[
    {
        "title": "Exploring the Carbon Footprint of Hugging Face's ML Models: A Repository\n  Mining Study",
        "url": "http://arxiv.org/abs/2305.11164v1",
        "pub_date": "2023-05-18",
        "summary": "The rise of machine learning (ML) systems has exacerbated their carbon\nfootprint due to increased capabilities and model sizes. However, there is\nscarce knowledge on how the carbon footprint of ML models is actually measured,\nreported, and evaluated. In light of this, the paper aims to analyze the\nmeasurement of the carbon footprint of 1,417 ML models and associated datasets\non Hugging Face, which is the most popular repository for pretrained ML models.\nThe goal is to provide insights and recommendations on how to report and\noptimize the carbon efficiency of ML models. The study includes the first\nrepository mining study on the Hugging Face Hub API on carbon emissions. This\nstudy seeks to answer two research questions: (1) how do ML model creators\nmeasure and report carbon emissions on Hugging Face Hub?, and (2) what aspects\nimpact the carbon emissions of training ML models? The study yielded several\nkey findings. These include a decreasing proportion of carbon\nemissions-reporting models, a slight decrease in reported carbon footprint on\nHugging Face over the past 2 years, and a continued dominance of NLP as the\nmain application domain. Furthermore, the study uncovers correlations between\ncarbon emissions and various attributes such as model size, dataset size, and\nML application domains. These results highlight the need for software\nmeasurements to improve energy reporting practices and promote carbon-efficient\nmodel development within the Hugging Face community. In response to this issue,\ntwo classifications are proposed: one for categorizing models based on their\ncarbon emission reporting practices and another for their carbon efficiency.\nThe aim of these classification proposals is to foster transparency and\nsustainable model development within the ML community.",
        "translated": "机器学习(ML)系统的兴起加剧了它们的碳足印，原因是功能和模型尺寸的增加。然而，对于机器学习模型的碳足印实际上是如何测量、报告和评估的，我们知之甚少。有鉴于此，本文旨在分析“拥抱脸”上对1417个机器学习模型及相关数据集的碳足印测量结果。“拥抱脸”是最受欢迎的预训机器学习模型库。目标是就如何报告和优化机器学习模型的碳效率提供见解和建议。这项研究包括第一个关于碳排放的拥抱面中心 API 的知识库挖掘研究。这项研究试图回答两个研究问题: (1)机器学习模型的创建者如何测量和报告拥抱面部中心的碳排放量？以及(2)哪些方面影响训练机器学习模型的碳排放量？这项研究产生了几个关键的发现。其中包括碳排放报告模型的比例下降，过去两年“拥抱脸”上的报告碳足印略有下降，以及自然语言处理作为主要应用领域的持续主导地位。此外，该研究还揭示了碳排放与模型大小、数据集大小和机器学习应用领域等各种属性之间的相关性。这些结果突出了软件测量的必要性，以改善能源报告做法，并促进在拥抱面社区的碳效率模型开发。针对这一问题，提出了两种分类: 一种是根据其碳排放报告做法对模型进行分类，另一种是根据其碳效率进行分类。这些分类建议的目的是在 ML 社区内促进透明度和可持续的模型开发。"
    },
    {
        "title": "TOME: A Two-stage Approach for Model-based Retrieval",
        "url": "http://arxiv.org/abs/2305.11161v1",
        "pub_date": "2023-05-18",
        "summary": "Recently, model-based retrieval has emerged as a new paradigm in text\nretrieval that discards the index in the traditional retrieval model and\ninstead memorizes the candidate corpora using model parameters. This design\nemploys a sequence-to-sequence paradigm to generate document identifiers, which\nenables the complete capture of the relevance between queries and documents and\nsimplifies the classic indexretrieval-rerank pipeline. Despite its attractive\nqualities, there remain several major challenges in model-based retrieval,\nincluding the discrepancy between pre-training and fine-tuning, and the\ndiscrepancy between training and inference. To deal with the above challenges,\nwe propose a novel two-stage model-based retrieval approach called TOME, which\nmakes two major technical contributions, including the utilization of tokenized\nURLs as identifiers and the design of a two-stage generation architecture. We\nalso propose a number of training strategies to deal with the training\ndifficulty as the corpus size increases. Extensive experiments and analysis on\nMS MARCO and Natural Questions demonstrate the effectiveness of our proposed\napproach, and we investigate the scaling laws of TOME by examining various\ninfluencing factors.",
        "translated": "近年来，基于模型的检索已经成为文本检索的一种新范式，它抛弃了传统检索模型中的索引，而是利用模型参数记忆候选语料库。该设计采用序列到序列的方法生成文档标识符，能够完全捕获查询和文档之间的相关性，简化了经典的索引检索-重排序流水线。尽管基于模型的检索具有吸引人的优点，但仍然存在一些主要的挑战，包括预训练和微调之间的差异，以及训练和推理之间的差异。为了应对上述挑战，我们提出了一种新的基于两阶段模型的检索方法，称为 TOME，它做出了两个主要的技术贡献，包括使用标记化 URL 作为标识符和设计一个两阶段生成体系结构。随着语料库规模的增大，我们提出了一些训练策略来解决训练难度。大量的实验和分析 MS MARCO 和自然问题证明了我们提出的方法的有效性，我们研究了 TOME 的缩放规律通过检查各种影响因素。"
    },
    {
        "title": "Preference or Intent? Double Disentangled Collaborative Filtering",
        "url": "http://arxiv.org/abs/2305.11084v1",
        "pub_date": "2023-05-18",
        "summary": "People usually have different intents for choosing items, while their\npreferences under the same intent may also different. In traditional\ncollaborative filtering approaches, both intent and preference factors are\nusually entangled in the modeling process, which significantly limits the\nrobustness and interpretability of recommendation performances. For example,\nthe low-rating items are always treated as negative feedback while they\nactually could provide positive information about user intent. To this end, in\nthis paper, we propose a two-fold representation learning approach, namely\nDouble Disentangled Collaborative Filtering (DDCF), for personalized\nrecommendations. The first-level disentanglement is for separating the\ninfluence factors of intent and preference, while the second-level\ndisentanglement is performed to build independent sparse preference\nrepresentations under individual intent with limited computational complexity.\nSpecifically, we employ two variational autoencoder networks, intent\nrecognition network and preference decomposition network, to learn the intent\nand preference factors, respectively. In this way, the low-rating items will be\ntreated as positive samples for modeling intents while the negative samples for\nmodeling preferences. Finally, extensive experiments on three real-world\ndatasets and four evaluation metrics clearly validate the effectiveness and the\ninterpretability of DDCF.",
        "translated": "人们通常有不同的意图选择项目，而他们的偏好下，相同的意图也可能有所不同。在传统的协同过滤建模方法中，意图和偏好因素通常会在建模过程中纠缠在一起，这极大地限制了推荐性能的稳健性和可解释性。例如，低等级的项目总是被视为负面反馈，而实际上它们可以提供关于用户意图的正面信息。为此，在本文中，我们提出了一种双重表征学习方法，即双重分离协同过滤(DDCF) ，用于个性化推荐。第一级解缠是为了分离意图和偏好的影响因素，而第二级解缠是为了在计算复杂度有限的个体意图下构建独立的稀疏偏好表示。具体来说，我们使用两个变分自动编码器网络，意图识别网络和偏好分解网络，分别学习意图和偏好因素。这样，低等级的项目将被视为建模意图的正面样本，而负面样本将被视为建模偏好。最后，在三个实际数据集和四个评价指标上进行了广泛的实验，验证了 DDCF 的有效性和可解释性。"
    },
    {
        "title": "Contrastive State Augmentations for Reinforcement Learning-Based\n  Recommender Systems",
        "url": "http://arxiv.org/abs/2305.11081v1",
        "pub_date": "2023-05-18",
        "summary": "Learning reinforcement learning (RL)-based recommenders from historical\nuser-item interaction sequences is vital to generate high-reward\nrecommendations and improve long-term cumulative benefits. However, existing RL\nrecommendation methods encounter difficulties (i) to estimate the value\nfunctions for states which are not contained in the offline training data, and\n(ii) to learn effective state representations from user implicit feedback due\nto the lack of contrastive signals. In this work, we propose contrastive state\naugmentations (CSA) for the training of RL-based recommender systems. To tackle\nthe first issue, we propose four state augmentation strategies to enlarge the\nstate space of the offline data. The proposed method improves the\ngeneralization capability of the recommender by making the RL agent visit the\nlocal state regions and ensuring the learned value functions are similar\nbetween the original and augmented states. For the second issue, we propose\nintroducing contrastive signals between augmented states and the state randomly\nsampled from other sessions to improve the state representation learning\nfurther. To verify the effectiveness of the proposed CSA, we conduct extensive\nexperiments on two publicly accessible datasets and one dataset collected from\na real-life e-commerce platform. We also conduct experiments on a simulated\nenvironment as the online evaluation setting. Experimental results demonstrate\nthat CSA can effectively improve recommendation performance.",
        "translated": "从历史用户项目交互序列中学习基于强化学习的推荐对于产生高回报的推荐和提高长期累积效益至关重要。然而，现有的 RL 推荐方法遇到了困难(i)估计不包含在离线训练数据中的状态的值函数，以及(ii)由于缺乏对比信号而从用户隐式反馈中学习有效的状态表示。在这项工作中，我们提出了对比状态增强(CSA)的训练基于 RL 的推荐系统。针对第一个问题，我们提出了四种状态增强策略来扩大离线数据的状态空间。该方法通过使 RL 代理访问局部状态区域，保证学习值函数在原状态和增广状态之间相似，提高了推荐器的泛化能力。对于第二个问题，我们提出在增广状态和从其他会话中随机采样的状态之间引入对比信号，以进一步改善状态表示学习。为了验证所提出的 CSA 的有效性，我们对从现实生活中的电子商务平台收集的两个可公开访问的数据集和一个数据集进行了广泛的实验。我们还进行了模拟环境的实验，作为在线评价设置。实验结果表明，CSA 能有效提高推荐性能。"
    },
    {
        "title": "BERM: Training the Balanced and Extractable Representation for Matching\n  to Improve Generalization Ability of Dense Retrieval",
        "url": "http://arxiv.org/abs/2305.11052v1",
        "pub_date": "2023-05-18",
        "summary": "Dense retrieval has shown promise in the first-stage retrieval process when\ntrained on in-domain labeled datasets. However, previous studies have found\nthat dense retrieval is hard to generalize to unseen domains due to its weak\nmodeling of domain-invariant and interpretable feature (i.e., matching signal\nbetween two texts, which is the essence of information retrieval). In this\npaper, we propose a novel method to improve the generalization of dense\nretrieval via capturing matching signal called BERM. Fully fine-grained\nexpression and query-oriented saliency are two properties of the matching\nsignal. Thus, in BERM, a single passage is segmented into multiple units and\ntwo unit-level requirements are proposed for representation as the constraint\nin training to obtain the effective matching signal. One is semantic unit\nbalance and the other is essential matching unit extractability. Unit-level\nview and balanced semantics make representation express the text in a\nfine-grained manner. Essential matching unit extractability makes passage\nrepresentation sensitive to the given query to extract the pure matching\ninformation from the passage containing complex context. Experiments on BEIR\nshow that our method can be effectively combined with different dense retrieval\ntraining methods (vanilla, hard negatives mining and knowledge distillation) to\nimprove its generalization ability without any additional inference overhead\nand target domain data.",
        "translated": "密集检索在域内标记数据集训练的第一阶段检索过程中显示出希望。然而，先前的研究发现，由于密集检索对领域不变性和可解释特征(即两个文本之间的匹配信号，这是信息检索的本质)的建模较弱，因此很难将其推广到不可见的领域。在本文中，我们提出了一种新的方法来提高通过捕获匹配信号密集检索的泛化称为 BERM。完全细粒度表达式和面向查询的显著性是匹配信号的两个属性。因此，在误码率模型中，将一个通道分割成多个单元，并提出了两个单元级的要求作为训练中获得有效匹配信号的约束条件。一个是语义单元平衡，另一个是必要的匹配单元可提取性。单元级视图和平衡语义使表示以细粒度的方式表示文本。基本匹配单元可提取性使得文本表示对给定的查询敏感，从包含复杂上下文的文本中提取纯匹配信息。在 BEIR 上的实验表明，该方法可以有效地结合不同的密集检索训练方法(普通方法、硬负数挖掘和知识提取) ，在不增加任何推理开销和目标域数据的情况下提高其泛化能力。"
    },
    {
        "title": "Improving Recommendation System Serendipity Through Lexicase Selection",
        "url": "http://arxiv.org/abs/2305.11044v1",
        "pub_date": "2023-05-18",
        "summary": "Recommender systems influence almost every aspect of our digital lives.\nUnfortunately, in striving to give us what we want, they end up restricting our\nopen-mindedness. Current recommender systems promote echo chambers, where\npeople only see the information they want to see, and homophily, where users of\nsimilar background see similar content. We propose a new serendipity metric to\nmeasure the presence of echo chambers and homophily in recommendation systems\nusing cluster analysis. We then attempt to improve the diversity-preservation\nqualities of well known recommendation techniques by adopting a parent\nselection algorithm from the evolutionary computation literature known as\nlexicase selection. Our results show that lexicase selection, or a mixture of\nlexicase selection and ranking, outperforms its purely ranked counterparts in\nterms of personalization, coverage and our specifically designed serendipity\nbenchmark, while only slightly under-performing in terms of accuracy (hit\nrate). We verify these results across a variety of recommendation list sizes.\nIn this work we show that lexicase selection is able to maintain multiple\ndiverse clusters of item recommendations that are each relevant for the\nspecific user, while still maintaining a high hit-rate accuracy, a trade off\nthat is not achieved by other methods.",
        "translated": "推荐系统几乎影响了我们数字生活的方方面面。不幸的是，在努力给予我们想要的东西的过程中，他们最终限制了我们思想的开放性。目前的推荐系统推广回声室，人们只看到他们想看到的信息，同质性，相似背景的用户看到相似的内容。我们提出了一种新的意外发现度量方法，用来衡量使用数据聚类的推荐系统中是否存在回声室和同质性。然后，我们试图通过采用来自进化计算文献的父选择算法(称为 lexicase 选择)来改进众所周知的推荐技术的多样性保持质量。我们的研究结果表明，词汇表选择，或词汇表选择和排名的混合，在个性化，覆盖率和我们专门设计的意外发现基准方面表现优于纯粹的排名对应方，而在准确性(命中率)方面表现稍差。我们通过各种推荐列表大小来验证这些结果。在这项工作中，我们表明，词汇表选择能够维护多个不同的项目推荐集群，每个相关的特定用户，同时仍然保持高命中率的准确性，这是一个权衡，没有实现的其他方法。"
    },
    {
        "title": "Query Performance Prediction: From Ad-hoc to Conversational Search",
        "url": "http://arxiv.org/abs/2305.10923v1",
        "pub_date": "2023-05-18",
        "summary": "Query performance prediction (QPP) is a core task in information retrieval.\nThe QPP task is to predict the retrieval quality of a search system for a query\nwithout relevance judgments. Research has shown the effectiveness and\nusefulness of QPP for ad-hoc search. Recent years have witnessed considerable\nprogress in conversational search (CS). Effective QPP could help a CS system to\ndecide an appropriate action to be taken at the next turn. Despite its\npotential, QPP for CS has been little studied. We address this research gap by\nreproducing and studying the effectiveness of existing QPP methods in the\ncontext of CS. While the task of passage retrieval remains the same in the two\nsettings, a user query in CS depends on the conversational history, introducing\nnovel QPP challenges. In particular, we seek to explore to what extent findings\nfrom QPP methods for ad-hoc search generalize to three CS settings: (i)\nestimating the retrieval quality of different query rewriting-based retrieval\nmethods, (ii) estimating the retrieval quality of a conversational dense\nretrieval method, and (iii) estimating the retrieval quality for top ranks vs.\ndeeper-ranked lists. Our findings can be summarized as follows: (i) supervised\nQPP methods distinctly outperform unsupervised counterparts only when a\nlarge-scale training set is available; (ii) point-wise supervised QPP methods\noutperform their list-wise counterparts in most cases; and (iii) retrieval\nscore-based unsupervised QPP methods show high effectiveness in assessing the\nconversational dense retrieval method, ConvDR.",
        "translated": "查询性能预测是信息检索的核心任务。QPP 任务是在没有相关性判断的情况下预测查询检索系统的检索质量。研究表明 QPP 在自组织搜索中的有效性和实用性。近年来，会话搜索取得了长足的进步。有效的质量保证计划可以帮助 CS 系统决定下一轮要采取的适当行动。尽管 QPP 具有很大的潜力，但是对它的研究还很少。我们通过再现和研究现有的 QPP 方法在 CS 背景下的有效性来弥补这一研究差距。虽然在这两种情况下，文章检索的任务是相同的，但用户在 CS 中的查询依赖于会话历史，引入了新的 QPP 挑战。特别是，我们试图探索用于特别搜索的 QPP 方法的结果在多大程度上概括为三种 CS 设置: (i)估计不同基于查询重写的检索方法的检索质量，(ii)估计会话密集检索方法的检索质量，以及(iii)估计顶级与更深级列表的检索质量。我们的研究结果可以总结如下: (i)监督 QPP 方法只有在大规模训练集可用时才明显优于无监督的对应方法; (ii)点式监督 QPP 方法在大多数情况下优于其列表式对应方法; 和(iii)基于检索评分的无监督 QPP 方法在评估会话密集检索方法，ConvDR 方面显示出高效性。"
    },
    {
        "title": "Adaptive Graph Contrastive Learning for Recommendation",
        "url": "http://arxiv.org/abs/2305.10837v1",
        "pub_date": "2023-05-18",
        "summary": "Recently, graph neural networks (GNNs) have been successfully applied to\nrecommender systems as an effective collaborative filtering (CF) approach. The\nkey idea of GNN-based recommender system is to recursively perform the message\npassing along the user-item interaction edge for refining the encoded\nembeddings, relying on sufficient and high-quality training data. Since user\nbehavior data in practical recommendation scenarios is often noisy and exhibits\nskewed distribution, some recommendation approaches, e.g., SGL and SimGCL,\nleverage self-supervised learning to improve user representations against the\nabove issues. Despite their effectiveness, however, they conduct\nself-supervised learning through creating contrastvie views, depending on the\nexploration of data augmentations with the problem of tedious trial-and-error\nselection of augmentation methods. In this paper, we propose a novel Adaptive\nGraph Contrastive Learning (AdaptiveGCL) framework which conducts graph\ncontrastive learning with two adaptive contrastive view generators to better\nempower CF paradigm. Specifically, we use two trainable view generators, which\nare a graph generative model and a graph denoising model respectively, to\ncreate contrastive views. Two generators are able to create adaptive\ncontrastive views, addressing the problem of model collapse and achieving\nadaptive contrastive learning. With two adaptive contrasive views, more\nadditionally high-quality training signals will be introduced into the CF\nparadigm and help to alleviate the data sparsity and noise issues. Extensive\nexperiments on three benchmark datasets demonstrate the superiority of our\nmodel over various state-of-the-art recommendation methods. Further visual\nanalysis intuitively explains why our AdaptiveGCL outperforms existing\ncontrastive learning approaches based on selected data augmentation methods.",
        "translated": "最近，图形神经网络(GNN)已成功应用于推荐系统，作为一种有效的协同过滤(CF)方法。基于 GNN 的推荐系统的关键思想是依靠充分和高质量的训练数据，递归地执行沿用户项目交互边缘传递的消息，以完善编码的嵌入。由于实际推荐场景中的用户行为数据通常是有噪音的，并且呈现出倾斜的分布，因此一些推荐方法，如 SGL 和 SimGCL，利用自监督学习来改善用户对上述问题的表示。然而，尽管他们的有效性，他们进行自我监督学习通过创建对比观点，依赖于探索数据增强与繁琐的试错选择增强方法的问题。本文提出了一种新的自适应图形对比学习(AdaptiveGCL)框架，该框架使用两个自适应对比视图生成器进行图形对比学习，以更好地支持 CF 范式。具体来说，我们使用两个可训练的视图生成器，分别是一个图形生成模型和一个图形去噪模型，来创建对比视图。两个生成器能够创建自适应对比视图，解决模型崩溃问题，实现自适应对比学习。通过两个自适应对立视图，在 CF 范式中引入更多高质量的训练信号，有助于缓解数据稀疏和噪声问题。在三个基准数据集上的大量实验证明了我们的模型优于各种最先进的推荐方法。进一步的可视化分析直观地解释了为什么我们的 AdaptiveGCL 优于基于所选数据增强方法的现有对比学习方法。"
    },
    {
        "title": "Integrating Item Relevance in Training Loss for Sequential Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2305.10824v1",
        "pub_date": "2023-05-18",
        "summary": "Sequential Recommender Systems (SRSs) are a popular type of recommender\nsystem that learns from a user's history to predict the next item they are\nlikely to interact with. However, user interactions can be affected by noise\nstemming from account sharing, inconsistent preferences, or accidental clicks.\nTo address this issue, we (i) propose a new evaluation protocol that takes\nmultiple future items into account and (ii) introduce a novel relevance-aware\nloss function to train a SRS with multiple future items to make it more robust\nto noise. Our relevance-aware models obtain an improvement of ~1.2% of NDCG@10\nand 0.88% in the traditional evaluation protocol, while in the new evaluation\nprotocol, the improvement is ~1.63% of NDCG@10 and ~1.5% of HR w.r.t the best\nperforming models.",
        "translated": "顺序推荐系统(SRSs)是一种流行的推荐系统，它可以从用户的历史中学习，预测他们可能接触到的下一个项目。然而，用户交互可能受到来自帐户共享、不一致的首选项或偶然点击的噪音的影响。为了解决这个问题，我们(i)提出了一个新的评估协议，考虑到多个未来项目，并且(ii)引入一个新的相关性感知损失函数来训练具有多个未来项目的 SRS，以使其对噪声更加鲁棒。我们的相关意识模型在传统的评估方案中获得了? 1.2% 的 NDCG@10和0.88% 的改善，而在新的评估方案中，改善是? 1.63% 的 NDCG@10和? 1.5% 的最佳表现模型。"
    },
    {
        "title": "When Search Meets Recommendation: Learning Disentangled Search\n  Representation for Recommendation",
        "url": "http://arxiv.org/abs/2305.10822v1",
        "pub_date": "2023-05-18",
        "summary": "Modern online service providers such as online shopping platforms often\nprovide both search and recommendation (S&amp;R) services to meet different user\nneeds. Rarely has there been any effective means of incorporating user behavior\ndata from both S&amp;R services. Most existing approaches either simply treat S&amp;R\nbehaviors separately, or jointly optimize them by aggregating data from both\nservices, ignoring the fact that user intents in S&amp;R can be distinctively\ndifferent. In our paper, we propose a Search-Enhanced framework for the\nSequential Recommendation (SESRec) that leverages users' search interests for\nrecommendation, by disentangling similar and dissimilar representations within\nS&amp;R behaviors. Specifically, SESRec first aligns query and item embeddings\nbased on users' query-item interactions for the computations of their\nsimilarities. Two transformer encoders are used to learn the contextual\nrepresentations of S&amp;R behaviors independently. Then a contrastive learning\ntask is designed to supervise the disentanglement of similar and dissimilar\nrepresentations from behavior sequences of S&amp;R. Finally, we extract user\ninterests by the attention mechanism from three perspectives, i.e., the\ncontextual representations, the two separated behaviors containing similar and\ndissimilar interests. Extensive experiments on both industrial and public\ndatasets demonstrate that SESRec consistently outperforms state-of-the-art\nmodels. Empirical studies further validate that SESRec successfully disentangle\nsimilar and dissimilar user interests from their S&amp;R behaviors.",
        "translated": "现代在线服务提供商，如在线购物平台，经常同时提供搜索和推荐(S & R)服务，以满足不同的用户需求。很少有任何有效的方法来整合来自 S & R 服务的用户行为数据。大多数现有的方法要么单独处理 S & R 行为，要么通过聚合来自两个服务的数据来共同优化它们，忽略了 S & R 中的用户意图可能截然不同的事实。在我们的论文中，我们提出了一个搜索增强的序列推荐框架(SESRec) ，它利用用户的搜索兴趣进行推荐，通过在 S & R 行为中分离相似和不相似的表示。具体来说，SESRec 首先根据用户的查询-项交互对查询和项嵌入进行对齐，以计算它们的相似性。使用两个变压器编码器分别学习 S & R 行为的上下文表示。然后设计了一个对比学习任务来监督 S & R 行为序列中相似和不相似表征的分离。最后，通过注意机制从三个方面提取用户的兴趣，即语境表征，两个分离的具有相似兴趣和不同兴趣的行为。在工业和公共数据集上的大量实验表明，SESRec 始终优于最先进的模型。实证研究进一步验证了 SESRec 成功地将相似和不同的用户兴趣从他们的 S & R 行为中分离出来。"
    },
    {
        "title": "How Does Generative Retrieval Scale to Millions of Passages?",
        "url": "http://arxiv.org/abs/2305.11841v1",
        "pub_date": "2023-05-19",
        "summary": "Popularized by the Differentiable Search Index, the emerging paradigm of\ngenerative retrieval re-frames the classic information retrieval problem into a\nsequence-to-sequence modeling task, forgoing external indices and encoding an\nentire document corpus within a single Transformer. Although many different\napproaches have been proposed to improve the effectiveness of generative\nretrieval, they have only been evaluated on document corpora on the order of\n100k in size. We conduct the first empirical study of generative retrieval\ntechniques across various corpus scales, ultimately scaling up to the entire MS\nMARCO passage ranking task with a corpus of 8.8M passages and evaluating model\nsizes up to 11B parameters. We uncover several findings about scaling\ngenerative retrieval to millions of passages; notably, the central importance\nof using synthetic queries as document representations during indexing, the\nineffectiveness of existing proposed architecture modifications when accounting\nfor compute cost, and the limits of naively scaling model parameters with\nrespect to retrieval performance. While we find that generative retrieval is\ncompetitive with state-of-the-art dual encoders on small corpora, scaling to\nmillions of passages remains an important and unsolved challenge. We believe\nthese findings will be valuable for the community to clarify the current state\nof generative retrieval, highlight the unique challenges, and inspire new\nresearch directions.",
        "translated": "由于可分辨搜索索引的普及，新兴的生成检索范式将经典的信息检索问题重新定义为一个序列到序列的建模任务，放弃外部索引，并在一个 former 中编码整个文档语料库。为了提高生成检索的有效性，人们提出了许多不同的方法，但这些方法在文献语料库中的检索效果只有10万次左右。我们进行了第一次实证研究的生成检索技术在不同的语料库尺度，最终扩大到整个 MS MARCO 段落排序任务与8.8 M 段落的语料库和评估模型大小高达11B 参数。我们发现了关于将生成性检索扩展到数百万段的一些发现; 值得注意的是，在索引过程中使用合成查询作为文档表示的核心重要性，在计算计算成本时现有提议的架构修改的无效性，以及天真地扩展模型参数对检索性能的限制。虽然我们发现生成检索与小型语料库上最先进的双编码器相比具有竞争力，但是扩展到数百万段仍然是一个重要的未解决的挑战。我们相信这些研究结果将有助于社区阐明生成性检索的现状，突出独特的挑战，并启发新的研究方向。"
    },
    {
        "title": "Visualization for Recommendation Explainability: A Survey and New\n  Perspectives",
        "url": "http://arxiv.org/abs/2305.11755v1",
        "pub_date": "2023-05-19",
        "summary": "Providing system-generated explanations for recommendations represents an\nimportant step towards transparent and trustworthy recommender systems.\nExplainable recommender systems provide a human-understandable rationale for\ntheir outputs. Over the last two decades, explainable recommendation has\nattracted much attention in the recommender systems research community. This\npaper aims to provide a comprehensive review of research efforts on visual\nexplanation in recommender systems. More concretely, we systematically review\nthe literature on explanations in recommender systems based on four dimensions,\nnamely explanation goal, explanation scope, explanation style, and explanation\nformat. Recognizing the importance of visualization, we approach the\nrecommender system literature from the angle of explanatory visualizations,\nthat is using visualizations as a display style of explanation. As a result, we\nderive a set of guidelines that might be constructive for designing explanatory\nvisualizations in recommender systems and identify perspectives for future work\nin this field. The aim of this review is to help recommendation researchers and\npractitioners better understand the potential of visually explainable\nrecommendation research and to support them in the systematic design of visual\nexplanations in current and future recommender systems.",
        "translated": "对建议提供系统生成的解释是朝向透明和可靠的推荐系统迈出的重要一步。可解释的推荐系统为其输出提供了一个人类可理解的理由。在过去的二十年中，可解释的推荐引起了推荐系统研究界的广泛关注。本文旨在对推荐系统中视觉解释的研究工作进行综述。更具体地，我们从解释目的、解释范围、解释风格和解释格式四个维度系统地回顾了关于推荐系统中解释的文献。认识到可视化的重要性，我们从解释性可视化的角度来看待推荐系统文献，即使用可视化作为一种解释的显示方式。因此，我们得出了一套指导方针，可能是建设性的设计解释性可视化在推荐系统，并确定未来的工作在这一领域的前景。本综述的目的是帮助推荐研究人员和从业人员更好地理解可视化解释推荐研究的潜力，并支持他们在目前和未来的推荐系统中系统地设计可视化解释。"
    },
    {
        "title": "Inference-time Re-ranker Relevance Feedback for Neural Information\n  Retrieval",
        "url": "http://arxiv.org/abs/2305.11744v1",
        "pub_date": "2023-05-19",
        "summary": "Neural information retrieval often adopts a retrieve-and-rerank framework: a\nbi-encoder network first retrieves K (e.g., 100) candidates that are then\nre-ranked using a more powerful cross-encoder model to rank the better\ncandidates higher. The re-ranker generally produces better candidate scores\nthan the retriever, but is limited to seeing only the top K retrieved\ncandidates, thus providing no improvements in retrieval performance as measured\nby Recall@K. In this work, we leverage the re-ranker to also improve retrieval\nby providing inference-time relevance feedback to the retriever. Concretely, we\nupdate the retriever's query representation for a test instance using a\nlightweight inference-time distillation of the re-ranker's prediction for that\ninstance. The distillation loss is designed to bring the retriever's candidate\nscores closer to those of the re-ranker. A second retrieval step is then\nperformed with the updated query vector. We empirically show that our approach,\nwhich can serve arbitrary retrieve-and-rerank pipelines, significantly improves\nretrieval recall in multiple domains, languages, and modalities.",
        "translated": "神经信息检索通常采用一个检索-重新排序框架: 一个双编码器网络首先检索 K (例如，100)候选人，然后使用一个更强大的交叉编码器模型重新排序，以排序更好的候选人更高。重新排名通常比检索器产生更好的候选人分数，但是仅限于看到被检索的最高 K 的候选人，因此没有提供根据 Recall@K 测量的检索性能的改善。在这项工作中，我们利用重新排名也提高检索提供推理时间关联反馈的检索。具体来说，我们使用重新排序器对测试实例的预测的轻量级推理时间精馏来更新检索器对该实例的查询表示。蒸馏损失的目的是使猎犬的候选分数更接近那些重新排名。然后使用更新的查询向量执行第二个检索步骤。我们的实验表明，我们的方法，可以服务任意的检索和重新排序管道，显着提高检索召回在多个领域，语言和模式。"
    },
    {
        "title": "Exploring the Upper Limits of Text-Based Collaborative Filtering Using\n  Large Language Models: Discoveries and Insights",
        "url": "http://arxiv.org/abs/2305.11700v1",
        "pub_date": "2023-05-19",
        "summary": "Text-based collaborative filtering (TCF) has become the mainstream approach\nfor text and news recommendation, utilizing text encoders, also known as\nlanguage models (LMs), to represent items. However, existing TCF models\nprimarily focus on using small or medium-sized LMs. It remains uncertain what\nimpact replacing the item encoder with one of the largest and most powerful\nLMs, such as the 175-billion parameter GPT-3 model, would have on\nrecommendation performance. Can we expect unprecedented results? To this end,\nwe conduct an extensive series of experiments aimed at exploring the\nperformance limits of the TCF paradigm. Specifically, we increase the size of\nitem encoders from one hundred million to one hundred billion to reveal the\nscaling limits of the TCF paradigm. We then examine whether these extremely\nlarge LMs could enable a universal item representation for the recommendation\ntask. Furthermore, we compare the performance of the TCF paradigm utilizing the\nmost powerful LMs to the currently dominant ID embedding-based paradigm and\ninvestigate the transferability of this TCF paradigm. Finally, we compare TCF\nwith the recently popularized prompt-based recommendation using ChatGPT. Our\nresearch findings have not only yielded positive results but also uncovered\nsome surprising and previously unknown negative outcomes, which can inspire\ndeeper reflection and innovative thinking regarding text-based recommender\nsystems. Codes and datasets will be released for further research.",
        "translated": "基于文本的协同过滤(TCF)已经成为文本和新闻推荐的主流方法，利用文本编码器(也称为语言模型(LMs))来表示项目。然而，现有的 TCF 模型主要侧重于使用中小型 LM。用最大最强的 LM (比如1750亿参数的 GPT-3模型)替换项目编码器会对推荐性能产生什么影响还不确定。我们能期待前所未有的结果吗？为此，我们进行了一系列广泛的实验，旨在探索 TCF 范式的性能极限。具体来说，我们将条目编码器的大小从1亿增加到1亿，以揭示 TCF 范例的伸缩限制。然后，我们检查这些极大的 LM 是否能够为推荐任务启用通用项表示。此外，我们比较了使用最强大的 LM 的 TCF 范式和目前占主导地位的基于 ID 嵌入的 TCF 范式的性能，并研究了这种 TCF 范式的可转移性。最后，我们使用 ChatGPT 比较 TCF 和最近推广的基于提示的推荐。我们的研究结果不仅产生了积极的结果，而且还揭示了一些令人惊讶的和以前未知的负面结果，这可以激发关于基于文本的推荐系统的更深层次的反思和创新思维。代码和数据集将被发布用于进一步的研究。"
    },
    {
        "title": "The Barriers to Online Clothing Websites for Visually Impaired People:\n  An Interview and Observation Approach to Understanding Needs",
        "url": "http://arxiv.org/abs/2305.11559v1",
        "pub_date": "2023-05-19",
        "summary": "Visually impaired (VI) people often face challenges when performing everyday\ntasks and identify shopping for clothes as one of the most challenging. Many\nengage in online shopping, which eliminates some challenges of physical\nshopping. However, clothes shopping online suffers from many other limitations\nand barriers. More research is needed to address these challenges, and extant\nworks often base their findings on interviews alone, providing only subjective,\nrecall-biased information. We conducted two complementary studies using both\nobservational and interview approaches to fill a gap in understanding about VI\npeople's behaviour when selecting and purchasing clothes online. Our findings\nshow that shopping websites suffer from inaccurate, misleading, and\ncontradictory clothing descriptions; that VI people mainly rely on (unreliable)\nsearch tools and check product descriptions by reviewing customer comments. Our\nfindings also indicate that VI people are hesitant to accept assistance from\nautomated, but that trust in such systems could be improved if researchers can\ndevelop systems that better accommodate users' needs and preferences.",
        "translated": "视力受损者在日常工作中经常面临挑战，购买衣服是最具挑战性的工作之一。许多人从事网上购物，这消除了实体购物的一些挑战。然而，网上购物的服装受到许多其他限制和障碍。需要更多的研究来解决这些挑战，现存的工作往往基于他们的调查结果仅仅访谈，只提供主观的，回忆偏见的信息。我们使用观察和访谈的方法进行了两个互补的研究，以填补在线选择和购买服装时对 VI 人群行为的理解差距。我们的研究结果表明，购物网站容易受到不准确、误导和自相矛盾的服装描述的困扰; VI 用户主要依靠(不可靠的)搜索工具，通过查看顾客评论来检查产品描述。我们的研究结果还表明，VI 人士不愿意接受自动化系统的帮助，但如果研究人员能够开发出更好地适应用户需求和偏好的系统，那么对这种系统的信任就可以得到改善。"
    },
    {
        "title": "InstructIE: A Chinese Instruction-based Information Extraction Dataset",
        "url": "http://arxiv.org/abs/2305.11527v1",
        "pub_date": "2023-05-19",
        "summary": "We introduce a new Information Extraction (IE) task dubbed Instruction-based\nIE, which aims to ask the system to follow specific instructions or guidelines\nto extract information. To facilitate research in this area, we construct a\ndataset called InstructIE, consisting of 270,000 weakly supervised data from\nChinese Wikipedia and 1,000 high-quality crowdsourced annotated instances. We\nfurther evaluate the performance of various baseline models on the InstructIE\ndataset. The results reveal that although current models exhibit promising\nperformance, there is still room for improvement. Furthermore, we conduct a\ncomprehensive case study analysis, underlining the challenges inherent in the\nInstruction-based IE task. Code and dataset are available at\nhttps://github.com/zjunlp/DeepKE/tree/main/example/llm.",
        "translated": "我们引入了一个新的基于指令的信息抽取(IE)任务，其目的是要求系统遵循特定的指令或指南来提取信息。为了促进这一领域的研究，我们构建了一个名为 DirectIE 的数据集，由来自中文维基百科的270,000个弱监督数据和1000个高质量的众包注释实例组成。我们进一步评估了各种基线模型在 DirectIE 数据集上的性能。结果表明，虽然目前的模型表现出良好的性能，仍然有改进的空间。此外，我们进行了一个全面的案例研究分析，强调了基于教学的 IE 任务固有的挑战。代码和数据集可在 https://github.com/zjunlp/deepke/tree/main/example/llm 下载。"
    },
    {
        "title": "Recouple Event Field via Probabilistic Bias for Event Extraction",
        "url": "http://arxiv.org/abs/2305.11498v1",
        "pub_date": "2023-05-19",
        "summary": "Event Extraction (EE), aiming to identify and classify event triggers and\narguments from event mentions, has benefited from pre-trained language models\n(PLMs). However, existing PLM-based methods ignore the information of\ntrigger/argument fields, which is crucial for understanding event schemas. To\nthis end, we propose a Probabilistic reCoupling model enhanced Event extraction\nframework (ProCE). Specifically, we first model the syntactic-related event\nfields as probabilistic biases, to clarify the event fields from ambiguous\nentanglement. Furthermore, considering multiple occurrences of the same\ntriggers/arguments in EE, we explore probabilistic interaction strategies among\nmultiple fields of the same triggers/arguments, to recouple the corresponding\nclarified distributions and capture more latent information fields. Experiments\non EE datasets demonstrate the effectiveness and generalization of our proposed\napproach.",
        "translated": "事件提取(EE) ，旨在从事件提及中识别和分类事件触发器和参数，已经受益于预训练语言模型(PLM)。然而，现有的基于 PLM 的方法忽略了触发器/参数字段的信息，这对于理解事件模式是至关重要的。为此，我们提出了一个概率重耦合模型增强的事件抽取框架(ProCE)。具体来说，我们首先将与句法相关的事件字段建模为概率偏差，以澄清来自模糊纠缠的事件字段。此外，考虑到同一触发器/参数在 EE 中的多次出现，我们探索了同一触发器/参数的多个域之间的概率交互策略，以重新耦合相应的澄清分布并捕获更多的潜在信息域。在 EE 数据集上的实验证明了该方法的有效性和推广性。"
    },
    {
        "title": "TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks",
        "url": "http://arxiv.org/abs/2305.11430v1",
        "pub_date": "2023-05-19",
        "summary": "While LLMs have shown great success in understanding and generating text in\ntraditional conversational settings, their potential for performing ill-defined\ncomplex tasks is largely under-studied. Indeed, we are yet to conduct\ncomprehensive benchmarking studies with multiple LLMs that are exclusively\nfocused on a complex task. However, conducting such benchmarking studies is\nchallenging because of the large variations in LLMs' performance when different\nprompt types/styles are used and different degrees of detail are provided in\nthe prompts. To address this issue, the paper proposes a general taxonomy that\ncan be used to design prompts with specific properties in order to perform a\nwide range of complex tasks. This taxonomy will allow future benchmarking\nstudies to report the specific categories of prompts used as part of the study,\nenabling meaningful comparisons across different studies. Also, by establishing\na common standard through this taxonomy, researchers will be able to draw more\naccurate conclusions about LLMs' performance on a specific complex task.",
        "translated": "虽然 LLM 在理解和生成传统会话环境中的文本方面取得了巨大的成功，但它们执行定义不清的复杂任务的潜力在很大程度上还没有得到充分的研究。事实上，我们还没有进行全面的基准研究与多个 LLM，专门集中在一个复杂的任务。然而，进行这样的基准测试研究是具有挑战性的，因为当使用不同的提示类型/风格和提示中提供不同程度的细节时，LLM 的性能有很大的差异。为了解决这个问题，本文提出了一个通用分类法，可用于设计具有特定属性的提示符，以便执行范围广泛的复杂任务。这种分类法将使未来的基准研究能够报告作为研究的一部分使用的特定类别的提示，使不同研究之间的有意义的比较成为可能。此外，通过建立一个共同的标准通过这个分类，研究人员将能够得出更准确的结论 LLM 的表现在一个特定的复杂的任务。"
    },
    {
        "title": "Online Learning in a Creator Economy",
        "url": "http://arxiv.org/abs/2305.11381v1",
        "pub_date": "2023-05-19",
        "summary": "The creator economy has revolutionized the way individuals can profit through\nonline platforms. In this paper, we initiate the study of online learning in\nthe creator economy by modeling the creator economy as a three-party game\nbetween the users, platform, and content creators, with the platform\ninteracting with the content creator under a principal-agent model through\ncontracts to encourage better content. Additionally, the platform interacts\nwith the users to recommend new content, receive an evaluation, and ultimately\nprofit from the content, which can be modeled as a recommender system.\n  Our study aims to explore how the platform can jointly optimize the contract\nand recommender system to maximize the utility in an online learning fashion.\nWe primarily analyze and compare two families of contracts: return-based\ncontracts and feature-based contracts. Return-based contracts pay the content\ncreator a fraction of the reward the platform gains. In contrast, feature-based\ncontracts pay the content creator based on the quality or features of the\ncontent, regardless of the reward the platform receives. We show that under\nsmoothness assumptions, the joint optimization of return-based contracts and\nrecommendation policy provides a regret $\\Theta(T^{2/3})$. For the\nfeature-based contract, we introduce a definition of intrinsic dimension $d$ to\ncharacterize the hardness of learning the contract and provide an upper bound\non the regret $\\mathcal{O}(T^{(d+1)/(d+2)})$. The upper bound is tight for the\nlinear family.",
        "translated": "创造者经济彻底改变了个人通过在线平台获利的方式。本文通过将创造者经济建模为用户、平台和内容创造者之间的三方博弈，平台与内容创造者在委托-代理模式下通过契约互动来鼓励更好的内容，开展了创造者经济中在线学习的研究。此外，该平台与用户互动，推荐新内容，接受评估，并最终从内容中获利，这些内容可以被建模为推荐系统。我们的研究旨在探讨这个平台如何能够共同优化合同和推荐系统，以便在网上学习的模式中最大限度地发挥效用。我们主要分析和比较了两类契约: 基于回报的契约和基于特征的契约。基于回报的合同支付给内容创作者的报酬只是平台收益的一小部分。相比之下，基于特性的合同根据内容的质量或特性支付给内容创作者，而不管平台获得什么报酬。结果表明，在平滑假设下，基于回报的合同和推荐策略的联合优化得到了遗憾的 $Θ (T ^ {2/3}) $。对于基于特征的契约，我们引入了本征维度 $d $的定义来描述学习契约的难度，并给出了遗憾 $数学{ O }(T ^ {(d + 1)/(d + 2)}) $的上界。线性族的上界是紧的。"
    },
    {
        "title": "Copy Recurrent Neural Network Structure Network",
        "url": "http://arxiv.org/abs/2305.13250v1",
        "pub_date": "2023-05-22",
        "summary": "Electronic Health Record (EHR) coding involves automatically classifying EHRs\ninto diagnostic codes. While most previous research treats this as a\nmulti-label classification task, generating probabilities for each code and\nselecting those above a certain threshold as labels, these approaches often\noverlook the challenge of identifying complex diseases. In this study, our\nfocus is on detecting complication diseases within EHRs.\n  We propose a novel coarse-to-fine ICD path generation framework called the\nCopy Recurrent Neural Network Structure Network (CRNNet), which employs a Path\nGenerator (PG) and a Path Discriminator (PD) for EHR coding. By using RNNs to\ngenerate sequential outputs and incorporating a copy module, we efficiently\nidentify complication diseases. Our method achieves a 57.30\\% ratio of complex\ndiseases in predictions, outperforming state-of-the-art and previous\napproaches.\n  Additionally, through an ablation study, we demonstrate that the copy\nmechanism plays a crucial role in detecting complex diseases.",
        "translated": "电子健康记录(EHR)编码涉及将 EHR 自动分类为诊断代码。虽然大多数以前的研究将其视为一个多标签分类任务，为每个代码产生概率，并选择那些高于某个阈值的代码作为标签，但这些方法往往忽视了识别复杂疾病的挑战。在这项研究中，我们的重点是检测并发症的 EHR 疾病。我们提出了一个新的由粗到精的 ICD 路径生成框架，称为拷贝递归神经网络结构网络(crnNet) ，它使用路径生成器(PG)和路径鉴别器(PD)进行电子健康记录(eHR)编码。通过使用 RNN 产生序列输出和合并拷贝模块，我们有效地识别并发症疾病。我们的方法在预测复杂疾病方面达到了57.30% 的比例，超过了最先进的方法和以前的方法。此外，通过消融研究，我们证明复制机制在检测复杂疾病中起着至关重要的作用。"
    },
    {
        "title": "Challenging Decoder helps in Masked Auto-Encoder Pre-training for Dense\n  Passage Retrieval",
        "url": "http://arxiv.org/abs/2305.13197v1",
        "pub_date": "2023-05-22",
        "summary": "Recently, various studies have been directed towards exploring dense passage\nretrieval techniques employing pre-trained language models, among which the\nmasked auto-encoder (MAE) pre-training architecture has emerged as the most\npromising. The conventional MAE framework relies on leveraging the passage\nreconstruction of decoder to bolster the text representation ability of\nencoder, thereby enhancing the performance of resulting dense retrieval\nsystems. Within the context of building the representation ability of the\nencoder through passage reconstruction of decoder, it is reasonable to\npostulate that a ``more demanding'' decoder will necessitate a corresponding\nincrease in the encoder's ability. To this end, we propose a novel token\nimportance aware masking strategy based on pointwise mutual information to\nintensify the challenge of the decoder. Importantly, our approach can be\nimplemented in an unsupervised manner, without adding additional expenses to\nthe pre-training phase. Our experiments verify that the proposed method is both\neffective and robust on large-scale supervised passage retrieval datasets and\nout-of-domain zero-shot retrieval benchmarks.",
        "translated": "近年来，各种研究都致力于探索使用预训练语言模型的密集通道检索技术，其中掩蔽自动编码器(MAE)预训练结构是最有前途的一种。传统的 MAE 框架依赖于利用解码器的通道重构来增强编码器的文本表示能力，从而提高密集检索系统的性能。在通过解码器的通道重构来建立编码器的表示能力的背景下，假设“更高要求”的解码器需要相应提高编码器的表示能力是合理的。为此，我们提出了一种新颖的基于点间互信息的标记重要性感知掩蔽策略，以增强解码器的挑战性。重要的是，我们的方法可以在一个无监督的方式实施，而不增加额外的费用，预培训阶段。实验结果表明，该方法对于大规模有监督通道检索数据集和域外零镜头检索基准具有良好的鲁棒性和有效性。"
    },
    {
        "title": "TEIMMA: The First Content Reuse Annotator for Text, Images, and Math",
        "url": "http://arxiv.org/abs/2305.13193v1",
        "pub_date": "2023-05-22",
        "summary": "This demo paper presents the first tool to annotate the reuse of text,\nimages, and mathematical formulae in a document pair -- TEIMMA. Annotating\ncontent reuse is particularly useful to develop plagiarism detection\nalgorithms. Real-world content reuse is often obfuscated, which makes it\nchallenging to identify such cases. TEIMMA allows entering the obfuscation type\nto enable novel classifications for confirmed cases of plagiarism. It enables\nrecording different reuse types for text, images, and mathematical formulae in\nHTML and supports users by visualizing the content reuse in a document pair\nusing similarity detection methods for text and math.",
        "translated": "本演示文件介绍了第一个注释文本、图像和数学公式在文档对中的重用的工具—— TEIMMA。注释内容重用对于开发剽窃检测算法特别有用。真实世界的内容重用通常是模糊的，这使得识别这种情况变得很困难。TEIMMA 允许输入模糊类型，以便对确认的剽窃案件进行新的分类。它支持在 HTML 中记录文本、图像和数学公式的不同重用类型，并通过使用文本和数学的相似性检测方法可视化文档对中的内容重用来支持用户。"
    },
    {
        "title": "Editing Large Language Models: Problems, Methods, and Opportunities",
        "url": "http://arxiv.org/abs/2305.13172v1",
        "pub_date": "2023-05-22",
        "summary": "Recent advancements in deep learning have precipitated the emergence of large\nlanguage models (LLMs) which exhibit an impressive aptitude for understanding\nand producing text akin to human language. Despite the ability to train highly\ncapable LLMs, the methodology for maintaining their relevancy and rectifying\nerrors remains elusive. To that end, the past few years have witnessed a surge\nin techniques for editing LLMs, the objective of which is to alter the behavior\nof LLMs within a specific domain without negatively impacting performance\nacross other inputs. This paper embarks on a deep exploration of the problems,\nmethods, and opportunities relating to model editing for LLMs. In particular,\nwe provide an exhaustive overview of the task definition and challenges\nassociated with model editing, along with an in-depth empirical analysis of the\nmost progressive methods currently at our disposal. We also build a new\nbenchmark dataset to facilitate a more robust evaluation and pinpoint enduring\nissues intrinsic to existing techniques. Our objective is to provide valuable\ninsights into the effectiveness and feasibility of each model editing\ntechnique, thereby assisting the research community in making informed\ndecisions when choosing the most appropriate method for a specific task or\ncontext. Code and datasets will be available at\nhttps://github.com/zjunlp/EasyEdit.",
        "translated": "深度学习的最新进展催生了大型语言模型(LLM)的出现，它们在理解和产生类似于人类语言的文本方面表现出令人印象深刻的天赋。尽管有能力培训高能力的 LLM，但保持其相关性和纠正错误的方法仍然是难以捉摸的。为此，过去几年见证了 LLM 编辑技术的激增，其目标是改变特定领域内 LLM 的行为，而不会对其他输入的性能产生负面影响。本文对 LLM 模型编辑的问题、方法和机会进行了深入的探讨。特别是，我们提供了任务定义和与模型编辑有关的挑战的详尽概述，以及对我们目前掌握的最进步的方法的深入实证分析。我们还建立了一个新的基准数据集，以促进更强大的评估，并确定持久的问题内在的现有技术。我们的目标是提供有价值的见解，每个模型编辑技术的有效性和可行性，从而协助研究界作出知情的决定时，选择最适当的方法，具体的任务或背景。代码和数据集将在 https://github.com/zjunlp/easyedit 提供。"
    },
    {
        "title": "LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities\n  and Future Opportunities",
        "url": "http://arxiv.org/abs/2305.13168v1",
        "pub_date": "2023-05-22",
        "summary": "This paper presents an exhaustive quantitative and qualitative evaluation of\nLarge Language Models (LLMs) for Knowledge Graph (KG) construction and\nreasoning. We employ eight distinct datasets that encompass aspects including\nentity, relation and event extraction, link prediction, and question answering.\nEmpirically, our findings suggest that GPT-4 outperforms ChatGPT in the\nmajority of tasks and even surpasses fine-tuned models in certain reasoning and\nquestion-answering datasets. Moreover, our investigation extends to the\npotential generalization ability of LLMs for information extraction, which\nculminates in the presentation of the Virtual Knowledge Extraction task and the\ndevelopment of the VINE dataset. Drawing on these empirical findings, we\nfurther propose AutoKG, a multi-agent-based approach employing LLMs for KG\nconstruction and reasoning, which aims to chart the future of this field and\noffer exciting opportunities for advancement. We anticipate that our research\ncan provide invaluable insights for future undertakings of KG\\footnote{Code and\ndatasets will be available in https://github.com/zjunlp/AutoKG.",
        "translated": "本文对知识图(KG)构造和推理的大语言模型(LLM)进行了详尽的定量和定性评价。我们使用八个不同的数据集，包括实体、关系和事件提取、链接预测和问题回答。经验上，我们的研究结果表明，GPT-4在大多数任务中的表现优于 ChatGPT，甚至在某些推理和问答数据集中优于微调模型。此外，我们的研究扩展到 LLM 对信息抽取的潜在推广能力，最终导致虚拟知识提取任务的介绍和 VINE 数据集的开发。基于这些实证研究结果，我们进一步提出了 AutoKG，这是一种基于多主体的方法，利用 LLM 进行 KG 的构建和推理，旨在描绘这一领域的未来，并提供令人兴奋的发展机会。我们期望我们的研究能为幼稚园日后的工作提供宝贵的意见。{ https://github.com/zjunlp/autokg 及数据集将可供参考。"
    },
    {
        "title": "Rethinking the Evaluation for Conversational Recommendation in the Era\n  of Large Language Models",
        "url": "http://arxiv.org/abs/2305.13112v1",
        "pub_date": "2023-05-22",
        "summary": "The recent success of large language models (LLMs) has shown great potential\nto develop more powerful conversational recommender systems (CRSs), which rely\non natural language conversations to satisfy user needs. In this paper, we\nembark on an investigation into the utilization of ChatGPT for conversational\nrecommendation, revealing the inadequacy of the existing evaluation protocol.\nIt might over-emphasize the matching with the ground-truth items or utterances\ngenerated by human annotators, while neglecting the interactive nature of being\na capable CRS. To overcome the limitation, we further propose an interactive\nEvaluation approach based on LLMs named iEvaLM that harnesses LLM-based user\nsimulators. Our evaluation approach can simulate various interaction scenarios\nbetween users and systems. Through the experiments on two publicly available\nCRS datasets, we demonstrate notable improvements compared to the prevailing\nevaluation protocol. Furthermore, we emphasize the evaluation of\nexplainability, and ChatGPT showcases persuasive explanation generation for its\nrecommendations. Our study contributes to a deeper comprehension of the\nuntapped potential of LLMs for CRSs and provides a more flexible and\neasy-to-use evaluation framework for future research endeavors. The codes and\ndata are publicly available at https://github.com/RUCAIBox/iEvaLM-CRS.",
        "translated": "大型语言模型(LLM)最近的成功显示了开发更强大的会话推荐系统(CRS)的巨大潜力，CRS 依赖于自然语言会话来满足用户的需求。本文对 ChatGPT 在会话推荐中的应用进行了研究，揭示了现有评价协议的不足之处。它可能过分强调与地面真相项目或人类注释者产生的话语的匹配，而忽视了作为一个有能力的 CRS 的互动性质。为了克服这一局限性，我们进一步提出了一种基于 LLM 的交互式评估方法 iEvaLM，该方法利用了基于 LLM 的用户模拟器。我们的评估方法可以模拟用户和系统之间的各种交互场景。通过对两个公开可用的 CRS 数据集的实验，我们发现与现行的评估协议相比有显著的改进。此外，我们强调可解释性的评价，ChatGPT 展示了其建议的说服性解释生成。我们的研究有助于更深入地了解 LLM 在 CRS 中尚未开发的潜力，并为未来的研究工作提供了一个更加灵活和易于使用的评估框架。这些代码和数据可以在 https://github.com/rucaibox/ievalm-crs 上公开获得。"
    },
    {
        "title": "Making Language Models Better Tool Learners with Execution Feedback",
        "url": "http://arxiv.org/abs/2305.13068v1",
        "pub_date": "2023-05-22",
        "summary": "Tools serve as pivotal interfaces that enable humans to understand and\nreshape the world. With the advent of foundational models, AI systems can\nutilize tools to expand their capabilities and interact with the world.\nExisting tool learning methodologies, encompassing supervised fine-tuning and\nprompt engineering approaches, often induce language models to utilize tools\nindiscriminately, as complex problems often exceed their own competencies.\nHowever, introducing tools for simple tasks, which the models themselves can\nreadily resolve, can inadvertently propagate errors rather than enhance\nperformance. This leads to the research question: can we teach language models\nwhen and how to use tools? To meet this need, we propose Tool leaRning wIth\nexeCution fEedback (TRICE), a two-stage end-to-end framework that enables the\nmodel to continually learn through feedback derived from tool execution,\nthereby learning when and how to use tools effectively. Experimental results,\nbacked by further analysis, show that TRICE can make the language model to\nselectively use tools by decreasing the model's dependency on tools while\nenhancing the performance. Code and datasets will be available in\nhttps://github.com/zjunlp/trice.",
        "translated": "工具作为关键的接口，使人类能够理解和重塑世界。随着基础模型的出现，人工智能系统可以利用工具来扩展它们的能力，并与世界互动。现有的工具学习方法，包括有监督的微调和及时的工程方法，经常诱导语言模型不加区分地使用工具，因为复杂的问题往往超出了它们自己的能力。然而，引入用于简单任务的工具(模型本身可以很容易地解决这些任务)可能会在无意中传播错误，而不是提高性能。这就引出了一个研究问题: 我们可以教语言模型何时以及如何使用工具吗？为了满足这一需求，我们提出工具学习与执行反馈(TRICE) ，一个两阶段的端到端框架，使模型能够通过反馈不断学习从工具执行，从而学习何时和如何有效地使用工具。实验结果表明，TRICE 能够使语言模型有选择地使用工具，降低模型对工具的依赖性，同时提高语言的性能。代码和数据集将以 https://github.com/zjunlp/trice 形式提供。"
    },
    {
        "title": "Evaluating and Enhancing Structural Understanding Capabilities of Large\n  Language Models on Tables via Input Designs",
        "url": "http://arxiv.org/abs/2305.13062v1",
        "pub_date": "2023-05-22",
        "summary": "Large language models (LLMs) are becoming attractive as few-shot reasoners to\nsolve NL-related tasks. However, there is still much to be learned about how\nwell LLMs understand structured data, such as tables. While it is true that\ntables can be used as inputs to LLMs with serialization, there lack\ncomprehensive studies examining whether LLMs can truly comprehend such data. In\nthis paper we try to understand this by designing a benchmark to evaluate\nstructural understanding capabilities (SUC) of LLMs. The benchmark we create\nincludes seven tasks, each with their own unique challenges, e.g,, cell lookup,\nrow retrieval and size detection. We run a series of evaluations on GPT-3\nfamily models (e.g., text-davinci-003). We discover that the performance varied\ndepending on a number of input choices, including table input format, content\norder, role prompting and partition marks. Drawing from the insights gained\nthrough the benchmark evaluations, we then propose self-augmentation for\neffective structural prompting, e.g., critical value / range identification\nusing LLMs' internal knowledge. When combined with carefully chosen input\nchoices, these structural prompting methods lead to promising improvements in\nLLM performance on a variety of tabular tasks, e.g., TabFact($\\uparrow2.31\\%$),\nHybridQA($\\uparrow2.13\\%$), SQA($\\uparrow2.72\\%$), Feverous($\\uparrow0.84\\%$),\nand ToTTo($\\uparrow5.68\\%$). We believe our benchmark and proposed prompting\nmethods can serve as a simple yet generic selection for future research. The\ncode and data are released in\nhttps://anonymous.4open.science/r/StructuredLLM-76F3.",
        "translated": "大型语言模型(LLM)作为解决大型语言相关任务的少量推理工具正变得越来越有吸引力。然而，关于 LLM 如何很好地理解结构化数据(如表) ，还有很多东西需要学习。虽然表确实可以用作具有序列化的 LLM 的输入，但是缺乏全面的研究来检查 LLM 是否能够真正理解这些数据。在本文中，我们试图通过设计一个基准来评估 LLM 的结构理解能力(SUC)来理解这一点。我们创建的基准测试包括七个任务，每个任务都有其独特的挑战，例如，单元格查找、行检索和大小检测。我们对 GPT-3家族模型(例如 text-davinci-003)进行了一系列的评估。我们发现，性能取决于许多输入选择，包括表输入格式、内容顺序、角色提示和分区标记。根据基准评估所获得的见解，我们提出自我增强的有效结构激励，例如，利用 LLM 的内部知识识识别临界值/范围。当结合精心选择的输入选择时，这些结构化的提示方法导致了各种表格任务的 LLM 性能的有希望的改善，例如 TabFact ($uparrow2.31% $) ，HybridQA ($uparrow2.13% $) ，SQA ($uparrow2.72% $) ，Feverous ($uparrow0.84% $)和 ToTTo ($uparrow5.68% $)。我们相信，我们的基准和提议的激励方法可以作为一个简单而通用的选择，为未来的研究。代码和数据以 https://anonymous.4open.science/r/structuredllm-76f3的形式发布。"
    },
    {
        "title": "Attentive Graph-based Text-aware Preference Modeling for Top-N\n  Recommendation",
        "url": "http://arxiv.org/abs/2305.12976v1",
        "pub_date": "2023-05-22",
        "summary": "Textual data are commonly used as auxiliary information for modeling user\npreference nowadays. While many prior works utilize user reviews for rating\nprediction, few focus on top-N recommendation, and even few try to incorporate\nitem textual contents such as title and description. Though delivering\npromising performance for rating prediction, we empirically find that many\nreview-based models cannot perform comparably well on top-N recommendation.\nAlso, user reviews are not available in some recommendation scenarios, while\nitem textual contents are more prevalent. On the other hand, recent graph\nconvolutional network (GCN) based models demonstrate state-of-the-art\nperformance for top-N recommendation. Thus, in this work, we aim to further\nimprove top-N recommendation by effectively modeling both item textual content\nand high-order connectivity in user-item graph. We propose a new model named\nAttentive Graph-based Text-aware Recommendation Model (AGTM). Extensive\nexperiments are provided to justify the rationality and effectiveness of our\nmodel design.",
        "translated": "文本数据是当今建立用户偏好模型的常用辅助信息。虽然许多以前的作品利用用户评论进行评分预测，但很少关注前 N 名的推荐，甚至很少尝试合并项目文本内容，如标题和描述。通过实证研究，我们发现许多基于评论的模型在排名前 N 的推荐中表现不佳。此外，在某些推荐场景中，用户评论是不可用的，而项目文本内容更为普遍。另一方面，最近基于图卷积网络(GCN)的模型展示了最高 N 推荐的最新性能。因此，在本研究中，我们的目标是通过有效地建立用户项目图中项目文本内容和高阶连通性的模型，进一步改善最高 N 推荐。提出了一种基于注意图的文本感知推荐模型(AGTM)。通过大量实验验证了模型设计的合理性和有效性。"
    },
    {
        "title": "It's Enough: Relaxing Diagonal Constraints in Linear Autoencoders for\n  Recommendation",
        "url": "http://arxiv.org/abs/2305.12922v1",
        "pub_date": "2023-05-22",
        "summary": "Linear autoencoder models learn an item-to-item weight matrix via convex\noptimization with L2 regularization and zero-diagonal constraints. Despite\ntheir simplicity, they have shown remarkable performance compared to\nsophisticated non-linear models. This paper aims to theoretically understand\nthe properties of two terms in linear autoencoders. Through the lens of\nsingular value decomposition (SVD) and principal component analysis (PCA), it\nis revealed that L2 regularization enhances the impact of high-ranked PCs.\nMeanwhile, zero-diagonal constraints reduce the impact of low-ranked PCs,\nleading to performance degradation for unpopular items. Inspired by this\nanalysis, we propose simple-yet-effective linear autoencoder models using\ndiagonal inequality constraints, called Relaxed Linear AutoEncoder (RLAE) and\nRelaxed Denoising Linear AutoEncoder (RDLAE). We prove that they generalize\nlinear autoencoders by adjusting the degree of diagonal constraints.\nExperimental results demonstrate that our models are comparable or superior to\nstate-of-the-art linear and non-linear models on six benchmark datasets; they\nsignificantly improve the accuracy of long-tail items. These results also\nsupport our theoretical insights on regularization and diagonal constraints in\nlinear autoencoders.",
        "translated": "线性自动编码器模型通过 L2正则化和零对角约束的凸优化学习一个项目对项目的权重矩阵。尽管它们很简单，但与复杂的非线性模型相比，它们表现出了显著的性能。本文旨在从理论上理解线性自动编码器中两项的性质。通过奇异值分解(SVD)和主成分分析(PCA)透镜，我们发现二语正规化增强了高等级个人电脑的影响。与此同时，零对角线约束减少了低排名个人电脑的影响，导致不受欢迎项目的性能下降。受此分析的启发，我们提出了使用对角不等式约束的简单而有效的线性自动编码器模型，称为松弛线性自动编码器(RLAE)和松弛去噪线性自动编码器(RDLAE)。我们证明了它们通过调整对角线约束的程度来推广线性自动编码器。实验结果表明，在六个基准数据集上，我们的模型与最先进的线性和非线性模型具有可比性或优越性，它们显著提高了长尾项目的准确性。这些结果也支持我们对线性自动编码器的正则化和对角线约束的理论认识。"
    },
    {
        "title": "Anchor Prediction: Automatic Refinement of Internet Links",
        "url": "http://arxiv.org/abs/2305.14337v1",
        "pub_date": "2023-05-23",
        "summary": "Internet links enable users to deepen their understanding of a topic by\nproviding convenient access to related information. However, the majority of\nlinks are unanchored -- they link to a target webpage as a whole, and readers\nmay expend considerable effort localizing the specific parts of the target\nwebpage that enrich their understanding of the link's source context. To help\nreaders effectively find information in linked webpages, we introduce the task\nof anchor prediction, where the goal is to identify the specific part of the\nlinked target webpage that is most related to the source linking context. We\nrelease the AuthorAnchors dataset, a collection of 34K naturally-occurring\nanchored links, which reflect relevance judgments by the authors of the source\narticle. To model reader relevance judgments, we annotate and release\nReaderAnchors, an evaluation set of anchors that readers find useful. Our\nanalysis shows that effective anchor prediction often requires jointly\nreasoning over lengthy source and target webpages to determine their implicit\nrelations and identify parts of the target webpage that are related but not\nredundant. We benchmark a performant T5-based ranking approach to establish\nbaseline performance on the task, finding ample room for improvement.",
        "translated": ""
    },
    {
        "title": "VIP5: Towards Multimodal Foundation Models for Recommendation",
        "url": "http://arxiv.org/abs/2305.14302v1",
        "pub_date": "2023-05-23",
        "summary": "Computer Vision (CV), Natural Language Processing (NLP), and Recommender\nSystems (RecSys) are three prominent AI applications that have traditionally\ndeveloped independently, resulting in disparate modeling and engineering\nmethodologies. This has impeded the ability for these fields to directly\nbenefit from each other's advancements. With the increasing availability of\nmultimodal data on the web, there is a growing need to consider various\nmodalities when making recommendations for users. With the recent emergence of\nfoundation models, large language models have emerged as a potential\ngeneral-purpose interface for unifying different modalities and problem\nformulations. In light of this, we propose the development of a multimodal\nfoundation model by considering both visual and textual modalities under the P5\nrecommendation paradigm (VIP5) to unify various modalities and recommendation\ntasks. This will enable the processing of vision, language, and personalization\ninformation in a shared architecture for improved recommendations. To achieve\nthis, we introduce multimodal personalized prompts to accommodate multiple\nmodalities under a shared format. Additionally, we propose a\nparameter-efficient training method for foundation models, which involves\nfreezing the backbone and fine-tuning lightweight adapters, resulting in\nimproved recommendation performance and increased efficiency in terms of\ntraining time and memory usage.",
        "translated": ""
    },
    {
        "title": "Simulating News Recommendation Ecosystem for Fun and Profit",
        "url": "http://arxiv.org/abs/2305.14103v1",
        "pub_date": "2023-05-23",
        "summary": "Understanding the evolution of online news communities is essential for\ndesigning more effective news recommender systems. However, due to the lack of\nappropriate datasets and platforms, the existing literature is limited in\nunderstanding the impact of recommender systems on this evolutionary process\nand the underlying mechanisms, resulting in sub-optimal system designs that may\naffect long-term utilities. In this work, we propose SimuLine, a simulation\nplatform to dissect the evolution of news recommendation ecosystems and present\na detailed analysis of the evolutionary process and underlying mechanisms.\nSimuLine first constructs a latent space well reflecting the human behaviors,\nand then simulates the news recommendation ecosystem via agent-based modeling.\nBased on extensive simulation experiments and the comprehensive analysis\nframework consisting of quantitative metrics, visualization, and textual\nexplanations, we analyze the characteristics of each evolutionary phase from\nthe perspective of life-cycle theory, and propose a relationship graph\nillustrating the key factors and affecting mechanisms. Furthermore, we explore\nthe impacts of recommender system designing strategies, including the\nutilization of cold-start news, breaking news, and promotion, on the\nevolutionary process, which shed new light on the design of recommender\nsystems.",
        "translated": ""
    },
    {
        "title": "BM25 Query Augmentation Learned End-to-End",
        "url": "http://arxiv.org/abs/2305.14087v1",
        "pub_date": "2023-05-23",
        "summary": "Given BM25's enduring competitiveness as an information retrieval baseline,\nwe investigate to what extent it can be even further improved by augmenting and\nre-weighting its sparse query-vector representation. We propose an approach to\nlearning an augmentation and a re-weighting end-to-end, and we find that our\napproach improves performance over BM25 while retaining its speed. We\nfurthermore find that the learned augmentations and re-weightings transfer well\nto unseen datasets.",
        "translated": ""
    },
    {
        "title": "Message Intercommunication for Inductive Relation Reasoning",
        "url": "http://arxiv.org/abs/2305.14074v1",
        "pub_date": "2023-05-23",
        "summary": "Inductive relation reasoning for knowledge graphs, aiming to infer missing\nlinks between brand-new entities, has drawn increasing attention. The models\ndeveloped based on Graph Inductive Learning, called GraIL-based models, have\nshown promising potential for this task. However, the uni-directional\nmessage-passing mechanism hinders such models from exploiting hidden mutual\nrelations between entities in directed graphs. Besides, the enclosing subgraph\nextraction in most GraIL-based models restricts the model from extracting\nenough discriminative information for reasoning. Consequently, the expressive\nability of these models is limited. To address the problems, we propose a novel\nGraIL-based inductive relation reasoning model, termed MINES, by introducing a\nMessage Intercommunication mechanism on the Neighbor-Enhanced Subgraph.\nConcretely, the message intercommunication mechanism is designed to capture the\nomitted hidden mutual information. It introduces bi-directed information\ninteractions between connected entities by inserting an undirected/bi-directed\nGCN layer between uni-directed RGCN layers. Moreover, inspired by the success\nof involving more neighbors in other graph-based tasks, we extend the\nneighborhood area beyond the enclosing subgraph to enhance the information\ncollection for inductive relation reasoning. Extensive experiments on twelve\ninductive benchmark datasets demonstrate that our MINES outperforms existing\nstate-of-the-art models, and show the effectiveness of our intercommunication\nmechanism and reasoning on the neighbor-enhanced subgraph.",
        "translated": ""
    },
    {
        "title": "When the Music Stops: Tip-of-the-Tongue Retrieval for Music",
        "url": "http://arxiv.org/abs/2305.14072v1",
        "pub_date": "2023-05-23",
        "summary": "We present a study of Tip-of-the-tongue (ToT) retrieval for music, where a\nsearcher is trying to find an existing music entity, but is unable to succeed\nas they cannot accurately recall important identifying information. ToT\ninformation needs are characterized by complexity, verbosity, uncertainty, and\npossible false memories. We make four contributions. (1) We collect a dataset -\n$ToT_{Music}$ - of 2,278 information needs and ground truth answers. (2) We\nintroduce a schema for these information needs and show that they often involve\nmultiple modalities encompassing several Music IR subtasks such as lyric\nsearch, audio-based search, audio fingerprinting, and text search. (3) We\nunderscore the difficulty of this task by benchmarking a standard text\nretrieval approach on this dataset. (4) We investigate the efficacy of query\nreformulations generated by a large language model (LLM), and show that they\nare not as effective as simply employing the entire information need as a query\n- leaving several open questions for future research.",
        "translated": ""
    },
    {
        "title": "DAPR: A Benchmark on Document-Aware Passage Retrieval",
        "url": "http://arxiv.org/abs/2305.13915v1",
        "pub_date": "2023-05-23",
        "summary": "Recent neural retrieval mainly focuses on ranking short texts and is\nchallenged with long documents. Existing work mainly evaluates either ranking\npassages or whole documents. However, there are many cases where the users want\nto find a relevant passage within a long document from a huge corpus, e.g.\nlegal cases, research papers, etc. In this scenario, the passage often provides\nlittle document context and thus challenges the current approaches to finding\nthe correct document and returning accurate results. To fill this gap, we\npropose and name this task Document-Aware Passage Retrieval (DAPR) and build a\nbenchmark including multiple datasets from various domains, covering both DAPR\nand whole-document retrieval. In experiments, we extend the state-of-the-art\nneural passage retrievers with document-level context via different approaches\nincluding prepending document summary, pooling over passage representations,\nand hybrid retrieval with BM25. The hybrid-retrieval systems, the overall best,\ncan only improve on the DAPR tasks marginally while significantly improving on\nthe document-retrieval tasks. This motivates further research in developing\nbetter retrieval systems for the new task. The code and the data are available\nat https://github.com/kwang2049/dapr",
        "translated": ""
    },
    {
        "title": "Term-Sets Can Be Strong Document Identifiers For Auto-Regressive Search\n  Engines",
        "url": "http://arxiv.org/abs/2305.13859v1",
        "pub_date": "2023-05-23",
        "summary": "Auto-regressive search engines emerge as a promising paradigm for next-gen\ninformation retrieval systems. These methods work with Seq2Seq models, where\neach query can be directly mapped to the identifier of its relevant document.\nAs such, they are praised for merits like being end-to-end differentiable.\nHowever, auto-regressive search engines also confront challenges in retrieval\nquality, given the requirement for the exact generation of the document\nidentifier. That's to say, the targeted document will be missed from the\nretrieval result if a false prediction about its identifier is made in any step\nof the generation process. In this work, we propose a novel framework, namely\nAutoTSG (Auto-regressive Search Engine with Term-Set Generation), which is\nfeatured by 1) the unordered term-based document identifier and 2) the\nset-oriented generation pipeline. With AutoTSG, any permutation of the term-set\nidentifier will lead to the retrieval of the corresponding document, thus\nlargely relaxing the requirement of exact generation. Besides, the Seq2Seq\nmodel is enabled to flexibly explore the optimal permutation of the document\nidentifier for the presented query, which may further contribute to the\nretrieval quality. AutoTSG is empirically evaluated with Natural Questions and\nMS MARCO, where notable improvements can be achieved against the existing\nauto-regressive search engines.",
        "translated": ""
    },
    {
        "title": "Advances and Challenges of Multi-task Learning Method in Recommender\n  System: A Survey",
        "url": "http://arxiv.org/abs/2305.13843v1",
        "pub_date": "2023-05-23",
        "summary": "Multi-task learning has been widely applied in computational vision, natural\nlanguage processing and other fields, which has achieved well performance. In\nrecent years, a lot of work about multi-task learning recommender system has\nbeen yielded, but there is no previous literature to summarize these works. To\nbridge this gap, we provide a systematic literature survey about multi-task\nrecommender systems, aiming to help researchers and practitioners quickly\nunderstand the current progress in this direction. In this survey, we first\nintroduce the background and the motivation of the multi-task learning-based\nrecommender systems. Then we provide a taxonomy of multi-task learning-based\nrecommendation methods according to the different stages of multi-task learning\ntechniques, which including task relationship discovery, model architecture and\noptimization strategy. Finally, we raise discussions on the application and\npromising future directions in this area.",
        "translated": ""
    },
    {
        "title": "Continual Learning on Dynamic Graphs via Parameter Isolation",
        "url": "http://arxiv.org/abs/2305.13825v1",
        "pub_date": "2023-05-23",
        "summary": "Many real-world graph learning tasks require handling dynamic graphs where\nnew nodes and edges emerge. Dynamic graph learning methods commonly suffer from\nthe catastrophic forgetting problem, where knowledge learned for previous\ngraphs is overwritten by updates for new graphs. To alleviate the problem,\ncontinual graph learning methods are proposed. However, existing continual\ngraph learning methods aim to learn new patterns and maintain old ones with the\nsame set of parameters of fixed size, and thus face a fundamental tradeoff\nbetween both goals. In this paper, we propose Parameter Isolation GNN (PI-GNN)\nfor continual learning on dynamic graphs that circumvents the tradeoff via\nparameter isolation and expansion. Our motivation lies in that different\nparameters contribute to learning different graph patterns. Based on the idea,\nwe expand model parameters to continually learn emerging graph patterns.\nMeanwhile, to effectively preserve knowledge for unaffected patterns, we find\nparameters that correspond to them via optimization and freeze them to prevent\nthem from being rewritten. Experiments on eight real-world datasets corroborate\nthe effectiveness of PI-GNN compared to state-of-the-art baselines.",
        "translated": ""
    },
    {
        "title": "NCHO: Unsupervised Learning for Neural 3D Composition of Humans and\n  Objects",
        "url": "http://arxiv.org/abs/2305.14345v1",
        "pub_date": "2023-05-23",
        "summary": "Deep generative models have been recently extended to synthesizing 3D digital\nhumans. However, previous approaches treat clothed humans as a single chunk of\ngeometry without considering the compositionality of clothing and accessories.\nAs a result, individual items cannot be naturally composed into novel\nidentities, leading to limited expressiveness and controllability of generative\n3D avatars. While several methods attempt to address this by leveraging\nsynthetic data, the interaction between humans and objects is not authentic due\nto the domain gap, and manual asset creation is difficult to scale for a wide\nvariety of objects. In this work, we present a novel framework for learning a\ncompositional generative model of humans and objects (backpacks, coats,\nscarves, and more) from real-world 3D scans. Our compositional model is\ninteraction-aware, meaning the spatial relationship between humans and objects,\nand the mutual shape change by physical contact is fully incorporated. The key\nchallenge is that, since humans and objects are in contact, their 3D scans are\nmerged into a single piece. To decompose them without manual annotations, we\npropose to leverage two sets of 3D scans of a single person with and without\nobjects. Our approach learns to decompose objects and naturally compose them\nback into a generative human model in an unsupervised manner. Despite our\nsimple setup requiring only the capture of a single subject with objects, our\nexperiments demonstrate the strong generalization of our model by enabling the\nnatural composition of objects to diverse identities in various poses and the\ncomposition of multiple objects, which is unseen in training data.",
        "translated": "深层生成模型最近已经扩展到合成3D 数字人类。然而，以前的方法没有考虑到衣服和配件的组合性，而是把穿着衣服的人当作一个单一的几何块来对待。因此，个别项目不能自然地组成新的身份，导致有限的表现力和可控性的生成3D 化身。虽然有几种方法试图通过利用合成数据来解决这个问题，但是由于领域间的差距，人与对象之间的交互并不真实，而且手动资产创建难以适用于各种各样的对象。在这项工作中，我们提出了一个新的框架来学习人类和物体(背包，外套，围巾等)的组合生成模型从现实世界的3 d 扫描。我们的构图模型是交互感知的，这意味着人与物体之间的空间关系，以及通过物理接触的相互形状变化被完全纳入。关键的挑战是，因为人类和物体是接触的，他们的3D 扫描合并成一个单一的部分。为了不用手动注释就可以分解它们，我们建议利用一个人的两组3D 扫描，有对象的和没有对象的。我们的方法学会了分解对象，并自然地以无监督的方式将它们重新组合成一个可生成的人类模型。尽管我们的简单设置只需要捕获具有对象的单个主体，但是我们的实验证明了我们的模型的强大泛化，通过使得对象的自然组合以不同姿势的不同身份和多个对象的组合，这在训练数据中是看不到的。"
    },
    {
        "title": "Siamese Masked Autoencoders",
        "url": "http://arxiv.org/abs/2305.14344v1",
        "pub_date": "2023-05-23",
        "summary": "Establishing correspondence between images or scenes is a significant\nchallenge in computer vision, especially given occlusions, viewpoint changes,\nand varying object appearances. In this paper, we present Siamese Masked\nAutoencoders (SiamMAE), a simple extension of Masked Autoencoders (MAE) for\nlearning visual correspondence from videos. SiamMAE operates on pairs of\nrandomly sampled video frames and asymmetrically masks them. These frames are\nprocessed independently by an encoder network, and a decoder composed of a\nsequence of cross-attention layers is tasked with predicting the missing\npatches in the future frame. By masking a large fraction ($95\\%$) of patches in\nthe future frame while leaving the past frame unchanged, SiamMAE encourages the\nnetwork to focus on object motion and learn object-centric representations.\nDespite its conceptual simplicity, features learned via SiamMAE outperform\nstate-of-the-art self-supervised methods on video object segmentation, pose\nkeypoint propagation, and semantic part propagation tasks. SiamMAE achieves\ncompetitive results without relying on data augmentation, handcrafted\ntracking-based pretext tasks, or other techniques to prevent representational\ncollapse.",
        "translated": "建立图像或场景之间的对应关系是计算机视觉中的一个重大挑战，特别是考虑到遮挡、视点变化和不同的物体外观。本文介绍了 Siamese 蒙版自动编码器(SiamMAE) ，它是蒙版自动编码器(MAE)的一个简单扩展，用于从视频中学习视觉对应。SiamMAE 对一对随机采样的视频帧进行操作，并非对称地屏蔽它们。这些帧由编码器网络独立处理，由一系列交叉注意层组成的解码器负责预测未来帧中丢失的补丁。SiamMAE 通过在未来帧中屏蔽大部分补丁(95%) ，同时保持过去帧不变，鼓励网络关注物体运动并学习以物体为中心的表示。尽管概念简单，通过 SiamMAE 学习的特征在视频对象分割、姿态关键点传播和语义部分传播任务方面优于最先进的自监督方法。SiamMAE 无需依赖数据增强、手工制作的基于跟踪的托辞任务或其他技术来防止表象崩溃，就能获得具有竞争力的结果。"
    },
    {
        "title": "Video Prediction Models as Rewards for Reinforcement Learning",
        "url": "http://arxiv.org/abs/2305.14343v1",
        "pub_date": "2023-05-23",
        "summary": "Specifying reward signals that allow agents to learn complex behaviors is a\nlong-standing challenge in reinforcement learning. A promising approach is to\nextract preferences for behaviors from unlabeled videos, which are widely\navailable on the internet. We present Video Prediction Rewards (VIPER), an\nalgorithm that leverages pretrained video prediction models as action-free\nreward signals for reinforcement learning. Specifically, we first train an\nautoregressive transformer on expert videos and then use the video prediction\nlikelihoods as reward signals for a reinforcement learning agent. VIPER enables\nexpert-level control without programmatic task rewards across a wide range of\nDMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction\nmodel allows us to derive rewards for an out-of-distribution environment where\nno expert data is available, enabling cross-embodiment generalization for\ntabletop manipulation. We see our work as starting point for scalable reward\nspecification from unlabeled videos that will benefit from the rapid advances\nin generative modeling. Source code and datasets are available on the project\nwebsite: https://escontrela.me",
        "translated": "指定奖励信号，让代理人学习复杂的行为，是强化学习研究中一个长期存在的挑战。一个有前途的方法是从未标记的视频中提取行为偏好，这些视频在互联网上广泛可用。我们提出了视频预测奖励(VIPER)算法，该算法利用预先训练的视频预测模型作为强化学习的无动作奖励信号。具体来说，我们首先在专家视频上训练一个自回归变换器，然后使用视频预测可能性作为强化学习代理的奖励信号。VIPER 可以实现专家级别的控制，无需程序任务奖励，跨越广泛的 DMC、 Atari 和 RLBench 任务。此外，视频预测模型的推广使我们能够在没有专家数据可用的分布外环境中获得奖励，从而能够对桌面操作进行跨实施例的推广。我们将我们的工作视为从未标记的视频中获得可扩展奖励规范的起点，这些视频将受益于生成建模的快速发展。源代码和数据集可在项目网站下载:  https://escontrela.me"
    },
    {
        "title": "Diffusion Hyperfeatures: Searching Through Time and Space for Semantic\n  Correspondence",
        "url": "http://arxiv.org/abs/2305.14334v1",
        "pub_date": "2023-05-23",
        "summary": "Diffusion models have been shown to be capable of generating high-quality\nimages, suggesting that they could contain meaningful internal representations.\nUnfortunately, the feature maps that encode a diffusion model's internal\ninformation are spread not only over layers of the network, but also over\ndiffusion timesteps, making it challenging to extract useful descriptors. We\npropose Diffusion Hyperfeatures, a framework for consolidating multi-scale and\nmulti-timestep feature maps into per-pixel feature descriptors that can be used\nfor downstream tasks. These descriptors can be extracted for both synthetic and\nreal images using the generation and inversion processes. We evaluate the\nutility of our Diffusion Hyperfeatures on the task of semantic keypoint\ncorrespondence: our method achieves superior performance on the SPair-71k real\nimage benchmark. We also demonstrate that our method is flexible and\ntransferable: our feature aggregation network trained on the inversion features\nof real image pairs can be used on the generation features of synthetic image\npairs with unseen objects and compositions. Our code is available at\n\\url{https://diffusion-hyperfeatures.github.io}.",
        "translated": "扩散模型已被证明能够产生高质量的图像，表明它们可以包含有意义的内部表示。遗憾的是，编码扩散模型内部信息的特征映射不仅分布在网络的各个层上，而且还分布在扩散时间步长上，这使得提取有用的描述符变得非常困难。我们提出了扩散超特征，一个框架，巩固多尺度和多时间步特征映射到每像素特征描述符，可用于下游任务。这些描述符可以提取合成图像和真实图像使用生成和反演过程。我们评估扩散超特征在语义关键点对应任务中的效用: 我们的方法在 SPair-71k 真实图像基准上获得了优越的性能。实验结果表明，该方法具有灵活性和可移植性: 基于实际图像对反演特征的特征聚合网络可以用于具有不可见物体和成分的合成图像对的特征生成。我们的代码可以在 url { https://diffusion-hyperfeatures.github.io }找到。"
    },
    {
        "title": "Prototype Adaption and Projection for Few- and Zero-shot 3D Point Cloud\n  Semantic Segmentation",
        "url": "http://arxiv.org/abs/2305.14335v1",
        "pub_date": "2023-05-23",
        "summary": "In this work, we address the challenging task of few-shot and zero-shot 3D\npoint cloud semantic segmentation. The success of few-shot semantic\nsegmentation in 2D computer vision is mainly driven by the pre-training on\nlarge-scale datasets like imagenet. The feature extractor pre-trained on\nlarge-scale 2D datasets greatly helps the 2D few-shot learning. However, the\ndevelopment of 3D deep learning is hindered by the limited volume and instance\nmodality of datasets due to the significant cost of 3D data collection and\nannotation. This results in less representative features and large intra-class\nfeature variation for few-shot 3D point cloud segmentation. As a consequence,\ndirectly extending existing popular prototypical methods of 2D few-shot\nclassification/segmentation into 3D point cloud segmentation won't work as well\nas in 2D domain. To address this issue, we propose a Query-Guided Prototype\nAdaption (QGPA) module to adapt the prototype from support point clouds feature\nspace to query point clouds feature space. With such prototype adaption, we\ngreatly alleviate the issue of large feature intra-class variation in point\ncloud and significantly improve the performance of few-shot 3D segmentation.\nBesides, to enhance the representation of prototypes, we introduce a\nSelf-Reconstruction (SR) module that enables prototype to reconstruct the\nsupport mask as well as possible. Moreover, we further consider zero-shot 3D\npoint cloud semantic segmentation where there is no support sample. To this\nend, we introduce category words as semantic information and propose a\nsemantic-visual projection model to bridge the semantic and visual spaces. Our\nproposed method surpasses state-of-the-art algorithms by a considerable 7.90%\nand 14.82% under the 2-way 1-shot setting on S3DIS and ScanNet benchmarks,\nrespectively. Code is available at https://github.com/heshuting555/PAP-FZS3D.",
        "translated": "在这项工作中，我们解决了具有挑战性的任务少镜头和零镜头三维点云语义分割。二维计算机视觉中少镜头语义分割的成功主要是由对大规模数据集(如图像网)的预训练所驱动的。在大规模二维数据集上预训练的特征提取器对二维少镜头学习有很大的帮助。然而，由于三维数据的采集和注释成本较高，数据集的体积和实例形式有限，阻碍了三维深度学习的发展。这导致了少镜头三维点云分割的代表性较差的特征和较大的类内特征变化。因此，将现有流行的二维少镜头分类/分割原型方法直接扩展到三维点云分割将不能像在二维领域那样有效。针对这一问题，提出了一种基于查询引导的原型适配(QGPA)模块，将原型从支持点云特征空间调整到查询点云特征空间。采用这种原型自适应方法，大大减轻了点云中特征类内变化大的问题，显著提高了少镜头三维分割的性能。此外，为了提高原型的表示能力，我们引入了自重构(SR)模块，使原型能够尽可能地重构支撑掩模。此外，在没有支持样本的情况下，我们进一步考虑了零拍3D 点云语义分割。为此，我们引入类别词作为语义信息，并提出一个语义-视觉投影模型，以连接语义和视觉空间。在 S3DIS 和 ScanNet 基准的双向单镜头设置下，我们提出的方法分别以7.90% 和14.82% 的优势超越了最先进的算法。密码可于 https://github.com/heshuting555/pap-fzs3d 索取。"
    },
    {
        "title": "Large Language Models are Frame-level Directors for Zero-shot\n  Text-to-Video Generation",
        "url": "http://arxiv.org/abs/2305.14330v1",
        "pub_date": "2023-05-23",
        "summary": "In the paradigm of AI-generated content (AIGC), there has been increasing\nattention in extending pre-trained text-to-image (T2I) models to text-to-video\n(T2V) generation. Despite their effectiveness, these frameworks face challenges\nin maintaining consistent narratives and handling rapid shifts in scene\ncomposition or object placement from a single user prompt. This paper\nintroduces a new framework, dubbed DirecT2V, which leverages instruction-tuned\nlarge language models (LLMs) to generate frame-by-frame descriptions from a\nsingle abstract user prompt. DirecT2V utilizes LLM directors to divide user\ninputs into separate prompts for each frame, enabling the inclusion of\ntime-varying content and facilitating consistent video generation. To maintain\ntemporal consistency and prevent object collapse, we propose a novel value\nmapping method and dual-softmax filtering. Extensive experimental results\nvalidate the effectiveness of the DirecT2V framework in producing visually\ncoherent and consistent videos from abstract user prompts, addressing the\nchallenges of zero-shot video generation.",
        "translated": "在人工智能生成内容(AIGC)的范式中，将预先训练好的文本到图像(T2I)模型扩展到文本到视频(T2V)生成已经受到越来越多的关注。尽管这些框架很有效，但它们在保持一致的叙述和处理来自单一用户提示的场景构成或物体放置的快速变化方面面临挑战。本文介绍了一个名为 DirecT2V 的新框架，它利用指令调优的大型语言模型(LLM)从单个抽象用户提示符生成一帧一帧的描述。DirecT2V 利用 LLM 控制器将用户输入划分为每个帧的单独提示，从而能够包含时变内容并促进一致的视频生成。为了保持时间一致性和防止对象崩溃，提出了一种新的值映射方法和双软极大值滤波。广泛的实验结果验证了 DirecT2V 框架在从抽象用户提示生成视觉一致性和一致性视频方面的有效性，解决了零拍摄视频生成的挑战。"
    },
    {
        "title": "Improving Factuality and Reasoning in Language Models through Multiagent\n  Debate",
        "url": "http://arxiv.org/abs/2305.14325v1",
        "pub_date": "2023-05-23",
        "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nlanguage generation, understanding, and few-shot learning in recent years. An\nextensive body of work has explored how their performance may be further\nimproved through the tools of prompting, ranging from verification,\nself-consistency, or intermediate scratchpads. In this paper, we present a\ncomplementary approach to improve language responses where multiple language\nmodel instances propose and debate their individual responses and reasoning\nprocesses over multiple rounds to arrive at a common final answer. Our findings\nindicate that this approach significantly enhances mathematical and strategic\nreasoning across a number of tasks. We also demonstrate that our approach\nimproves the factual validity of generated content, reducing fallacious answers\nand hallucinations that contemporary models are prone to. Our approach may be\ndirectly applied to existing black-box models and uses identical procedure and\nprompts for all tasks we investigate. Overall, our findings suggest that such\n\"society of minds\" approach has the potential to significantly advance the\ncapabilities of LLMs and pave the way for further breakthroughs in language\ngeneration and understanding.",
        "translated": "近年来，大语言模型(LLM)在语言生成、理解和短镜头学习等方面表现出了显著的能力。大量的工作已经探索了如何通过提示工具进一步提高他们的性能，从验证，自我一致性，或中间的暂存器。在本文中，我们提出了一个互补的方法来改善语言反应，其中多个语言模型实例提出和辩论他们的个别反应和推理过程，多轮得出一个共同的最终答案。我们的研究结果表明，这种方法显着提高了数学和战略推理的一些任务。我们还证明，我们的方法提高了生成内容的实际有效性，减少了当代模型容易产生的谬误答案和幻觉。我们的方法可以直接应用于现有的黑盒模型，并对我们调查的所有任务使用相同的过程和提示。总的来说，我们的研究结果表明，这种“心灵社会”的方法有潜力显着提高语言学习者的能力，并为语言生成和理解的进一步突破铺平道路。"
    },
    {
        "title": "Text-guided 3D Human Generation from 2D Collections",
        "url": "http://arxiv.org/abs/2305.14312v1",
        "pub_date": "2023-05-23",
        "summary": "3D human modeling has been widely used for engaging interaction in gaming,\nfilm, and animation. The customization of these characters is crucial for\ncreativity and scalability, which highlights the importance of controllability.\nIn this work, we introduce Text-guided 3D Human Generation (\\texttt{T3H}),\nwhere a model is to generate a 3D human, guided by the fashion description.\nThere are two goals: 1) the 3D human should render articulately, and 2) its\noutfit is controlled by the given text. To address this \\texttt{T3H} task, we\npropose Compositional Cross-modal Human (CCH). CCH adopts cross-modal attention\nto fuse compositional human rendering with the extracted fashion semantics.\nEach human body part perceives relevant textual guidance as its visual\npatterns. We incorporate the human prior and semantic discrimination to enhance\n3D geometry transformation and fine-grained consistency, enabling it to learn\nfrom 2D collections for data efficiency. We conduct evaluations on DeepFashion\nand SHHQ with diverse fashion attributes covering the shape, fabric, and color\nof upper and lower clothing. Extensive experiments demonstrate that CCH\nachieves superior results for \\texttt{T3H} with high efficiency.",
        "translated": "3D 人体建模已经广泛应用于游戏、电影和动画中的交互。这些字符的定制对于创造性和可扩展性至关重要，这突出了可控性的重要性。在这项工作中，我们介绍了文本引导的三维人类生成(texttt { T3H }) ，其中一个模型是生成一个三维人，在时尚描述的指导下。有两个目标: 1)3D 人应该清晰地渲染，2)它的装备是由给定的文本控制。为了解决这个文本{ T3H }任务，我们提出了组合跨模态人(CCH)。CCH 采用交叉模态注意将提取的时尚语义融合到组合人体绘制中。人体的每个部分都将相关的文本指导视为其视觉模式。我们结合了人类先验和语义识别，以增强三维几何变换和细粒度的一致性，使其能够学习从2D 集合的数据效率。我们对 DeepFashion 和 SHHQ 进行评估，它们具有多种时尚属性，包括上衣和下衣的形状、面料和颜色。大量的实验表明，CCH 对 texttt { T3H }具有较高的效率，取得了较好的效果。"
    },
    {
        "title": "Hierarchical Adaptive Voxel-guided Sampling for Real-time Applications\n  in Large-scale Point Clouds",
        "url": "http://arxiv.org/abs/2305.14306v1",
        "pub_date": "2023-05-23",
        "summary": "While point-based neural architectures have demonstrated their efficacy, the\ntime-consuming sampler currently prevents them from performing real-time\nreasoning on scene-level point clouds. Existing methods attempt to overcome\nthis issue by using random sampling strategy instead of the commonly-adopted\nfarthest point sampling~(FPS), but at the expense of lower performance. So the\neffectiveness/efficiency trade-off remains under-explored. In this paper, we\nreveal the key to high-quality sampling is ensuring an even spacing between\npoints in the subset, which can be naturally obtained through a grid. Based on\nthis insight, we propose a hierarchical adaptive voxel-guided point sampler\nwith linear complexity and high parallelization for real-time applications.\nExtensive experiments on large-scale point cloud detection and segmentation\ntasks demonstrate that our method achieves competitive performance with the\nmost powerful FPS, at an amazing speed that is more than 100 times faster. This\nbreakthrough in efficiency addresses the bottleneck of the sampling step when\nhandling scene-level point clouds. Furthermore, our sampler can be easily\nintegrated into existing models and achieves a 20$\\sim$80\\% reduction in\nruntime with minimal effort. The code will be available at\nhttps://github.com/OuyangJunyuan/pointcloud-3d-detector-tensorrt",
        "translated": "虽然基于点的神经结构已经证明了它们的功效，但耗时的采样器目前阻止它们对场景级别的点云进行实时推理。现有的方法试图用随机抽样策略代替常用的最远点抽样，但是以牺牲较低的性能为代价来克服这一问题。因此，有效性/效率之间的权衡仍未得到充分探讨。在本文中，我们揭示了高质量采样的关键是确保子集中点之间的均匀间距，这可以通过网格自然获得。在此基础上，我们提出了一种分层自适应体素引导的点采样器，它具有线性复杂度和高并行性，适用于实时应用。大规模点云检测和分割任务的大量实验表明，我们的方法实现了与最强大的 FPS 具有竞争力的性能，以惊人的速度，超过100倍的速度。这一效率上的突破解决了处理场景级点云时采样步骤的瓶颈。此外，我们的采样器可以很容易地集成到现有的模型，并实现20美元西姆 $80% 的运行时减少最小的努力。代码将在 https://github.com/ouyangjunyuan/pointcloud-3d-detector-tensorrt 公布"
    },
    {
        "title": "A Laplacian Pyramid Based Generative H&amp;E Stain Augmentation Network",
        "url": "http://arxiv.org/abs/2305.14301v1",
        "pub_date": "2023-05-23",
        "summary": "Hematoxylin and Eosin (H&amp;E) staining is a widely used sample preparation\nprocedure for enhancing the saturation of tissue sections and the contrast\nbetween nuclei and cytoplasm in histology images for medical diagnostics.\nHowever, various factors, such as the differences in the reagents used, result\nin high variability in the colors of the stains actually recorded. This\nvariability poses a challenge in achieving generalization for machine-learning\nbased computer-aided diagnostic tools. To desensitize the learned models to\nstain variations, we propose the Generative Stain Augmentation Network (G-SAN)\n-- a GAN-based framework that augments a collection of cell images with\nsimulated yet realistic stain variations. At its core, G-SAN uses a novel and\nhighly computationally efficient Laplacian Pyramid (LP) based generator\narchitecture, that is capable of disentangling stain from cell morphology.\nThrough the task of patch classification and nucleus segmentation, we show that\nusing G-SAN-augmented training data provides on average 15.7% improvement in F1\nscore and 7.3% improvement in panoptic quality, respectively. Our code is\navailable at https://github.com/lifangda01/GSAN-Demo.",
        "translated": "苏木精-伊红(H & E)染色是一种广泛应用的提高组织切片饱和度和增强组织学图像中细胞核和细胞质对比度的样品制备方法。然而，各种因素，例如使用的试剂的差异，导致实际记录的污渍颜色的高度可变性。这种可变性对基于机器学习的计算机辅助诊断工具的普及提出了挑战。为了使学习的模型对染色变化不敏感，我们提出生成染色增强网络(G-SAN)——一种基于 GAN 的框架，用模拟但真实的染色变化增强细胞图像集合。在其核心，G-SAN 使用了一种新的和高度计算效率的拉普拉斯金字塔(LP)为基础的生成器架构，这是能够从细胞形态学分离染色。通过斑块分类和细胞核分割实验，我们发现使用 G-SAN 增强训练数据，F1评分平均提高15.7% ，视觉质量平均提高7.3% 。我们的代码可以在 https://github.com/lifangda01/gsan-demo 找到。"
    },
    {
        "title": "Balancing the Picture: Debiasing Vision-Language Datasets with Synthetic\n  Contrast Sets",
        "url": "http://arxiv.org/abs/2305.15407v1",
        "pub_date": "2023-05-24",
        "summary": "Vision-language models are growing in popularity and public visibility to\ngenerate, edit, and caption images at scale; but their outputs can perpetuate\nand amplify societal biases learned during pre-training on uncurated image-text\npairs from the internet. Although debiasing methods have been proposed, we\nargue that these measurements of model bias lack validity due to dataset bias.\nWe demonstrate there are spurious correlations in COCO Captions, the most\ncommonly used dataset for evaluating bias, between background context and the\ngender of people in-situ. This is problematic because commonly-used bias\nmetrics (such as Bias@K) rely on per-gender base rates. To address this issue,\nwe propose a novel dataset debiasing pipeline to augment the COCO dataset with\nsynthetic, gender-balanced contrast sets, where only the gender of the subject\nis edited and the background is fixed. However, existing image editing methods\nhave limitations and sometimes produce low-quality images; so, we introduce a\nmethod to automatically filter the generated images based on their similarity\nto real images. Using our balanced synthetic contrast sets, we benchmark bias\nin multiple CLIP-based models, demonstrating how metrics are skewed by\nimbalance in the original COCO images. Our results indicate that the proposed\napproach improves the validity of the evaluation, ultimately contributing to\nmore realistic understanding of bias in vision-language models.",
        "translated": "视觉语言模型在大规模生成、编辑和标题图像方面越来越受欢迎和公众可见度越来越高; 但是它们的输出可以延续和放大在互联网上未经策划的图像-文本对的预培训中学到的社会偏见。虽然已经提出了消除偏差的方法，但是我们认为由于数据集的偏差，这些模型偏差的测量缺乏有效性。我们证明在 COCO 标题中存在虚假的相关性，COCO 标题是最常用于评估偏倚的数据集，背景环境和现场人员的性别之间存在虚假的相关性。这是有问题的，因为常用的偏见指标(如偏见@K)依赖于每个性别的基础比率。为了解决这一问题，我们提出了一种新的数据集去偏流水线，以增加合成的，性别平衡的对比集 COCO 数据集，其中只编辑主题的性别和背景是固定的。然而，现有的图像编辑方法存在一定的局限性，有时会产生低质量的图像，因此，我们提出了一种基于图像与真实图像相似度的自动过滤方法。使用我们的平衡合成对比度集，我们在多个基于 CLIP 的模型基准偏差，演示了如何在原始 COCO 图像的不平衡度量偏斜。我们的研究结果表明，提出的方法提高了评价的有效性，最终有助于更现实的理解偏见的视觉语言模型。"
    },
    {
        "title": "RoMa: Revisiting Robust Losses for Dense Feature Matching",
        "url": "http://arxiv.org/abs/2305.15404v1",
        "pub_date": "2023-05-24",
        "summary": "Dense feature matching is an important computer vision task that involves\nestimating all correspondences between two images of a 3D scene. In this paper,\nwe revisit robust losses for matching from a Markov chain perspective, yielding\ntheoretical insights and large gains in performance. We begin by constructing a\nunifying formulation of matching as a Markov chain, based on which we identify\ntwo key stages which we argue should be decoupled for matching. The first is\nthe coarse stage, where the estimated result needs to be globally consistent.\nThe second is the refinement stage, where the model needs precise localization\ncapabilities. Inspired by the insight that these stages concern distinct\nissues, we propose a coarse matcher following the regression-by-classification\nparadigm that provides excellent globally consistent, albeit not exactly\nlocalized, matches. This is followed by a local feature refinement stage using\nwell-motivated robust regression losses, yielding extremely precise matches.\nOur proposed approach, which we call RoMa, achieves significant improvements\ncompared to the state-of-the-art. Code is available at\nhttps://github.com/Parskatt/RoMa",
        "translated": "密集特征匹配是一项重要的计算机视觉任务，它涉及到估计三维场景中两幅图像之间的所有对应关系。在本文中，我们从马尔可夫链的角度重新审视匹配的鲁棒性损失，产生理论见解和性能的大幅提高。我们首先构造一个统一的匹配公式作为一个马尔可夫链，在此基础上，我们确定了两个关键的阶段，我们认为应该解耦匹配。第一个阶段是粗略阶段，其中估计的结果需要具有全局一致性。第二个阶段是精化阶段，在这个阶段模型需要精确的定位能力。受到这些阶段关注不同问题的洞察力的启发，我们提出了遵循分类回归范式的粗匹配器，它提供了优秀的全局一致性匹配，尽管不是完全本地化的匹配。然后是局部特征细化阶段，使用动机良好的鲁棒回归损失，产生非常精确的匹配。我们提出的方法，我们称之为 RoMa，与最先进的技术相比，取得了显著的改进。密码可于 https://github.com/parskatt/roma 索取"
    },
    {
        "title": "Sin3DM: Learning a Diffusion Model from a Single 3D Textured Shape",
        "url": "http://arxiv.org/abs/2305.15399v1",
        "pub_date": "2023-05-24",
        "summary": "Synthesizing novel 3D models that resemble the input example has long been\npursued by researchers and artists in computer graphics. In this paper, we\npresent Sin3DM, a diffusion model that learns the internal patch distribution\nfrom a single 3D textured shape and generates high-quality variations with fine\ngeometry and texture details. Training a diffusion model directly in 3D would\ninduce large memory and computational cost. Therefore, we first compress the\ninput into a lower-dimensional latent space and then train a diffusion model on\nit. Specifically, we encode the input 3D textured shape into triplane feature\nmaps that represent the signed distance and texture fields of the input. The\ndenoising network of our diffusion model has a limited receptive field to avoid\noverfitting, and uses triplane-aware 2D convolution blocks to improve the\nresult quality. Aside from randomly generating new samples, our model also\nfacilitates applications such as retargeting, outpainting and local editing.\nThrough extensive qualitative and quantitative evaluation, we show that our\nmodel can generate 3D shapes of various types with better quality than prior\nmethods.",
        "translated": "长期以来，计算机图形学的研究人员和艺术家一直致力于合成类似于输入样本的新颖3D 模型。本文提出了一种扩散模型 Sin3DM，该模型从单个三维纹理形状中学习内部斑块分布，并生成具有精细几何和纹理细节的高质量变化。直接在三维空间中训练扩散模型会产生大量的内存和计算开销。因此，我们首先将输入压缩到一个低维的潜在空间，然后在其上训练一个扩散模型。具体来说，我们将输入的三维纹理形状编码成三平面特征映射，表示输入的有符号距离和纹理字段。为了避免过拟合，扩散模型的去噪网络具有有限的接收域，并且使用了三平面感知的二维卷积块来提高结果的质量。除了随机产生新的样本，我们的模型还促进应用程序，如重定向，外绘和本地编辑。通过广泛的定性和定量评价，我们表明我们的模型可以生成各种类型的三维形状，比以往的方法更好的质量。"
    },
    {
        "title": "LayoutGPT: Compositional Visual Planning and Generation with Large\n  Language Models",
        "url": "http://arxiv.org/abs/2305.15393v1",
        "pub_date": "2023-05-24",
        "summary": "Attaining a high degree of user controllability in visual generation often\nrequires intricate, fine-grained inputs like layouts. However, such inputs\nimpose a substantial burden on users when compared to simple text inputs. To\naddress the issue, we study how Large Language Models (LLMs) can serve as\nvisual planners by generating layouts from text conditions, and thus\ncollaborate with visual generative models. We propose LayoutGPT, a method to\ncompose in-context visual demonstrations in style sheet language to enhance the\nvisual planning skills of LLMs. LayoutGPT can generate plausible layouts in\nmultiple domains, ranging from 2D images to 3D indoor scenes. LayoutGPT also\nshows superior performance in converting challenging language concepts like\nnumerical and spatial relations to layout arrangements for faithful\ntext-to-image generation. When combined with a downstream image generation\nmodel, LayoutGPT outperforms text-to-image models/systems by 20-40% and\nachieves comparable performance as human users in designing visual layouts for\nnumerical and spatial correctness. Lastly, LayoutGPT achieves comparable\nperformance to supervised methods in 3D indoor scene synthesis, demonstrating\nits effectiveness and potential in multiple visual domains.",
        "translated": "在可视化生成中获得高度的用户可控性通常需要复杂的、细粒度的输入，如布局。然而，与简单的文本输入相比，这种输入对用户造成了相当大的负担。为了解决这个问题，我们研究了大语言模型(LLM)如何通过根据文本条件生成布局来充当可视化规划器，从而与可视化生成模型进行协作。我们提出了 LayoutGPT，一种用样式表语言组合上下文视觉演示的方法，以提高 LLM 的视觉规划技巧。LayoutGPT 可以在多个领域生成合理的布局，从2D 图像到3D 室内场景。LayoutGPT 在将具有挑战性的语言概念(如数值和空间关系)转换为可靠的文本到图像生成的布局安排方面也表现出优越的性能。结合下游图像生成模型，LayoutGPT 的性能比文本到图像的模型/系统高出20-40% ，并且在数值和空间正确性的可视化布局设计方面达到与人类用户相当的性能。最后，LayoutGPT 在三维室内场景合成中实现了与监督方法相当的性能，证明了其在多视觉领域中的有效性和潜力。"
    },
    {
        "title": "A Neural Space-Time Representation for Text-to-Image Personalization",
        "url": "http://arxiv.org/abs/2305.15391v1",
        "pub_date": "2023-05-24",
        "summary": "A key aspect of text-to-image personalization methods is the manner in which\nthe target concept is represented within the generative process. This choice\ngreatly affects the visual fidelity, downstream editability, and disk space\nneeded to store the learned concept. In this paper, we explore a new\ntext-conditioning space that is dependent on both the denoising process\ntimestep (time) and the denoising U-Net layers (space) and showcase its\ncompelling properties. A single concept in the space-time representation is\ncomposed of hundreds of vectors, one for each combination of time and space,\nmaking this space challenging to optimize directly. Instead, we propose to\nimplicitly represent a concept in this space by optimizing a small neural\nmapper that receives the current time and space parameters and outputs the\nmatching token embedding. In doing so, the entire personalized concept is\nrepresented by the parameters of the learned mapper, resulting in a compact,\nyet expressive, representation. Similarly to other personalization methods, the\noutput of our neural mapper resides in the input space of the text encoder. We\nobserve that one can significantly improve the convergence and visual fidelity\nof the concept by introducing a textual bypass, where our neural mapper\nadditionally outputs a residual that is added to the output of the text\nencoder. Finally, we show how one can impose an importance-based ordering over\nour implicit representation, providing users control over the reconstruction\nand editability of the learned concept using a single trained model. We\ndemonstrate the effectiveness of our approach over a range of concepts and\nprompts, showing our method's ability to generate high-quality and controllable\ncompositions without fine-tuning any parameters of the generative model itself.",
        "translated": "文本到图像个性化方法的一个关键方面是目标概念在生成过程中的表示方式。这种选择极大地影响了视觉保真度、下游可编辑性和存储所学概念所需的磁盘空间。在本文中，我们探索了一个新的文本条件空间，它同时依赖于去噪过程的时间步长(时间)和去噪的 U-Net 层(空间) ，并展示了其引人注目的性质。时空表示中的一个概念由数百个向量组成，每个向量对应于时间和空间的组合，这使得直接优化这个空间具有挑战性。相反，我们建议通过优化一个接收当前时间和空间参数并输出匹配令牌嵌入的小型神经映射器来隐式表示这个空间中的一个概念。在这样做时，整个个性化的概念是由学习映射器的参数表示，导致一个紧凑，但表达，表示。与其他个性化方法类似，我们的神经映射器的输出驻留在文本编码器的输入空间中。我们观察到，通过引入文本旁路，可以显著提高概念的收敛性和视觉保真度，其中我们的神经映射器额外输出一个残差，添加到文本编码器的输出。最后，我们展示了如何在我们的隐式表示上强加一个基于重要性的排序，使用一个单一的训练模型为用户提供对所学概念的重构和可编辑性的控制。我们通过一系列的概念和提示展示了我们的方法的有效性，展示了我们的方法能够生成高质量和可控的合成物，而不需要对生成模型本身的任何参数进行微调。"
    },
    {
        "title": "What can generic neural networks learn from a child's visual experience?",
        "url": "http://arxiv.org/abs/2305.15372v1",
        "pub_date": "2023-05-24",
        "summary": "Young children develop sophisticated internal models of the world based on\ntheir egocentric visual experience. How much of this is driven by innate\nconstraints and how much is driven by their experience? To investigate these\nquestions, we train state-of-the-art neural networks on a realistic proxy of a\nchild's visual experience without any explicit supervision or domain-specific\ninductive biases. Specifically, we train both embedding models and generative\nmodels on 200 hours of headcam video from a single child collected over two\nyears. We train a total of 72 different models, exploring a range of model\narchitectures and self-supervised learning algorithms, and comprehensively\nevaluate their performance in downstream tasks. The best embedding models\nperform at 70% of a highly performant ImageNet-trained model on average. They\nalso learn broad semantic categories without any labeled examples and learn to\nlocalize semantic categories in an image without any location supervision.\nHowever, these models are less object-centric and more background-sensitive\nthan comparable ImageNet-trained models. Generative models trained with the\nsame data successfully extrapolate simple properties of partially masked\nobjects, such as their texture, color, orientation, and rough outline, but\nstruggle with finer object details. We replicate our experiments with two other\nchildren and find very similar results. Broadly useful high-level visual\nrepresentations are thus robustly learnable from a representative sample of a\nchild's visual experience without strong inductive biases.",
        "translated": "幼儿根据自我中心的视觉经验，发展出复杂的内在世界模型。这其中有多少是由先天约束驱动的，又有多少是由他们的经验驱动的？为了研究这些问题，我们训练最先进的神经网络，在没有任何明确的监督或领域特定的归纳偏见的情况下，对儿童的视觉经验进行现实的替代。具体来说，我们训练嵌入模型和生成模型的200个小时的头部摄像头视频从一个孩子收集了两年多。我们总共训练了72个不同的模型，探索了一系列模型结构和自我监督学习算法，并全面评估了它们在下游任务中的表现。最好的嵌入模型在高性能 ImageNet 训练模型中的平均执行率为70% 。他们还学习广泛的语义类别没有任何标记的例子和学习本地化的语义类别在一个图像没有任何位置监督。然而，与可比较的 ImageNet 训练模型相比，这些模型更少的以对象为中心，更多的是对背景敏感。使用相同数据训练的生成模型成功地推断出部分遮蔽对象的简单属性，例如它们的纹理、颜色、方向和粗略轮廓，但是难以获得更精细的对象细节。我们在另外两个孩子身上做了同样的实验，得到了非常相似的结果。因此，广泛有用的高层次视觉表征可以从一个具有代表性的儿童视觉经验样本中强有力地学习，而没有强烈的归纳偏见。"
    },
    {
        "title": "SAMScore: A Semantic Structural Similarity Metric for Image Translation\n  Evaluation",
        "url": "http://arxiv.org/abs/2305.15367v1",
        "pub_date": "2023-05-24",
        "summary": "Image translation has wide applications, such as style transfer and modality\nconversion, usually aiming to generate images having both high degrees of\nrealism and faithfulness. These problems remain difficult, especially when it\nis important to preserve semantic structures. Traditional image-level\nsimilarity metrics are of limited use, since the semantics of an image are\nhigh-level, and not strongly governed by pixel-wise faithfulness to an original\nimage. Towards filling this gap, we introduce SAMScore, a generic semantic\nstructural similarity metric for evaluating the faithfulness of image\ntranslation models. SAMScore is based on the recent high-performance Segment\nAnything Model (SAM), which can perform semantic similarity comparisons with\nstandout accuracy. We applied SAMScore on 19 image translation tasks, and found\nthat it is able to outperform all other competitive metrics on all of the\ntasks. We envision that SAMScore will prove to be a valuable tool that will\nhelp to drive the vibrant field of image translation, by allowing for more\nprecise evaluations of new and evolving translation models. The code is\navailable at https://github.com/Kent0n-Li/SAMScore.",
        "translated": "意象翻译在文体转换、情态转换等方面有着广泛的应用，通常意象翻译的目的是生成逼真度高、忠实度高的意象。这些问题仍然很难解决，特别是在保护语义结构很重要的情况下。传统的图像级相似度指标的用途有限，因为图像的语义是高级的，并且不受像素级对原始图像的忠实度的强烈支配。为了填补这个空白，我们引入了 SAMScore，这是一个通用的语义结构相似性指标，用于评估图像翻译模型的可靠性。SAMScore 是基于最新的高性能分段任意模型(Segment AnyModel，SAM) ，它可以执行语义相似度比较，具有突出的准确性。我们将 SAMScore 应用于19个图像翻译任务，发现它在所有任务中的表现都优于其他竞争指标。我们设想 SAMScore 将被证明是一个有价值的工具，通过允许对新的和不断发展的翻译模式进行更精确的评估，将有助于推动图像翻译这一充满活力的领域。密码可在 https://github.com/kent0n-li/samscore 查阅。"
    },
    {
        "title": "Boundary Attention Mapping (BAM): Fine-grained saliency maps for\n  segmentation of Burn Injuries",
        "url": "http://arxiv.org/abs/2305.15365v1",
        "pub_date": "2023-05-24",
        "summary": "Burn injuries can result from mechanisms such as thermal, chemical, and\nelectrical insults. A prompt and accurate assessment of burns is essential for\ndeciding definitive clinical treatments. Currently, the primary approach for\nburn assessments, via visual and tactile observations, is approximately 60%-80%\naccurate. The gold standard is biopsy and a close second would be non-invasive\nmethods like Laser Doppler Imaging (LDI) assessments, which have up to 97%\naccuracy in predicting burn severity and the required healing time. In this\npaper, we introduce a machine learning pipeline for assessing burn severities\nand segmenting the regions of skin that are affected by burn. Segmenting 2D\ncolour images of burns allows for the injured versus non-injured skin to be\ndelineated, clearly marking the extent and boundaries of the localized\nburn/region-of-interest, even during remote monitoring of a burn patient. We\ntrained a convolutional neural network (CNN) to classify four severities of\nburns. We built a saliency mapping method, Boundary Attention Mapping (BAM),\nthat utilises this trained CNN for the purpose of accurately localizing and\nsegmenting the burn regions from skin burn images. We demonstrated the\neffectiveness of our proposed pipeline through extensive experiments and\nevaluations using two datasets; 1) A larger skin burn image dataset consisting\nof 1684 skin burn images of four burn severities, 2) An LDI dataset that\nconsists of a total of 184 skin burn images with their associated LDI scans.\nThe CNN trained using the first dataset achieved an average F1-Score of 78% and\nmicro/macro- average ROC of 85% in classifying the four burn severities.\nMoreover, a comparison between the BAM results and LDI results for measuring\ninjury boundary showed that the segmentations generated by our method achieved\n91.60% accuracy, 78.17% sensitivity, and 93.37% specificity.",
        "translated": "烧伤可能是由热、化学和电气等机制造成的。及时和准确的评估烧伤是决定明确的临床治疗是必不可少的。目前，烧伤评估的主要方法，通过视觉和触觉观察，大约60% -80% 的准确率。黄金标准是活组织检查，紧随其后的是非侵入性方法，如激光多普勒成像(LDI)评估，它在预测烧伤严重程度和所需的愈合时间方面有高达97% 的准确性。在本文中，我们介绍了一个机器学习管道，用于评估烧伤的严重程度和分割皮肤受烧伤影响的区域。分割烧伤的二维彩色图像允许描绘受伤与未受伤的皮肤，即使在远程监测烧伤患者期间，也清楚地标记局部烧伤/感兴趣区域的范围和边界。我们训练了一个卷积神经网络(CNN)来分类烧伤的四种严重程度。我们建立了一个突出映射方法，边界注意映射(BAM) ，利用这个训练的 CNN 的目的是准确定位和分割烧伤区域从皮肤烧伤图像。我们通过使用两个数据集进行广泛的实验和评估，证明了我们提出的管道的有效性; 1)由1684个四种烧伤严重程度的皮肤烧伤图像组成的更大的皮肤烧伤图像数据集，2)总共由184个皮肤烧伤图像及其相关的 LDI 扫描组成的 LDI 数据集。使用第一个数据集训练的美国有线电视新闻网在对四种烧伤严重程度进行分类时，平均达到了78% 的 f1-得分和85% 的微观/宏观平均 ROC。此外，测量损伤边界的 BAM 结果与 LDI 结果之间的比较显示，由我们的方法产生的分割达到91.60% 的准确性，78.17% 的灵敏度和93.37% 的特异性。"
    },
    {
        "title": "Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image\n  Super-Resolution",
        "url": "http://arxiv.org/abs/2305.15357v1",
        "pub_date": "2023-05-24",
        "summary": "Diffusion models, as a kind of powerful generative model, have given\nimpressive results on image super-resolution (SR) tasks. However, due to the\nrandomness introduced in the reverse process of diffusion models, the\nperformances of diffusion-based SR models are fluctuating at every time of\nsampling, especially for samplers with few resampled steps. This inherent\nrandomness of diffusion models results in ineffectiveness and instability,\nmaking it challenging for users to guarantee the quality of SR results.\nHowever, our work takes this randomness as an opportunity: fully analyzing and\nleveraging it leads to the construction of an effective plug-and-play sampling\nmethod that owns the potential to benefit a series of diffusion-based SR\nmethods. More in detail, we propose to steadily sample high-quality SR images\nfrom pretrained diffusion-based SR models by solving diffusion ordinary\ndifferential equations (diffusion ODEs) with optimal boundary conditions (BCs)\nand analyze the characteristics between the choices of BCs and their\ncorresponding SR results. Our analysis shows the route to obtain an\napproximately optimal BC via an efficient exploration in the whole space. The\nquality of SR results sampled by the proposed method with fewer steps\noutperforms the quality of results sampled by current methods with randomness\nfrom the same pretrained diffusion-based SR model, which means that our\nsampling method ``boosts'' current diffusion-based SR models without any\nadditional training.",
        "translated": "扩散模型作为一种强大的生成模型，在图像超分辨率(SR)任务中给出了令人印象深刻的结果。然而，由于扩散模型反向过程的随机性，基于扩散的 SR 模型在每次采样时的性能都会发生波动，特别是对于重采样步数较少的采样者。这种扩散模型固有的随机性导致无效性和不稳定性，使得用户难以保证 SR 结果的质量。然而，我们的工作把这种随机性作为一个机会: 充分分析和利用它导致建立一个有效的即插即用的抽样方法，拥有受益于一系列基于扩散的 SR 方法的潜力。具体来说，我们提出了通过求解具有最优边界条件的扩散常微分方程，从基于扩散的预训练 SR 模型中稳定地采集高质量的 SR 图像，并分析了扩散常微分方程的选择与其相应 SR 结果之间的特点。我们的分析表明，通过在整个空间进行有效的探索，可以获得一个近似最优 BC 的路径。本文提出的方法采样的 SR 结果的质量较少的步骤优于目前的方法采样的结果的质量与随机性从相同的预先训练的扩散为基础的 SR 模型，这意味着我们的采样方法“推动”目前的扩散为基础的 SR 模型没有任何额外的训练。"
    },
    {
        "title": "Mitigating Biased Activation in Weakly-supervised Object Localization\n  via Counterfactual Learning",
        "url": "http://arxiv.org/abs/2305.15354v1",
        "pub_date": "2023-05-24",
        "summary": "In this paper, we focus on an under-explored issue of biased activation in\nprior weakly-supervised object localization methods based on Class Activation\nMapping (CAM). We analyze the cause of this problem from a causal view and\nattribute it to the co-occurring background confounders. Following this\ninsight, we propose a novel Counterfactual Co-occurring Learning (CCL) paradigm\nto synthesize the counterfactual representations via coupling constant\nforeground and unrealized backgrounds in order to cut off their co-occurring\nrelationship. Specifically, we design a new network structure called\nCounterfactual-CAM, which embeds the counterfactual representation perturbation\nmechanism into the vanilla CAM-based model. This mechanism is responsible for\ndecoupling foreground as well as background and synthesizing the counterfactual\nrepresentations. By training the detection model with these synthesized\nrepresentations, we compel the model to focus on the constant foreground\ncontent while minimizing the influence of distracting co-occurring background.\nTo our best knowledge, it is the first attempt in this direction. Extensive\nexperiments on several benchmarks demonstrate that Counterfactual-CAM\nsuccessfully mitigates the biased activation problem, achieving improved object\nlocalization accuracy.",
        "translated": "本文研究了基于类激活映射(CAM)的弱监督目标定位方法中存在的偏向激活问题。我们从因果关系的角度分析了这一问题的原因，并将其归因于共同发生的背景混杂因素。根据这一观点，我们提出了一种新的反事实共现学习范式，通过耦合常数前景和未实现背景来综合反事实表征，以切断它们之间的共现关系。具体来说，我们设计了一种新的网络结构，称为反事实-CAM，它将反事实表示扰动机制嵌入到普通的基于 CAM 的模型中。该机制负责解耦前景和背景，并综合反事实表示。通过训练这些综合表征的检测模型，我们迫使模型集中在恒定的前景内容，同时尽量减少干扰共现背景的影响。据我们所知，这是朝这个方向的第一次尝试。在几个基准上的大量实验表明，反事实 CAM 成功地缓解了有偏激活问题，提高了目标定位精度。"
    },
    {
        "title": "Uni-ControlNet: All-in-One Control to Text-to-Image Diffusion Models",
        "url": "http://arxiv.org/abs/2305.16322v1",
        "pub_date": "2023-05-25",
        "summary": "Text-to-Image diffusion models have made tremendous progress over the past\ntwo years, enabling the generation of highly realistic images based on\nopen-domain text descriptions. However, despite their success, text\ndescriptions often struggle to adequately convey detailed controls, even when\ncomposed of long and complex texts. Moreover, recent studies have also shown\nthat these models face challenges in understanding such complex texts and\ngenerating the corresponding images. Therefore, there is a growing need to\nenable more control modes beyond text description. In this paper, we introduce\nUni-ControlNet, a novel approach that allows for the simultaneous utilization\nof different local controls (e.g., edge maps, depth map, segmentation masks)\nand global controls (e.g., CLIP image embeddings) in a flexible and composable\nmanner within one model. Unlike existing methods, Uni-ControlNet only requires\nthe fine-tuning of two additional adapters upon frozen pre-trained\ntext-to-image diffusion models, eliminating the huge cost of training from\nscratch. Moreover, thanks to some dedicated adapter designs, Uni-ControlNet\nonly necessitates a constant number (i.e., 2) of adapters, regardless of the\nnumber of local or global controls used. This not only reduces the fine-tuning\ncosts and model size, making it more suitable for real-world deployment, but\nalso facilitate composability of different conditions. Through both\nquantitative and qualitative comparisons, Uni-ControlNet demonstrates its\nsuperiority over existing methods in terms of controllability, generation\nquality and composability. Code is available at\n\\url{https://github.com/ShihaoZhaoZSH/Uni-ControlNet}.",
        "translated": "在过去的两年中，文本到图像的扩散模型取得了巨大的进展，使得基于开放域文本描述的高度真实感图像的生成成为可能。然而，尽管成功，文本描述往往难以充分传达详细的控制，即使是组成了长期和复杂的文本。此外，最近的研究也表明，这些模型在理解这些复杂的文本和生成相应的图像面临挑战。因此，越来越需要在文本描述之外启用更多的控制模式。在本文中，我们介绍了 Uni-ControlNet，一种新的方法，允许同时利用不同的局部控件(例如，边缘映射，深度映射，分割掩码)和全局控件(例如，CLIP 图像嵌入)在一个灵活和可组合的方式在一个模型。与现有的方法不同，Uni-ControlNet 只需要在冻结的预先训练的文本到图像扩散模型上对另外两个适配器进行微调，从而消除了从头开始训练的巨大成本。此外，由于一些专用的适配器设计，Uni-ControlNet 只需要一个常数(即2)的适配器，而不管所使用的本地或全局控件的数量。这不仅降低了微调成本和模型大小，使其更适合于实际部署，而且还促进了不同条件的可组合性。通过定量和定性比较，Uni-ControlNet 在可控性、生成质量和可组合性等方面均优于现有方法。代码可在网址{ https://github.com/shihaozhaozsh/uni-controlnet }下载。"
    },
    {
        "title": "Eclipse: Disambiguating Illumination and Materials using Unintended\n  Shadows",
        "url": "http://arxiv.org/abs/2305.16321v1",
        "pub_date": "2023-05-25",
        "summary": "Decomposing an object's appearance into representations of its materials and\nthe surrounding illumination is difficult, even when the object's 3D shape is\nknown beforehand. This problem is ill-conditioned because diffuse materials\nseverely blur incoming light, and is ill-posed because diffuse materials under\nhigh-frequency lighting can be indistinguishable from shiny materials under\nlow-frequency lighting. We show that it is possible to recover precise\nmaterials and illumination -- even from diffuse objects -- by exploiting\nunintended shadows, like the ones cast onto an object by the photographer who\nmoves around it. These shadows are a nuisance in most previous inverse\nrendering pipelines, but here we exploit them as signals that improve\nconditioning and help resolve material-lighting ambiguities. We present a\nmethod based on differentiable Monte Carlo ray tracing that uses images of an\nobject to jointly recover its spatially-varying materials, the surrounding\nillumination environment, and the shapes of the unseen light occluders who\ninadvertently cast shadows upon it.",
        "translated": "分解一个物体的外观到其材料和周围照明的表示是困难的，即使当物体的三维形状是已知的。这个问题是病态的，因为漫反射材料严重模糊入射光，而且是病态的，因为在高频照明下漫反射材料可以与低频照明下发光材料难以区分。我们展示了通过利用意想不到的阴影(如摄影师在物体周围移动时投射到物体上的阴影)来恢复精确的材料和照明——即使是从漫反射的物体上也是可能的。这些阴影在大多数以前的反向渲染管道中是一个麻烦，但在这里我们利用它们作为改善条件反射和帮助解决材质-照明模糊的信号。我们提出了一种基于可微蒙特卡罗射线追踪的方法，该方法利用一个物体的图像来共同恢复其空间变化的材料，周围的照明环境，以及无意中在其上投射阴影的看不见的光遮挡物的形状。"
    },
    {
        "title": "Image is First-order Norm+Linear Autoregressive",
        "url": "http://arxiv.org/abs/2305.16319v1",
        "pub_date": "2023-05-25",
        "summary": "This paper reveals that every image can be understood as a first-order\nnorm+linear autoregressive process, referred to as FINOLA, where norm+linear\ndenotes the use of normalization before the linear model. We demonstrate that\nimages of size 256$\\times$256 can be reconstructed from a compressed vector\nusing autoregression up to a 16$\\times$16 feature map, followed by upsampling\nand convolution. This discovery sheds light on the underlying partial\ndifferential equations (PDEs) governing the latent feature space. Additionally,\nwe investigate the application of FINOLA for self-supervised learning through a\nsimple masked prediction technique. By encoding a single unmasked quadrant\nblock, we can autoregressively predict the surrounding masked region.\nRemarkably, this pre-trained representation proves effective for image\nclassification and object detection tasks, even in lightweight networks,\nwithout requiring fine-tuning. The code will be made publicly available.",
        "translated": "本文揭示了每幅图像都可以理解为一阶范数 + 线性自回归过程，称为 FINOLA，其中范数 + 线性表示在线性模型之前使用归一化。我们证明了大小为256美元乘以256美元的图像可以从一个压缩向量重建使用自回归高达16美元乘以16美元的特征映射，然后上采样和卷积。这一发现揭示了控制潜在特征空间的基本偏微分方程(PDE)。此外，我们还通过一个简单的掩蔽预测技术研究了 FINOLA 在自监督学习中的应用。通过编码一个未遮蔽的象限块，我们可以自回归地预测周围的遮蔽区域。值得注意的是，这种预先训练的表示被证明对图像分类和目标检测任务非常有效，即使在轻量级网络中，也不需要进行微调。代码将公开发布。"
    },
    {
        "title": "Referred by Multi-Modality: A Unified Temporal Transformer for Video\n  Object Segmentation",
        "url": "http://arxiv.org/abs/2305.16318v1",
        "pub_date": "2023-05-25",
        "summary": "Recently, video object segmentation (VOS) referred by multi-modal signals,\ne.g., language and audio, has evoked increasing attention in both industry and\nacademia. It is challenging for exploring the semantic alignment within\nmodalities and the visual correspondence across frames. However, existing\nmethods adopt separate network architectures for different modalities, and\nneglect the inter-frame temporal interaction with references. In this paper, we\npropose MUTR, a Multi-modal Unified Temporal transformer for Referring video\nobject segmentation. With a unified framework for the first time, MUTR adopts a\nDETR-style transformer and is capable of segmenting video objects designated by\neither text or audio reference. Specifically, we introduce two strategies to\nfully explore the temporal relations between videos and multi-modal signals.\nFirstly, for low-level temporal aggregation before the transformer, we enable\nthe multi-modal references to capture multi-scale visual cues from consecutive\nvideo frames. This effectively endows the text or audio signals with temporal\nknowledge and boosts the semantic alignment between modalities. Secondly, for\nhigh-level temporal interaction after the transformer, we conduct inter-frame\nfeature communication for different object embeddings, contributing to better\nobject-wise correspondence for tracking along the video. On Ref-YouTube-VOS and\nAVSBench datasets with respective text and audio references, MUTR achieves\n+4.2% and +4.2% J&amp;F improvements to state-of-the-art methods, demonstrating our\nsignificance for unified multi-modal VOS. Code is released at\nhttps://github.com/OpenGVLab/MUTR.",
        "translated": "近年来，基于语言、音频等多模态信号的视频对象分割技术引起了业界和学术界的广泛关注。这对于探索模式内的语义对齐和跨框架的视觉对应是一个挑战。然而，现有的方法针对不同的模式采用不同的网络结构，而忽略了帧间与参考文献的时间交互。本文提出了一种多模态统一时态转换器 MUTR，用于参考视频对象分割。MUTR 首次采用了统一的框架，采用了 DETR 风格的变换器，能够对文本或音频参考指定的视频对象进行分割。具体来说，我们引入了两种策略来充分探索视频和多模态信号之间的时间关系。首先，对于转换前的低级时间聚合，我们使多模态参考能够从连续的视频帧中捕获多尺度的视觉线索。这有效地赋予了文本或音频信号时间知识，并提高了形态之间的语义对齐。其次，对于变压器后的高层次时间交互，针对不同的目标嵌入进行帧间特征通信，有助于提高视频跟踪的目标对应性。在 Ref-YouTube-VOS 和 AVSBench 数据集上，各自的文本和音频参考，MUTR 对最先进的方法实现了 + 4.2% 和 + 4.2% 的 J & F 改进，表明了我们对统一的多模态 VOS 的重要性。代码在 https://github.com/opengvlab/mutr 发布。"
    },
    {
        "title": "Making Vision Transformers Truly Shift-Equivariant",
        "url": "http://arxiv.org/abs/2305.16316v1",
        "pub_date": "2023-05-25",
        "summary": "For computer vision tasks, Vision Transformers (ViTs) have become one of the\ngo-to deep net architectures. Despite being inspired by Convolutional Neural\nNetworks (CNNs), ViTs remain sensitive to small shifts in the input image. To\naddress this, we introduce novel designs for each of the modules in ViTs, such\nas tokenization, self-attention, patch merging, and positional encoding. With\nour proposed modules, we achieve truly shift-equivariant ViTs on four\nwell-established models, namely, Swin, SwinV2, MViTv2, and CvT, both in theory\nand practice. Empirically, we tested these models on image classification and\nsemantic segmentation, achieving competitive performance across three different\ndatasets while maintaining 100% shift consistency.",
        "translated": "在计算机视觉任务中，视觉变换器(ViTs)已经成为一种常用的深层网络体系结构。尽管受到卷积神经网络(CNN)的启发，ViTs 仍然对输入图像的微小变化敏感。为了解决这个问题，我们为 ViT 中的每个模块引入了新的设计，例如标记化、自注意、补丁合并和位置编码。通过我们提出的模块，我们在理论和实践上实现了四个已经建立的模型，即 Swin，SwinV2，MViTv2和 CvT 上的真正的移位等变 VIT。经验上，我们在图像分类和语义分割上测试了这些模型，在保持100% 移位一致性的同时，在三个不同的数据集上实现了竞争性能。"
    },
    {
        "title": "NAP: Neural 3D Articulation Prior",
        "url": "http://arxiv.org/abs/2305.16315v1",
        "pub_date": "2023-05-25",
        "summary": "We propose Neural 3D Articulation Prior (NAP), the first 3D deep generative\nmodel to synthesize 3D articulated object models. Despite the extensive\nresearch on generating 3D objects, compositions, or scenes, there remains a\nlack of focus on capturing the distribution of articulated objects, a common\nobject category for human and robot interaction. To generate articulated\nobjects, we first design a novel articulation tree/graph parameterization and\nthen apply a diffusion-denoising probabilistic model over this representation\nwhere articulated objects can be generated via denoising from random complete\ngraphs. In order to capture both the geometry and the motion structure whose\ndistribution will affect each other, we design a graph-attention denoising\nnetwork for learning the reverse diffusion process. We propose a novel distance\nthat adapts widely used 3D generation metrics to our novel task to evaluate\ngeneration quality, and experiments demonstrate our high performance in\narticulated object generation. We also demonstrate several conditioned\ngeneration applications, including Part2Motion, PartNet-Imagination,\nMotion2Part, and GAPart2Object.",
        "translated": "我们提出了神经三维关节优先级(nAP) ，这是第一个合成三维关节物体模型的三维深度生成模型。尽管在生成三维物体、构图或场景方面有着广泛的研究，但是对于捕捉关节物体的分布这一人机交互的常见对象类别仍然缺乏关注。为了生成关节对象，我们首先设计了一个新的关节树/图形参量化，然后应用扩散去噪概率模型，通过从随机完整图中去噪来生成关节对象。为了捕捉反向扩散过程中几何和运动结构的相互影响，我们设计了一个图注意去噪网络来学习反向扩散过程。我们提出了一个新的距离，适应广泛使用的三维生成度量的新任务，以评估生成质量，实验表明我们的高性能的铰接对象生成。我们还演示了几个条件生成应用程序，包括 Part2Motion、 PartNet-Imagination、 Motion2Part 和 GAPart2Object。"
    },
    {
        "title": "Banana: Banach Fixed-Point Network for Pointcloud Segmentation with\n  Inter-Part Equivariance",
        "url": "http://arxiv.org/abs/2305.16314v1",
        "pub_date": "2023-05-25",
        "summary": "Equivariance has gained strong interest as a desirable network property that\ninherently ensures robust generalization. However, when dealing with complex\nsystems such as articulated objects or multi-object scenes, effectively\ncapturing inter-part transformations poses a challenge, as it becomes entangled\nwith the overall structure and local transformations. The interdependence of\npart assignment and per-part group action necessitates a novel equivariance\nformulation that allows for their co-evolution. In this paper, we present\nBanana, a Banach fixed-point network for equivariant segmentation with\ninter-part equivariance by construction. Our key insight is to iteratively\nsolve a fixed-point problem, where point-part assignment labels and per-part\nSE(3)-equivariance co-evolve simultaneously. We provide theoretical derivations\nof both per-step equivariance and global convergence, which induces an\nequivariant final convergent state. Our formulation naturally provides a strict\ndefinition of inter-part equivariance that generalizes to unseen inter-part\nconfigurations. Through experiments conducted on both articulated objects and\nmulti-object scans, we demonstrate the efficacy of our approach in achieving\nstrong generalization under inter-part transformations, even when confronted\nwith substantial changes in pointcloud geometry and topology.",
        "translated": "等方差作为一种理想的网络属性，在本质上确保了鲁棒的推广，已经引起了人们的极大兴趣。然而，当处理复杂的系统，如铰接对象或多对象场景，有效地捕获部分间的转换提出了一个挑战，因为它成为纠缠在整体结构和局部转换。部分分配和每部分群体行为的相互依赖性需要一个新的等方差公式，允许它们的协同演化。本文提出了一种基于构造的 Banana 不动点网络，用于部分间等方差的等变分割。我们的主要见解是迭代求解一个不动点问题，其中点部分分配标签和每部分 SE (3)-等方差同时共同演化。我们给出了每步等方差和全局收敛的理论推导，得到了一个等变的最终收敛状态。我们的公式自然提供了一个严格的部分间等方差的定义，推广到看不见的部分间配置。通过对关联对象和多目标扫描的实验，我们证明了该方法在部分间转换下实现强泛化的有效性，即使在面临点云几何和拓扑结构的实质性变化时也是如此。"
    },
    {
        "title": "Break-A-Scene: Extracting Multiple Concepts from a Single Image",
        "url": "http://arxiv.org/abs/2305.16311v1",
        "pub_date": "2023-05-25",
        "summary": "Text-to-image model personalization aims to introduce a user-provided concept\nto the model, allowing its synthesis in diverse contexts. However, current\nmethods primarily focus on the case of learning a single concept from multiple\nimages with variations in backgrounds and poses, and struggle when adapted to a\ndifferent scenario. In this work, we introduce the task of textual scene\ndecomposition: given a single image of a scene that may contain several\nconcepts, we aim to extract a distinct text token for each concept, enabling\nfine-grained control over the generated scenes. To this end, we propose\naugmenting the input image with masks that indicate the presence of target\nconcepts. These masks can be provided by the user or generated automatically by\na pre-trained segmentation model. We then present a novel two-phase\ncustomization process that optimizes a set of dedicated textual embeddings\n(handles), as well as the model weights, striking a delicate balance between\naccurately capturing the concepts and avoiding overfitting. We employ a masked\ndiffusion loss to enable handles to generate their assigned concepts,\ncomplemented by a novel loss on cross-attention maps to prevent entanglement.\nWe also introduce union-sampling, a training strategy aimed to improve the\nability of combining multiple concepts in generated images. We use several\nautomatic metrics to quantitatively compare our method against several\nbaselines, and further affirm the results using a user study. Finally, we\nshowcase several applications of our method. Project page is available at:\nhttps://omriavrahami.com/break-a-scene/",
        "translated": "文本到图像模型个性化旨在向模型引入用户提供的概念，允许在不同的上下文中进行合成。然而，目前的方法主要集中在从背景和姿势不同的多幅图像中学习单一概念的情况下，并在适应不同情景时进行斗争。在这项工作中，我们介绍了文本场景分解的任务: 给定一个场景的单个图像，可能包含几个概念，我们的目标是提取一个不同的文本标记为每个概念，使细粒度控制生成的场景。为此，我们提出用掩码来增强输入图像，以表明目标概念的存在。这些掩码可以由用户提供，也可以由预先训练好的分割模型自动生成。然后，我们提出了一个新的两阶段定制过程，优化了一组专用的文本嵌入(处理) ，以及模型权重，在准确捕捉概念和避免过度拟合之间找到人海万花筒(电影)。我们使用一个掩蔽的扩散损失，使处理能够生成其指定的概念，补充交叉注意地图上的一个新的损失，以防止纠缠。我们还引入了联合采样，这是一种旨在提高生成图像中多个概念组合能力的训练策略。我们使用几个自动指标来定量比较我们的方法与几个基线，并进一步确认结果使用用户研究。最后，我们展示了我们的方法的几个应用。项目网页可于以下 https://omriavrahami.com/break-a-scene/下载:"
    },
    {
        "title": "UMat: Uncertainty-Aware Single Image High Resolution Material Capture",
        "url": "http://arxiv.org/abs/2305.16312v1",
        "pub_date": "2023-05-25",
        "summary": "We propose a learning-based method to recover normals, specularity, and\nroughness from a single diffuse image of a material, using microgeometry\nappearance as our primary cue. Previous methods that work on single images tend\nto produce over-smooth outputs with artifacts, operate at limited resolution,\nor train one model per class with little room for generalization. Previous\nmethods that work on single images tend to produce over-smooth outputs with\nartifacts, operate at limited resolution, or train one model per class with\nlittle room for generalization. In contrast, in this work, we propose a novel\ncapture approach that leverages a generative network with attention and a U-Net\ndiscriminator, which shows outstanding performance integrating global\ninformation at reduced computational complexity. We showcase the performance of\nour method with a real dataset of digitized textile materials and show that a\ncommodity flatbed scanner can produce the type of diffuse illumination required\nas input to our method. Additionally, because the problem might be illposed\n-more than a single diffuse image might be needed to disambiguate the specular\nreflection- or because the training dataset is not representative enough of the\nreal distribution, we propose a novel framework to quantify the model's\nconfidence about its prediction at test time. Our method is the first one to\ndeal with the problem of modeling uncertainty in material digitization,\nincreasing the trustworthiness of the process and enabling more intelligent\nstrategies for dataset creation, as we demonstrate with an active learning\nexperiment.",
        "translated": "我们提出了一个基于学习的方法来恢复法线，反射率和粗糙度从一个单一的漫反射图像的材料，使用显微几何外观作为我们的主要线索。以前用于单幅图像的方法倾向于产生带有工件的过于平滑的输出，在有限的分辨率下操作，或者每类训练一个模型，几乎没有泛化的空间。以前用于单幅图像的方法倾向于产生带有工件的过于平滑的输出，在有限的分辨率下操作，或者每类训练一个模型，几乎没有泛化的空间。相比之下，在这项工作中，我们提出了一种新的捕获方法，利用生成网络的注意力和 U-Net 鉴别器，它显示了在降低计算复杂度的情况下集成全局信息的出色性能。我们展示了我们的方法的性能与数字化纺织材料的真实数据集，并表明，一个商品平板扫描仪可以产生所需的漫反射照明类型作为输入我们的方法。此外，由于问题可能是病态的——消除镜面反射(物理)可能需要不止一张漫反射图像——或者由于训练数据集不足以代表真实分布，我们提出了一个新的框架来量化模型在测试时对其预测的信心。我们的方法是第一个处理材料数字化建模不确定性的问题，提高过程的可信度，使数据集创建更智能的策略，正如我们用一个积极的学习实验所证明的那样。"
    },
    {
        "title": "Securing Deep Generative Models with Universal Adversarial Signature",
        "url": "http://arxiv.org/abs/2305.16310v1",
        "pub_date": "2023-05-25",
        "summary": "Recent advances in deep generative models have led to the development of\nmethods capable of synthesizing high-quality, realistic images. These models\npose threats to society due to their potential misuse. Prior research attempted\nto mitigate these threats by detecting generated images, but the varying traces\nleft by different generative models make it challenging to create a universal\ndetector capable of generalizing to new, unseen generative models. In this\npaper, we propose to inject a universal adversarial signature into an arbitrary\npre-trained generative model, in order to make its generated contents more\ndetectable and traceable. First, the imperceptible optimal signature for each\nimage can be found by a signature injector through adversarial training.\nSubsequently, the signature can be incorporated into an arbitrary generator by\nfine-tuning it with the images processed by the signature injector. In this\nway, the detector corresponding to the signature can be reused for any\nfine-tuned generator for tracking the generator identity. The proposed method\nis validated on the FFHQ and ImageNet datasets with various state-of-the-art\ngenerative models, consistently showing a promising detection rate. Code will\nbe made publicly available at \\url{https://github.com/zengxianyu/genwm}.",
        "translated": "深度生成模型的最新进展导致了能够合成高质量、真实图像的方法的发展。这些模式由于可能被滥用而对社会构成威胁。先前的研究试图通过检测生成的图像来减轻这些威胁，但不同的生成模型留下的不同痕迹使得创建一个能够推广到新的、看不见的生成模型的通用检测器具有挑战性。在这篇文章中，我们建议将一个通用的对抗性签名注入到一个任意的预先训练的生成模型中，以使其生成的内容更加可检测和可追踪。首先，通过对抗训练，利用签名注入器可以找到每幅图像的不可察觉的最优签名。随后，可以通过使用签名注入器处理的图像对签名进行微调，从而将签名合并到任意生成器中。这样，对应于签名的检测器可以重用于任何微调发生器，用于跟踪发生器标识。该方法在 FFHQ 和 ImageNet 数据集上通过各种最先进的生成模型进行了验证，一致地显示出有希望的检测率。代码将在 url { https://github.com/zengxianyu/genwm }公开发布。"
    },
    {
        "title": "NeuManifold: Neural Watertight Manifold Reconstruction with Efficient\n  and High-Quality Rendering Support",
        "url": "http://arxiv.org/abs/2305.17134v1",
        "pub_date": "2023-05-26",
        "summary": "We present a method for generating high-quality watertight manifold meshes\nfrom multi-view input images. Existing volumetric rendering methods are robust\nin optimization but tend to generate noisy meshes with poor topology.\nDifferentiable rasterization-based methods can generate high-quality meshes but\nare sensitive to initialization. Our method combines the benefits of both\nworlds; we take the geometry initialization obtained from neural volumetric\nfields, and further optimize the geometry as well as a compact neural texture\nrepresentation with differentiable rasterizers. Through extensive experiments,\nwe demonstrate that our method can generate accurate mesh reconstructions with\nfaithful appearance that are comparable to previous volume rendering methods\nwhile being an order of magnitude faster in rendering. We also show that our\ngenerated mesh and neural texture reconstruction is compatible with existing\ngraphics pipelines and enables downstream 3D applications such as simulation.\nProject page: https://sarahweiii.github.io/neumanifold/",
        "translated": ""
    },
    {
        "title": "Manifold Regularization for Memory-Efficient Training of Deep Neural\n  Networks",
        "url": "http://arxiv.org/abs/2305.17119v1",
        "pub_date": "2023-05-26",
        "summary": "One of the prevailing trends in the machine- and deep-learning community is\nto gravitate towards the use of increasingly larger models in order to keep\npushing the state-of-the-art performance envelope. This tendency makes access\nto the associated technologies more difficult for the average practitioner and\nruns contrary to the desire to democratize knowledge production in the field.\nIn this paper, we propose a framework for achieving improved memory efficiency\nin the process of learning traditional neural networks by leveraging\ninductive-bias-driven network design principles and layer-wise\nmanifold-oriented regularization objectives. Use of the framework results in\nimproved absolute performance and empirical generalization error relative to\ntraditional learning techniques. We provide empirical validation of the\nframework, including qualitative and quantitative evidence of its effectiveness\non two standard image datasets, namely CIFAR-10 and CIFAR-100. The proposed\nframework can be seamlessly combined with existing network compression methods\nfor further memory savings.",
        "translated": ""
    },
    {
        "title": "Random-Access Neural Compression of Material Textures",
        "url": "http://arxiv.org/abs/2305.17105v1",
        "pub_date": "2023-05-26",
        "summary": "The continuous advancement of photorealism in rendering is accompanied by a\ngrowth in texture data and, consequently, increasing storage and memory\ndemands. To address this issue, we propose a novel neural compression technique\nspecifically designed for material textures. We unlock two more levels of\ndetail, i.e., 16x more texels, using low bitrate compression, with image\nquality that is better than advanced image compression techniques, such as AVIF\nand JPEG XL. At the same time, our method allows on-demand, real-time\ndecompression with random access similar to block texture compression on GPUs,\nenabling compression on disk and memory. The key idea behind our approach is\ncompressing multiple material textures and their mipmap chains together, and\nusing a small neural network, that is optimized for each material, to\ndecompress them. Finally, we use a custom training implementation to achieve\npractical compression speeds, whose performance surpasses that of general\nframeworks, like PyTorch, by an order of magnitude.",
        "translated": ""
    },
    {
        "title": "GeoVLN: Learning Geometry-Enhanced Visual Representation with Slot\n  Attention for Vision-and-Language Navigation",
        "url": "http://arxiv.org/abs/2305.17102v1",
        "pub_date": "2023-05-26",
        "summary": "Most existing works solving Room-to-Room VLN problem only utilize RGB images\nand do not consider local context around candidate views, which lack sufficient\nvisual cues about surrounding environment. Moreover, natural language contains\ncomplex semantic information thus its correlations with visual inputs are hard\nto model merely with cross attention. In this paper, we propose GeoVLN, which\nlearns Geometry-enhanced visual representation based on slot attention for\nrobust Visual-and-Language Navigation. The RGB images are compensated with the\ncorresponding depth maps and normal maps predicted by Omnidata as visual\ninputs. Technically, we introduce a two-stage module that combine local slot\nattention and CLIP model to produce geometry-enhanced representation from such\ninput. We employ V&amp;L BERT to learn a cross-modal representation that\nincorporate both language and vision informations. Additionally, a novel\nmultiway attention module is designed, encouraging different phrases of input\ninstruction to exploit the most related features from visual input. Extensive\nexperiments demonstrate the effectiveness of our newly designed modules and\nshow the compelling performance of the proposed method.",
        "translated": ""
    },
    {
        "title": "ControlVideo: Adding Conditional Control for One Shot Text-to-Video\n  Editing",
        "url": "http://arxiv.org/abs/2305.17098v1",
        "pub_date": "2023-05-26",
        "summary": "In this paper, we present ControlVideo, a novel method for text-driven video\nediting. Leveraging the capabilities of text-to-image diffusion models and\nControlNet, ControlVideo aims to enhance the fidelity and temporal consistency\nof videos that align with a given text while preserving the structure of the\nsource video. This is achieved by incorporating additional conditions such as\nedge maps, fine-tuning the key-frame and temporal attention on the source\nvideo-text pair with carefully designed strategies. An in-depth exploration of\nControlVideo's design is conducted to inform future research on one-shot tuning\nvideo diffusion models. Quantitatively, ControlVideo outperforms a range of\ncompetitive baselines in terms of faithfulness and consistency while still\naligning with the textual prompt. Additionally, it delivers videos with high\nvisual realism and fidelity w.r.t. the source content, demonstrating\nflexibility in utilizing controls containing varying degrees of source video\ninformation, and the potential for multiple control combinations. The project\npage is available at\n\\href{https://ml.cs.tsinghua.edu.cn/controlvideo/}{https://ml.cs.tsinghua.edu.cn/controlvideo/}.",
        "translated": ""
    },
    {
        "title": "GRAtt-VIS: Gated Residual Attention for Auto Rectifying Video Instance\n  Segmentation",
        "url": "http://arxiv.org/abs/2305.17096v1",
        "pub_date": "2023-05-26",
        "summary": "Recent trends in Video Instance Segmentation (VIS) have seen a growing\nreliance on online methods to model complex and lengthy video sequences.\nHowever, the degradation of representation and noise accumulation of the online\nmethods, especially during occlusion and abrupt changes, pose substantial\nchallenges. Transformer-based query propagation provides promising directions\nat the cost of quadratic memory attention. However, they are susceptible to the\ndegradation of instance features due to the above-mentioned challenges and\nsuffer from cascading effects. The detection and rectification of such errors\nremain largely underexplored. To this end, we introduce \\textbf{GRAtt-VIS},\n\\textbf{G}ated \\textbf{R}esidual \\textbf{Att}ention for \\textbf{V}ideo\n\\textbf{I}nstance \\textbf{S}egmentation. Firstly, we leverage a\nGumbel-Softmax-based gate to detect possible errors in the current frame. Next,\nbased on the gate activation, we rectify degraded features from its past\nrepresentation. Such a residual configuration alleviates the need for dedicated\nmemory and provides a continuous stream of relevant instance features.\nSecondly, we propose a novel inter-instance interaction using gate activation\nas a mask for self-attention. This masking strategy dynamically restricts the\nunrepresentative instance queries in the self-attention and preserves vital\ninformation for long-term tracking. We refer to this novel combination of Gated\nResidual Connection and Masked Self-Attention as \\textbf{GRAtt} block, which\ncan easily be integrated into the existing propagation-based framework.\nFurther, GRAtt blocks significantly reduce the attention overhead and simplify\ndynamic temporal modeling. GRAtt-VIS achieves state-of-the-art performance on\nYouTube-VIS and the highly challenging OVIS dataset, significantly improving\nover previous methods. Code is available at\n\\url{https://github.com/Tanveer81/GRAttVIS}.",
        "translated": ""
    },
    {
        "title": "SSSegmenation: An Open Source Supervised Semantic Segmentation Toolbox\n  Based on PyTorch",
        "url": "http://arxiv.org/abs/2305.17091v1",
        "pub_date": "2023-05-26",
        "summary": "This paper presents SSSegmenation, which is an open source supervised\nsemantic image segmentation toolbox based on PyTorch. The design of this\ntoolbox is motivated by MMSegmentation while it is easier to use because of\nfewer dependencies and achieves superior segmentation performance under a\ncomparable training and testing setup. Moreover, the toolbox also provides\nplenty of trained weights for popular and contemporary semantic segmentation\nmethods, including Deeplab, PSPNet, OCRNet, MaskFormer, \\emph{etc}. We expect\nthat this toolbox can contribute to the future development of semantic\nsegmentation. Codes and model zoos are available at\n\\href{https://github.com/SegmentationBLWX/sssegmentation/}{SSSegmenation}.",
        "translated": ""
    },
    {
        "title": "Mindstorms in Natural Language-Based Societies of Mind",
        "url": "http://arxiv.org/abs/2305.17066v1",
        "pub_date": "2023-05-26",
        "summary": "Both Minsky's \"society of mind\" and Schmidhuber's \"learning to think\" inspire\ndiverse societies of large multimodal neural networks (NNs) that solve problems\nby interviewing each other in a \"mindstorm.\" Recent implementations of NN-based\nsocieties of minds consist of large language models (LLMs) and other NN-based\nexperts communicating through a natural language interface. In doing so, they\novercome the limitations of single LLMs, improving multimodal zero-shot\nreasoning. In these natural language-based societies of mind (NLSOMs), new\nagents -- all communicating through the same universal symbolic language -- are\neasily added in a modular fashion. To demonstrate the power of NLSOMs, we\nassemble and experiment with several of them (having up to 129 members),\nleveraging mindstorms in them to solve some practical AI tasks: visual question\nanswering, image captioning, text-to-image synthesis, 3D generation, egocentric\nretrieval, embodied AI, and general language-based task solving. We view this\nas a starting point towards much larger NLSOMs with billions of agents-some of\nwhich may be humans. And with this emergence of great societies of\nheterogeneous minds, many new research questions have suddenly become paramount\nto the future of artificial intelligence. What should be the social structure\nof an NLSOM? What would be the (dis)advantages of having a monarchical rather\nthan a democratic structure? How can principles of NN economies be used to\nmaximize the total reward of a reinforcement learning NLSOM? In this work, we\nidentify, discuss, and try to answer some of these questions.",
        "translated": ""
    },
    {
        "title": "Extremely weakly-supervised blood vessel segmentation with\n  physiologically based synthesis and domain adaptation",
        "url": "http://arxiv.org/abs/2305.17054v1",
        "pub_date": "2023-05-26",
        "summary": "Accurate analysis and modeling of renal functions require a precise\nsegmentation of the renal blood vessels. Micro-CT scans provide image data at\nhigher resolutions, making more small vessels near the renal cortex visible.\nAlthough deep-learning-based methods have shown state-of-the-art performance in\nautomatic blood vessel segmentations, they require a large amount of labeled\ntraining data. However, voxel-wise labeling in micro-CT scans is extremely\ntime-consuming given the huge volume sizes. To mitigate the problem, we\nsimulate synthetic renal vascular trees physiologically while generating\ncorresponding scans of the simulated trees by training a generative model on\nunlabeled scans. This enables the generative model to learn the mapping\nimplicitly without the need for explicit functions to emulate the image\nacquisition process. We further propose an additional segmentation branch over\nthe generative model trained on the generated scans. We demonstrate that the\nmodel can directly segment blood vessels on real scans and validate our method\non both 3D micro-CT scans of rat kidneys and a proof-of-concept experiment on\n2D retinal images. Code and 3D results are available at\nhttps://github.com/miccai2023anony/RenalVesselSeg",
        "translated": ""
    },
    {
        "title": "SelfClean: A Self-Supervised Data Cleaning Strategy",
        "url": "http://arxiv.org/abs/2305.17048v1",
        "pub_date": "2023-05-26",
        "summary": "Most commonly used benchmark datasets for computer vision contain irrelevant\nimages, near duplicates, and label errors. Consequently, model performance on\nthese benchmarks may not be an accurate estimate of generalization ability.\nThis is a particularly acute concern in computer vision for medicine where\ndatasets are typically small, stakes are high, and annotation processes are\nexpensive and error-prone. In this paper, we propose SelfClean, a general\nprocedure to clean up image datasets exploiting a latent space learned with\nself-supervision. By relying on self-supervised learning, our approach focuses\non intrinsic properties of the data and avoids annotation biases. We formulate\ndataset cleaning as either a set of ranking problems, where human experts can\nmake decisions with significantly reduced effort, or a set of scoring problems,\nwhere decisions can be fully automated based on score distributions. We compare\nSelfClean against other algorithms on common computer vision benchmarks\nenhanced with synthetic noise and demonstrate state-of-the-art performance on\ndetecting irrelevant images, near duplicates, and label errors. In addition, we\napply our method to multiple image datasets and confirm an improvement in\nevaluation reliability.",
        "translated": ""
    },
    {
        "title": "RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths",
        "url": "http://arxiv.org/abs/2305.18295v1",
        "pub_date": "2023-05-29",
        "summary": "Text-to-image generation has recently witnessed remarkable achievements. We\nintroduce a text-conditional image diffusion model, termed RAPHAEL, to generate\nhighly artistic images, which accurately portray the text prompts, encompassing\nmultiple nouns, adjectives, and verbs. This is achieved by stacking tens of\nmixture-of-experts (MoEs) layers, i.e., space-MoE and time-MoE layers, enabling\nbillions of diffusion paths (routes) from the network input to the output. Each\npath intuitively functions as a \"painter\" for depicting a particular textual\nconcept onto a specified image region at a diffusion timestep. Comprehensive\nexperiments reveal that RAPHAEL outperforms recent cutting-edge models, such as\nStable Diffusion, ERNIE-ViLG 2.0, DeepFloyd, and DALL-E 2, in terms of both\nimage quality and aesthetic appeal. Firstly, RAPHAEL exhibits superior\nperformance in switching images across diverse styles, such as Japanese comics,\nrealism, cyberpunk, and ink illustration. Secondly, a single model with three\nbillion parameters, trained on 1,000 A100 GPUs for two months, achieves a\nstate-of-the-art zero-shot FID score of 6.61 on the COCO dataset. Furthermore,\nRAPHAEL significantly surpasses its counterparts in human evaluation on the\nViLG-300 benchmark. We believe that RAPHAEL holds the potential to propel the\nfrontiers of image generation research in both academia and industry, paving\nthe way for future breakthroughs in this rapidly evolving field. More details\ncan be found on a project webpage: https://raphael-painter.github.io/.",
        "translated": ""
    },
    {
        "title": "Mix-of-Show: Decentralized Low-Rank Adaptation for Multi-Concept\n  Customization of Diffusion Models",
        "url": "http://arxiv.org/abs/2305.18292v1",
        "pub_date": "2023-05-29",
        "summary": "Public large-scale text-to-image diffusion models, such as Stable Diffusion,\nhave gained significant attention from the community. These models can be\neasily customized for new concepts using low-rank adaptations (LoRAs). However,\nthe utilization of multiple concept LoRAs to jointly support multiple\ncustomized concepts presents a challenge. We refer to this scenario as\ndecentralized multi-concept customization, which involves single-client concept\ntuning and center-node concept fusion. In this paper, we propose a new\nframework called Mix-of-Show that addresses the challenges of decentralized\nmulti-concept customization, including concept conflicts resulting from\nexisting single-client LoRA tuning and identity loss during model fusion.\nMix-of-Show adopts an embedding-decomposed LoRA (ED-LoRA) for single-client\ntuning and gradient fusion for the center node to preserve the in-domain\nessence of single concepts and support theoretically limitless concept fusion.\nAdditionally, we introduce regionally controllable sampling, which extends\nspatially controllable sampling (e.g., ControlNet and T2I-Adaptor) to address\nattribute binding and missing object problems in multi-concept sampling.\nExtensive experiments demonstrate that Mix-of-Show is capable of composing\nmultiple customized concepts with high fidelity, including characters, objects,\nand scenes.",
        "translated": ""
    },
    {
        "title": "LaFTer: Label-Free Tuning of Zero-shot Classifier using Language and\n  Unlabeled Image Collections",
        "url": "http://arxiv.org/abs/2305.18287v1",
        "pub_date": "2023-05-29",
        "summary": "Recently, large-scale pre-trained Vision and Language (VL) models have set a\nnew state-of-the-art (SOTA) in zero-shot visual classification enabling\nopen-vocabulary recognition of potentially unlimited set of categories defined\nas simple language prompts. However, despite these great advances, the\nperformance of these zeroshot classifiers still falls short of the results of\ndedicated (closed category set) classifiers trained with supervised fine\ntuning. In this paper we show, for the first time, how to reduce this gap\nwithout any labels and without any paired VL data, using an unlabeled image\ncollection and a set of texts auto-generated using a Large Language Model (LLM)\ndescribing the categories of interest and effectively substituting labeled\nvisual instances of those categories. Using our label-free approach, we are\nable to attain significant performance improvements over the zero-shot\nperformance of the base VL model and other contemporary methods and baselines\non a wide variety of datasets, demonstrating absolute improvement of up to\n11.7% (3.8% on average) in the label-free setting. Moreover, despite our\napproach being label-free, we observe 1.3% average gains over leading few-shot\nprompting baselines that do use 5-shot supervision.",
        "translated": ""
    },
    {
        "title": "Photoswap: Personalized Subject Swapping in Images",
        "url": "http://arxiv.org/abs/2305.18286v1",
        "pub_date": "2023-05-29",
        "summary": "In an era where images and visual content dominate our digital landscape, the\nability to manipulate and personalize these images has become a necessity.\nEnvision seamlessly substituting a tabby cat lounging on a sunlit window sill\nin a photograph with your own playful puppy, all while preserving the original\ncharm and composition of the image. We present Photoswap, a novel approach that\nenables this immersive image editing experience through personalized subject\nswapping in existing images. Photoswap first learns the visual concept of the\nsubject from reference images and then swaps it into the target image using\npre-trained diffusion models in a training-free manner. We establish that a\nwell-conceptualized visual subject can be seamlessly transferred to any image\nwith appropriate self-attention and cross-attention manipulation, maintaining\nthe pose of the swapped subject and the overall coherence of the image.\nComprehensive experiments underscore the efficacy and controllability of\nPhotoswap in personalized subject swapping. Furthermore, Photoswap\nsignificantly outperforms baseline methods in human ratings across subject\nswapping, background preservation, and overall quality, revealing its vast\napplication potential, from entertainment to professional editing.",
        "translated": ""
    },
    {
        "title": "Contextual Object Detection with Multimodal Large Language Models",
        "url": "http://arxiv.org/abs/2305.18279v1",
        "pub_date": "2023-05-29",
        "summary": "Recent Multimodal Large Language Models (MLLMs) are remarkable in\nvision-language tasks, such as image captioning and question answering, but\nlack the essential perception ability, i.e., object detection. In this work, we\naddress this limitation by introducing a novel research problem of contextual\nobject detection -- understanding visible objects within different human-AI\ninteractive contexts. Three representative scenarios are investigated,\nincluding the language cloze test, visual captioning, and question answering.\nMoreover, we present ContextDET, a unified multimodal model that is capable of\nend-to-end differentiable modeling of visual-language contexts, so as to\nlocate, identify, and associate visual objects with language inputs for\nhuman-AI interaction. Our ContextDET involves three key submodels: (i) a visual\nencoder for extracting visual representations, (ii) a pre-trained LLM for\nmultimodal context decoding, and (iii) a visual decoder for predicting bounding\nboxes given contextual object words. The new generate-then-detect framework\nenables us to detect object words within human vocabulary. Extensive\nexperiments show the advantages of ContextDET on our proposed CODE benchmark,\nopen-vocabulary detection, and referring image segmentation. Github:\nhttps://github.com/yuhangzang/ContextDET.",
        "translated": ""
    },
    {
        "title": "3DTeethSeg'22: 3D Teeth Scan Segmentation and Labeling Challenge",
        "url": "http://arxiv.org/abs/2305.18277v1",
        "pub_date": "2023-05-29",
        "summary": "Teeth localization, segmentation, and labeling from intra-oral 3D scans are\nessential tasks in modern dentistry to enhance dental diagnostics, treatment\nplanning, and population-based studies on oral health. However, developing\nautomated algorithms for teeth analysis presents significant challenges due to\nvariations in dental anatomy, imaging protocols, and limited availability of\npublicly accessible data. To address these challenges, the 3DTeethSeg'22\nchallenge was organized in conjunction with the International Conference on\nMedical Image Computing and Computer Assisted Intervention (MICCAI) in 2022,\nwith a call for algorithms tackling teeth localization, segmentation, and\nlabeling from intraoral 3D scans. A dataset comprising a total of 1800 scans\nfrom 900 patients was prepared, and each tooth was individually annotated by a\nhuman-machine hybrid algorithm. A total of 6 algorithms were evaluated on this\ndataset. In this study, we present the evaluation results of the 3DTeethSeg'22\nchallenge. The 3DTeethSeg'22 challenge code can be accessed at:\nhttps://github.com/abenhamadou/3DTeethSeg22_challenge",
        "translated": ""
    },
    {
        "title": "Reconstructing the Mind's Eye: fMRI-to-Image with Contrastive Learning\n  and Diffusion Priors",
        "url": "http://arxiv.org/abs/2305.18274v1",
        "pub_date": "2023-05-29",
        "summary": "We present MindEye, a novel fMRI-to-image approach to retrieve and\nreconstruct viewed images from brain activity. Our model comprises two parallel\nsubmodules that are specialized for retrieval (using contrastive learning) and\nreconstruction (using a diffusion prior). MindEye can map fMRI brain activity\nto any high dimensional multimodal latent space, like CLIP image space,\nenabling image reconstruction using generative models that accept embeddings\nfrom this latent space. We comprehensively compare our approach with other\nexisting methods, using both qualitative side-by-side comparisons and\nquantitative evaluations, and show that MindEye achieves state-of-the-art\nperformance in both reconstruction and retrieval tasks. In particular, MindEye\ncan retrieve the exact original image even among highly similar candidates\nindicating that its brain embeddings retain fine-grained image-specific\ninformation. This allows us to accurately retrieve images even from large-scale\ndatabases like LAION-5B. We demonstrate through ablations that MindEye's\nperformance improvements over previous methods result from specialized\nsubmodules for retrieval and reconstruction, improved training techniques, and\ntraining models with orders of magnitude more parameters. Furthermore, we show\nthat MindEye can better preserve low-level image features in the\nreconstructions by using img2img, with outputs from a separate autoencoder. All\ncode is available on GitHub.",
        "translated": ""
    },
    {
        "title": "Pix2Repair: Implicit Shape Restoration from Images",
        "url": "http://arxiv.org/abs/2305.18273v1",
        "pub_date": "2023-05-29",
        "summary": "We present Pix2Repair, an automated shape repair approach that generates\nrestoration shapes from images to repair fractured objects. Prior repair\napproaches require a high-resolution watertight 3D mesh of the fractured object\nas input. Input 3D meshes must be obtained using expensive 3D scanners, and\nscanned meshes require manual cleanup, limiting accessibility and scalability.\nPix2Repair takes an image of the fractured object as input and automatically\ngenerates a 3D printable restoration shape. We contribute a novel shape\nfunction that deconstructs a latent code representing the fractured object into\na complete shape and a break surface. We show restorations for synthetic\nfractures from the Geometric Breaks and Breaking Bad datasets, and cultural\nheritage objects from the QP dataset, and for real fractures from the Fantastic\nBreaks dataset. We overcome challenges in restoring axially symmetric objects\nby predicting view-centered restorations. Our approach outperforms shape\ncompletion approaches adapted for shape repair in terms of chamfer distance,\nearth mover's distance, normal consistency, and percent restorations generated.",
        "translated": ""
    },
    {
        "title": "Gen-L-Video: Multi-Text to Long Video Generation via Temporal\n  Co-Denoising",
        "url": "http://arxiv.org/abs/2305.18264v1",
        "pub_date": "2023-05-29",
        "summary": "Leveraging large-scale image-text datasets and advancements in diffusion\nmodels, text-driven generative models have made remarkable strides in the field\nof image generation and editing. This study explores the potential of extending\nthe text-driven ability to the generation and editing of multi-text conditioned\nlong videos. Current methodologies for video generation and editing, while\ninnovative, are often confined to extremely short videos (typically less than\n24 frames) and are limited to a single text condition. These constraints\nsignificantly limit their applications given that real-world videos usually\nconsist of multiple segments, each bearing different semantic information. To\naddress this challenge, we introduce a novel paradigm dubbed as Gen-L-Video,\ncapable of extending off-the-shelf short video diffusion models for generating\nand editing videos comprising hundreds of frames with diverse semantic segments\nwithout introducing additional training, all while preserving content\nconsistency. We have implemented three mainstream text-driven video generation\nand editing methodologies and extended them to accommodate longer videos imbued\nwith a variety of semantic segments with our proposed paradigm. Our\nexperimental outcomes reveal that our approach significantly broadens the\ngenerative and editing capabilities of video diffusion models, offering new\npossibilities for future research and applications. The code is available at\nhttps://github.com/G-U-N/Gen-L-Video.",
        "translated": ""
    },
    {
        "title": "Synfeal: A Data-Driven Simulator for End-to-End Camera Localization",
        "url": "http://arxiv.org/abs/2305.18260v1",
        "pub_date": "2023-05-29",
        "summary": "Collecting real-world data is often considered the bottleneck of Artificial\nIntelligence, stalling the research progress in several fields, one of which is\ncamera localization. End-to-end camera localization methods are still\noutperformed by traditional methods, and we argue that the inconsistencies\nassociated with the data collection techniques are restraining the potential of\nend-to-end methods. Inspired by the recent data-centric paradigm, we propose a\nframework that synthesizes large localization datasets based on realistic 3D\nreconstructions of the real world. Our framework, termed Synfeal: Synthetic\nfrom Real, is an open-source, data-driven simulator that synthesizes RGB images\nby moving a virtual camera through a realistic 3D textured mesh, while\ncollecting the corresponding ground-truth camera poses. The results validate\nthat the training of camera localization algorithms on datasets generated by\nSynfeal leads to better results when compared to datasets generated by\nstate-of-the-art methods. Using Synfeal, we conducted the first analysis of the\nrelationship between the size of the dataset and the performance of camera\nlocalization algorithms. Results show that the performance significantly\nincreases with the dataset size. Our results also suggest that when a large\nlocalization dataset with high quality is available, training from scratch\nleads to better performances. Synfeal is publicly available at\nhttps://github.com/DanielCoelho112/synfeal.",
        "translated": ""
    },
    {
        "title": "Forensic Video Steganalysis in Spatial Domain by Noise Residual\n  Convolutional Neural Network",
        "url": "http://arxiv.org/abs/2305.18070v1",
        "pub_date": "2023-05-29",
        "summary": "This research evaluates a convolutional neural network (CNN) based approach\nto forensic video steganalysis. A video steganography dataset is created to\ntrain a CNN to conduct forensic steganalysis in the spatial domain. We use a\nnoise residual convolutional neural network to detect embedded secrets since a\nsteganographic embedding process will always result in the modification of\npixel values in video frames. Experimental results show that the CNN-based\napproach can be an effective method for forensic video steganalysis and can\nreach a detection rate of 99.96%. Keywords: Forensic, Steganalysis, Deep\nSteganography, MSU StegoVideo, Convolutional Neural Networks",
        "translated": "这项研究评估了一种基于卷积神经网络(CNN)的法医视频隐写分析方法。建立了一个视频隐写数据集，用于训练 CNN 在空间域中进行取证隐写分析。我们使用噪声残余卷积神经网络来检测嵌入的秘密，因为隐写嵌入过程总是会导致视频帧中像素值的修改。实验结果表明，基于细胞神经网络的方法是一种有效的法医视频隐写分析方法，检测率可达99.96% 。关键词: 取证，隐写分析，深度隐写术，MSU 隐写视频，卷积神经网络"
    },
    {
        "title": "Key Rate Analysis of a 3-State Twin-Field Quantum Key Distribution\n  Protocol in the Finite-key Regime",
        "url": "http://arxiv.org/abs/2305.18006v2",
        "pub_date": "2023-05-29",
        "summary": "When analysing Quantum Key Distribution (QKD) protocols several metrics can\nbe determined, but one of the most important is the Secret Key Rate. The Secret\nKey Rate is the number of bits per transmission that result in being part of a\nSecret Key between two parties. There are equations that give the Secret Key\nRate, for example, for the BB84 protocol, equation 52 from [1, p.1032] gives\nthe Secret Key Rate for a given Quantum Bit Error Rate (QBER). However, the\nanalysis leading to equations such as these often rely on an Asymptotic\napproach, where it is assumed that an infinite number of transmissions are sent\nbetween the two communicating parties (henceforth denoted as Alice and Bob). In\na practical implementation this is obviously impossible. Moreover, some QKD\nprotocols belong to a category called Asymmetric protocols, for which it is\nsignificantly more difficult to perform such an analysis. As such, there is\ncurrently a lot of investigation into a different approach called the\nFinite-key regime. Work by Bunandar et al. [2] has produced code that used\nSemi-Definite Programming to produce lower bounds on the Secret Key Rate of\neven Asymmetric protocols. Our work looks at devising a novel QKD protocol\ntaking inspiration from both the 3-state version of BB84 [3], and the\nTwin-Field protocol [4], and then using this code to perform analysis of the\nnew protocol.",
        "translated": "在分析量子密钥分配(QKD)协议时，可以确定几个度量，但其中最重要的是密钥速率。密钥速率是每次传输的比特数，它导致成为双方之间密钥的一部分。有些公式给出了密钥速率，例如 BB84协议，[1，p. 1032]中的公式52给出了给定量子误码率(QBER)的密钥速率。然而，导致这些方程的分析往往依赖于渐近方法，其中假定在两个通信方之间发送无限数量的传输(以下称为 Alice 和 Bob)。在实际的实现中，这显然是不可能的。此外，一些 QKD 协议属于一类称为非对称协议，其中执行这样的分析是非常困难的。因此，目前有很多研究的不同的方法称为有限键制度。Bunandar 等人的工作[2]已经产生了使用半定规划产生甚至非对称协议的秘密密钥速率的下界的代码。我们的工作着眼于设计一个新颖的 QKD 协议，从 BB84的三态版本[3]和双场协议[4]中获得灵感，然后使用这个代码来执行新协议的分析。"
    },
    {
        "title": "On the Minimal Knowledge Required for Solving Stellar Consensus",
        "url": "http://arxiv.org/abs/2305.17989v1",
        "pub_date": "2023-05-29",
        "summary": "Byzantine Consensus is fundamental for building consistent and fault-tolerant\ndistributed systems. In traditional quorum-based consensus protocols, quorums\nare defined using globally known assumptions shared among all participants.\nMotivated by decentralized applications on open networks, the Stellar\nblockchain relaxes these global assumptions by allowing each participant to\ndefine its quorums using local information. A similar model called Consensus\nwith Unknown Participants (CUP) studies the minimal knowledge required to solve\nconsensus in ad-hoc networks where each participant knows only a subset of\nother participants of the system. We prove that Stellar cannot solve consensus\nusing the initial knowledge provided to participants in the CUP model, even\nthough CUP can. We propose an oracle called sink detector that augments this\nknowledge, enabling Stellar participants to solve consensus.",
        "translated": "拜占庭共识是构建一致性和容错分布式系统的基础。在传统的基于仲裁的协商一致协议中，仲裁是使用所有参与者共享的全局已知假设来定义的。受到开放网络上分散应用的激励，Stellar 区块链通过允许每个参与者使用本地信息定义其法定人数来放松这些全球假设。一个类似的模型称为“与未知参与者达成共识”(CUP) ，它研究在每个参与者只知道系统中其他参与者的一个子集的特定网络中解决达成共识所需的最小知识。我们证明，Stellar 不能利用 CUP 模型中提供给参与者的初始知识来解决一致性问题，即使 CUP 可以。我们提出了一个称为汇检测器的甲骨文，以增加这方面的知识，使恒星的参与者解决共识。"
    },
    {
        "title": "An Experimental Analysis of RowHammer in HBM2 DRAM Chips",
        "url": "http://arxiv.org/abs/2305.17918v1",
        "pub_date": "2023-05-29",
        "summary": "RowHammer (RH) is a significant and worsening security, safety, and\nreliability issue of modern DRAM chips that can be exploited to break memory\nisolation. Therefore, it is important to understand real DRAM chips' RH\ncharacteristics. Unfortunately, no prior work extensively studies the RH\nvulnerability of modern 3D-stacked high-bandwidth memory (HBM) chips, which are\ncommonly used in modern GPUs.\n  In this work, we experimentally characterize the RH vulnerability of a real\nHBM2 DRAM chip. We show that 1) different 3D-stacked channels of HBM2 memory\nexhibit significantly different levels of RH vulnerability (up to 79%\ndifference in bit error rate), 2) the DRAM rows at the end of a DRAM bank (rows\nwith the highest addresses) exhibit significantly fewer RH bitflips than other\nrows, and 3) a modern HBM2 DRAM chip implements undisclosed RH defenses that\nare triggered by periodic refresh operations. We describe the implications of\nour observations on future RH attacks and defenses and discuss future work for\nunderstanding RH in 3D-stacked memories.",
        "translated": "RowHammer (RH)是现代 DRAM 芯片的一个重要且日益严重的安全性、安全性和可靠性问题，可用于打破内存隔离。因此，了解真实 DRAM 芯片的 RH 特性具有重要意义。不幸的是，没有先前的工作广泛研究现代3D 堆叠高带宽存储器(HBM)芯片的 RH 漏洞，这是在现代 GPU 中常用的。在这项工作中，我们实验性地刻画了一个真正的 HBM2 DRAM 芯片的 RH 漏洞。我们表明: 1)不同的3D 堆叠通道的 HBM2存储器表现出显着不同的 RH 脆弱性水平(比特错误率高达79% 的差异) ，2) DRAM 银行结束的 DRAM 行(具有最高地址的行)表现出明显较少的 RH 位翻转比其他行，3)现代 HBM2 DRAM 芯片实现未公开的 RH 防御，由定期刷新操作触发。我们描述了我们的观察对未来 RH 攻击和防御的影响，并讨论了未来在3D 堆积记忆中理解 RH 的工作。"
    },
    {
        "title": "ACETest: Automated Constraint Extraction for Testing Deep Learning\n  Operators",
        "url": "http://arxiv.org/abs/2305.17914v1",
        "pub_date": "2023-05-29",
        "summary": "Deep learning (DL) applications are prevalent nowadays as they can help with\nmultiple tasks. DL libraries are essential for building DL applications.\nFurthermore, DL operators are the important building blocks of the DL\nlibraries, that compute the multi-dimensional data (tensors). Therefore, bugs\nin DL operators can have great impacts. Testing is a practical approach for\ndetecting bugs in DL operators. In order to test DL operators effectively, it\nis essential that the test cases pass the input validity check and are able to\nreach the core function logic of the operators. Hence, extracting the input\nvalidation constraints is required for generating high-quality test cases.\nExisting techniques rely on either human effort or documentation of DL library\nAPIs to extract the constraints. They cannot extract complex constraints and\nthe extracted constraints may differ from the actual code implementation.\n  To address the challenge, we propose ACETest, a technique to automatically\nextract input validation constraints from the code to build valid yet diverse\ntest cases which can effectively unveil bugs in the core function logic of DL\noperators. For this purpose, ACETest can automatically identify the input\nvalidation code in DL operators, extract the related constraints and generate\ntest cases according to the constraints. The experimental results on popular DL\nlibraries, TensorFlow and PyTorch, demonstrate that ACETest can extract\nconstraints with higher quality than state-of-the-art (SOTA) techniques.\nMoreover, ACETest is capable of extracting 96.4% more constraints and detecting\n1.95 to 55 times more bugs than SOTA techniques. In total, we have used ACETest\nto detect 108 previously unknown bugs on TensorFlow and PyTorch, with 87 of\nthem confirmed by the developers. Lastly, five of the bugs were assigned with\nCVE IDs due to their security impacts.",
        "translated": "深度学习(DL)应用程序现在很流行，因为它们可以帮助完成多个任务。DL 库对于构建 DL 应用程序是必不可少的。此外，DL 运算符是计算多维数据(张量)的 DL 库的重要组成部分。因此，DL 操作符中的 bug 会产生很大的影响。测试是检测 DL 操作符错误的一种实用方法。为了有效地测试 DL 算子，测试用例必须通过输入有效性检验，并且能够达到算子的核心函数逻辑。因此，提取输入验证约束是生成高质量测试用例所必需的。现有技术依赖于人工或 DL 库 API 的文档来提取约束。它们不能提取复杂的约束，并且提取的约束可能与实际的代码实现不同。为了应对这一挑战，我们提出了 ACETest，这是一种从代码中自动提取输入验证约束的技术，可以构建有效且多样化的测试用例，有效地揭示 DL 操作符核心功能逻辑中的缺陷。为此，ACETest 可以自动识别 DL 运算符中的输入验证代码，提取相关约束，并根据约束生成测试用例。在流行的 DL 库 TensorFlow 和 PyTorch 上的实验结果表明，ACETest 提取约束的质量比最先进的 SOTA (state-of-art)技术更高。此外，与 SOTA 技术相比，ACETest 能够提取96.4% 以上的约束，检测到1.95到55倍以上的错误。总的来说，我们已经使用 ACETest 检测了 TensorFlow 和 PyTorch 上108个以前未知的 bug，其中87个已经被开发人员证实。最后，由于安全影响，五个 bug 被分配到 CVE ID 中。"
    },
    {
        "title": "Reversible Deep Neural Network Watermarking:Matching the Floating-point\n  Weights",
        "url": "http://arxiv.org/abs/2305.17879v1",
        "pub_date": "2023-05-29",
        "summary": "Static deep neural network (DNN) watermarking embeds watermarks into the\nweights of DNN model by irreversible methods, but this will cause permanent\ndamage to watermarked model and can not meet the requirements of integrity\nauthentication. For these reasons, reversible data hiding (RDH) seems more\nattractive for the copyright protection of DNNs. This paper proposes a novel\nRDH-based static DNN watermarking method by improving the non-reversible\nquantization index modulation (QIM). Targeting the floating-point weights of\nDNNs, the idea of our RDH method is to add a scaled quantization error back to\nthe cover object. Two schemes are designed to realize the integrity protection\nand legitimate authentication of DNNs. Simulation results on training loss and\nclassification accuracy justify the superior feasibility, effectiveness and\nadaptability of the proposed method over histogram shifting (HS).",
        "translated": "静态深度神经网络(DNN)水印通过不可逆的方法将水印嵌入到 DNN 模型的权值中，但这会对水印模型造成永久性的损伤，不能满足完整性认证的要求。由于这些原因，可逆数据隐藏(RDH)似乎对 DNN 的版权保护更具吸引力。提出了一种基于 RDH 的静态 DNN 水印算法，改进了不可逆量化指数调制(QIM)算法。针对 DNN 的浮点权重，我们的 RDH 方法的想法是将一个按比例缩放的量化噪声重新添加到盖对象。设计了两种方案来实现 DNN 的完整性保护和合法认证。训练损失和分类精度的仿真结果证明了该方法优于直方图移位(HS)的可行性、有效性和适应性。"
    },
    {
        "title": "NaturalFinger: Generating Natural Fingerprint with Generative\n  Adversarial Networks",
        "url": "http://arxiv.org/abs/2305.17868v1",
        "pub_date": "2023-05-29",
        "summary": "Deep neural network (DNN) models have become a critical asset of the model\nowner as training them requires a large amount of resource (i.e. labeled data).\nTherefore, many fingerprinting schemes have been proposed to safeguard the\nintellectual property (IP) of the model owner against model extraction and\nillegal redistribution. However, previous schemes adopt unnatural images as the\nfingerprint, such as adversarial examples and noisy images, which can be easily\nperceived and rejected by the adversary. In this paper, we propose\nNaturalFinger which generates natural fingerprint with generative adversarial\nnetworks (GANs). Besides, our proposed NaturalFinger fingerprints the decision\ndifference areas rather than the decision boundary, which is more robust. The\napplication of GAN not only allows us to generate more imperceptible samples,\nbut also enables us to generate unrestricted samples to explore the decision\nboundary.To demonstrate the effectiveness of our fingerprint approach, we\nevaluate our approach against four model modification attacks including\nadversarial training and two model extraction attacks. Experiments show that\nour approach achieves 0.91 ARUC value on the FingerBench dataset (154 models),\nexceeding the optimal baseline (MetaV) over 17\\%.",
        "translated": "深度神经网络(DNN)模型已经成为模型所有者的重要资产，因为训练它们需要大量的资源(即标记数据)。因此，人们提出了许多指纹识别方案来保护模型所有者的知识产权，防止模型提取和非法再分配。然而，以往的方案都是采用非自然的图像作为指纹，如对手的例子和噪声图像，这些都很容易被对手察觉和排斥。本文提出了一种利用生成对抗网络(GAN)生成自然指纹的 NaturalFinger 方法。此外，我们提出的 NaturalFinger 能够识别决策差异区域，而不是更加健壮的决策边界。GAN 的应用不仅使我们能够生成更多不易察觉的样本，而且使我们能够生成不受限制的样本来探索决策边界。为了证明我们的指纹方法的有效性，我们评估了我们的方法对抗四种模型修改攻击，包括对抗性训练和两种模型提取攻击。实验表明，我们的方法在 FingerBench 数据集(154个模型)上达到了0.91 ARUC 值，超过了最佳基线(MetaV)的17% 以上。"
    },
    {
        "title": "NOTABLE: Transferable Backdoor Attacks Against Prompt-based NLP Models",
        "url": "http://arxiv.org/abs/2305.17826v1",
        "pub_date": "2023-05-28",
        "summary": "Prompt-based learning is vulnerable to backdoor attacks. Existing backdoor\nattacks against prompt-based models consider injecting backdoors into the\nentire embedding layers or word embedding vectors. Such attacks can be easily\naffected by retraining on downstream tasks and with different prompting\nstrategies, limiting the transferability of backdoor attacks. In this work, we\npropose transferable backdoor attacks against prompt-based models, called\nNOTABLE, which is independent of downstream tasks and prompting strategies.\nSpecifically, NOTABLE injects backdoors into the encoders of PLMs by utilizing\nan adaptive verbalizer to bind triggers to specific words (i.e., anchors). It\nactivates the backdoor by pasting input with triggers to reach\nadversary-desired anchors, achieving independence from downstream tasks and\nprompting strategies. We conduct experiments on six NLP tasks, three popular\nmodels, and three prompting strategies. Empirical results show that NOTABLE\nachieves superior attack performance (i.e., attack success rate over 90% on all\nthe datasets), and outperforms two state-of-the-art baselines. Evaluations on\nthree defenses show the robustness of NOTABLE. Our code can be found at\nhttps://github.com/RU-System-Software-and-Security/Notable.",
        "translated": "基于提示的学习很容易受到后门攻击。针对基于提示的模型的现有后门攻击考虑将后门注入到整个嵌入层或单词嵌入向量中。这种攻击很容易受到下游任务的再培训和不同的激励策略的影响，从而限制了后门攻击的可转移性。在这项工作中，我们提出了可转移的后门攻击对提示为基础的模型，称为 NOTABLE，这是独立于下游任务和提示策略。具体来说，NOTABLE 通过使用自适应语言表达器将触发器绑定到特定的单词(即锚) ，从而向 PLM 的编码器注入后门。它通过粘贴带有触发器的输入来激活后门，从而达到对手想要的锚，实现对下游任务的独立性并提示策略。我们对六个自然语言处理任务、三个流行的模型和三种激励策略进行了实验。实验结果表明，NOTABLE 具有更好的攻击性能(即所有数据集的攻击成功率都超过90%) ，并且优于两个最先进的基线。对三种防御机制的评估显示了 NOTABLE 的鲁棒性。我们的代码可以在 https://github.com/ru-system-software-and-security/notable 找到。"
    },
    {
        "title": "Ceibaco: REST API and Single Page Application for the generation and\n  evaluation of bijective S-boxes",
        "url": "http://arxiv.org/abs/2305.17798v1",
        "pub_date": "2023-05-28",
        "summary": "In this paper we present the first REST API for the generation and evaluation\nof bijective S-boxes. We also present the first Single Page Application tool\nfor researchers and students that allows the use of a graphical interface. We\ngive a small dataset of classical S-boxes to test the properties evaluations.\nWe show how to define experiments and we include two local search experiments\ninto the proposed tool.",
        "translated": "在本文中，我们提出了第一个 RESTAPI 的生成和评价双射 S 盒。我们还为研究人员和学生提供了第一个允许使用图形界面的单页应用程序工具。我们给出了一个经典 S- 盒的小数据集来检验性质的评价。我们展示了如何定义实验，我们包括两个局部搜索实验到提出的工具。"
    },
    {
        "title": "Amplification trojan network: Attack deep neural networks by amplifying\n  their inherent weakness",
        "url": "http://arxiv.org/abs/2305.17688v1",
        "pub_date": "2023-05-28",
        "summary": "Recent works found that deep neural networks (DNNs) can be fooled by\nadversarial examples, which are crafted by adding adversarial noise on clean\ninputs. The accuracy of DNNs on adversarial examples will decrease as the\nmagnitude of the adversarial noise increase. In this study, we show that DNNs\ncan be also fooled when the noise is very small under certain circumstances.\nThis new type of attack is called Amplification Trojan Attack (ATAttack).\nSpecifically, we use a trojan network to transform the inputs before sending\nthem to the target DNN. This trojan network serves as an amplifier to amplify\nthe inherent weakness of the target DNN. The target DNN, which is infected by\nthe trojan network, performs normally on clean data while being more vulnerable\nto adversarial examples. Since it only transforms the inputs, the trojan\nnetwork can hide in DNN-based pipelines, e.g. by infecting the pre-processing\nprocedure of the inputs before sending them to the DNNs. This new type of\nthreat should be considered in developing safe DNNs.",
        "translated": "最近的研究发现，深层神经网络(DNN)可以被敌对的例子所愚弄，这些例子是通过在干净的输入上添加敌对的噪音而精心设计的。随着对抗噪声的大小增加，DNN 对对抗性实例的准确性将下降。在这项研究中，我们表明，DNN 也可以被愚弄，当噪音是非常小，在某些情况下。这种新型的攻击被称为放大木马攻击。具体来说，我们使用特洛伊木马网络来转换输入，然后将它们发送给目标 DNN。这种木马网络充当放大器，放大目标 DNN 固有的弱点。目标 DNN 被特洛伊木马网络感染，正常执行干净的数据，但更容易受到对手的例子。由于木马网络只转换输入，因此它可以隐藏在基于 DNN 的管道中，例如在将输入发送给 DNN 之前感染输入的预处理过程。在开发安全的 DNN 时，应该考虑这种新型威胁。"
    },
    {
        "title": "What Can We Learn from Unlearnable Datasets?",
        "url": "http://arxiv.org/abs/2305.19254v1",
        "pub_date": "2023-05-30",
        "summary": "In an era of widespread web scraping, unlearnable dataset methods have the\npotential to protect data privacy by preventing deep neural networks from\ngeneralizing. But in addition to a number of practical limitations that make\ntheir use unlikely, we make a number of findings that call into question their\nability to safeguard data. First, it is widely believed that neural networks\ntrained on unlearnable datasets only learn shortcuts, simpler rules that are\nnot useful for generalization. In contrast, we find that networks actually can\nlearn useful features that can be reweighed for high test performance,\nsuggesting that image privacy is not preserved. Unlearnable datasets are also\nbelieved to induce learning shortcuts through linear separability of added\nperturbations. We provide a counterexample, demonstrating that linear\nseparability of perturbations is not a necessary condition. To emphasize why\nlinearly separable perturbations should not be relied upon, we propose an\northogonal projection attack which allows learning from unlearnable datasets\npublished in ICML 2021 and ICLR 2023. Our proposed attack is significantly less\ncomplex than recently proposed techniques.",
        "translated": "在一个广泛使用网页抓取的时代，不可学习的数据集方法有可能通过阻止深层神经网络泛化来保护数据隐私。但是，除了一些实际的限制，使他们的使用不太可能，我们作出了一些调查结果，质疑他们的能力，以保护数据。首先，人们普遍认为，在不可学习的数据集上训练的神经网络只学习快捷方式，简单的规则对泛化没有用处。相比之下，我们发现网络实际上可以学习有用的功能，可以重新权衡高测试性能，这表明图像隐私没有得到保护。无法学习的数据集也被认为能够通过线性可分的的扰动诱导学习捷径。我们提供了一个反例，证明了扰动的线性可分的不是一个必要条件。为了强调为什么不应该依赖于线性可分离的扰动，我们提出了一种正交投影攻击，它允许从 ICML 2021和 ICLR 2023发布的不可学习的数据集中学习。我们提出的攻击比最近提出的技术要简单得多。"
    },
    {
        "title": "Accountable authentication with privacy protection: The Larch system for\n  universal login",
        "url": "http://arxiv.org/abs/2305.19241v1",
        "pub_date": "2023-05-30",
        "summary": "Credential compromise is hard to detect and hard to mitigate. To address this\nproblem, we present larch, an accountable authentication framework with strong\nsecurity and privacy properties. Larch protects user privacy while ensuring\nthat the larch log server correctly records every authentication. Specifically,\nan attacker who compromises a user's device cannot authenticate without\ncreating evidence in the log, and the log cannot learn which web service\n(relying party) the user is authenticating to. To enable fast adoption, larch\nis backwards-compatible with relying parties that support FIDO2, TOTP, and\npassword-based login. Furthermore, larch does not degrade the security and\nprivacy a user already expects: the log server cannot authenticate on behalf of\na user, and larch does not allow relying parties to link a user across\naccounts. We implement larch for FIDO2, TOTP, and password-based login. Given a\nclient with four cores and a log server with eight cores, an authentication\nwith larch takes 150ms for FIDO2, 91ms for TOTP, and 74ms for passwords\n(excluding preprocessing, which takes 1.23s for TOTP).",
        "translated": "证书妥协很难被发现，也很难被减轻。为了解决这个问题，我们提出了落叶松，一个可靠的认证框架，具有强大的安全性和隐私属性。落叶松保护用户隐私，同时确保落叶松日志服务器正确记录每个身份验证。具体来说，攻击用户设备的攻击者如果不在日志中创建证据，就无法进行身份验证，而日志也无法知道用户正在向哪个 Web 服务(依赖方)进行身份验证。为了能够快速采用，落叶松是向后兼容的依赖方，支持 FIDO2，TOTP 和基于密码的登录。此外，落叶松不会降低用户已经预期的安全性和隐私: 日志服务器不能代表用户进行身份验证，而且落叶松不允许依赖方跨账户链接用户。我们为 FIDO2、 TOTP 和基于密码的登录实现了落叶松。给定一个有四个核心的客户机和一个有八个核心的日志服务器，用落叶松进行身份验证需要150ms 的 FIDO2,91ms 的 TOTP 和74ms 的密码(不包括预处理，TOTP 需要1.23 s)。"
    },
    {
        "title": "Adversarial Attacks on Online Learning to Rank with Stochastic Click\n  Models",
        "url": "http://arxiv.org/abs/2305.19218v1",
        "pub_date": "2023-05-30",
        "summary": "We propose the first study of adversarial attacks on online learning to rank.\nThe goal of the adversary is to misguide the online learning to rank algorithm\nto place the target item on top of the ranking list linear times to time\nhorizon $T$ with a sublinear attack cost. We propose generalized list poisoning\nattacks that perturb the ranking list presented to the user. This strategy can\nefficiently attack any no-regret ranker in general stochastic click models.\nFurthermore, we propose a click poisoning-based strategy named attack-then-quit\nthat can efficiently attack two representative OLTR algorithms for stochastic\nclick models. We theoretically analyze the success and cost upper bound of the\ntwo proposed methods. Experimental results based on synthetic and real-world\ndata further validate the effectiveness and cost-efficiency of the proposed\nattack strategies.",
        "translated": "我们首次提出了对网络学习中的对抗性攻击进行排名的研究。对手的目标是误导在线学习排序算法，将目标项目放在排序列表的顶部，时间跨度为线性时间跨度 $T $，带有次线性攻击成本。我们提出了广义列表中毒攻击，扰乱了提供给用户的排名列表。该策略可以有效地攻击一般随机点击模型中的任意无后悔排名。此外，我们提出了一个基于点击中毒的攻击-然后退出策略，可以有效地攻击随机点击模型的两个代表性的 OLTR 算法。我们从理论上分析了这两种方法的成功率和成本上限。基于合成数据和真实数据的实验结果进一步验证了该攻击策略的有效性和性价比。"
    },
    {
        "title": "Design and implementation of intelligent packet filtering in IoT\n  microcontroller-based devices",
        "url": "http://arxiv.org/abs/2305.19214v1",
        "pub_date": "2023-05-30",
        "summary": "Internet of Things (IoT) devices are increasingly pervasive and essential\ncomponents in enabling new applications and services. However, their widespread\nuse also exposes them to exploitable vulnerabilities and flaws that can lead to\nsignificant losses. In this context, ensuring robust cybersecurity measures is\nessential to protect IoT devices from malicious attacks. However, the current\nsolutions that provide flexible policy specifications and higher security\nlevels for IoT devices are scarce. To address this gap, we introduce T800, a\nlow-resource packet filter that utilizes machine learning (ML) algorithms to\nclassify packets in IoT devices. We present a detailed performance benchmarking\nframework and demonstrate T800's effectiveness on the ESP32 system-on-chip\nmicrocontroller and ESP-IDF framework. Our evaluation shows that T800 is an\nefficient solution that increases device computational capacity by excluding\nunsolicited malicious traffic from the processing pipeline. Additionally, T800\nis adaptable to different systems and provides a well-documented performance\nevaluation strategy for security ML-based mechanisms on ESP32-based IoT\nsystems. Our research contributes to improving the cybersecurity of\nresource-constrained IoT devices and provides a scalable, efficient solution\nthat can be used to enhance the security of IoT systems.",
        "translated": "物联网(IoT)设备在支持新的应用程序和服务方面越来越普遍和重要。然而，它们的广泛使用也使它们暴露在可利用的漏洞和缺陷之下，这些漏洞和缺陷可能导致重大损失。在这种情况下，确保强有力的网络安全措施对于保护物联网设备免受恶意攻击至关重要。然而，目前为物联网设备提供灵活策略规范和更高安全级别的解决方案很少。为了解决这个问题，我们引入了 T800，这是一个低资源包过滤器，它利用机器学习(ML)算法对物联网设备中的数据包进行分类。我们提出了一个详细的性能基准测试框架，并在 ESP32系统单片机和 ESP-IDF 框架上验证了 T800的有效性。我们的评估表明，T800是一个有效的解决方案，它通过从处理流水线中排除未经请求的恶意流量来增加设备的计算能力。此外，T800可以适应不同的系统，并且为基于 ESP32的物联网系统上基于 ML 的安全机制提供了一个有据可查的性能评估策略。我们的研究有助于改善资源受限的物联网设备的网络安全，并提供了一个可扩展的、高效的解决方案，可用于提高物联网系统的安全性。"
    },
    {
        "title": "Inferring Private Personal Attributes of Virtual Reality Users from Head\n  and Hand Motion Data",
        "url": "http://arxiv.org/abs/2305.19198v1",
        "pub_date": "2023-05-30",
        "summary": "Motion tracking \"telemetry\" data lies at the core of nearly all modern\nvirtual reality (VR) and metaverse experiences. While generally presumed\ninnocuous, recent studies have demonstrated that motion data actually has the\npotential to uniquely identify VR users. In this study, we go a step further,\nshowing that a variety of private user information can be inferred just by\nanalyzing motion data recorded by VR devices. We conducted a large-scale survey\nof VR users (N=1,006) with dozens of questions ranging from background and\ndemographics to behavioral patterns and health information. We then collected\nVR motion samples of each user playing the game ``Beat Saber,'' and attempted\nto infer their survey responses using just their head and hand motion patterns.\nUsing simple machine learning models, many of these attributes could accurately\nand consistently be inferred from VR motion data alone, highlighting the\npressing need for privacy-preserving mechanisms in multi-user VR applications.",
        "translated": "运动跟踪“遥测”数据是几乎所有现代虚拟现实(VR)和元宇宙体验的核心。虽然一般认为无害，最近的研究已经证明，运动数据实际上有可能唯一识别虚拟现实用户。在这项研究中，我们更进一步，表明各种各样的私人用户信息可以推断只是分析运动数据记录的虚拟现实设备。我们对虚拟现实用户(N = 1,006)进行了一次大规模的调查，问题涉及背景、人口统计学、行为模式和健康信息等多个方面。然后，我们收集了每个玩游戏的用户的虚拟现实运动样本“击败军刀”，并试图推断他们的调查反应，只使用他们的头部和手部运动模式。使用简单的机器学习模型，许多这些属性可以准确和一致地从 VR 运动数据中推断出来，突出了在多用户 VR 应用中保护隐私机制的迫切需要。"
    },
    {
        "title": "Reviewing BPMN as a Modeling Notation for CACAO Security Playbooks",
        "url": "http://arxiv.org/abs/2305.18928v1",
        "pub_date": "2023-05-30",
        "summary": "As cyber systems become increasingly complex and cybersecurity threats become\nmore prominent, defenders must prepare, coordinate, automate, document, and\nshare their response methodologies to the extent possible. The CACAO standard\nwas developed to satisfy the above requirements providing a common\nmachine-readable framework and schema to document cybersecurity operations\nprocesses, including defensive tradecraft and tactics, techniques, and\nprocedures. Although this approach is compelling, a remaining limitation is\nthat CACAO provides no native modeling notation for graphically representing\nplaybooks, which is crucial for simplifying their creation, modification, and\nunderstanding. In contrast, the industry is familiar with BPMN, a\nstandards-based modeling notation for business processes that has also found\nits place in representing cybersecurity processes. This research examines BPMN\nand CACAO and explores the feasibility of using the BPMN modeling notation to\ngraphically represent CACAO security playbooks. The results indicate that\nmapping CACAO and BPMN is attainable at an abstract level; however, conversion\nfrom one encoding to another introduces a degree of complexity due to the\nmultiple ways CACAO constructs can be represented in BPMN and the extensions\nrequired in BPMN to fully support CACAO.",
        "translated": "随着网络系统变得越来越复杂，网络安全威胁变得越来越突出，防御者必须尽可能地准备、协调、自动化、记录和分享他们的应对方法。CACAO 标准是为了满足上述要求而开发的，它提供了一个通用的机器可读框架和模式来记录网络安全操作过程，包括防御性间谍技术和战术、技术和程序。尽管这种方法很有说服力，但是仍然存在一个局限性，即 CACAO 没有提供用于以图形方式表示剧本的本地建模符号，这对于简化剧本的创建、修改和理解至关重要。相比之下，业界熟悉 BPMN，这是一种基于标准的业务流程建模符号，在表示网络安全流程方面也占有一席之地。本研究检视 BPMN 和 CACAO，并探讨使用 BPMN 建模符号以图形化方式表示 CACAO 安全剧本的可行性。结果表明，CACAO 和 BPMN 的映射在抽象层次上是可以实现的; 然而，由于 CACAO 构造可以在 BPMN 中以多种方式表示，并且 BPMN 需要扩展来完全支持 CACAO，因此从一种编码到另一种编码的转换引入了一定程度的复杂性。"
    },
    {
        "title": "Password-Based Authentication and The Experiences of End Users",
        "url": "http://arxiv.org/abs/2305.18909v1",
        "pub_date": "2023-05-30",
        "summary": "Passwords are used majorly for end-user authentication in information and\ncommunication technology (ICT) systems due to its perceived ease of use. The\nuse for end-user authentication extends through mobile, computers and\nnetwork-based products and services. But with the attendant issues relating to\npassword hacks, leakages, and theft largely due to weak, reuse and poor\npassword habits of end-users, the call for passwordless authentication as\nalternative intensifies. All the same, there are missing knowledge of whether\nthese password-based experiences are associated with societal economic status,\neducational qualification of citizens, their age and gender, technological\nadvancements, and depth of penetration. In line with the above, understanding\nthe experience of end-users in developing economy to ascertain their\npassword-based experience has become of interest to the researchers. This paper\naims at measuring the experience of staff and students in University\ncommunities within southeastern Nigeria on password-based authentication\nsystems. These communities have population whose age brackets are majorly\nwithin the ages of 16 and 60 years; have people with requisite educational\nqualifications ranging from Diploma to Doctorate degrees and constitutes good\nnumber of ICT tools consumers. The survey had 291 respondents, and collected\ndata about age, educational qualifications, and gender from these respondents.\nIt also collected information about their password experience in social media\nnetwork, online shopping, electronic health care services, and internet\nbanking. Our analysis using SPSS and report by means of descriptive statistics,\nfrequency distribution, and Chi-Square tests showed that account compromise in\nthe geographical area is not common with the respondents reporting good\nexperience with passwords usage.",
        "translated": "在信息和通信技术(ICT)系统中，由于密码易于使用，它主要用于最终用户身份验证。最终用户认证的使用范围扩大到移动、计算机和基于网络的产品和服务。但是由于最终用户的软弱、重用和糟糕的密码习惯，与密码破解、泄漏和盗窃相关的问题随之而来，要求无密码身份验证作为替代方案的呼声越来越高。尽管如此，对于这些基于密码的体验是否与社会经济地位、公民的教育资格、年龄和性别、技术进步以及渗透深度有关，人们仍然缺乏知识。据此，了解最终用户在发展经济中的经验，以确定其基于密码的经验已成为研究人员感兴趣的问题。本文旨在测量尼日利亚东南部大学社区的教职员工和学生使用基于密码的认证系统的经验。这些社区的人口年龄段主要在16岁和60岁之间; 他们拥有从文凭到博士学位的必要教育资格，是信息和通信技术工具的大量消费者。这项调查有291名受访者，并收集了这些受访者的年龄、学历和性别等方面的数据。它还收集了有关他们在社交媒体网络、网上购物、电子医疗服务和网上银行中的密码体验的信息。我们使用 SPSS 软件进行分析，并通过描述统计学、频率分布和卡方检验进行报告。结果显示，受访者报告密码使用经验丰富的情况并不常见，但经纬度中的帐户泄露情况却不常见。"
    },
    {
        "title": "Majority Voting Approach to Ransomware Detection",
        "url": "http://arxiv.org/abs/2305.18852v1",
        "pub_date": "2023-05-30",
        "summary": "Crypto-ransomware remains a significant threat to governments and companies\nalike, with high-profile cyber security incidents regularly making headlines.\nMany different detection systems have been proposed as solutions to the\never-changing dynamic landscape of ransomware detection. In the majority of\ncases, these described systems propose a method based on the result of a single\ntest performed on either the executable code, the process under investigation,\nits behaviour, or its output. In a small subset of ransomware detection\nsystems, the concept of a scorecard is employed where multiple tests are\nperformed on various aspects of a process under investigation and their results\nare then analysed using machine learning. The purpose of this paper is to\npropose a new majority voting approach to ransomware detection by developing a\nmethod that uses a cumulative score derived from discrete tests based on\ncalculations using algorithmic rather than heuristic techniques. The paper\ndescribes 23 candidate tests, as well as 9 Windows API tests which are\nvalidated to determine both their accuracy and viability for use within a\nransomware detection system. Using a cumulative score calculation approach to\nransomware detection has several benefits, such as the immunity to the\noccasional inaccuracy of individual tests when making its final classification.\nThe system can also leverage multiple tests that can be both comprehensive and\ncomplimentary in an attempt to achieve a broader, deeper, and more robust\nanalysis of the program under investigation. Additionally, the use of multiple\ncollaborative tests also significantly hinders ransomware from masking or\nmodifying its behaviour in an attempt to bypass detection.",
        "translated": "加密勒索软件仍然是对政府和企业的重大威胁，高调的网络安全事件经常成为头条新闻。许多不同的检测系统已被提出作为解决不断变化的动态景观的勒索软件检测。在大多数情况下，这些描述的系统提出一种方法，该方法基于对可执行代码、正在调查的过程、其行为或其输出执行的单个测试的结果。在一小部分勒索软件检测系统中，使用记分卡的概念，对所调查过程的各个方面进行多个测试，然后使用机器学习分析测试结果。本文的目的是提出一种新的多数表决方法来检测勒索软件，通过开发一种方法，使用累积得分从离散测试的基础上计算使用算法而不是启发式技术。本文描述了23个候选测试，以及9个 Windows API 测试，这些测试经过验证，以确定其准确性和可行性，以便在勒索软件检测系统中使用。使用累积分数计算方法进行勒索软件检测有几个好处，例如在进行最终分类时可以免受个别测试偶尔出现的不准确性的影响。该系统还可以利用多个测试，这些测试可以是全面的，也可以是互补的，以便对正在调查的程序进行更广泛、更深入和更健壮的分析。此外，多个协作测试的使用也显著地阻碍了勒索软件掩盖或修改其行为，试图绕过检测。"
    },
    {
        "title": "Methods for Collisions in Some Algebraic Hash Functions",
        "url": "http://arxiv.org/abs/2305.18799v1",
        "pub_date": "2023-05-30",
        "summary": "This paper focuses on devising methods for producing collisions in algebraic\nhash functions that may be seen as generalized forms of the well-known Z\\'emor\nand Tillich-Z\\'emor hash functions. In contrast to some of the previous\napproaches, we attempt to construct collisions in a structured and\ndeterministic manner by constructing messages with triangular or diagonal\nhashes messages. Our method thus provides an alternate deterministic approach\nto the method for finding triangular hashes. We also consider the generalized\nTillich-Z\\'emor hash functions over ${\\mathbb{F}_p}^k$ for $p\\neq 2$, relating\nthe generator matrices to a polynomial recurrence relation, and derive a closed\nform for any arbitrary power of the generators. We then provide conditions for\ncollisions, and a method to maliciously design the system so as to facilitate\neasy collisions, in terms of this polynomial recurrence relation. Our general\nconclusion is that it is very difficult in practice to achieve the theoretical\ncollision conditions efficiently, in both the generalized Z\\'emor and the\ngeneralized Tillich-Z\\'emor cases. Therefore, although the techniques are\ninteresting theoretically, in practice the collision-resistance of the\ngeneralized Z\\'emor functions is reinforced.",
        "translated": "本文重点研究了代数哈希函数中产生碰撞的方法，这些碰撞可以看作是著名的 Z’em 或 Tillich-Z’em 或哈希函数的广义形式。与以前的一些方法相反，我们尝试通过构造具有三角形或对角散列消息的消息来以结构化和确定性的方式构造冲突。因此，我们的方法为寻找三角形散列的方法提供了一种替代的确定性方法。我们还考虑了 ${ mathbb { F } _ p } ^ k $for $p neq 2 $上的广义 Tillich-Z’em 或 hash 函数，将生成器矩阵与多项式递回关系式联系起来，并对生成器的任意幂给出了一个封闭形式。然后，我们提供碰撞的条件，以及一种恶意设计系统的方法，以便根据这个多项式递回关系式方便地进行碰撞。我们的一般结论是，无论是在广义 Z’em 还是在广义 Tillich-Z’em 或情形下，有效地实现理论碰撞条件在实践中都是非常困难的。因此，虽然这些技术在理论上很有趣，但在实践中，广义 Z’em 或函数的抗碰撞能力得到了加强。"
    },
    {
        "title": "Phase Correction using Deep Learning for Satellite-to-Ground CV-QKD",
        "url": "http://arxiv.org/abs/2305.18737v1",
        "pub_date": "2023-05-30",
        "summary": "Coherent measurement of quantum signals used for continuous-variable (CV)\nquantum key distribution (QKD) across satellite-to-ground channels requires\ncompensation of phase wavefront distortions caused by atmospheric turbulence.\nOne compensation technique involves multiplexing classical reference pulses\n(RPs) and the quantum signal, with direct phase measurements on the RPs then\nused to modulate a real local oscillator (RLO) on the ground - a solution that\nalso removes some known attacks on CV-QKD. However, this is a cumbersome task\nin practice - requiring substantial complexity in equipment requirements and\ndeployment. As an alternative to this traditional practice, here we introduce a\nnew method for estimating phase corrections for an RLO by using only intensity\nmeasurements from RPs as input to a convolutional neural network, mitigating\ncompletely the necessity to measure phase wavefronts directly. Conventional\nwisdom dictates such an approach would likely be fruitless. However, we show\nthat the phase correction accuracy needed to provide for non-zero secure key\nrates through satellite-to-ground channels is achieved by our intensity-only\nmeasurements. Our work shows, for the first time, how artificial intelligence\nalgorithms can replace phase-measuring equipment in the context of CV-QKD\ndelivered from space, thereby delivering an alternate deployment paradigm for\nthis global quantum-communication application.",
        "translated": "用于连续变量量子密钥分配(QKD)的量子信号的相干测量需要对大气湍流引起的相位波前畸变进行补偿。一种补偿技术包括复用经典参考脉冲(RPs)和量子信号，对 RPs 进行直接相位测量，然后用于调制地面上的实际本机振荡器(RLO)——这种解决方案也消除了对 CV-QKD 的一些已知攻击。然而，这在实践中是一项繁琐的任务——需要在设备需求和部署方面有相当大的复杂性。作为这种传统做法的替代，我们在这里介绍一种新的估计相位修正的方法，通过只使用来自 RP 的强度测量作为输入到卷积神经网络，完全减少了直接测量相位波阵面的必要性。传统观点认为，这种做法可能是徒劳的。然而，我们表明，所需的相位校正精度，以提供非零安全密钥率通过卫星到地面信道是通过我们的强度只有测量。我们的工作首次展示了人工智能算法如何取代从太空传送的 CV-QKD 环境下的相位测量设备，从而为这种全球量子通信应用提供了一种替代部署范式。"
    },
    {
        "title": "Understanding and Mitigating Copying in Diffusion Models",
        "url": "http://arxiv.org/abs/2305.20086v1",
        "pub_date": "2023-05-31",
        "summary": "Images generated by diffusion models like Stable Diffusion are increasingly\nwidespread. Recent works and even lawsuits have shown that these models are\nprone to replicating their training data, unbeknownst to the user. In this\npaper, we first analyze this memorization problem in text-to-image diffusion\nmodels. While it is widely believed that duplicated images in the training set\nare responsible for content replication at inference time, we observe that the\ntext conditioning of the model plays a similarly important role. In fact, we\nsee in our experiments that data replication often does not happen for\nunconditional models, while it is common in the text-conditional case.\nMotivated by our findings, we then propose several techniques for reducing data\nreplication at both training and inference time by randomizing and augmenting\nimage captions in the training set.",
        "translated": "由稳定扩散等扩散模型产生的图像越来越广泛。最近的工作，甚至诉讼已经表明，这些模型倾向于复制他们的训练数据，用户不知道。本文首先分析了文本-图像扩散模型中的记忆问题。虽然人们普遍认为训练集中的重复图像负责推理时的内容复制，但是我们观察到模型的文本条件作用也起着类似的重要作用。事实上，在我们的实验中，我们看到数据复制通常不会发生在无条件模型中，而在文本条件的情况下却很常见。在我们的研究结果的激励下，我们提出了几种技术，通过在训练集中随机化和增强图像标题来减少训练和推理时间的数据复制。"
    },
    {
        "title": "Tree-Ring Watermarks: Fingerprints for Diffusion Images that are\n  Invisible and Robust",
        "url": "http://arxiv.org/abs/2305.20030v2",
        "pub_date": "2023-05-31",
        "summary": "Watermarking the outputs of generative models is a crucial technique for\ntracing copyright and preventing potential harm from AI-generated content. In\nthis paper, we introduce a novel technique called Tree-Ring Watermarking that\nrobustly fingerprints diffusion model outputs. Unlike existing methods that\nperform post-hoc modifications to images after sampling, Tree-Ring Watermarking\nsubtly influences the entire sampling process, resulting in a model fingerprint\nthat is invisible to humans. The watermark embeds a pattern into the initial\nnoise vector used for sampling. These patterns are structured in Fourier space\nso that they are invariant to convolutions, crops, dilations, flips, and\nrotations. After image generation, the watermark signal is detected by\ninverting the diffusion process to retrieve the noise vector, which is then\nchecked for the embedded signal. We demonstrate that this technique can be\neasily applied to arbitrary diffusion models, including text-conditioned Stable\nDiffusion, as a plug-in with negligible loss in FID. Our watermark is\nsemantically hidden in the image space and is far more robust than watermarking\nalternatives that are currently deployed. Code is available at\nhttps://github.com/YuxinWenRick/tree-ring-watermark.",
        "translated": "对生成模型的输出进行水印是追踪版权和防止人工智能生成内容潜在危害的关键技术。本文介绍了一种新的指纹扩散模型鲁棒输出的树环数字水印技术。与现有的采样后对图像进行事后修改的方法不同，树环水印微妙地影响了整个采样过程，产生了人类看不见的模型指纹。该水印在用于采样的初始噪声矢量中嵌入一个模式。这些图案是在傅里叶空间中构成的，因此它们对卷积、作物、膨胀、翻转和旋转都是不变的。图像生成后，通过反向扩散过程检测水印信号，提取噪声矢量，然后对嵌入信号进行检测。我们证明了这种技术可以很容易地应用于任意的扩散模型，包括文本条件的稳定扩散，作为一个插件，在 FID 的损失可以忽略不计。我们的水印在语义上隐藏在图像空间中，并且比目前部署的水印方案更加健壮。密码可于 https://github.com/yuxinwenrick/tree-ring-watermark 索取。"
    },
    {
        "title": "Hidden Stabilizers, the Isogeny To Endomorphism Ring Problem and the\n  Cryptanalysis of pSIDH",
        "url": "http://arxiv.org/abs/2305.19897v1",
        "pub_date": "2023-05-31",
        "summary": "The Isogeny to Endomorphism Ring Problem (IsERP) asks to compute the\nendomorphism ring of the codomain of an isogeny between supersingular curves in\ncharacteristic $p$ given only a representation for this isogeny, i.e. some data\nand an algorithm to evaluate this isogeny on any torsion point. This problem\nplays a central role in isogeny-based cryptography; it underlies the security\nof pSIDH protocol (ASIACRYPT 2022) and it is at the heart of the recent attacks\nthat broke the SIDH key exchange. Prior to this work, no efficient algorithm\nwas known to solve IsERP for a generic isogeny degree, the hardest case\nseemingly when the degree is prime.\n  In this paper, we introduce a new quantum polynomial-time algorithm to solve\nIsERP for isogenies whose degrees are odd and have $O(\\log\\log p)$ many prime\nfactors. As main technical tools, our algorithm uses a quantum algorithm for\ncomputing hidden Borel subgroups, a group action on supersingular isogenies\nfrom EUROCRYPT 2021, various algorithms for the Deuring correspondence and a\nnew algorithm to lift arbitrary quaternion order elements modulo an odd integer\n$N$ with $O(\\log\\log p)$ many prime factors to powersmooth elements.\n  As a main consequence for cryptography, we obtain a quantum polynomial-time\nkey recovery attack on pSIDH. The technical tools we use may also be of\nindependent interest.",
        "translated": "等值自同态环问题(isERP)要求计算特征 $p $中超奇异曲线之间等值自同态环的余域，只给出这个等值的一个表示，即一些数据和一个算法来评估这个等值在任何扭转点上。这个问题在基于等基因的密码学中起着核心作用; 它是 pSIDH 协议(ASIACRYPT 2022)安全性的基础，也是最近破坏 SIDH 密钥交换的攻击的核心。在这项工作之前，还没有有效的算法来解决一般的等同度 IsERP 问题，最困难的情况似乎是当该度为素数时。本文提出了一种新的量子多项式时间算法，用于求解度数为奇数且具有 $O (log log p) $多素因子的同构体的 IsERP 问题。作为主要的技术工具，该算法采用了计算隐 Borel 子群的量子算法、 EUROCRYPT 2021中对超奇异同构的群作用、 Deuring 通信的各种算法以及提升任意四元数阶元素模的奇数整数 $N $和 $O (log log p) $多素因子的新算法。作为密码学的一个主要结果，我们得到了对 pSIDH 的量子多项式时间密钥恢复攻击。我们使用的技术工具也可能是独立的兴趣。"
    },
    {
        "title": "Lattice-Aided Extraction of Spread-Spectrum Hidden Data",
        "url": "http://arxiv.org/abs/2305.19855v1",
        "pub_date": "2023-05-31",
        "summary": "This paper discusses the problem of extracting spread spectrum hidden data\nfrom the perspective of lattice decoding. Since the conventional blind\nextraction scheme multi-carrier iterative generalize least-squares (M-IGLS) and\nnon-blind extraction scheme minimum mean square error (MMSE) suffer from\nperformance degradation when the carriers lack sufficient orthogonality, we\npresent two novel schemes from the viewpoint of lattice decoding, namely\nmulti-carrier iterative successive interference cancellation (M-ISIC) and\nsphere decoding (SD). The better performance of M-ISIC and SD are confirmed by\nboth theoretical justification and numerical simulations.",
        "translated": "本文从格解码的角度讨论了扩频隐藏数据的提取问题。针对传统的盲提取方案多载波迭代广义最小二乘(M-IGLS)和非盲提取方案最小均方误差(MMSE)在载波缺乏足够正交性时性能下降的问题，从格形译码的角度提出了两种新方案，即多载波迭代逐次干扰抵消(M-ISIC)和球形译码(SD)。理论分析和数值模拟均证实了 M-ISIC 和 SD 具有较好的性能。"
    },
    {
        "title": "Aggregated Zero-knowledge Proof and Blockchain-Empowered Authentication\n  for Autonomous Truck Platooning",
        "url": "http://arxiv.org/abs/2305.19813v1",
        "pub_date": "2023-05-31",
        "summary": "Platooning technologies enable trucks to drive cooperatively and\nautomatically, providing benefits including less fuel consumption, greater road\ncapacity, and safety. This paper introduces an aggregated zero-knowledge proof\nand blockchain-empowered system for privacy-preserving identity verification in\nthe mixed fleet platooning environment. The correctness proof and the security\nanalysis of the proposed authentication scheme are provided, highlighting its\nincreased security and fast performance in comparison to a single-proof design.\nThe blockchain performs the role of verifier within the authentication scheme,\nreducing unnecessary communication overhead. Moreover, the blockchain improves\nsystem resilience by providing fault tolerance to the decentralized\nverification process. Platooning records are stored directly on the digital\nledger to guarantee data immutability and integrity, while the programmable\naccess control policies ensure data privacy. The experimental results\ndemonstrate that the proposed approach can perform authentication on the order\nof milliseconds, regardless of the number of proofs, highlighting feasibility\nfor real-world deployment in truck platooning.",
        "translated": "排队技术使卡车能够合作和自动驾驶，提供的好处包括更少的燃料消耗，更大的道路容量和安全。提出了一种基于区块链授权的混合编队环境下的集成零知识证明系统。给出了该认证方案的正确性证明和安全性分析，与单一证明方案相比，该方案具有更高的安全性和更快的性能。块链在身份验证方案中扮演验证者的角色，减少了不必要的通信开销。此外，区块链通过对分散验证过程提供容错能力，提高了系统的恢复能力。排队记录直接存储在数字分类账上，保证数据的不变性和完整性，而可编程访问控制策略保证数据的隐私性。实验结果表明，该方法可以在毫秒级的时间内进行身份验证，无论验证次数多少，突出了在现实世界中部署卡车排的可行性。"
    },
    {
        "title": "A Hybrid Blockchain-Edge Architecture for Electronic Health Records\n  Management with Attribute-based Cryptographic Mechanisms",
        "url": "http://arxiv.org/abs/2305.19797v1",
        "pub_date": "2023-05-31",
        "summary": "This paper presents a hybrid blockchain-edge architecture for managing\nElectronic Health Records (EHRs) with attribute-based cryptographic mechanisms.\nThe architecture introduces a novel attribute-based signature aggregation\n(ABSA) scheme and multi-authority attribute-based encryption (MA-ABE)\nintegrated with Paillier homomorphic encryption (HE) to protect patients'\nanonymity and safeguard their EHRs. All the EHR activities and access control\nevents are recorded permanently as blockchain transactions. We develop the ABSA\nmodule on Hyperledger Ursa cryptography library, MA-ABE module on OpenABE\ntoolset, and blockchain network on Hyperledger Fabric. We measure the execution\ntime of ABSA's signing and verification functions, MA-ABE with different access\npolicies and homomorphic encryption schemes, and compare the results with other\nexisting blockchain-based EHR systems. We validate the access activities and\nauthentication events recorded in blockchain transactions and evaluate the\ntransaction throughput and latency using Hyperledger Caliper. The results show\nthat the performance meets real-world scenarios' requirements while\nsafeguarding EHR and is robust against unauthorized retrievals.",
        "translated": "提出了一种基于属性加密机制的区块链-边缘混合体系结构来管理电子健康记录(EHR)。该体系结构引入了一种新的基于属性的签名聚合(ABSA)方案和集成了 Paillier 同态加密(HE)的基于多权限属性的加密(MA-ABE) ，以保护患者的匿名性和保护他们的电子病历。所有的 EHR 活动和访问控制事件都被永久地记录为区块链事务。我们在 Hyperledger Ursa 密码库上开发了 ABSA 模块，在 OpenABE 工具集上开发了 MA-ABE 模块，在 Hyperledger Fabric 上开发了区块链网络。我们测量了 ABSA 的签名和验证功能的执行时间，使用不同接入策略和同态加密方案的 MA-ABE，并将结果与其他现有的基于区块链的电子健康记录系统进行了比较。我们验证区块链事务中记录的访问活动和认证事件，并使用 Hyperledger Caliper 评估事务吞吐量和延迟。结果表明，该算法在保护 EHR 的同时，性能满足现实场景的要求，对未经授权的检索具有较强的鲁棒性。"
    },
    {
        "title": "Off-By-One Implementation Error in J-UNIWARD",
        "url": "http://arxiv.org/abs/2305.19776v1",
        "pub_date": "2023-05-31",
        "summary": "J-UNIWARD is a popular steganography method for hiding secret messages in\nJPEG cover images. As a content-adaptive method, J-UNIWARD aims to embed into\ntextured image regions where changes are difficult to detect. To this end,\nJ-UNIWARD first assigns to each DCT coefficient an embedding cost calculated\nbased on the image's Wavelet residual, and then uses a coding method that\nminimizes the cost while embedding the desired payload. Changing one DCT\ncoefficient affects a 23x23 window of Wavelet coefficients. To speed up the\ncostmap computation, the original implementation pre-computes the Wavelet\nresidual and then considers per changed DCT coefficient a 23x23 window of the\nWavelet residual. However, the implementation accesses a window accidentally\nshifted by one pixel to the bottom right. In this report, we evaluate the\neffect of this off-by-one error on the resulting costmaps. Some image blocks\nare over-priced while other image blocks are under-priced, but the difference\nis relatively small. The off-by-one error seems to make little difference for\nlearning-based steganalysis.",
        "translated": "UNIWARD 是一种流行的隐写方法，用于在 JPEG 封面图像中隐藏秘密信息。作为一种内容自适应方法，J-UNIWARD 的目标是嵌入到纹理图像区域的变化是难以检测。为此，J-UNIWARD 首先给每个 DCT 系数分配一个基于图像小波残差计算的嵌入代价，然后使用一种编码方法，在嵌入所需有效载荷的同时使代价最小。改变一个 DCT 系数影响一个23x23窗口的小波系数。为了提高代价图的计算速度，原始实现先对小波残差进行预计算，然后考虑每变化一个 DCT 系数的小波残差窗口为23x23。但是，该实现访问的窗口意外地向右下方移动了一个像素。在这份报告中，我们评估了这种差一错误对最终成本图的影响。一些图像块定价过高，而其他图像块定价过低，但差别相对较小。这种差一错误似乎对基于学习的隐写分析没什么影响。"
    },
    {
        "title": "You Can Run But You Can't Hide: Runtime Protection Against Malicious\n  Package Updates For Node.js",
        "url": "http://arxiv.org/abs/2305.19760v1",
        "pub_date": "2023-05-31",
        "summary": "Maliciously prepared software packages are an extensively leveraged weapon\nfor software supply chain attacks. The detection of malicious packages is\nundoubtedly of high priority and many academic and commercial approaches have\nbeen developed. In the inevitable case of an attack, one needs resilience\nagainst malicious code. To this end, we present a runtime protection for\nNode.js that automatically limits a package's capabilities to an established\nminimum. The detection of required capabilities as well as their enforcement at\nruntime has been implemented and evaluated against known malicious attacks. Our\napproach was able to prevent 9/10 historic attacks with a median install-time\noverhead of less than 0.6 seconds and a median runtime overhead of less than\n0.2 seconds.",
        "translated": "恶意准备的软件包是软件供应链攻击的一种广泛利用的武器。检测恶意软件包无疑是高度优先事项，已经开发了许多学术和商业方法。在攻击不可避免的情况下，人们需要对恶意代码的弹性。为此，我们为 Node.js 提供了一个运行时保护，它自动将包的功能限制到一个确定的最小值。针对已知的恶意攻击，已经实现并评估了所需功能的检测及其在运行时的执行。我们的方法能够阻止9/10的历史性攻击，中位安装时间开销小于0.6秒，中位运行时间开销小于0.2秒。"
    },
    {
        "title": "Concentrated Geo-Privacy",
        "url": "http://arxiv.org/abs/2305.19756v1",
        "pub_date": "2023-05-31",
        "summary": "This paper proposes concentrated geo-privacy (CGP), a privacy notion that can\nbe considered as the counterpart of concentrated differential privacy (CDP) for\ngeometric data. Compared with the previous notion of geo-privacy [ABCP13,\nCABP13], which is the counterpart of standard differential privacy, CGP offers\nmany benefits including simplicity of the mechanism, lower noise scale in high\ndimensions, and better composability known as advanced composition. The last\none is the most important, as it allows us to design complex mechanisms using\nsmaller building blocks while achieving better utilities. To complement this\nresult, we show that the previous notion of geo-privacy inherently does not\nadmit advanced composition even using its approximate version. Next, we study\nthree problems on private geometric data: the identity query, k nearest\nneighbors, and convex hulls. While the first problem has been previously\nstudied, we give the first mechanisms for the latter two under geo-privacy. For\nall three problems, composability is essential in obtaining good utility\nguarantees on the privatized query answer.",
        "translated": "本文提出了集中地理隐私权(cGP) ，这一隐私概念可以被视为几何数据集中差分隐私(cDP)的对应物。与之前的地理隐私概念[ ABCP13，CABP13](与标准差分隐私相对应)相比，CGP 提供了许多优点，包括机制的简单性、高维度的低噪声尺度以及被称为高级合成的更好的可组合性。最后一个是最重要的，因为它使我们能够设计复杂的机制，使用更小的构件，同时实现更好的效用。为了补充这一结果，我们表明，先前的地理隐私概念本质上不承认先进的组成，即使使用其近似版本。接下来，我们研究了私有几何数据的三个问题: 身份查询、 k 最近邻和凸壳。在前人研究的基础上，我们给出了地理隐私条件下后两种情况下的第一种机制。对于所有这三个问题，可组合性对于获得私有化查询答案的良好效用保证是必不可少的。"
    },
    {
        "title": "Adversarial Clean Label Backdoor Attacks and Defenses on Text\n  Classification Systems",
        "url": "http://arxiv.org/abs/2305.19607v1",
        "pub_date": "2023-05-31",
        "summary": "Clean-label (CL) attack is a form of data poisoning attack where an adversary\nmodifies only the textual input of the training data, without requiring access\nto the labeling function. CL attacks are relatively unexplored in NLP, as\ncompared to label flipping (LF) attacks, where the latter additionally requires\naccess to the labeling function as well. While CL attacks are more resilient to\ndata sanitization and manual relabeling methods than LF attacks, they often\ndemand as high as ten times the poisoning budget than LF attacks. In this work,\nwe first introduce an Adversarial Clean Label attack which can adversarially\nperturb in-class training examples for poisoning the training set. We then show\nthat an adversary can significantly bring down the data requirements for a CL\nattack, using the aforementioned approach, to as low as 20% of the data\notherwise required. We then systematically benchmark and analyze a number of\ndefense methods, for both LF and CL attacks, some previously employed solely\nfor LF attacks in the textual domain and others adapted from computer vision.\nWe find that text-specific defenses greatly vary in their effectiveness\ndepending on their properties.",
        "translated": "Clean-label (CL)攻击是数据中毒攻击的一种形式，攻击者只修改训练数据的文本输入，而不需要访问标记函数。与标签翻转(LF)攻击相比，自然语言处理(NLP)中的 CL 攻击相对较少涉及，后者还需要访问标签功能。虽然 CL 攻击比 LF 攻击对数据清理和手动重新标记方法更有弹性，但它们往往需要比 LF 攻击高出10倍的中毒预算。在这项工作中，我们首先介绍了一个对抗性干净标签攻击，它可以对抗性地扰乱课堂上的训练例子中毒的训练集。然后，我们展示了使用上述方法，对手可以显著降低 CL 攻击的数据需求，使其降低到原来需要的数据的20% 。然后，我们系统地基准和分析了一些防御方法，包括 LF 和 CL 攻击，其中一些以前只用于文本领域的 LF 攻击和其他改编自计算机视觉。我们发现，文本特定的防御措施根据其属性的不同，其有效性差异很大。"
    },
    {
        "title": "Interpreting GNN-based IDS Detections Using Provenance Graph Structural\n  Features",
        "url": "http://arxiv.org/abs/2306.00934v1",
        "pub_date": "2023-06-01",
        "summary": "The black-box nature of complex Neural Network (NN)-based models has hindered\ntheir widespread adoption in security domains due to the lack of logical\nexplanations and actionable follow-ups for their predictions. To enhance the\ntransparency and accountability of Graph Neural Network (GNN) security models\nused in system provenance analysis, we propose PROVEXPLAINER, a framework for\nprojecting abstract GNN decision boundaries onto interpretable feature spaces.\n  We first replicate the decision-making process of GNNbased security models\nusing simpler and explainable models such as Decision Trees (DTs). To maximize\nthe accuracy and fidelity of the surrogate models, we propose novel graph\nstructural features founded on classical graph theory and enhanced by extensive\ndata study with security domain knowledge. Our graph structural features are\nclosely tied to problem-space actions in the system provenance domain, which\nallows the detection results to be explained in descriptive, human language.\nPROVEXPLAINER allowed simple DT models to achieve 95% fidelity to the GNN on\nprogram classification tasks with general graph structural features, and 99%\nfidelity on malware detection tasks with a task-specific feature package\ntailored for direct interpretation. The explanations for malware classification\nare demonstrated with case studies of five real-world malware samples across\nthree malware families.",
        "translated": "基于神经网络(NN)的复杂模型的黑箱特性阻碍了它们在安全领域的广泛应用，因为它们的预测缺乏逻辑解释和可操作的后续行动。为了提高图形神经网络(GNN)安全模型在系统起源分析中的透明度和可靠性，提出了 PROVEXPLAINER 框架，该框架将抽象的 GNN 决策边界投影到可解释的特征空间中。我们首先使用更简单和可解释的模型(比如决策树)复制基于 GNN 的安全模型的决策过程。为了最大限度地提高代理模型的准确性和保真度，我们提出了基于经典图论的新的图结构特征，并利用安全领域的知识进行了广泛的数据研究。我们的图形结构特征与系统起源域中的问题空间操作紧密相关，这使得检测结果可以用描述性的人工语言来解释。PROVEXPLAINER 允许简单的 DT 模型在具有一般图形结构特征的程序分类任务上达到95% 的 GNN 保真度，在具有专门用于直接解释的任务特定功能包的恶意软件检测任务上达到99% 的保真度。通过对三个恶意软件家族的五个现实世界恶意软件样本的案例研究，说明了恶意软件分类的解释。"
    },
    {
        "title": "Better Private Linear Regression Through Better Private Feature\n  Selection",
        "url": "http://arxiv.org/abs/2306.00920v1",
        "pub_date": "2023-06-01",
        "summary": "Existing work on differentially private linear regression typically assumes\nthat end users can precisely set data bounds or algorithmic hyperparameters.\nEnd users often struggle to meet these requirements without directly examining\nthe data (and violating privacy). Recent work has attempted to develop\nsolutions that shift these burdens from users to algorithms, but they struggle\nto provide utility as the feature dimension grows. This work extends these\nalgorithms to higher-dimensional problems by introducing a differentially\nprivate feature selection method based on Kendall rank correlation. We prove a\nutility guarantee for the setting where features are normally distributed and\nconduct experiments across 25 datasets. We find that adding this private\nfeature selection step before regression significantly broadens the\napplicability of ``plug-and-play'' private linear regression algorithms at\nlittle additional cost to privacy, computation, or decision-making by the end\nuser.",
        "translated": "关于差别私有线性回归的现有工作通常假设终端用户可以精确设置数据界限或算法超参数。最终用户经常在不直接检查数据(并侵犯隐私)的情况下难以满足这些需求。最近的工作试图开发解决方案，将这些负担从用户转移到算法，但随着功能维度的增长，他们很难提供实用性。本文通过引入一种基于肯德尔秩相关的差分私有特征选择方法，将这些算法推广到高维问题。我们证明了一个实用的设置功能是正态分布的保证，并在25个数据集进行实验。我们发现，在回归之前添加这个私有特性选择步骤，可以显著扩大“即插即用”私有线性回归算法的适用性，而不会增加终端用户的隐私、计算或决策成本。"
    },
    {
        "title": "Robust Backdoor Attack with Visible, Semantic, Sample-Specific, and\n  Compatible Triggers",
        "url": "http://arxiv.org/abs/2306.00816v1",
        "pub_date": "2023-06-01",
        "summary": "Deep neural networks (DNNs) can be manipulated to exhibit specific behaviors\nwhen exposed to specific trigger patterns, without affecting their performance\non normal samples. This type of attack is known as a backdoor attack. Recent\nresearch has focused on designing invisible triggers for backdoor attacks to\nensure visual stealthiness. These triggers have demonstrated strong attack\nperformance even under backdoor defense, which aims to eliminate or suppress\nthe backdoor effect in the model. However, through experimental observations,\nwe have noticed that these carefully designed invisible triggers are often\nsusceptible to visual distortion during inference, such as Gaussian blurring or\nenvironmental variations in real-world scenarios. This phenomenon significantly\nundermines the effectiveness of attacks in practical applications.\nUnfortunately, this issue has not received sufficient attention and has not\nbeen thoroughly investigated. To address this limitation, we propose a novel\napproach called the Visible, Semantic, Sample-Specific, and Compatible trigger\n(VSSC-trigger), which leverages a recent powerful image method known as the\nstable diffusion model. In this approach, a text trigger is utilized as a\nprompt and combined with a benign image. The resulting combination is then\nprocessed by a pre-trained stable diffusion model, generating a corresponding\nsemantic object. This object is seamlessly integrated with the original image,\nresulting in a new realistic image, referred to as the poisoned image.\nExtensive experimental results and analysis validate the effectiveness and\nrobustness of our proposed attack method, even in the presence of visual\ndistortion. We believe that the new trigger proposed in this work, along with\nthe proposed idea to address the aforementioned issues, will have significant\nprospective implications for further advancements in this direction.",
        "translated": "深度神经网络(DNN)可以被操纵，当暴露于特定的触发模式时表现出特定的行为，而不会影响它们在正常样本上的表现。这种类型的攻击被称为后门攻击。最近的研究集中在为后门攻击设计隐形触发器，以确保视觉隐蔽性。这些触发器表现出强大的攻击性能，甚至在后门防御下，旨在消除或抑制后门效应的模型。然而，通过实验观察，我们已经注意到这些精心设计的不可见触发器在推理过程中往往容易受到视觉扭曲的影响，如高斯模糊或现实世界场景中的环境变化。这种现象极大地削弱了攻击在实际应用中的有效性。不幸的是，这个问题没有得到足够的重视，也没有得到彻底的调查。为了解决这一局限性，我们提出了一种新的方法，称为可见的，语义的，样本特定的，兼容的触发器(VSSC 触发器) ，它利用了最近的一个强大的图像方法称为稳定扩散模型。该方法利用文本触发器作为提示，并与良性图像相结合。然后由预先训练好的稳定扩散模型处理得到的组合，生成相应的语义对象。这个物体与原始图像无缝地结合在一起，产生了一个新的逼真的图像，称为中毒图像。大量的实验结果和分析验证了我们提出的攻击方法的有效性和鲁棒性，即使在存在视觉失真的情况下。我们认为，在这项工作中提出的新的触发点，以及处理上述问题的提议，将对朝这个方向取得进一步进展产生重大的预期影响。"
    },
    {
        "title": "SlothSpeech: Denial-of-service Attack Against Speech Recognition Models",
        "url": "http://arxiv.org/abs/2306.00794v1",
        "pub_date": "2023-06-01",
        "summary": "Deep Learning (DL) models have been popular nowadays to execute different\nspeech-related tasks, including automatic speech recognition (ASR). As ASR is\nbeing used in different real-time scenarios, it is important that the ASR model\nremains efficient against minor perturbations to the input. Hence, evaluating\nefficiency robustness of the ASR model is the need of the hour. We show that\npopular ASR models like Speech2Text model and Whisper model have dynamic\ncomputation based on different inputs, causing dynamic efficiency. In this\nwork, we propose SlothSpeech, a denial-of-service attack against ASR models,\nwhich exploits the dynamic behaviour of the model. SlothSpeech uses the\nprobability distribution of the output text tokens to generate perturbations to\nthe audio such that efficiency of the ASR model is decreased. We find that\nSlothSpeech generated inputs can increase the latency up to 40X times the\nlatency induced by benign input.",
        "translated": "深度学习(DL)模型是当今流行的用于执行各种语音相关任务的模型，其中包括自动语音识别(ASR)。由于 ASR 正在不同的实时场景中使用，因此 ASR 模型对于输入的微小扰动保持有效是非常重要的。因此，评估 ASR 模型的效率鲁棒性是当前的需要。我们表明，流行的 ASR 模型，如 Speech2Text 模型和 Whisper 模型，可以基于不同的输入进行动态计算，从而产生动态效率。在这项工作中，我们提出了懒人语音，一个反对 ASR 模型的分布式拒绝服务攻击，它利用了模型的动态行为。SlothLanguage 利用输出文本令牌的概率分布对音频产生扰动，从而降低了 ASR 模型的效率。我们发现懒语音产生的输入可以增加延迟高达40倍的良性输入所引起的延迟。"
    },
    {
        "title": "Impact of using a privacy model on smart buildings data for CO2\n  prediction",
        "url": "http://arxiv.org/abs/2306.00766v1",
        "pub_date": "2023-06-01",
        "summary": "There is a constant trade-off between the utility of the data collected and\nprocessed by the many systems forming the Internet of Things (IoT) revolution\nand the privacy concerns of the users living in the spaces hosting these\nsensors. Privacy models, such as the SITA (Spatial, Identity, Temporal, and\nActivity) model, can help address this trade-off. In this paper, we focus on\nthe problem of $CO_2$ prediction, which is crucial for health monitoring but\ncan be used to monitor occupancy, which might reveal some private information.\nWe apply a number of transformations on a real dataset from a Smart Building to\nsimulate different SITA configurations on the collected data. We use the\ntransformed data with multiple Machine Learning (ML) techniques to analyse the\nperformance of the models to predict $CO_{2}$ levels. Our results show that,\nfor different algorithms, different SITA configurations do not make one\nalgorithm perform better or worse than others, compared to the baseline data;\nalso, in our experiments, the temporal dimension was particularly sensitive,\nwith scores decreasing up to $18.9\\%$ between the original and the transformed\ndata. The results can be useful to show the effect of different levels of data\nprivacy on the data utility of IoT applications, and can also help to identify\nwhich parameters are more relevant for those systems so that higher privacy\nsettings can be adopted while data utility is still preserved.",
        "translated": "在构成物联网革命的许多系统所收集和处理的数据的效用和生活在这些传感器所在空间的用户的隐私问题之间有一个持续的权衡。隐私模型，比如 SITA (空间、身份、时间和活动)模型，可以帮助解决这种权衡。本文主要研究二氧化碳预测问题，这个问题对健康监测至关重要，但可以用来监测入住率，可能会泄露一些私人信息。我们应用一个智能建筑的实际数据集上的一些变换，以模拟不同的 SITA 配置上收集的数据。我们使用多机器学习(ML)技术的转换数据来分析模型的性能，以预测 $CO _ {2} $水平。我们的研究结果表明，对于不同的算法，与基线数据相比，不同的 SITA 配置不会使一种算法执行得更好或更差; 此外，在我们的实验中，时间维度特别敏感，原始数据和转换数据之间的分数下降高达18.9% $。研究结果有助于显示不同级别的数据隐私对物联网应用程序数据效用的影响，也有助于确定哪些参数与这些系统更相关，以便在保留数据效用的同时采用更高的隐私设置。"
    },
    {
        "title": "UNGOML: Automated Classification of unsafe Usages in Go",
        "url": "http://arxiv.org/abs/2306.00694v1",
        "pub_date": "2023-06-01",
        "summary": "The Go programming language offers strong protection from memory corruption.\nAs an escape hatch of these protections, it provides the unsafe package.\nPrevious studies identified that this unsafe package is frequently used in\nreal-world code for several purposes, e.g., serialization or casting types. Due\nto the variety of these reasons, it may be possible to refactor specific usages\nto avoid potential vulnerabilities. However, the classification of unsafe\nusages is challenging and requires the context of the call and the program's\nstructure. In this paper, we present the first automated classifier for unsafe\nusages in Go, UNGOML, to identify what is done with the unsafe package and why\nit is used. For UNGOML, we built four custom deep learning classifiers trained\non a manually labeled data set. We represent Go code as enriched control-flow\ngraphs (CFGs) and solve the label prediction task with one single-vertex and\nthree context-aware classifiers. All three context-aware classifiers achieve a\ntop-1 accuracy of more than 86% for both dimensions, WHAT and WHY. Furthermore,\nin a set-valued conformal prediction setting, we achieve accuracies of more\nthan 93% with mean label set sizes of 2 for both dimensions. Thus, UNGOML can\nbe used to efficiently filter unsafe usages for use cases such as refactoring\nor a security audit. UNGOML: https://github.com/stg-tud/ungoml Artifact:\nhttps://dx.doi.org/10.6084/m9.figshare.22293052",
        "translated": "Go 编程语言提供了强大的内存损坏保护。作为这些保护的逃生出口，它提供了不安全的包。以前的研究表明，这个不安全的包经常用于现实世界的代码中的几个目的，例如，序列化或强制转换类型。由于这些原因的多样性，有可能重构特定的用法以避免潜在的漏洞。然而，不安全使用的分类是具有挑战性的，需要调用的上下文和程序的结构。在本文中，我们提出了第一个自动分类器的不安全使用在围棋，UNGOML，以确定是什么做了不安全的包和为什么使用它。对于 UNGOML，我们建立了四个自定义深度学习分类器，这些分类器用手工标记的数据集进行训练。我们将 Go 代码表示为丰富的控制流图(CFG) ，并用一个单顶点分类器和三个上下文感知分类器解决标签预测任务。所有三个上下文感知分类器在“什么”和“为什么”这两个维度上都达到了86% 以上的最高准确率。此外，在一个集值保角预测集合中，当两个维度的平均标签集大小为2时，我们可以达到93% 以上的准确率。因此，可以使用 UNGOML 对重构或安全审计等用例的不安全使用进行有效过滤。Https://github.com/stg-tud/UNGOML 艺术品:  https://dx.doi.org/10.6084/m9.figshare.22293052"
    },
    {
        "title": "Adversarial Robustness in Unsupervised Machine Learning: A Systematic\n  Review",
        "url": "http://arxiv.org/abs/2306.00687v1",
        "pub_date": "2023-06-01",
        "summary": "As the adoption of machine learning models increases, ensuring robust models\nagainst adversarial attacks is increasingly important. With unsupervised\nmachine learning gaining more attention, ensuring it is robust against attacks\nis vital. This paper conducts a systematic literature review on the robustness\nof unsupervised learning, collecting 86 papers. Our results show that most\nresearch focuses on privacy attacks, which have effective defenses; however,\nmany attacks lack effective and general defensive measures. Based on the\nresults, we formulate a model on the properties of an attack on unsupervised\nlearning, contributing to future research by providing a model to use.",
        "translated": "随着机器学习模型的应用越来越广泛，确保模型对抗对手攻击变得越来越重要。随着非监督式学习受到越来越多的关注，确保它能够抵御攻击至关重要。本文对非监督式学习的稳健性进行了系统的文献回顾，共收集了86篇论文。我们的研究结果表明，大多数的研究集中在隐私攻击，有效的防御，然而，许多攻击缺乏有效的和一般的防御措施。根据研究结果，我们建立了一个攻击非监督式学习的特性模型，通过提供一个可以使用的模型，为未来的研究做出了贡献。"
    },
    {
        "title": "CRS-FL: Conditional Random Sampling for Communication-Efficient and\n  Privacy-Preserving Federated Learning",
        "url": "http://arxiv.org/abs/2306.00674v1",
        "pub_date": "2023-06-01",
        "summary": "Federated Learning (FL), a privacy-oriented distributed ML paradigm, is being\ngaining great interest in Internet of Things because of its capability to\nprotect participants data privacy. Studies have been conducted to address\nchallenges existing in standard FL, including communication efficiency and\nprivacy-preserving. But they cannot achieve the goal of making a tradeoff\nbetween communication efficiency and model accuracy while guaranteeing privacy.\nThis paper proposes a Conditional Random Sampling (CRS) method and implements\nit into the standard FL settings (CRS-FL) to tackle the above-mentioned\nchallenges. CRS explores a stochastic coefficient based on Poisson sampling to\nachieve a higher probability of obtaining zero-gradient unbiasedly, and then\ndecreases the communication overhead effectively without model accuracy\ndegradation. Moreover, we dig out the relaxation Local Differential Privacy\n(LDP) guarantee conditions of CRS theoretically. Extensive experiment results\nindicate that (1) in communication efficiency, CRS-FL performs better than the\nexisting methods in metric accuracy per transmission byte without model\naccuracy reduction in more than 7% sampling ratio (# sampling size / # model\nsize); (2) in privacy-preserving, CRS-FL achieves no accuracy reduction\ncompared with LDP baselines while holding the efficiency, even exceeding them\nin model accuracy under more sampling ratio conditions.",
        "translated": "联邦学习(FL)作为一种面向隐私的分布式机器学习模式，由于其保护参与者数据隐私的能力，正在引起物联网的极大兴趣。为了解决标准 FL 中存在的挑战，包括通信效率和隐私保护，已经进行了多项研究。但是他们不能在保证隐私的同时在通信效率和模型精确性之间取得平衡。本文提出了一种条件随机抽样(CRS)方法，并将其应用到标准 FL 设置(CRS-FL)中，以解决上述问题。CRS 探索一个基于泊松试验的随机系数，以获得更高的无偏获得零梯度的概率，然后在不降低模型精度的情况下有效地降低通信开销。此外，我们从理论上挖掘了 CRS 的松弛局部差分隐私(lDP)保证条件。广泛的实验结果表明: (1)在保持通信效率的同时，CRS-FL 在每传输字节的度量精度方面优于现有的方法，在采样率不降低模型精度7% 以上(# 采样大小/# 模型大小)的情况下; (2)在保护隐私方面，CRS-FL 与 LDP 基线相比，在保持通信效率的情况下，在模型精度方面优于现有方法，甚至在更多采样率条件下优于 LDP 基线。"
    },
    {
        "title": "Physical Attacks on the Railway System",
        "url": "http://arxiv.org/abs/2306.00623v1",
        "pub_date": "2023-06-01",
        "summary": "Recent attacks encouraged public interest in physical security for railways.\nKnowing about and learning from previous attacks is necessary to secure against\nthem. This paper presents a structured data set of physical attacks against\nrailways. We analyze the data regarding the used means, the railway system's\ntarget component, the attacker type, and the geographical distribution of\nattacks. The results indicate a growing heterogeneity of observed attacks in\nthe recent decade compared to the previous decades and centuries, making\nprotecting railways more complex.",
        "translated": "最近的袭击事件激发了公众对铁路实体安全的兴趣。了解并从以前的攻击中吸取教训是防范它们的必要条件。本文提出了一个针对铁路的物理攻击的结构化数据集。我们分析了有关使用的手段，铁路系统的目标组件，攻击者的类型和地理分布的攻击的数据。结果表明，与过去几十年和几个世纪相比，近十年来观察到的攻击日益多样化，使得保护铁路变得更加复杂。"
    },
    {
        "title": "Spying on the Spy: Security Analysis of Hidden Cameras",
        "url": "http://arxiv.org/abs/2306.00610v1",
        "pub_date": "2023-06-01",
        "summary": "Hidden cameras, also called spy cameras, are surveillance tools commonly used\nto spy on people without their knowledge. Whilst previous studies largely\nfocused on investigating the detection of such a camera and the privacy\nimplications, the security of the camera itself has received limited attention.\nCompared with ordinary IP cameras, spy cameras are normally sold in bulk at\ncheap prices and are ubiquitously deployed in hidden places within homes and\nworkplaces. A security compromise of these cameras can have severe\nconsequences. In this paper, we analyse a generic IP camera module, which has\nbeen packaged and re-branded for sale by several spy camera vendors. The module\nis controlled by mobile phone apps. By analysing the Android app and the\ntraffic data, we reverse-engineered the security design of the whole system,\nincluding the module's Linux OS environment, the file structure, the\nauthentication mechanism, the session management, and the communication with a\nremote server. Serious vulnerabilities have been identified in every component.\nCombined together, they allow an adversary to take complete control of a spy\ncamera from anywhere over the Internet, enabling arbitrary code execution. This\nis possible even if the camera is behind a firewall. All that an adversary\nneeds to launch an attack is the camera's serial number, which users sometimes\nunknowingly share in online reviews. We responsibly disclosed our findings to\nthe manufacturer. Whilst the manufacturer acknowledged our work, they showed no\nintention to fix the problems. Patching or recalling the affected cameras is\ninfeasible due to complexities in the supply chain. However, it is prudent to\nassume that bad actors have already been exploiting these flaws. We provide\ndetails of the identified vulnerabilities in order to raise public awareness,\nespecially on the grave danger of disclosing a spy camera's serial number.",
        "translated": "隐藏式摄像机，也称为间谍摄像机，是一种监视工具，通常用于在人们不知情的情况下监视他们。虽然以前的研究主要集中在调查这种相机的检测和隐私的影响，相机本身的安全受到有限的关注。与普通的 IP 摄像头相比，间谍摄像头通常以低廉的价格成批出售，并且无处不在地部署在家庭和工作场所的隐蔽地点。这些摄像头的安全隐患会造成严重后果。本文分析了一种通用的 IP 摄像头模块，该模块已被多家间谍摄像头厂商包装并重新命名销售。该模块由手机应用程序控制。通过对 Android 应用程序和流量数据的分析，对整个系统的安全设计进行了逆向工程，包括模块的 Linux 操作系统环境、文件结构、认证机制、会话管理以及与远程服务器的通信。每个部件都有严重的漏洞。结合在一起，他们允许对手从互联网上的任何地方完全控制间谍摄像头，使任意代码执行。即使摄像头位于防火墙之后，这也是可能的。对手发动攻击所需要的只是相机的序列号，用户有时会在不知情的情况下在网上评论中分享这些序列号。我们负责任地向制造商透露了我们的发现。虽然制造商承认我们的工作，但他们没有表现出解决问题的意图。由于供应链的复杂性，修补或召回受影响的相机是不可行的。然而，假设坏人已经在利用这些缺陷是谨慎的。我们提供已识别漏洞的细节，以提高公众意识，特别是对泄露间谍相机序列号的严重危险。"
    },
    {
        "title": "Harnessing large-language models to generate private synthetic text",
        "url": "http://arxiv.org/abs/2306.01684v1",
        "pub_date": "2023-06-02",
        "summary": "Differentially private (DP) training methods like DP-SGD can protect\nsensitive training data by ensuring that ML models will not reveal private\ninformation. An alternative approach, which this paper studies, is to use a\nsensitive dataset to generate a new synthetic dataset which is differentially\nprivate with respect to the original data. Doing so has several advantages:\nsynthetic data can be reused for other tasks (including for hyper parameter\ntuning), retained indefinitely, or shared with third parties without\nsacrificing privacy.\n  However, obtaining DP data is much harder than introducing DP during\ntraining. To make it feasible for text, recent work has utilized public data by\nstarting with a pre-trained generative language model and privately finetuning\nit on sensitive data. This model can be used to sample a DP synthetic dataset.\nWhile this strategy seems straightforward, executing it has proven problematic.\nPrevious approaches either show significant performance loss, or have, as we\nshow, critical design flaws.\n  In this paper we demonstrate that a proper training objective along with\ntuning fewer parameters results in excellent DP synthetic data quality. Our\napproach is competitive with direct DP-training of downstream classifiers in\nterms of performance on downstream tasks. We also demonstrate that our DP\nsynthetic data is not only useful for downstream classifier training, but also\nto tune those same models.",
        "translated": "差异私有(DP)训练方法，如 DP-SGD，可以保护敏感的训练数据，确保机器学习模型不会显示私人信息。本文研究的另一种方法是使用一个敏感数据集生成一个新的合成数据集，该数据集与原始数据相比具有差异私有性。这样做有几个好处: 合成数据可以重用于其他任务(包括超参数调优) ，可以无限期保留，或者在不牺牲隐私的情况下与第三方共享。然而，在训练过程中获取 DP 数据比引入 DP 要困难得多。为了使其适用于文本，最近的工作利用了公共数据，从预先训练的生成语言模型开始，并私下对敏感数据进行微调。此模型可用于对 DP 合成数据集进行采样。虽然这个策略看起来很简单，但是执行起来却有问题。以前的方法要么显示出显著的性能损失，要么如我们所示，存在严重的设计缺陷。本文证明了适当的训练目标和较少的参数调整可以获得优良的 DP 合成数据质量。在下游任务的性能方面，我们的方法与下游分类器的直接 DP 训练是有竞争力的。我们还证明了我们的 DP 合成数据不仅对于下游分类器的训练是有用的，而且对于调整这些相同的模型也是有用的。"
    },
    {
        "title": "Poisoning Network Flow Classifiers",
        "url": "http://arxiv.org/abs/2306.01655v1",
        "pub_date": "2023-06-02",
        "summary": "As machine learning (ML) classifiers increasingly oversee the automated\nmonitoring of network traffic, studying their resilience against adversarial\nattacks becomes critical. This paper focuses on poisoning attacks, specifically\nbackdoor attacks, against network traffic flow classifiers. We investigate the\nchallenging scenario of clean-label poisoning where the adversary's\ncapabilities are constrained to tampering only with the training data - without\nthe ability to arbitrarily modify the training labels or any other component of\nthe training process. We describe a trigger crafting strategy that leverages\nmodel interpretability techniques to generate trigger patterns that are\neffective even at very low poisoning rates. Finally, we design novel strategies\nto generate stealthy triggers, including an approach based on generative\nBayesian network models, with the goal of minimizing the conspicuousness of the\ntrigger, and thus making detection of an ongoing poisoning campaign more\nchallenging. Our findings provide significant insights into the feasibility of\npoisoning attacks on network traffic classifiers used in multiple scenarios,\nincluding detecting malicious communication and application classification.",
        "translated": "随着机器学习(ML)分类器越来越多地监督网络流量的自动化监控，研究它们对抗对手攻击的弹性变得至关重要。本文主要研究针对网络流量分类器的中毒攻击，特别是后门攻击。我们调查的具有挑战性的场景，清洁标签中毒，其中对手的能力被限制篡改只与训练数据-没有能力任意修改训练标签或任何其他组成部分的训练过程。我们描述了一种触发器制作策略，它利用模型可解释性技术来生成触发器模式，即使在非常低的中毒率下也是有效的。最后，我们设计了新的策略来产生隐秘的触发器，包括一种基于生成贝氏网路模型的方法，目标是最小化触发器的显著性，从而使正在进行的中毒活动的检测更具挑战性。我们的研究结果提供了重要的见解，中毒攻击的网络流量分类器使用在多个场景的可行性，包括检测恶意通信和应用程序分类。"
    },
    {
        "title": "Blockchain Model for Environment/Infrastructure Monitoring in\n  Cloud-Enabled High-Altitude Platform Systems",
        "url": "http://arxiv.org/abs/2306.01616v1",
        "pub_date": "2023-06-02",
        "summary": "The recently accentuated features of augmenting conventional wireless\nnetworks with high altitude platform systems (HAPS) have fueled a plethora of\napplications, which promise to offer new services to ground users, as well to\nenhance the efficiency and pervasion of existing applications. Cloud-enabled\nHAPS, which aims to create HAPS-based datacenters that offer cloud services to\nusers, has particularly emerged as a promising key enabler to provide\nlarge-scale equitable services from the sky. Although offering cloud services\nfrom the HAPS proves to be efficient, its practical deployment at the\nstratosphere level still faces many challenges such as high energy\nrequirements, physical maintenance, and is particularly prone to security\nconsiderations. Safeguarding the cloud-enabled HAPS against various\ncyberattacks is a necessity to guarantee its safe operation. This paper\nproposes a blockchain model to secure cloud-enabled HAPS networks that contain\na large number of HAPS stations from recurring cyberattacks within the context\nof the environment and infrastructure monitoring (EIM) application. To this\nend, the paper first presents a detailed blockchain framework, and describes\nthe ways of integrating the developed framework into the various system\ncomponents. We then discuss the details of the system implementation, including\nthe storing and consuming of cloud transactions, the generation of new blocks,\nand the blockchain consensus protocol that is tailored to the EIM requirements.\nFinally, we present numerical simulations that illustrate the performance of\nthe system in terms of throughput, latency, and resilience to attacks.",
        "translated": "最近强调的用高海拔平台系统(HAPS)增强传统无线网络的特点推动了大量应用，这些应用有望为地面用户提供新的服务，并提高现有应用的效率和普及程度。支持云计算的 HAPS，旨在创建基于 HAPS 的数据中心，为用户提供云服务，已经成为从天而降提供大规模公平服务的一个有前途的关键推动者。尽管 HAPS 提供的云服务被证明是有效的，但它在平流层的实际部署仍然面临许多挑战，如高能耗要求、物理维护，并且特别容易受到安全考虑的影响。保护云端 HAPS 免受各种网络攻击是保证其安全运行的必要条件。本文提出了一种区块链模型，以保护包含大量 HAPS 站点的云支持的 HAPS 网络在环境和基础设施监测(EIM)应用的背景下免受反复出现的网络攻击。为此，本文首先提出了一个详细的区块链框架，并描述了如何将开发的框架集成到各种系统组件中。然后，我们讨论系统实现的细节，包括云事务的存储和使用、新块的生成以及根据 EIM 需求量身定制的块链共识协议。最后，我们给出了数值模拟，说明了系统在吞吐量、延迟和抗攻击能力方面的性能。"
    },
    {
        "title": "Hyperparameter Learning under Data Poisoning: Analysis of the Influence\n  of Regularization via Multiobjective Bilevel Optimization",
        "url": "http://arxiv.org/abs/2306.01613v1",
        "pub_date": "2023-06-02",
        "summary": "Machine Learning (ML) algorithms are vulnerable to poisoning attacks, where a\nfraction of the training data is manipulated to deliberately degrade the\nalgorithms' performance. Optimal attacks can be formulated as bilevel\noptimization problems and help to assess their robustness in worst-case\nscenarios. We show that current approaches, which typically assume that\nhyperparameters remain constant, lead to an overly pessimistic view of the\nalgorithms' robustness and of the impact of regularization. We propose a novel\noptimal attack formulation that considers the effect of the attack on the\nhyperparameters and models the attack as a multiobjective bilevel optimization\nproblem. This allows to formulate optimal attacks, learn hyperparameters and\nevaluate robustness under worst-case conditions. We apply this attack\nformulation to several ML classifiers using $L_2$ and $L_1$ regularization. Our\nevaluation on multiple datasets confirms the limitations of previous strategies\nand evidences the benefits of using $L_2$ and $L_1$ regularization to dampen\nthe effect of poisoning attacks.",
        "translated": "机器学习(ML)算法容易受到中毒攻击，其中一小部分训练数据被操纵，故意降低算法的性能。最优攻击可以表述为两级优化问题，并帮助评估它们在最坏情况下的鲁棒性。我们表明，当前的方法，通常假设超参数保持不变，导致过度悲观的看法，算法的健壮性和正则化的影响。我们提出了一种新的最优攻击公式，该公式考虑了攻击对超参数的影响，并将攻击建模为一个多目标双层最佳化问题。这允许制定最佳攻击，学习超参数和评估最坏情况下的健壮性。我们使用 $L _ 2 $和 $L _ 1 $正则化将这种攻击公式应用于多个 ML 分类器。我们对多个数据集的评估证实了以往策略的局限性，并证明了使用 $L _ 2 $和 $L _ 1 $正则化抑制中毒攻击效果的好处。"
    },
    {
        "title": "PassGPT: Password Modeling and (Guided) Generation with Large Language\n  Models",
        "url": "http://arxiv.org/abs/2306.01545v1",
        "pub_date": "2023-06-02",
        "summary": "Large language models (LLMs) successfully model natural language from vast\namounts of text without the need for explicit supervision. In this paper, we\ninvestigate the efficacy of LLMs in modeling passwords. We present PassGPT, a\nLLM trained on password leaks for password generation. PassGPT outperforms\nexisting methods based on generative adversarial networks (GAN) by guessing\ntwice as many previously unseen passwords. Furthermore, we introduce the\nconcept of guided password generation, where we leverage PassGPT sampling\nprocedure to generate passwords matching arbitrary constraints, a feat lacking\nin current GAN-based strategies. Lastly, we conduct an in-depth analysis of the\nentropy and probability distribution that PassGPT defines over passwords and\ndiscuss their use in enhancing existing password strength estimators.",
        "translated": "大语言模型(LLM)成功地从大量的文本中建模自然语言，而不需要显式的监督。在本文中，我们研究了 LLM 建模密码的功效。我们介绍 PassGPT，它是一个受过密码泄漏培训的 LLM，用于生成密码。PassGPT 的性能优于现有的基于生成对抗网络(GAN)的方法，因为它可以猜测两倍以前未见过的密码。此外，我们还引入了引导密码生成的概念，利用 PassGPT 采样过程生成匹配任意约束的密码，这是目前基于 GAN 的策略所缺乏的。最后，我们对 PassGPT 定义的密码熵和概率分布进行了深入的分析，并讨论了它们在增强现有密码强度估计中的应用。"
    },
    {
        "title": "Guiding Text-to-Text Privatization by Syntax",
        "url": "http://arxiv.org/abs/2306.01471v1",
        "pub_date": "2023-06-02",
        "summary": "Metric Differential Privacy is a generalization of differential privacy\ntailored to address the unique challenges of text-to-text privatization. By\nadding noise to the representation of words in the geometric space of\nembeddings, words are replaced with words located in the proximity of the noisy\nrepresentation. Since embeddings are trained based on word co-occurrences, this\nmechanism ensures that substitutions stem from a common semantic context.\nWithout considering the grammatical category of words, however, this mechanism\ncannot guarantee that substitutions play similar syntactic roles. We analyze\nthe capability of text-to-text privatization to preserve the grammatical\ncategory of words after substitution and find that surrogate texts consist\nalmost exclusively of nouns. Lacking the capability to produce surrogate texts\nthat correlate with the structure of the sensitive texts, we encompass our\nanalysis by transforming the privatization step into a candidate selection\nproblem in which substitutions are directed to words with matching grammatical\nproperties. We demonstrate a substantial improvement in the performance of\ndownstream tasks by up to $4.66\\%$ while retaining comparative privacy\nguarantees.",
        "translated": "公制差分隐私是为了应对文本到文本私有化的独特挑战而量身定做的差分隐私的概括。通过在嵌入的几何空间中的词语表示中添加噪声，将词语替换为位于噪声表示附近的词语。由于嵌入是基于词的共现进行训练的，所以这种机制确保替换来自一个共同的语义上下文。然而，如果不考虑单词的语法范畴，这种机制不能保证替换能够发挥类似的句法作用。我们分析了文本到文本私有化的能力，以保存替换后的单词的语法范畴，并发现替代文本几乎完全由名词组成。由于缺乏产生与敏感文本结构相关的替代文本的能力，我们通过将私有化步骤转化为候选人选择问题来包含我们的分析，其中替换指向具有匹配语法属性的单词。我们证明了在保留相对隐私保障的情况下，下游任务的性能提高了4.66% 。"
    },
    {
        "title": "Network Agnostic MPC with Statistical Security",
        "url": "http://arxiv.org/abs/2306.01401v1",
        "pub_date": "2023-06-02",
        "summary": "We initiate the study of the network agnostic MPC protocols with statistical\nsecurity. Network agnostic protocols give the best possible security guarantees\nirrespective of the underlying network type. We consider the general-adversary\nmodel, where the adversary is characterized by an adversary structure which\nenumerates all possible candidate subsets of corrupt parties. The\n$\\mathcal{Q}^{(k)}$ condition enforces that the union of no $k$ subsets from\nthe adversary structure covers the party set. Given an unconditionally-secure\nPKI setup, known statistically-secure synchronous MPC protocols are secure\nagainst adversary structures satisfying the $\\mathcal{Q}^{(2)}$ condition.\nKnown statistically-secure asynchronous MPC protocols can tolerate\n$\\mathcal{Q}^{(3)}$ adversary structures. Fix a set of $n$ parties $\\mathcal{P}\n= \\{P_1, ... ,P_n\\}$ and adversary structures $\\mathcal{Z}_s$ and\n$\\mathcal{Z}_a$, satisfying the $\\mathcal{Q}^{(2)}$ and $\\mathcal{Q}^{(3)}$\nconditions respectively, where $\\mathcal{Z}_a \\subset \\mathcal{Z}_s$. Then,\ngiven an unconditionally-secure PKI, we ask whether it is possible to design a\nstatistically-secure MPC protocol resilient against $\\mathcal{Z}_s$ and\n$\\mathcal{Z}_a$ in a synchronous and an asynchronous network respectively if\nthe parties in $\\mathcal{P}$ are unaware of the network type. We show that it\nis possible iff $\\mathcal{Z}_s$ and $\\mathcal{Z}_a$ satisfy the\n$\\mathcal{Q}^{(2,1)}$ condition, meaning that the union of any two subsets from\n$\\mathcal{Z}_s$ and any one subset from $\\mathcal{Z}_a$ is a proper subset of\n$\\mathcal{P}$. We design several important network agnostic building blocks\nwith the $\\mathcal{Q}^{(2,1)}$ condition, such as Byzantine broadcast,\nByzantine agreement, information checking protocol, verifiable secret-sharing\nand secure multiplication protocol, whose complexity is polynomial in $n$ and\n$|\\mathcal{Z}_s|$.",
        "translated": "我们开始研究具有统计安全性的网络不可知 MPC 协议。无论底层网络类型如何，网络不可知协议都能提供最好的安全保证。我们考虑一般对手模型，在这个模型中，对手的对手拥有属性结构列举了腐败政党的所有可能的候选子集。$mathcal { Q } ^ {(k)} $条件强制要求对手结构中没有 $k $子集的并集覆盖方集。给定一个无条件安全的 PKI 设置，已知的统计安全的同步 MPC 协议可以安全地对抗满足 $数学{ Q } ^ {(2)} $条件的对手结构。已知的统计安全异步 MPC 协议可以容忍 $精确{ Q } ^ {(3)} $对手结构。修正了一组 $n $方 $数学{ P } = { P _ 1，... ，P _ n } $和对手结构 $数学{ Z } _ s $和 $数学{ Z } _ a $，分别满足 $数学{ Q } ^ {(2)} $和 $数学{ Q } ^ {(3)} $条件，其中 $数学{ Z } _ a 子集数学{ Z } _ s $。然后，给定一个无条件安全的 PKI，我们询问是否有可能设计一个统计安全的 MPC 协议，分别在同步网络和异步网络中分别对抗 $mathcal { Z } s $和 $mathcal { Z } a $，如果 $mathcal { P } $中的各方不知道网络类型。我们证明了 $mathal { Z } _ s $和 $mathal { Z } _ a $满足 $mathal { Q } ^ {(2,1)} $条件的可能性，这意味着 $mathal { Z } _ s $的任意两个子集和 $mathal { Z } _ a $的任意一个子集的并是 $mathal { P } $的一个真子集。设计了几个具有 $数学{ Q } ^ {(2,1)} $条件的重要网络不可知构件，如拜占庭广播、拜占庭协议、信息检查协议、可验证秘密共享和安全乘法协议，其复杂度为 $n $和 $| 数学{ Z } _ s | $。"
    },
    {
        "title": "Adaptive Attractors: A Defense Strategy against ML Adversarial Collusion\n  Attacks",
        "url": "http://arxiv.org/abs/2306.01400v1",
        "pub_date": "2023-06-02",
        "summary": "In the seller-buyer setting on machine learning models, the seller generates\ndifferent copies based on the original model and distributes them to different\nbuyers, such that adversarial samples generated on one buyer's copy would\nlikely not work on other copies. A known approach achieves this using\nattractor-based rewriter which injects different attractors to different\ncopies. This induces different adversarial regions in different copies, making\nadversarial samples generated on one copy not replicable on others. In this\npaper, we focus on a scenario where multiple malicious buyers collude to\nattack. We first give two formulations and conduct empirical studies to analyze\neffectiveness of collusion attack under different assumptions on the attacker's\ncapabilities and properties of the attractors. We observe that existing\nattractor-based methods do not effectively mislead the colluders in the sense\nthat adversarial samples found are influenced more by the original model\ninstead of the attractors as number of colluders increases. Based on this\nobservation, we propose using adaptive attractors whose weight is guided by a\nU-shape curve to cover the shortfalls. Experimentation results show that when\nusing our approach, the attack success rate of a collusion attack converges to\naround 15% even when lots of copies are applied for collusion. In contrast,\nwhen using the existing attractor-based rewriter with fixed weight, the attack\nsuccess rate increases linearly with the number of copies used for collusion.",
        "translated": "在机器学习模型中，卖方根据原始模型生成不同的副本，并将其分发给不同的买方，因此在一个买方副本上生成的对抗性样本可能不适用于其他副本。一个已知的方法实现这一点使用基于吸引子的重写器注入不同的吸引子不同的副本。这会在不同的拷贝中产生不同的对抗性区域，使得在一个拷贝上产生的对抗性样本不能在其他拷贝上复制。在本文中，我们关注的是多个恶意买家串通攻击的场景。我们首先给出了两个公式并进行了实证研究，分析了在不同的攻击者能力和吸引子性质的假设下，合谋攻击的有效性。我们观察到现有的基于吸引子的方法并不能有效地误导合谋者，因为随着合谋者数量的增加，发现的对手样本受原始模型的影响更大，而不是吸引子。在此基础上，我们提出了利用自适应吸引子的权重指导下的 U 形曲线来弥补这一不足。实验结果表明，当使用该方法时，即使使用大量拷贝进行合谋攻击，合谋攻击的成功率也会收敛到15% 左右。相比之下，当使用现有的具有固定权重的基于吸引子的重写器时，攻击成功率随着用于合谋的拷贝数的增加而线性增加。"
    },
    {
        "title": "Towards Robust GAN-generated Image Detection: a Multi-view Completion\n  Representation",
        "url": "http://arxiv.org/abs/2306.01364v1",
        "pub_date": "2023-06-02",
        "summary": "GAN-generated image detection now becomes the first line of defense against\nthe malicious uses of machine-synthesized image manipulations such as\ndeepfakes. Although some existing detectors work well in detecting clean, known\nGAN samples, their success is largely attributable to overfitting unstable\nfeatures such as frequency artifacts, which will cause failures when facing\nunknown GANs or perturbation attacks. To overcome the issue, we propose a\nrobust detection framework based on a novel multi-view image completion\nrepresentation. The framework first learns various view-to-image tasks to model\nthe diverse distributions of genuine images. Frequency-irrelevant features can\nbe represented from the distributional discrepancies characterized by the\ncompletion models, which are stable, generalized, and robust for detecting\nunknown fake patterns. Then, a multi-view classification is devised with\nelaborated intra- and inter-view learning strategies to enhance view-specific\nfeature representation and cross-view feature aggregation, respectively. We\nevaluated the generalization ability of our framework across six popular GANs\nat different resolutions and its robustness against a broad range of\nperturbation attacks. The results confirm our method's improved effectiveness,\ngeneralization, and robustness over various baselines.",
        "translated": "GAN 生成的图像检测现在成为对抗恶意使用机器合成图像操作(如深度伪造)的第一道防线。虽然现有的一些探测器在检测干净的已知 GAN 样本方面表现良好，但是它们的成功主要归功于过度拟合不稳定特征，例如频率伪影，这些特征在面对未知 GAN 或扰动攻击时会导致故障。为了克服这一问题，我们提出了一种基于多视图图像完成表示的鲁棒检测框架。该框架首先学习各种视图到图像的任务，以建模真实图像的不同分布。频率无关的特征可以通过完成模型的分布差异来表示，完成模型具有稳定性、广义性和鲁棒性，能够检测未知的虚假拥有属性。在此基础上，设计了多视图分类方法，并详细阐述了视图内和视图间的学习策略，分别增强了视图特征表示和跨视图特征聚合。我们评估了我们的框架在六个不同分辨率的流行 GAN 上的泛化能力，以及它对大范围扰动攻击的鲁棒性。实验结果证实了该方法在不同基线条件下的有效性、泛化性和鲁棒性得到了提高。"
    },
    {
        "title": "FedCIP: Federated Client Intellectual Property Protection with Traitor\n  Tracking",
        "url": "http://arxiv.org/abs/2306.01356v1",
        "pub_date": "2023-06-02",
        "summary": "Federated learning is an emerging privacy-preserving distributed machine\nlearning that enables multiple parties to collaboratively learn a shared model\nwhile keeping each party's data private. However, federated learning faces two\nmain problems: semi-honest server privacy inference attacks and malicious\nclient-side model theft. To address privacy inference attacks, parameter-based\nencrypted federated learning secure aggregation can be used. To address model\ntheft, a watermark-based intellectual property protection scheme can verify\nmodel ownership. Although watermark-based intellectual property protection\nschemes can help verify model ownership, they are not sufficient to address the\nissue of continuous model theft by uncaught malicious clients in federated\nlearning. Existing IP protection schemes that have the ability to track\ntraitors are also not compatible with federated learning security aggregation.\nThus, in this paper, we propose a Federated Client-side Intellectual Property\nProtection (FedCIP), which is compatible with federated learning security\naggregation and has the ability to track traitors. To the best of our\nknowledge, this is the first IP protection scheme in federated learning that is\ncompatible with secure aggregation and tracking capabilities.",
        "translated": "联合学习是一种新兴的保护隐私的分布式机器学习，它使多方能够协作学习共享模型，同时保持各方数据的私有性。然而，联邦学习面临两个主要问题: 半诚实的服务器隐私推理攻击和恶意的客户端模型盗窃。为了解决隐私推理攻击，可以使用基于参数的加密联邦学习安全聚合。为了解决模型盗窃问题，一种基于水印的知识产权保护方案可以验证模型所有权。尽管基于水印的知识产权保护方案可以帮助验证模型所有权，但它们不足以解决联邦学习中未被抓获的恶意客户端持续盗窃模型的问题。具有跟踪叛徒能力的现有 IP 保护方案也不兼容联邦学习安全聚合。因此，本文提出了一种联邦客户端知识产权保护协议(FedCIP) ，该协议兼容联邦学习安全集成，具有追踪叛徒的能力。据我们所知，这是联邦学习中第一个与安全聚合和跟踪能力兼容的 IP 保护方案。"
    },
    {
        "title": "Discriminative Adversarial Privacy: Balancing Accuracy and Membership\n  Privacy in Neural Networks",
        "url": "http://arxiv.org/abs/2306.03054v1",
        "pub_date": "2023-06-05",
        "summary": "The remarkable proliferation of deep learning across various industries has\nunderscored the importance of data privacy and security in AI pipelines. As the\nevolution of sophisticated Membership Inference Attacks (MIAs) threatens the\nsecrecy of individual-specific information used for training deep learning\nmodels, Differential Privacy (DP) raises as one of the most utilized techniques\nto protect models against malicious attacks. However, despite its proven\ntheoretical properties, DP can significantly hamper model performance and\nincrease training time, turning its use impractical in real-world scenarios.\nTackling this issue, we present Discriminative Adversarial Privacy (DAP), a\nnovel learning technique designed to address the limitations of DP by achieving\na balance between model performance, speed, and privacy. DAP relies on\nadversarial training based on a novel loss function able to minimise the\nprediction error while maximising the MIA's error. In addition, we introduce a\nnovel metric named Accuracy Over Privacy (AOP) to capture the\nperformance-privacy trade-off. Finally, to validate our claims, we compare DAP\nwith diverse DP scenarios, providing an analysis of the results from\nperformance, time, and privacy preservation perspectives.",
        "translated": "深度学习在各个行业的显著扩散突出了人工智能管道中数据隐私和安全的重要性。随着复杂的成员推理攻击(MIAs)的发展，威胁到用于训练深度学习模型的个人特定信息的保密性，差分隐私(DP)成为保护模型免受恶意攻击的最常用技术之一。然而，尽管 DP 的理论性能得到了证明，但它会显著地阻碍模型的性能并增加训练时间，使其在现实世界中的使用变得不切实际。针对这一问题，我们提出了一种新的学习技术——歧视性对抗性隐私(DAP) ，旨在通过实现模型性能、速度和隐私之间的平衡来解决 DP 的局限性。DAP 依赖于基于一种新的损失函数的对抗性训练，能够最小化预测误差，同时最大化 MIA 的误差。此外，我们还引入了一个新的度量标准，称为隐私精确度(AOP) ，以捕获性能-隐私权衡。最后，为了验证我们的主张，我们将 DAP 与不同的 DP 场景进行了比较，从性能、时间和隐私保护的角度对结果进行了分析。"
    },
    {
        "title": "Hiding in Plain Sight: Disguising Data Stealing Attacks in Federated\n  Learning",
        "url": "http://arxiv.org/abs/2306.03013v2",
        "pub_date": "2023-06-05",
        "summary": "Malicious server (MS) attacks have enabled the scaling of data stealing in\nfederated learning to large batch sizes and secure aggregation, settings\npreviously considered private. However, many concerns regarding client-side\ndetectability of MS attacks were raised, questioning their practicality once\nthey are publicly known. In this work, for the first time, we thoroughly study\nthe problem of client-side detectability.We demonstrate that most prior MS\nattacks, which fundamentally rely on one of two key principles, are detectable\nby principled client-side checks. Further, we formulate desiderata for\npractical MS attacks and propose SEER, a novel attack framework that satisfies\nall desiderata, while stealing user data from gradients of realistic networks,\neven for large batch sizes (up to 512 in our experiments) and under secure\naggregation. The key insight of SEER is the use of a secret decoder, which is\njointly trained with the shared model. Our work represents a promising first\nstep towards more principled treatment of MS attacks, paving the way for\nrealistic data stealing that can compromise user privacy in real-world\ndeployments.",
        "translated": "恶意服务器(MS)攻击使联邦学习中的数据窃取扩展到大批量和安全聚合，这些设置以前被认为是私有的。然而，许多关于 MS 攻击的客户端可检测性的担忧被提出，质疑它们的实用性，一旦它们被公开知道。在这项工作中，我们首次深入研究了客户端的可检测性问题。我们证明了大多数先前的 MS 攻击，基本上依赖于两个关键原则之一，是可以通过原则性的客户端检查来检测的。此外，我们为实际的 MS 攻击制定了急需数据，并提出了 SEER，一种满足所有急需数据的新型攻击框架，同时从现实网络的梯度中窃取用户数据，即使是在大批量(在我们的实验中高达512)和安全聚合的情况下。SEER 的关键见解是使用秘密解码器，该解码器与共享模型共同训练。我们的工作是朝着更有原则地处理 MS 攻击迈出的有希望的第一步，为在现实世界部署中可能损害用户隐私的现实数据窃取铺平了道路。"
    },
    {
        "title": "Evading Black-box Classifiers Without Breaking Eggs",
        "url": "http://arxiv.org/abs/2306.02895v1",
        "pub_date": "2023-06-05",
        "summary": "Decision-based evasion attacks repeatedly query a black-box classifier to\ngenerate adversarial examples. Prior work measures the cost of such attacks by\nthe total number of queries made to the classifier. We argue this metric is\nflawed. Most security-critical machine learning systems aim to weed out \"bad\"\ndata (e.g., malware, harmful content, etc). Queries to such systems carry a\nfundamentally asymmetric cost: queries detected as \"bad\" come at a higher cost\nbecause they trigger additional security filters, e.g., usage throttling or\naccount suspension. Yet, we find that existing decision-based attacks issue a\nlarge number of \"bad\" queries, which likely renders them ineffective against\nsecurity-critical systems. We then design new attacks that reduce the number of\nbad queries by $1.5$-$7.3\\times$, but often at a significant increase in total\n(non-bad) queries. We thus pose it as an open problem to build black-box\nattacks that are more effective under realistic cost metrics.",
        "translated": "基于决策的规避攻击通过重复查询黑盒分类器来生成对抗性例子。之前的工作通过对分类器的查询总数来度量这种攻击的成本。我们认为这个度量标准是有缺陷的。大多数安全性关键的机器学习系统旨在清除“坏”数据(例如，恶意软件、有害内容等)。对这种系统的查询有一个根本上不对称的成本: 被检测为“坏”的查询的成本更高，因为它们会触发额外的安全过滤器，例如，使用节流或帐户暂停。然而，我们发现现有的基于决策的攻击会发出大量“坏”查询，这可能使它们对安全关键系统无效。然后我们设计新的攻击，将坏查询的数量减少1.5美元-7.3美元乘以 $，但是总的(非坏的)查询数量通常会显著增加。因此，我们提出了一个开放的问题来构建在现实成本指标下更有效的黑盒攻击。"
    },
    {
        "title": "Modular zk-Rollup On-Demand",
        "url": "http://arxiv.org/abs/2306.02785v1",
        "pub_date": "2023-06-05",
        "summary": "The rapid expansion of the use of blockchain-based systems often leads to a\nchoice between customizable private blockchains and more secure, scalable and\ndecentralized but expensive public blockchains. This choice represents the\ntrade-off between privacy and customization at a low cost and security,\nscalability, and a large user base but at a high cost. In order to improve the\nscalability of secure public blockchains while enabling privacy and cost\nreduction, zk-rollups, a layer 2 solution, appear to be a promising avenue.\nThis paper explores the benefits of zk-rollups, including improved privacy, as\nwell as their potential to support transactions designed for specific\napplications. We propose an innovative design that allows multiple zk-rollups\nto co-exist on the same smart contracts, simplifying their creation and\ncustomization. We then evaluate the first implementation of our system\nhighlighting a low overhead on existing transaction types and on proof\ngeneration while strongly decreasing the cost of new transaction types and\ndrastically reducing zk-rollup creation costs.",
        "translated": "基于区块链系统使用的快速扩展通常导致在可定制的私人区块链和更安全、可扩展和分散但昂贵的公共区块链之间进行选择。这种选择代表了在低成本、安全性、可伸缩性和大用户基础(但成本较高)的隐私和定制之间的权衡。为了在保护隐私和降低成本的同时提高安全公共区块链的可伸缩性，第二层解决方案 zk-rollup 似乎是一个有前途的途径。本文探讨 zk-rollup 的好处，包括提高隐私性，以及它们支持为特定应用程序设计的事务的潜力。我们提出了一个创新的设计，允许多 zk-rollup 共存在相同的智能合同，简化他们的创建和定制。然后，我们评估了我们系统的第一个实现，突出了现有事务类型和证明生成的低开销，同时大大降低了新事务类型的成本，并大大降低了 zk-rolup 创建成本。"
    },
    {
        "title": "Federated Intrusion Detection System based on Deep Belief Networks",
        "url": "http://arxiv.org/abs/2306.02715v1",
        "pub_date": "2023-06-05",
        "summary": "The vast increase of IoT technologies and the ever-evolving attack vectors\nand threat actors have increased cyber-security risks dramatically. Novel\nattacks can compromise IoT devices to gain access to sensitive data or control\nthem to deploy further malicious activities. The detection of novel attacks\noften relies upon AI solutions. A common approach to implementing AI-based IDS\nin distributed IoT systems is in a centralised manner. However, this approach\nmay violate data privacy and secrecy. In addition, centralised data collection\nprohibits the scale-up of IDSs. Therefore, intrusion detection solutions in IoT\necosystems need to move towards a decentralised direction. FL has attracted\nsignificant interest in recent years due to its ability to perform\ncollaborative learning while preserving data confidentiality and locality.\nNevertheless, most FL-based IDS for IoT systems are designed under unrealistic\ndata distribution conditions. To that end, we design an experiment\nrepresentative of the real world and evaluate the performance of two FL IDS\nimplementations, one based on DNNs and another on our previous work on DBNs.\nFor our experiments, we rely on TON-IoT, a realistic IoT network traffic\ndataset, associating each IP address with a single FL client. Additionally, we\nexplore pre-training and investigate various aggregation methods to mitigate\nthe impact of data heterogeneity. Lastly, we benchmark our approach against a\ncentralised solution. The comparison shows that the heterogeneous nature of the\ndata has a considerable negative impact on the model performance when trained\nin a distributed manner. However, in the case of a pre-trained initial global\nFL model, we demonstrate a performance improvement of over 20% (F1-score) when\ncompared against a randomly initiated global model.",
        "translated": "物联网技术的飞速发展以及不断演变的攻击媒介和威胁行为者已经极大地增加了网络安全风险。新型的攻击可以危害物联网设备，以获得敏感数据的访问或控制它们部署进一步的恶意活动。新型攻击的检测常常依赖于人工智能的解决方案。在分布式物联网系统中实现基于人工智能的入侵检测系统的一种常见方法是采用集中方式。然而，这种方法可能会侵犯数据的隐私性和保密性。此外，中央数据收集禁止扩大入侵检测系统的规模。因此，物联网生态系统中的入侵检测解决方案需要朝着分散化的方向发展。由于能够在保护数据机密性和本地性的同时执行合作学习操作，FL 近年来引起了人们的极大兴趣。然而，大多数基于 FL 的物联网入侵检测系统都是在不切实际的数据分发条件下设计的。为此，我们设计了一个真实世界的实验代表，并评估了两个 FL IDS 实现的性能，一个基于 DNN，另一个基于我们以前在 DBN 上的工作。对于我们的实验，我们依赖 TON-IoT，一个真实的物联网流量数据集，将每个 IP 地址与一个 FL 客户端关联起来。此外，我们探索预训练和调查各种聚合方法，以减轻数据异构性的影响。最后，我们以集中解决方案为基准来衡量我们的方法。比较表明，当以分布式方式训练时，数据的异构性质对模型性能有相当大的负面影响。然而，在预先训练的初始全局 FL 模型的情况下，与随机启动的全局模型相比，我们证明了超过20% 的性能提高(F1分数)。"
    },
    {
        "title": "A Privacy-Preserving Federated Learning Approach for Kernel methods",
        "url": "http://arxiv.org/abs/2306.02677v1",
        "pub_date": "2023-06-05",
        "summary": "It is challenging to implement Kernel methods, if the data sources are\ndistributed and cannot be joined at a trusted third party for privacy reasons.\nIt is even more challenging, if the use case rules out privacy-preserving\napproaches that introduce noise. An example for such a use case is machine\nlearning on clinical data. To realize exact privacy preserving computation of\nkernel methods, we propose FLAKE, a Federated Learning Approach for KErnel\nmethods on horizontally distributed data. With FLAKE, the data sources mask\ntheir data so that a centralized instance can compute a Gram matrix without\ncompromising privacy. The Gram matrix allows to calculate many kernel matrices,\nwhich can be used to train kernel-based machine learning algorithms such as\nSupport Vector Machines. We prove that FLAKE prevents an adversary from\nlearning the input data or the number of input features under a semi-honest\nthreat model. Experiments on clinical and synthetic data confirm that FLAKE is\noutperforming the accuracy and efficiency of comparable methods. The time\nneeded to mask the data and to compute the Gram matrix is several orders of\nmagnitude less than the time a Support Vector Machine needs to be trained.\nThus, FLAKE can be applied to many use cases.",
        "translated": "如果数据源是分布式的，并且由于隐私原因不能在可信的第三方连接，那么实现内核方法就很有挑战性。如果用例排除了引入噪音的保护隐私的方法，那么就更具挑战性了。这种用例的一个例子是基于临床数据的机器学习。为了实现核方法的精确隐私保护计算，我们提出了一种水平分布数据核方法的联邦学习方法 FLAKE。在 FLAKE，数据源掩盖了他们的数据，这样一个集中的实例就可以在不损害隐私的情况下计算出一个 Gram 矩阵。Gram 矩阵允许计算许多核矩阵，可用于训练基于核的机器学习算法，如支持向量机。证明了在半诚实威胁模型下，FLAKE 可以防止对手学习输入数据或输入特征的个数。临床和合成数据的实验证实，FLAKE 的准确性和效率优于可比较的方法。掩盖数据和计算格拉姆矩阵所需的时间比训练数量级支持向量机所需的时间少了好几倍。因此，FLAKE 可以应用于许多用例。"
    },
    {
        "title": "Efficient Algorithms for Modeling SBoxes Using MILP",
        "url": "http://arxiv.org/abs/2306.02642v1",
        "pub_date": "2023-06-05",
        "summary": "Mixed Integer Linear Programming (MILP) is a well-known approach for the\ncryptanalysis of a symmetric cipher. A number of MILP-based security analyses\nhave been reported for non-linear (SBoxes) and linear layers. Researchers\nproposed word- and bit-wise SBox modeling techniques using a set of\ninequalities which helps in searching differential trails for a cipher. In this\npaper, we propose two new techniques to reduce the number of inequalities to\nrepresent the valid differential transitions for SBoxes. Our first technique\nchooses the best greedy solution with a random tiebreaker and achieves improved\nresults for the 4-bit SBoxes of MIBS, LBlock, and Serpent over the existing\nresults of Sun et al. [25]. Subset addition, our second approach, is an\nimprovement over the algorithm proposed by Boura and Coggia. Subset addition\ntechnique is faster than Boura and Coggia [10] and also improves the count of\ninequalities. Our algorithm emulates the existing results for the 4-bit SBoxes\nof Minalpher, LBlock, Serpent, Prince, and Rectangle. The subset addition\nmethod also works for 5-bit and 6-bit SBoxes. We improve the boundary of\nminimum number inequalities from the existing results for 5-bit SBoxes of ASCON\nand SC2000. Application of subset addition technique for 6-bit SBoxes of APN,\nFIDES, and SC2000 enhances the existing results. By applying multithreading, we\nreduced the execution time needed to find the minimum inequality set over the\nexisting techniques.",
        "translated": "混合整数线性规划(mILP)是一种著名的对称密码密码分析方法。针对非线性(SBox)和线性层，已经报道了许多基于 MILP 的安全性分析。研究人员提出了使用一组不等式的字和位 SBox 建模技术，这些不等式有助于搜索密码的差分路径。在本文中，我们提出了两个新的技巧，以减少不等式的数目来表示有效的微分过渡。我们的第一个技术使用随机抢七选择最好的贪婪解决方案，并且比 Sun 等人的现有结果更好地实现了 MIBS、 LBlock 和 Snake 的4位 SBox 的结果[25]。子集加法是我们的第二种方法，是对 Boura 和 Coggia 提出的算法的改进。子集加法比 Boura 和 Coggia [10]更快，也提高了不等式的计数。我们的算法模拟了 Minalpher、 LBlock、 Snake、 Prince 和矩形的4位 SBoxes 的现有结果。子集添加方法也适用于5位和6位 SBox。从 ASCON 和 SC2000的5位 SBox 的已有结果出发，改进了最小数不等式的边界。对 APN、 FIDES 和 SC2000等6位 SBox 的子集加入技术的应用增强了已有的结果。通过应用多线程，我们减少了在现有技术上寻找最小不等式集所需的执行时间。"
    },
    {
        "title": "Building Resilient SMEs: Harnessing Large Language Models for Cyber\n  Security in Australia",
        "url": "http://arxiv.org/abs/2306.02612v1",
        "pub_date": "2023-06-05",
        "summary": "The escalating digitalisation of our lives and enterprises has led to a\nparallel growth in the complexity and frequency of cyber-attacks. Small and\nmedium-sized enterprises (SMEs), particularly in Australia, are experiencing\nincreased vulnerability to cyber threats, posing a significant challenge to the\nnation's cyber security landscape. Embracing transformative technologies such\nas Artificial Intelligence (AI), Machine Learning (ML) and Large Language\nModels (LLMs) can potentially strengthen cyber security policies for Australian\nSMEs. However, their practical application, advantages, and limitations remain\nunderexplored, with prior research mainly focusing on large corporations. This\nstudy aims to address this gap by providing a comprehensive understanding of\nthe potential role of LLMs in enhancing cyber security policies for Australian\nSMEs. Employing a mixed-methods study design, this research includes a\nliterature review, qualitative analysis of SME case studies, and a quantitative\nassessment of LLM performance metrics in cyber security applications. The\nfindings highlight the promising potential of LLMs across various performance\ncriteria, including relevance, accuracy, and applicability, though gaps remain\nin areas such as completeness and clarity. The study underlines the importance\nof integrating human expertise with LLM technology and refining model\ndevelopment to address these limitations. By proposing a robust conceptual\nframework guiding the effective adoption of LLMs, this research aims to\ncontribute to a safer and more resilient cyber environment for Australian SMEs,\nenabling sustainable growth and competitiveness in the digital era.",
        "translated": "我们的生活和企业日益数字化，导致网络攻击的复杂性和频率也随之增加。中小型企业，尤其是澳大利亚的中小型企业，面对网络威胁的脆弱性正在增加，这对澳大利亚的网络安全形势构成了重大挑战。拥抱人工智能(AI)、机器学习(ML)和大语言模型(LLM)等变革性技术，可能会加强澳大利亚中小企业的网络安全政策。然而，它们的实际应用、优势和局限性仍然没有得到充分的探索，以前的研究主要集中在大公司。本研究旨在通过全面了解澳大利亚中小企业在加强网络安全政策方面的潜在作用，弥补这一差距。本研究采用混合研究方法设计，包括文献回顾、中小企业个案研究的定性分析、网络安全应用中 LLM 绩效指标的定量评估。研究结果强调了 LLM 在各种绩效标准(包括相关性、准确性和适用性)方面的潜力，尽管在完整性和清晰度等方面仍然存在差距。该研究强调了将人类专业知识与 LLM 技术相结合并精炼模型开发以解决这些局限性的重要性。本研究旨在透过建议一套稳健的概念框架，指导澳洲中小企业有效采用 LLM 模式，为澳洲中小企业缔造一个更安全及更具弹性的网络环境，促进数码时代的持续增长及竞争力。"
    },
    {
        "title": "Jammer classification with Federated Learning",
        "url": "http://arxiv.org/abs/2306.02587v1",
        "pub_date": "2023-06-05",
        "summary": "Jamming signals can jeopardize the operation of GNSS receivers until denying\nits operation. Given their ubiquity, jamming mitigation and localization\ntechniques are of crucial importance, for which jammer classification is of\nhelp. Data-driven models have been proven useful in detecting these threats,\nwhile their training using crowdsourced data still poses challenges when it\ncomes to private data sharing. This article investigates the use of federated\nlearning to train jamming signal classifiers locally on each device, with model\nupdates aggregated and averaged at the central server. This allows for\nprivacy-preserving training procedures that do not require centralized data\nstorage or access to client local data. The used framework FedAvg is assessed\non a dataset consisting of spectrogram images of simulated interfered GNSS\nsignal. Six different jammer types are effectively classified with comparable\nresults to a fully centralized solution that requires vast amounts of data\ncommunication and involves privacy-preserving concerns.",
        "translated": "干扰信号可以危及 GNSS 接收机的运行，直到拒绝其运行。鉴于干扰的普遍性，干扰抑制和定位技术具有非常重要的意义，干扰分类对此有着重要的帮助。事实证明，数据驱动模型在检测这些威胁方面非常有用，而使用众包数据的培训在私人数据共享方面仍然构成挑战。本文研究了使用联邦学习在每个设备上本地训练干扰信号分类器，在中央服务器上汇总和平均模型更新。这使得保护隐私的培训过程不需要集中的数据存储或访问客户端本地数据。所使用的框架 FedAvg 是在一个由模拟干扰 GNSS 信号的光谱图像组成的数据集上进行评估的。六种不同的干扰机类型被有效地分类，其结果与需要大量数据通信并涉及隐私保护问题的完全集中的解决方案相当。"
    },
    {
        "title": "Large-Scale Distributed Learning via Private On-Device\n  Locality-Sensitive Hashing",
        "url": "http://arxiv.org/abs/2306.02563v1",
        "pub_date": "2023-06-05",
        "summary": "Locality-sensitive hashing (LSH) based frameworks have been used efficiently\nto select weight vectors in a dense hidden layer with high cosine similarity to\nan input, enabling dynamic pruning. While this type of scheme has been shown to\nimprove computational training efficiency, existing algorithms require repeated\nrandomized projection of the full layer weight, which is impractical for\ncomputational- and memory-constrained devices. In a distributed setting,\ndeferring LSH analysis to a centralized host is (i) slow if the device cluster\nis large and (ii) requires access to input data which is forbidden in a\nfederated context. Using a new family of hash functions, we develop one of the\nfirst private, personalized, and memory-efficient on-device LSH frameworks. Our\nframework enables privacy and personalization by allowing each device to\ngenerate hash tables, without the help of a central host, using device-specific\nhashing hyper-parameters (e.g. number of hash tables or hash length). Hash\ntables are generated with a compressed set of the full weights, and can be\nserially generated and discarded if the process is memory-intensive. This\nallows devices to avoid maintaining (i) the fully-sized model and (ii) large\namounts of hash tables in local memory for LSH analysis. We prove several\nstatistical and sensitivity properties of our hash functions, and\nexperimentally demonstrate that our framework is competitive in training\nlarge-scale recommender networks compared to other LSH frameworks which assume\nunrestricted on-device capacity.",
        "translated": "基于局部性敏感哈希(lSH)的框架已被有效地用于选择密集隐藏层中的权重向量，该层对输入具有较高的余弦距离，从而实现了动态修剪。虽然这种类型的方案已被证明可以提高计算训练效率，但现有的算法需要重复随机投影的全层权重，这是不切实际的计算和内存约束的设备。在分布式设置中，如果设备集群很大，将 LSH 分析延迟到集中式主机会很慢，并且(ii)需要访问联邦上下文中禁止的输入数据。使用一个新的散列函数家族，我们开发了第一个私有的、个性化的、内存高效的设备上 LSH 框架。我们的框架允许每个设备在没有中央主机的帮助下，使用特定设备的哈希超参数(例如哈希表的数量或哈希长度)生成哈希表，从而实现了隐私和个性化。哈希表是用一组压缩的全权值生成的，如果进程占用大量内存，则可以连续生成和丢弃哈希表。这允许设备避免维护(i)完全大小的模型和(ii)本地内存中用于 LSH 分析的大量哈希表。我们证明了我们的哈希函数的一些统计和灵敏度特性，并且实验证明了我们的框架在训练大规模的推荐网络相比，其他 LSH 框架假设无限制的在设备上的能力是有竞争力的。"
    },
    {
        "title": "Interest-disclosing Mechanisms for Advertising are Privacy-Exposing (not\n  Preserving)",
        "url": "http://arxiv.org/abs/2306.03825v1",
        "pub_date": "2023-06-06",
        "summary": "Today, targeted online advertising relies on unique identifiers assigned to\nusers through third-party cookies--a practice at odds with user privacy. While\nthe web and advertising communities have proposed interest-disclosing\nmechanisms, including Google's Topics API, as solutions, an independent\nanalysis of these proposals in realistic scenarios has yet to be performed. In\nthis paper, we attempt to validate the privacy (i.e., preventing unique\nidentification) and utility (i.e., enabling ad targeting) claims of Google's\nTopics proposal in the context of realistic user behavior. Through new\nstatistical models of the distribution of user behaviors and resulting\ntargeting topics, we analyze the capabilities of malicious advertisers\nobserving users over time and colluding with other third parties. Our analysis\nshows that even in the best case, individual users' identification across sites\nis possible, as 0.4% of the 250k users we simulate are re-identified. These\nguarantees weaken further over time and when advertisers collude: 57% of users\nare uniquely re-identified after 15 weeks of browsing, increasing to 75% after\n30 weeks. While measuring that the Topics API provides moderate utility, we\nalso find that advertisers and publishers can abuse the Topics API to\npotentially assign unique identifiers to users, defeating the desired privacy\nguarantees. As a result, the inherent diversity of users' interests on the web\nis directly at odds with the privacy objectives of interest-disclosing\nmechanisms; we discuss how any replacement of third-party cookies may have to\nseek other avenues to achieve privacy for the web.",
        "translated": "今天，有针对性的在线广告依赖于通过第三方 cookie 分配给用户的唯一标识符——这种做法与用户隐私不符。虽然网络和广告社区已经提出了利益披露机制，包括谷歌的主题 API，作为解决方案，这些建议在现实情况下的独立分析尚未执行。在本文中，我们试图验证的隐私(即，防止唯一的识别)和实用程序(即，启用广告定位)的声明，谷歌的主题的建议在现实的用户行为的背景下。通过新的用户行为分布统计模型和由此产生的目标话题，我们分析了恶意广告商随时间观察用户和与其他第三方合谋的能力。我们的分析表明，即使在最好的情况下，个人用户的识别跨网站是可能的，因为0.4% 的25万用户我们模拟重新识别。随着时间的推移和广告商的串通，这些保证会进一步削弱: 57% 的用户在浏览15周后被唯一地重新识别，30周后增加到75% 。虽然测量主题 API 提供适度的实用性，我们也发现广告商和发布者可以滥用主题 API 来潜在地为用户分配唯一的标识符，破坏所需的隐私保障。因此，用户在网络上兴趣的内在多样性与利益披露机制的隐私目标直接相悖; 我们讨论任何第三方 cookie 的替代品可能必须寻找其他途径来实现网络的隐私。"
    },
    {
        "title": "A Novel Approach To User Agent String Parsing For Vulnerability Analysis\n  Using Mutli-Headed Attention",
        "url": "http://arxiv.org/abs/2306.03733v1",
        "pub_date": "2023-06-06",
        "summary": "The increasing reliance on the internet has led to the proliferation of a\ndiverse set of web-browsers and operating systems (OSs) capable of browsing the\nweb. User agent strings (UASs) are a component of web browsing that are\ntransmitted with every Hypertext Transfer Protocol (HTTP) request. They contain\ninformation about the client device and software, which is used by web servers\nfor various purposes such as content negotiation and security. However, due to\nthe proliferation of various browsers and devices, parsing UASs is a\nnon-trivial task due to a lack of standardization of UAS formats. Current\nrules-based approaches are often brittle and can fail when encountering such\nnon-standard formats. In this work, a novel methodology for parsing UASs using\nMulti-Headed Attention Based transformers is proposed. The proposed methodology\nexhibits strong performance in parsing a variety of UASs with differing\nformats. Furthermore, a framework to utilize parsed UASs to estimate the\nvulnerability scores for large sections of publicly visible IT networks or\nregions is also discussed. The methodology present here can also be easily\nextended or deployed for real-time parsing of logs in enterprise settings.",
        "translated": "对互联网的日益依赖导致了一系列不同的网络浏览器和操作系统(OS)能够浏览网页的激增。用户代理字符串(User agent string，UAS)是网页浏览的一个组成部分，每个超文本传输协议(HTTP)请求都会传输这些字符串。它们包含有关客户端设备和软件的信息，这些信息被 Web 服务器用于各种目的，例如内容协商和安全性。然而，由于各种浏览器和设备的普及，由于 UAS 格式缺乏标准化，解析 UAS 是一项非常重要的任务。当前基于规则的方法往往是脆弱的，在遇到这种非标准格式时可能会失败。提出了一种基于多头注意变换的无人机分析方法。提议的方法在解析不同格式的各种无人机系统方面表现出很强的性能。此外，还讨论了一个框架，利用已解析的 UAS 来估计大部分公开可见的 IT 网络或区域的脆弱性分数。这里提供的方法也可以很容易地扩展或部署，以便在企业设置中实时解析日志。"
    },
    {
        "title": "Exploring Model Dynamics for Accumulative Poisoning Discovery",
        "url": "http://arxiv.org/abs/2306.03726v1",
        "pub_date": "2023-06-06",
        "summary": "Adversarial poisoning attacks pose huge threats to various machine learning\napplications. Especially, the recent accumulative poisoning attacks show that\nit is possible to achieve irreparable harm on models via a sequence of\nimperceptible attacks followed by a trigger batch. Due to the limited\ndata-level discrepancy in real-time data streaming, current defensive methods\nare indiscriminate in handling the poison and clean samples. In this paper, we\ndive into the perspective of model dynamics and propose a novel information\nmeasure, namely, Memorization Discrepancy, to explore the defense via the\nmodel-level information. By implicitly transferring the changes in the data\nmanipulation to that in the model outputs, Memorization Discrepancy can\ndiscover the imperceptible poison samples based on their distinct dynamics from\nthe clean samples. We thoroughly explore its properties and propose\nDiscrepancy-aware Sample Correction (DSC) to defend against accumulative\npoisoning attacks. Extensive experiments comprehensively characterized\nMemorization Discrepancy and verified its effectiveness. The code is publicly\navailable at: https://github.com/tmlr-group/Memorization-Discrepancy.",
        "translated": "对抗性中毒攻击对各种机器学习应用构成了巨大的威胁。特别是，最近的累积中毒攻击表明，有可能通过一系列不易察觉的攻击，然后是触发批次，对模型造成不可弥补的伤害。由于实时数据流中有限的数据级别差异，当前的防御方法在处理毒物和干净样本时是不加区分的。本文从模型动力学的角度出发，提出了一种新的信息度量方法——记忆差异，通过模型层次的信息来探索防御策略。通过隐式地将数据操作中的变化传递给模型输出中的变化，记忆差异可以基于其与干净样本不同的动态发现不可察觉的有毒样本。我们深入探讨了它的性质，并提出了差异感知样本修正(DSC)来防范累积中毒攻击。广泛的实验全面描述了记忆差异，并验证了其有效性。该守则可于以下 https://github.com/tmlr-group/memorization-discrepancy 公开索取:。"
    },
    {
        "title": "Effective Intrusion Detection in Highly Imbalanced IoT Networks with\n  Lightweight S2CGAN-IDS",
        "url": "http://arxiv.org/abs/2306.03707v1",
        "pub_date": "2023-06-06",
        "summary": "Since the advent of the Internet of Things (IoT), exchanging vast amounts of\ninformation has increased the number of security threats in networks. As a\nresult, intrusion detection based on deep learning (DL) has been developed to\nachieve high throughput and high precision. Unlike general deep learning-based\nscenarios, IoT networks contain benign traffic far more than abnormal traffic,\nwith some rare attacks. However, most existing studies have been focused on\nsacrificing the detection rate of the majority class in order to improve the\ndetection rate of the minority class in class-imbalanced IoT networks. Although\nthis way can reduce the false negative rate of minority classes, it both wastes\nresources and reduces the credibility of the intrusion detection systems. To\naddress this issue, we propose a lightweight framework named S2CGAN-IDS. The\nproposed framework leverages the distribution characteristics of network\ntraffic to expand the number of minority categories in both data space and\nfeature space, resulting in a substantial increase in the detection rate of\nminority categories while simultaneously ensuring the detection precision of\nmajority categories. To reduce the impact of sparsity on the experiments, the\nCICIDS2017 numeric dataset is utilized to demonstrate the effectiveness of the\nproposed method. The experimental results indicate that our proposed approach\noutperforms the superior method in both Precision and Recall, particularly with\na 10.2% improvement in the F1-score.",
        "translated": "自从物联网(IoT)出现以来，大量的信息交换增加了网络安全威胁的数量。因此，基于深度学习(DL)的入侵检测技术得到了发展，以实现高吞吐量和高精度的入侵检测。与一般的基于深度学习的场景不同，物联网包含的良性流量远远多于异常流量，还有一些罕见的攻击。然而，为了提高类不平衡物联网中少数类的检测率，现有的研究大多集中在牺牲多数类的检测率上。虽然这种方法可以降低少数类的假阴性率，但是它不仅浪费资源，而且降低了入侵检测系统的可信度。为了解决这个问题，我们提出了一个轻量级框架 S2CGAN-IDS。拟议框架利用网络流量的分布特点，扩大了数据空间和特征空间中少数群体类别的数量，从而大幅度提高了少数群体类别的检测率，同时确保了多数群体类别的检测精度。为了减少稀疏性对实验的影响，利用 CICIDS2017数值数据集验证了该方法的有效性。实验结果表明，我们提出的方法在两个准确率召回率都优于优秀的方法，特别是在 f1分数提高了10.2% 的情况下。"
    },
    {
        "title": "Human-imperceptible, Machine-recognizable Images",
        "url": "http://arxiv.org/abs/2306.03679v1",
        "pub_date": "2023-06-06",
        "summary": "Massive human-related data is collected to train neural networks for computer\nvision tasks. A major conflict is exposed relating to software engineers\nbetween better developing AI systems and distancing from the sensitive training\ndata. To reconcile this conflict, this paper proposes an efficient\nprivacy-preserving learning paradigm, where images are first encrypted to\nbecome ``human-imperceptible, machine-recognizable'' via one of the two\nencryption strategies: (1) random shuffling to a set of equally-sized patches\nand (2) mixing-up sub-patches of the images. Then, minimal adaptations are made\nto vision transformer to enable it to learn on the encrypted images for vision\ntasks, including image classification and object detection. Extensive\nexperiments on ImageNet and COCO show that the proposed paradigm achieves\ncomparable accuracy with the competitive methods. Decrypting the encrypted\nimages requires solving an NP-hard jigsaw puzzle or an ill-posed inverse\nproblem, which is empirically shown intractable to be recovered by various\nattackers, including the powerful vision transformer-based attacker. We thus\nshow that the proposed paradigm can ensure the encrypted images have become\nhuman-imperceptible while preserving machine-recognizable information. The code\nis available at \\url{https://github.com/FushengHao/PrivacyPreservingML.}",
        "translated": "大量的人类相关数据被收集来训练计算机视觉任务的神经网络。软件工程师在更好地开发人工智能系统和远离敏感的培训数据之间暴露出一个重大的冲突。为了调和这种冲突，本文提出了一种有效的保护隐私的学习范式，首先通过两种加密策略中的一种将图像加密成“人类感知不到的，机器可识别的”: (1)随机移动到一组大小相等的补丁和(2)混合图像的子补丁。然后，对视觉转换器进行最小限度的适应，使其能够在加密图像上学习视觉任务，包括图像分类和目标检测。在 ImageNet 和 COCO 上的大量实验表明，本文提出的方法与竞争方法具有可比的精度。加密图像的解密需要解决一个 NP 难题或一个病态逆问题，经验表明，这是难以恢复的各种攻击者，包括强大的视觉变压器为基础的攻击者。结果表明，该算法能够在保留机器可识别信息的同时，保证加密后的图像不被人类察觉。该代码可在 url { https://github.com/fushenghao/privacypreservingml 获得。}"
    },
    {
        "title": "TALUS: Reinforcing TEE Confidentiality with Cryptographic Coprocessors\n  (Technical Report)",
        "url": "http://arxiv.org/abs/2306.03643v1",
        "pub_date": "2023-06-06",
        "summary": "Platforms are nowadays typically equipped with tristed execution environments\n(TEES), such as Intel SGX and ARM TrustZone. However, recent microarchitectural\nattacks on TEEs repeatedly broke their confidentiality guarantees, including\nthe leakage of long-term cryptographic secrets. These systems are typically\nalso equipped with a cryptographic coprocessor, such as a TPM or Google Titan.\nThese coprocessors offer a unique set of security features focused on\nsafeguarding cryptographic secrets. Still, despite their simultaneous\navailability, the integration between these technologies is practically\nnonexistent, which prevents them from benefitting from each other's strengths.\nIn this paper, we propose TALUS, a general design and a set of three main\nrequirements for a secure symbiosis between TEEs and cryptographic\ncoprocessors. We implement a proof-of-concept of TALUS based on Intel SGX and a\nhardware TPM. We show that with TALUS, the long-term secrets used in the SGX\nlife cycle can be moved to the TPM. We demonstrate that our design is robust\neven in the presence of transient execution attacks, preventing an entire class\nof attacks due to the reduced attack surface on the shared hardware.",
        "translated": "现在的平台通常都配备了三级执行环境(TEES) ，比如 Intel SGX 和 ARM TrustZone。然而，最近对 TEE 的微架构攻击屡次打破了它们的保密保障，包括泄露长期的加密机密。这些系统通常还配备了加密协处理器，如 TPM 或 Google Titan。这些协处理器提供了一组独特的安全特性，专注于保护加密秘密。然而，尽管它们同时可用，但这些技术之间的集成实际上是不存在的，这使它们无法从彼此的优势中受益。在本文中，我们提出 TALUS，一个通用的设计和一组三个主要要求的安全共生之间的 TEE 和密码协处理器。我们实现了一个基于 Intel SGX 和硬件 TPM 的 TALUS 概念验证。我们展示了使用 TALUS，在新交所生命周期中使用的长期秘密可以移动到 TPM。我们证明了我们的设计即使在存在瞬态执行攻击的情况下也是健壮的，可以防止由于共享硬件上的攻击面减少而导致的整个类别的攻击。"
    },
    {
        "title": "mdTLS: How to Make middlebox-aware TLS more efficient?",
        "url": "http://arxiv.org/abs/2306.03573v1",
        "pub_date": "2023-06-06",
        "summary": "The more data transmission over TLS protocol becomes increasingly common in\nIT Systems, the more middleboxes are deployed in networks. These middleboxes\nhave several advantages, however, they become the target of cyber-attacks. Many\nresearchers proposed revised versions of TLS protocols to make them secure,\nhowever, their approaches had some limitations. In this paper, we propose a\nmiddlebox-delegated TLS (mdTLS) protocol to improve performance based on the\nmiddlebox-aware TLS (maTLS), one of the most secure TLS protocols. We found out\nthat the computational complexity of mdTLS is about twice as low as that of\nmaTLS. Furthermore, we formally verified that our proposal meets newly defined\nsecurity goals as well as those verified by maTLS. All of the formal models and\nlemmas are open to the public through following url\nhttps://github.com/HackProof/mdTLS.",
        "translated": "在 IT 系统中，TLS 协议的数据传输越多，网络中部署的中间盒就越多。这些中间盒有几个优点，然而，他们成为网络攻击的目标。许多研究人员提出了 TLS 协议的修订版本，以使它们更加安全，然而，他们的方法有一些局限性。本文基于最安全的 TLS 协议之一——中间盒感知 TLS (maTLS) ，提出了一种中间盒委托 TLS (mdTLS)协议来提高性能。我们发现 mdTLS 的计算复杂度大约是 maTLS 的两倍。此外，我们正式验证了我们的提案满足新定义的安全目标以及那些由 maTLS 验证的目标。所有的正式模型和引理都是通过下面的 url  https://github.com/hackproof/mdtls 向公众开放的。"
    },
    {
        "title": "Machine Unlearning: A Survey",
        "url": "http://arxiv.org/abs/2306.03558v1",
        "pub_date": "2023-06-06",
        "summary": "Machine learning has attracted widespread attention and evolved into an\nenabling technology for a wide range of highly successful applications, such as\nintelligent computer vision, speech recognition, medical diagnosis, and more.\nYet a special need has arisen where, due to privacy, usability, and/or the\nright to be forgotten, information about some specific samples needs to be\nremoved from a model, called machine unlearning. This emerging technology has\ndrawn significant interest from both academics and industry due to its\ninnovation and practicality. At the same time, this ambitious problem has led\nto numerous research efforts aimed at confronting its challenges. To the best\nof our knowledge, no study has analyzed this complex topic or compared the\nfeasibility of existing unlearning solutions in different kinds of scenarios.\nAccordingly, with this survey, we aim to capture the key concepts of unlearning\ntechniques. The existing solutions are classified and summarized based on their\ncharacteristics within an up-to-date and comprehensive review of each\ncategory's advantages and limitations. The survey concludes by highlighting\nsome of the outstanding issues with unlearning techniques, along with some\nfeasible directions for new research opportunities.",
        "translated": "机器学习已经引起了人们的广泛关注，并逐渐发展成为一种能够应用于各种非常成功的应用领域的技术，例如智能计算机视觉、语音识别、医疗诊断等等。然而，由于隐私、可用性和/或被遗忘的权利，一种特殊的需求已经出现，关于某些特定样本的信息需要从一个模型中删除，这种模型称为机器去学习。这种新兴技术因其创新性和实用性而引起了学术界和工业界的极大兴趣。与此同时，这个雄心勃勃的问题已经导致了许多旨在应对其挑战的研究工作。据我们所知，还没有研究分析过这个复杂的主题，也没有比较过现有的不学习解决方案在不同情景下的可行性。因此，通过这个调查，我们的目标是捕捉忘记技术的关键概念。现有的解决方案根据其特点进行了分类和总结，并对每个类别的优点和局限性进行了最新和全面的审查。该调查最后强调了一些突出的问题与忘却技术，以及一些新的研究机会的可行方向。"
    },
    {
        "title": "A Practical Framework for Storing and Searching Encrypted Data on Cloud\n  Storage",
        "url": "http://arxiv.org/abs/2306.03547v1",
        "pub_date": "2023-06-06",
        "summary": "Security has become a significant concern with the increased popularity of\ncloud storage services. It comes with the vulnerability of being accessed by\nthird parties. Security is one of the major hurdles in the cloud server for the\nuser when the user data that reside in local storage is outsourced to the\ncloud. It has given rise to security concerns involved in data confidentiality\neven after the deletion of data from cloud storage. Though, it raises a serious\nproblem when the encrypted data needs to be shared with more people than the\ndata owner initially designated. However, searching on encrypted data is a\nfundamental issue in cloud storage. The method of searching over encrypted data\nrepresents a significant challenge in the cloud.\n  Searchable encryption allows a cloud server to conduct a search over\nencrypted data on behalf of the data users without learning the underlying\nplaintexts. While many academic SE schemes show provable security, they usually\nexpose some query information, making them less practical, weak in usability,\nand challenging to deploy. Also, sharing encrypted data with other authorized\nusers must provide each document's secret key. However, this way has many\nlimitations due to the difficulty of key management and distribution.\n  We have designed the system using the existing cryptographic approaches,\nensuring the search on encrypted data over the cloud. The primary focus of our\nproposed model is to ensure user privacy and security through a less\ncomputationally intensive, user-friendly system with a trusted third party\nentity. To demonstrate our proposed model, we have implemented a web\napplication called CryptoSearch as an overlay system on top of a well-known\ncloud storage domain. It exhibits secure search on encrypted data with no\ncompromise to the user-friendliness and the scheme's functional performance in\nreal-world applications.",
        "translated": "随着云存储服务的日益普及，安全性已经成为一个重要的问题。它伴随着被第三方访问的脆弱性。当驻留在本地存储中的用户数据被外包到云服务器时，安全性是用户在云服务器中面临的主要障碍之一。即使在从云存储中删除数据之后，这也引起了与数据保密有关的安全问题。尽管如此，当需要与比数据所有者最初指定的更多的人共享加密数据时，它会引起一个严重的问题。然而，对加密数据的搜索是云存储中的一个基本问题。搜索加密数据的方法是云中的一个重大挑战。可搜索加密允许云服务器代表数据用户对加密数据进行搜索，而无需了解底层明文。虽然许多学术 SE 方案表现出可证明的安全性，但是它们通常会暴露一些查询信息，使得它们不太实用、易用性差、部署困难。此外，与其他授权用户共享加密数据必须提供每个文档的密钥。然而，由于密钥管理和分配的困难，这种方法有很多局限性。我们使用现有的加密方法设计了系统，确保在云上搜索加密数据。我们提出的模型的主要重点是通过一个计算密集度较低、用户友好的系统和一个可信的第三方实体来确保用户的隐私和安全。为了演示我们提出的模型，我们在一个著名的云存储域上实现了一个名为 CryptoSearch 的 Web 应用程序作为覆盖系统。该方案对加密数据进行安全搜索，不影响用户友好性和实际应用中的性能。"
    },
    {
        "title": "Greedy-Mine: A Profitable Mining Attack Strategy in Bitcoin-NG",
        "url": "http://arxiv.org/abs/2306.03540v1",
        "pub_date": "2023-06-06",
        "summary": "Bitcoin-NG is an extensible blockchain protocol based on the same trust model\nas Bitcoin. It divides each epoch into one Key-Block and multiple Micro-Blocks,\neffectively improving transaction processing capacity. Bitcoin-NG adopts a\nspecial incentive mechanism (i.e., the transaction fees in each epoch are split\nto the current and next leader) to maintain its security. However, there are\nsome limitations to the existing incentive analysis of Bitcoin-NG in recent\nworks. First, the incentive division method of Bitcoin-NG only includes some\nspecific mining attack strategies of adversary, while ignoring more stubborn\nattack strategies. Second, once adversaries find a whale transaction, they will\ndeviate from honest mining strategy to obtain extra reward. In this paper, we\nare committed to solving these two limitations. First, we propose a novel\nmining strategy named Greedy-Mine attack. Then, we formulate a Markov Decision\nProcess (MDP) model to analyze the competition of honest miners and\nadversaries. Furthermore, we analysis the extra reward of adversaries and\nsummarize the mining power proportion range required for malicious adversaries\nto launch Greedy-Mine to obtain extra returns. Finally, we make a\nbackward-compatibility progressive modification to Bitcoin-NG protocol that\nwould raise the threshold of propagation factor from 0 to 1. Meanwhile, we get\nthe winning condition of adversaries when adopting Greedy-Mine, compared with\nhonest mining. Simulation and experimental results indicate that Bitcoin-NG is\nnot incentive compatible, which is vulnerable to Greedy-Mine attack.",
        "translated": "比特币 -NG 是一个可扩展的区块链协议，它基于与比特币相同的信任模型。它将每个时代划分为一个关键块和多个微块，有效地提高了事务处理能力。比特币 -NG 采用了一种特殊的激励机制(即每个时代的交易费用分摊给当前和下一个领导者)来维持其安全性。然而，现有的比特币 -NG 激励分析在近期的研究中存在一定的局限性。首先，比特币 -NG 的激励划分方法只包括一些特定的挖掘攻击策略，而忽略了较为顽固的攻击策略。其次，一旦对手发现了鲸鱼的交易，他们就会偏离诚实的采矿策略来获得额外的报酬。在本文中，我们致力于解决这两个局限性。首先，本文提出了一种新的挖掘策略——贪婪-地雷攻击。然后，我们制定了一个马可夫决策过程模型来分析诚实的矿工和对手之间的竞争。此外，本文还分析了对手的额外报酬，总结了恶意对手为获得额外报酬而发射贪婪矿井所需的采矿权比例范围。最后，对比特币 -NG 协议进行了向后兼容的渐进修改，将传播因子的阈值从0提高到1。同时，通过与诚实采矿法的比较，得出了采用贪婪采矿法时对手的获胜条件。仿真和实验结果表明，比特币-NG 不具备激励兼容性，容易受到 Greedy-Mine 攻击。"
    },
    {
        "title": "On the Reliability of Watermarks for Large Language Models",
        "url": "http://arxiv.org/abs/2306.04634v1",
        "pub_date": "2023-06-07",
        "summary": "Large language models (LLMs) are now deployed to everyday use and positioned\nto produce large quantities of text in the coming decade. Machine-generated\ntext may displace human-written text on the internet and has the potential to\nbe used for malicious purposes, such as spearphishing attacks and social media\nbots. Watermarking is a simple and effective strategy for mitigating such harms\nby enabling the detection and documentation of LLM-generated text. Yet, a\ncrucial question remains: How reliable is watermarking in realistic settings in\nthe wild? There, watermarked text might be mixed with other text sources,\nparaphrased by human writers or other language models, and used for\napplications in a broad number of domains, both social and technical. In this\npaper, we explore different detection schemes, quantify their power at\ndetecting watermarks, and determine how much machine-generated text needs to be\nobserved in each scenario to reliably detect the watermark. We especially\nhighlight our human study, where we investigate the reliability of watermarking\nwhen faced with human paraphrasing. We compare watermark-based detection to\nother detection strategies, finding overall that watermarking is a reliable\nsolution, especially because of its sample complexity - for all attacks we\nconsider, the watermark evidence compounds the more examples are given, and the\nwatermark is eventually detected.",
        "translated": "大型语言模型(LLM)现在被部署到日常使用中，并定位于在未来十年生成大量文本。机器生成的文本可能取代互联网上人写的文本，并有可能被用于恶意目的，如鱼叉式钓鱼攻击和社交媒体机器人。水印是一种简单而有效的策略，通过检测和记录 LLM 生成的文本来减轻这种危害。然而，一个关键的问题仍然存在: 在野外的现实环境中，水印的可靠性如何？在那里，水印文本可能与其他文本来源混合，由人类作家或其他语言模型转述，并用于广泛的领域中的应用，包括社会和技术。在本文中，我们探讨了不同的检测方案，量化它们在检测水印方面的能力，并确定在每个场景中需要观察多少机器生成的文本才能可靠地检测水印。我们特别强调我们的人类研究，我们调查的可靠性水印时，面对人类释义。我们比较了基于水印的检测和其他检测策略，发现水印是一个可靠的解决方案，特别是因为它的样本复杂性-对于所有的攻击，我们考虑，水印证据复合越多的例子，并最终检测水印。"
    },
    {
        "title": "Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis,\n  and LLMs Evaluations",
        "url": "http://arxiv.org/abs/2306.04618v1",
        "pub_date": "2023-06-07",
        "summary": "This paper reexamines the research on out-of-distribution (OOD) robustness in\nthe field of NLP. We find that the distribution shift settings in previous\nstudies commonly lack adequate challenges, hindering the accurate evaluation of\nOOD robustness. To address these issues, we propose a benchmark construction\nprotocol that ensures clear differentiation and challenging distribution\nshifts. Then we introduce BOSS, a Benchmark suite for Out-of-distribution\nrobustneSS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we\nconduct a series of experiments on pre-trained language models for analysis and\nevaluation of OOD robustness. First, for vanilla fine-tuning, we examine the\nrelationship between in-distribution (ID) and OOD performance. We identify\nthree typical types that unveil the inner learning mechanism, which could\npotentially facilitate the forecasting of OOD robustness, correlating with the\nadvancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and\nfind that, despite exhibiting some effectiveness in specific cases, they do not\noffer significant improvement compared to vanilla fine-tuning. Further, we\nevaluate 5 LLMs with various adaptation paradigms and find that when sufficient\nID data is available, fine-tuning domain-specific models outperform LLMs on ID\nexamples significantly. However, in the case of OOD instances, prioritizing\nLLMs with in-context learning yields better results. We identify that both\nfine-tuned small models and LLMs face challenges in effectively addressing\ndownstream tasks. The code is public at\n\\url{https://github.com/lifan-yuan/OOD_NLP}.",
        "translated": "本文重新审视了自然语言处理领域中分布外(OOD)鲁棒性的研究。我们发现在以往的研究中，分布移位设置通常缺乏足够的挑战，阻碍了面向对象的鲁棒性的准确评估。为了解决这些问题，我们提出了一个基准建设协议，以确保明确的差异和挑战性的分配转移。然后我们介绍了 BOSS，一个用于分布外鲁棒性评估的基准套件，包括5个任务和20个数据集。基于 BOSS 系统，我们对预训练语言模型进行了一系列的实验，用于分析和评估面向对象的鲁棒性。首先，对于普通的微调，我们研究分发内(ID)和 OOD 性能之间的关系。我们确定了三种典型的类型，揭示了内部学习机制，这可能有助于面向对象的鲁棒性预测，与 ID 数据集的进步相关。然后，我们对 BOSS 上的5种经典方法进行了评估，发现尽管它们在特定情况下显示出一些有效性，但与普通的微调相比，它们并没有提供显著的改进。此外，我们评估了5个具有不同适应范例的 LLM，发现当有足够的 ID 数据可用时，针对特定领域的微调模型在 ID 示例上的表现明显优于 LLM。但是，在面向对象的实例中，使用上下文内学习对 LLM 进行优先排序会产生更好的结果。我们发现，微调小型模型和 LLM 在有效处理下游任务方面都面临挑战。该代码在 url { https://github.com/lifan-yuan/ood_nlp }是公共的。"
    },
    {
        "title": "Prefix Siphoning: Exploiting LSM-Tree Range Filters For Information\n  Disclosure (Full Version)",
        "url": "http://arxiv.org/abs/2306.04602v1",
        "pub_date": "2023-06-07",
        "summary": "Key-value stores typically leave access control to the systems for which they\nact as storage engines. Unfortunately, attackers may circumvent such read\naccess controls via timing attacks on the key-value store, which use\ndifferences in query response times to glean information about stored data.\n  To date, key-value store timing attacks have aimed to disclose stored values\nand have exploited external mechanisms that can be disabled for protection. In\nthis paper, we point out that key disclosure is also a security threat -- and\ndemonstrate key disclosure timing attacks that exploit mechanisms of the\nkey-value store itself.\n  We target LSM-tree based key-value stores utilizing range filters, which have\nbeen recently proposed to optimize LSM-tree range queries. We analyze the\nimpact of the range filters SuRF and prefix Bloom filter on LSM-trees through a\nsecurity lens, and show that they enable a key disclosure timing attack, which\nwe call prefix siphoning. Prefix siphoning successfully leverages benign\nqueries for non-present keys to identify prefixes of actual keys -- and in some\ncases, full keys -- in scenarios where brute force searching for keys (via\nexhaustive enumeration or random guesses) is infeasible.",
        "translated": "键值存储通常将访问控制留给它们作为存储引擎的系统。不幸的是，攻击者可能通过对键值存储的定时攻击来规避这种读访问控制，这种攻击使用查询响应时间的差异来收集有关存储数据的信息。到目前为止，键值存储定时攻击的目的是披露存储值，并利用可以禁用的外部机制进行保护。在本文中，我们指出密钥公开也是一种安全威胁——并论证了利用密钥价值存储本身机制的密钥公开时间攻击。我们利用最近提出的用于优化 LSM 树范围查询的范围过滤器来实现基于 LSM 树的键值存储。我们通过安全透镜分析了距离滤波器 SuRF 和前缀 Bloom 滤波器对 LSM 树的影响，并指出它们能够实现一种密钥泄露定时攻击，我们称之为前缀虹吸。前缀虹吸成功地利用对非存在键的良性查询来识别实际键的前缀——在某些情况下，识别全键——在强制搜索键(通过穷举或随机猜测)是不可行的情况下。"
    },
    {
        "title": "The Effect of Length on Key Fingerprint Verification Security and\n  Usability",
        "url": "http://arxiv.org/abs/2306.04574v1",
        "pub_date": "2023-06-07",
        "summary": "In applications such as end-to-end encrypted instant messaging, secure email,\nand device pairing, users need to compare key fingerprints to detect\nimpersonation and adversary-in-the-middle attacks. Key fingerprints are usually\ncomputed as truncated hashes of each party's view of the channel keys, encoded\nas an alphanumeric or numeric string, and compared out-of-band, e.g. manually,\nto detect any inconsistencies. Previous work has extensively studied the\nusability of various verification strategies and encoding formats, however, the\nexact effect of key fingerprint length on the security and usability of key\nfingerprint verification has not been rigorously investigated. We present a\n162-participant study on the effect of numeric key fingerprint length on\ncomparison time and error rate. While the results confirm some widely-held\nintuitions such as general comparison times and errors increasing significantly\nwith length, a closer look reveals interesting nuances. The significant rise in\ncomparison time only occurs when highly similar fingerprints are compared, and\ncomparison time remains relatively constant otherwise. On errors, our results\nclearly distinguish between security non-critical errors that remain low\nirrespective of length and security critical errors that significantly rise,\nespecially at higher fingerprint lengths. A noteworthy implication of this\nlatter result is that Signal/WhatsApp key fingerprints provide a considerably\nlower level of security than usually assumed.",
        "translated": "在端到端加密即时通讯、安全电子邮件和设备配对等应用程序中，用户需要比较密钥指纹来检测模仿和中间对手攻击。密钥指纹通常计算为各方频道密钥视图的截断哈希，编码为字母数字或数字字符串，并进行带外比较，例如手动比较，以检测任何不一致之处。以往的工作已经广泛地研究了各种验证策略和编码格式的可用性，但是，密钥指纹长度对密钥指纹验证的安全性和可用性的确切影响还没有得到严格的研究。我们提出了一个162人参与的研究数字键指纹长度对比较时间和错误率的影响。虽然研究结果证实了一些普遍接受的直觉，比如一般的比较时间和误差随着长度的增加而显著增加，但仔细观察就会发现有趣的细微差别。只有当高度相似的指纹进行比较时，比较时间才会显著增加，否则比较时间保持相对恒定。在错误方面，我们的结果清楚地区分了无论长度如何都保持较低的安全性非关键性错误和显著上升的安全性关键性错误，特别是在指纹长度较高的情况下。后一种结果的一个值得注意的含义是，Signal/WhatsApp 密钥指纹提供了比通常假设的低得多的安全级别。"
    },
    {
        "title": "Differentially Private Selection from Secure Distributed Computing",
        "url": "http://arxiv.org/abs/2306.04564v2",
        "pub_date": "2023-06-07",
        "summary": "Given a collection of vectors $x^{(1)},\\dots,x^{(n)} \\in \\{0,1\\}^d$, the\nselection problem asks to report the index of an \"approximately largest\" entry\nin $x=\\sum_{j=1}^n x^{(j)}$. Selection abstracts a host of problems--in machine\nlearning it can be used for hyperparameter tuning, feature selection, or to\nmodel empirical risk minimization. We study selection under differential\nprivacy, where a released index guarantees privacy for each vectors. Though\nselection can be solved with an excellent utility guarantee in the central\nmodel of differential privacy, the distributed setting lacks solutions.\nSpecifically, strong privacy guarantees with high utility are offered in high\ntrust settings, but not in low trust settings. For example, in the popular\nshuffle model of distributed differential privacy, there are strong lower\nbounds suggesting that the utility of the central model cannot be obtained. In\nthis paper we design a protocol for differentially private selection in a trust\nsetting similar to the shuffle model--with the crucial difference that our\nprotocol tolerates corrupted servers while maintaining privacy. Our protocol\nuses techniques from secure multi-party computation (MPC) to implement a\nprotocol that: (i) has utility on par with the best mechanisms in the central\nmodel, (ii) scales to large, distributed collections of high-dimensional\nvectors, and (iii) uses $k\\geq 3$ servers that collaborate to compute the\nresult, where the differential privacy holds assuming an honest majority. Since\ngeneral-purpose MPC techniques are not sufficiently scalable, we propose a\nnovel application of integer secret sharing, and evaluate the utility and\nefficiency of our protocol theoretically and empirically. Our protocol is the\nfirst to demonstrate that large-scale differentially private selection is\npossible in a distributed setting.",
        "translated": "给定{0,1} ^ d $中的向量 $x ^ {(1)} ，点，x ^ {(n)}的集合，选择问题要求报告 $x = sum _ { j = 1} ^ n x ^ {(j)} $中“近似最大”条目的索引。选择抽象出许多问题——在机器学习中，它可以用于超参数调整、特征选择或经验风险最小化建模。我们研究差分隐私下的选择，其中发布的索引保证每个向量的隐私。尽管在中心差分隐私模型中，选择可以通过极好的效用保证得到解决，但分布式设置缺乏解决方案。具体来说，在高信任设置中提供具有高效用的强隐私保护，但在低信任设置中不提供这种保护。例如，在流行的分布式差分隐私的洗牌模型中，有很强的下限，表明不能获得中央模型的效用。在本文中，我们设计了一个类似于 shuffle 模型的信任设置中的差异私有选择协议，其关键区别在于我们的协议能够容忍被破坏的服务器，同时保持私有性。我们的协议使用来自安全多方计算(mPC)的技术来实现一个协议: (i)与中央模型中最好的机制具有同等效用，(ii)扩展到高维向量的大型分布式集合，以及(iii)使用合作计算结果的 $k geq 3 $服务器，其中差分隐私持有假定的诚实多数。由于通用的 MPC 技术不具有足够的可扩展性，本文提出了一种新的整数秘密共享应用，并从理论和经验上对该协议的效用和效率进行了评估。我们的协议首次证明了在分布式环境中可以进行大规模的差异私有选择。"
    },
    {
        "title": "Vulnerable Smart Contract Function Locating Based on Multi-Relational\n  Nested Graph Convolutional Network",
        "url": "http://arxiv.org/abs/2306.04479v1",
        "pub_date": "2023-06-07",
        "summary": "The immutable and trustable characteristics of blockchain enable smart\ncontracts to be applied in various fields. Unfortunately, smart contracts are\nsubject to various vulnerabilities, which are frequently exploited by\nattackers, causing financial damage to users.In this paper, we study the\nproblem of vulnerable smart contract function locating. We construct a novel\nMulti-Relational Nested contract Graph (MRNG) to better characterize the rich\nsyntactic and semantic information in the smart contract code, including the\nrelationships between data and instructions. An MRNG represents a smart\ncontract, where each node represents a function in the smart contract and each\nedge describes the calling relationship between the functions. In addition, we\ncreate a Multi-Relational Function Graph (MRFG) for each function, which\ncharacterizes the corresponding function code. That is, each function is\ncharacterized as an MRFG, which corresponds to a node in the MRNG. Each MRFG\nuses different types of edges to represent the different control and data\nrelationships between nodes within a function. We also propose a\nMulti-Relational Nested Graph Convolutional Network (MRN-GCN) to process the\nMRNG. MRN-GCN first extracts and aggregates features from each MRFG, using the\nedge-enhanced graph convolution network and self-attention mechanism. The\nextracted feature vector is then assigned to the corresponding node in the MRNG\nto obtain a new Featured Contract Graph (FCG) for the smart contract. Graph\nconvolution is used to further extract features from the FCG. Finally, a feed\nforward network with a Sigmoid function is used to locate the vulnerable\nfunctions. Experimental results on the real-world smart contract datasets show\nthat model MRN-GCN can effectively improve the accuracy, precision, recall and\nF1-score performance of vulnerable smart contract function locating.",
        "translated": "区块链的不可变性和可信性使得智能契约能够应用于各个领域。不幸的是，智能合同容易受到各种漏洞的攻击，这些漏洞经常被攻击者利用，给用户造成经济损失。为了更好地描述智能契约代码中丰富的语法和语义信息，包括数据和指令之间的关系，我们构建了一个新颖的多关系嵌套契约图(Multi-Relational Nested ContractGraph，MRNG)。MRNG 表示智能契约，其中每个节点表示智能契约中的一个函数，每个边描述函数之间的调用关系。此外，我们为每个函数创建一个多关系函数图(MRFG) ，它描述了相应的函数代码。也就是说，每个函数都被描述为一个 MRFG，它对应于 MRNG 中的一个节点。每个 MRFG 使用不同类型的边来表示函数中节点之间不同的控制和数据关系。我们还提出了一个多关系嵌套图卷积网络(MRN-GCN)来处理 MRNG。GCN 首先利用边增强的图卷积网络和自注意机制从每个 MRFG 中提取和聚集特征。然后将提取的特征向量分配给 MRNG 中的相应节点，得到新的特征契约图(FCG)。图卷积用于进一步从 FCG 中提取特征。最后，使用一个带有 S形函数的前馈网络来定位易受攻击的功能。实验结果表明，MRN-GCN 模型能够有效地提高脆弱智能契约功能定位的准确性、精度、召回率和 F1分数性能。"
    },
    {
        "title": "Hardening and Speeding Up Zero-interaction Pairing and Authentication",
        "url": "http://arxiv.org/abs/2306.04458v1",
        "pub_date": "2023-06-07",
        "summary": "Establishing and maintaining secure communications in the Internet of Things\n(IoT) is vital to protect smart devices. Zero-interaction pairing (ZIP) and\nzero-interaction authentication (ZIA) enable IoT devices to establish and\nmaintain secure communications without user interaction by utilizing devices'\nambient context, e.g., audio. For autonomous operation, ZIP and ZIA require the\ncontext to have enough entropy to resist attacks and complete in a timely\nmanner. Despite the low-entropy context being the norm, like inside an\nunoccupied room, the research community has yet to come up with ZIP and ZIA\nschemes operating under such conditions. We propose HARDZIPA, a novel approach\nthat turns commodity IoT actuators into injecting devices, generating\nhigh-entropy context. Here, we combine the capability of IoT actuators to\nimpact the environment, e.g., emitting a sound, with a pseudorandom number\ngenerator (PRNG) featured by many actuators to craft hard-to-predict context\nstimuli. To demonstrate the feasibility of HARDZIPA, we implement it on\noff-the-shelf IoT actuators, i.e., smart speakers, lights, and humidifiers. We\ncomprehensively evaluate HARDZIPA, collecting over 80 hours of various context\ndata in real-world scenarios. Our results show that HARDZIPA is able to thwart\nadvanced active attacks on ZIP and ZIA schemes, while doubling the amount of\ncontext entropy in many cases, which allows two times faster pairing and\nauthentication.",
        "translated": "在物联网(IoT)中建立和维护安全通信对于保护智能设备至关重要。零交互配对(ZIP)和零交互认证(ZIA)使物联网设备能够在没有用户交互的情况下，通过利用设备的环境上下文(如音频)建立和维护安全通信。对于自主操作，ZIP 和 ZIA 需要上下文具有足够的熵来抵抗攻击并及时完成。尽管低熵环境是常态，就像在一个空房间里一样，研究团体还没有提出 ZIP 和 ZIA 方案在这样的条件下运作。我们提出了 HARDZIPA，一种新的方法，将商品物联网执行器注入设备，产生高熵上下文。在这里，我们结合了物联网执行器的能力，以影响环境，例如，发出声音，一个伪随机数生成器(PRNG)的特点，许多执行器制造难以预测的背景刺激。为了证明 HARDZIPA 的可行性，我们在现成的物联网执行器上实现了它，例如，智能扬声器、灯和加湿器。我们全面评估 HARDZIPA，收集超过80个小时的各种上下文数据在现实世界的情景。我们的研究结果表明，HARDZIPA 能够阻止针对 ZIP 和 ZIA 方案的高级主动攻击，同时在许多情况下使上下文熵增加一倍，这使得配对和认证速度提高了两倍。"
    },
    {
        "title": "Fast Optimal Locally Private Mean Estimation via Random Projections",
        "url": "http://arxiv.org/abs/2306.04444v1",
        "pub_date": "2023-06-07",
        "summary": "We study the problem of locally private mean estimation of high-dimensional\nvectors in the Euclidean ball. Existing algorithms for this problem either\nincur sub-optimal error or have high communication and/or run-time complexity.\nWe propose a new algorithmic framework, ProjUnit, for private mean estimation\nthat yields algorithms that are computationally efficient, have low\ncommunication complexity, and incur optimal error up to a $1+o(1)$-factor. Our\nframework is deceptively simple: each randomizer projects its input to a random\nlow-dimensional subspace, normalizes the result, and then runs an optimal\nalgorithm such as PrivUnitG in the lower-dimensional space. In addition, we\nshow that, by appropriately correlating the random projection matrices across\ndevices, we can achieve fast server run-time. We mathematically analyze the\nerror of the algorithm in terms of properties of the random projections, and\nstudy two instantiations. Lastly, our experiments for private mean estimation\nand private federated learning demonstrate that our algorithms empirically\nobtain nearly the same utility as optimal ones while having significantly lower\ncommunication and computational cost.",
        "translated": "研究了欧氏球中高维向量的局部私有均值估计问题。针对这个问题的现有算法要么会产生次优误差，要么具有较高的通信和/或运行时复杂度。我们提出了一个新的算法框架，ProjUnit，用于私有均值估计，它产生的算法计算效率高，通信复杂度低，并产生最优误差高达 $1 + o (1) $因子。我们的框架看似简单: 每个随机发生器将其输入投射到一个随机的低维子空间，规范化结果，然后在低维空间运行一个最优算法，如 PrivUnitG。此外，我们表明，通过适当关联跨设备的随机投影矩阵，我们可以实现快速的服务器运行时。我们利用随机投影的性质对算法的误差进行了数学分析，并研究了两个实例。最后，我们对私有均值估计和私有联邦学习的实验表明，我们的算法在经验上获得了与最优算法几乎相同的效用，同时具有明显较低的通信和计算成本。"
    },
    {
        "title": "Security Analysis of WG-7 Lightweight Stream Cipher against Cube Attack",
        "url": "http://arxiv.org/abs/2306.04352v1",
        "pub_date": "2023-06-07",
        "summary": "Welch--Gong (WG) is a hardware-oriented LFSR-based stream cipher. WG-7 is a\nversion of the eStream submission Welch--Gong, used for RFID encryption and\nauthentication purposes. It offers 80-bit cryptographic security. In modern\ndays, almost all ciphers achieve the security by exploiting the nonlinear\nfeedback structure. In this paper, we investigate the security of the nonlinear\nfeedback-based initialization phase of the WG-7 stream cipher using the\nconventional bit-based division property of cube attack, by considering the\ncipher in a non-blackbox polynomial setting. In our work, we mount the cube\nattack using mixed-integer-linear-programming(MILP) models. The results of our\nattack enable us to recover the secret key of WG-7 after 20 rounds of\ninitialization utilizing $2^{10}$ keystream bits in $2^{73}$ time. We show that\nour proposed attack takes significantly lower data complexity. To the best of\nour knowledge, our attack is the first one that investigates the security of\nthe nonlinear feedback-based initialization phase of WG-7 cipher.",
        "translated": "WG 是一种面向硬件的基于 LFSR 的流密码。WG-7是 eStream 提交的韦尔奇——龚的一个版本，用于 RFID 加密和认证目的。它提供80位加密安全性。在现代，几乎所有的密码都是通过利用非线性反馈结构来实现安全的。本文利用立方体攻击的传统比特基除法性质，考虑非黑盒多项式背景下的非线性反馈初始化阶段的安全性，研究了 WG-7流密码的初始化阶段的安全性。在我们的工作中，我们使用混合整数线性规划(MILP)模型来安装立方体攻击。我们的攻击结果使我们能够在20轮初始化之后恢复 WG-7的密钥，在 $2 ^ {73} $时间内使用 $2 ^ {10} $keystream 位。我们表明，我们提出的攻击采取显着降低数据复杂性。据我们所知，我们的攻击是第一个研究 WG-7密码基于非线性反馈初始化阶段的安全性的攻击。"
    },
    {
        "title": "Development of a Multi-purpose Fuzzer to Perform Assessment as Input to\n  a Cybersecurity Risk Assessment and Analysis System",
        "url": "http://arxiv.org/abs/2306.04284v1",
        "pub_date": "2023-06-07",
        "summary": "Fuzzing is utilized for testing software and systems for cybersecurity risk\nvia the automated adaptation of inputs. It facilitates the identification of\nsoftware bugs and misconfigurations that may create vulnerabilities, cause\nabnormal operations or result in systems' failure. While many fuzzers have been\npurpose-developed for testing specific systems, this paper proposes a\ngeneralized fuzzer that provides a specific capability for testing software and\ncyber-physical systems which utilize configuration files. While this fuzzer\nfacilitates the detection of system and software defects and vulnerabilities,\nit also facilitates the determination of the impact of settings on device\noperations. This later capability facilitates the modeling of the devices in a\ncybersecurity risk assessment and analysis system. This paper describes and\nassesses the performance of the proposed fuzzer technology. It also details how\nthe fuzzer operates as part of the broader cybersecurity risk assessment and\nanalysis system.",
        "translated": "Fuzzing 通过自动调整输入，用于测试软件和系统的网络安全风险。它有助于识别可能导致漏洞、导致异常操作或导致系统故障的软件错误和错误配置。针对测试特定系统的需要，本文提出了一种通用的模糊检测器，该模糊检测器为测试软件和利用配置文件的网络物理系统提供了特定的功能。虽然这个模糊检测器有助于检测系统和软件缺陷和漏洞，但它也有助于确定设置对设备操作的影响。此后的功能有助于在网络安全风险评估和分析系统中对设备进行建模。本文描述并评估了所提出的模糊控制技术的性能。报告还详细介绍了作为更广泛的网络安全风险评估和分析系统的一部分，模糊控制器是如何运作的。"
    },
    {
        "title": "Fully Robust Federated Submodel Learning in a Distributed Storage System",
        "url": "http://arxiv.org/abs/2306.05402v1",
        "pub_date": "2023-06-08",
        "summary": "We consider the federated submodel learning (FSL) problem in a distributed\nstorage system. In the FSL framework, the full learning model at the server\nside is divided into multiple submodels such that each selected client needs to\ndownload only the required submodel(s) and upload the corresponding update(s)\nin accordance with its local training data. The server comprises multiple\nindependent databases and the full model is stored across these databases. An\neavesdropper passively observes all the storage and listens to all the\ncommunicated data, of its controlled databases, to gain knowledge about the\nremote client data and the submodel information. In addition, a subset of\ndatabases may fail, negatively affecting the FSL process, as FSL process may\ntake a non-negligible amount of time for large models. To resolve these two\nissues together (i.e., security and database repair), we propose a novel coding\nmechanism coined ramp secure regenerating coding (RSRC), to store the full\nmodel in a distributed manner. Using our new RSRC method, the eavesdropper is\npermitted to learn a controllable amount of submodel information for the sake\nof reducing the communication and storage costs. Further, during the database\nrepair process, in the construction of the replacement database, the submodels\nto be updated are stored in the form of their latest version from updating\nclients, while the remaining submodels are obtained from the previous version\nin other databases through routing clients. Our new RSRC-based distributed FSL\napproach is constructed on top of our earlier two-database FSL scheme which\nuses private set union (PSU). A complete one-round FSL process consists of\nFSL-PSU phase, FSL-write phase and additional auxiliary phases. Our proposed\nFSL scheme is also robust against database drop-outs, client drop-outs, client\nlate-arrivals and an active adversary controlling databases.",
        "translated": "研究了分布式存储系统中的联邦子模型学习(FSL)问题。在 FSL 框架中，服务器端的完整学习模型被划分为多个子模型，每个选定的客户端只需要下载所需的子模型，并根据其本地训练数据上传相应的更新。服务器由多个独立的数据库组成，完整的模型存储在这些数据库中。窃听者被动地观察所有的存储器，监听所有被控数据库的通信数据，以获取远程客户端数据和子模型信息的相关知识。此外，数据库的一个子集可能会失败，从而对 FSL 过程产生负面影响，因为对于大型模型，FSL 过程可能需要花费不可忽视的时间。为了同时解决这两个问题(即安全性和数据库修复) ，我们提出了一种新的编码机制——斜坡安全再生编码(RSRC) ，以分布式方式存储完整的模型。使用我们新的 RSRC 方法，窃听者可以学习可控量的子模型信息，从而降低通信和存储成本。此外，在数据库修复过程中，在建立替换数据库时，需要更新的子模型以更新客户端的最新版本的形式存储，而其余子模型则通过路由客户端从其他数据库的上一版本获得。我们的新的基于 RSRC 的分布式 FSL 方法是在我们早期的两数据库 FSL 方案的基础上构建的，该方案使用私有集合联合(PSU)。一个完整的一轮 FSL 工艺包括 FSL-PSU 阶段、 FSL 写入阶段和附加辅助阶段。我们提出的 FSL 方案对于数据库丢失、客户端丢失、客户端延迟到达以及活跃的对手控制数据库也具有很强的鲁棒性。"
    },
    {
        "title": "Detecting Neural Trojans Through Merkle Trees",
        "url": "http://arxiv.org/abs/2306.05368v1",
        "pub_date": "2023-06-08",
        "summary": "Deep neural networks are utilized in a growing number of industries. Much of\nthe current literature focuses on the applications of deep neural networks\nwithout discussing the security of the network itself. One security issue\nfacing deep neural networks is neural trojans. Through a neural trojan, a\nmalicious actor may force the deep neural network to act in unintended ways.\nSeveral potential defenses have been proposed, but they are computationally\nexpensive, complex, or unusable in commercial applications. We propose Merkle\ntrees as a novel way to detect and isolate neural trojans.",
        "translated": "深层神经网络在越来越多的行业中得到了应用。目前的大部分文献都集中在深层神经网络的应用上，而没有讨论网络本身的安全性。深度神经网络面临的一个安全问题是神经木马。通过神经木马，恶意行为者可能会强迫深层神经网络以意想不到的方式行动。已经提出了几种潜在的防御措施，但它们在计算上代价高昂、复杂，或者在商业应用中无法使用。我们提出默克尔树作为一种新的方法来检测和分离神经木马。"
    },
    {
        "title": "Federated Linear Contextual Bandits with User-level Differential Privacy",
        "url": "http://arxiv.org/abs/2306.05275v1",
        "pub_date": "2023-06-08",
        "summary": "This paper studies federated linear contextual bandits under the notion of\nuser-level differential privacy (DP). We first introduce a unified federated\nbandits framework that can accommodate various definitions of DP in the\nsequential decision-making setting. We then formally introduce user-level\ncentral DP (CDP) and local DP (LDP) in the federated bandits framework, and\ninvestigate the fundamental trade-offs between the learning regrets and the\ncorresponding DP guarantees in a federated linear contextual bandits model. For\nCDP, we propose a federated algorithm termed as \\robin and show that it is\nnear-optimal in terms of the number of clients $M$ and the privacy budget\n$\\varepsilon$ by deriving nearly-matching upper and lower regret bounds when\nuser-level DP is satisfied. For LDP, we obtain several lower bounds, indicating\nthat learning under user-level $(\\varepsilon,\\delta)$-LDP must suffer a regret\nblow-up factor at least {$\\min\\{1/\\varepsilon,M\\}$ or\n$\\min\\{1/\\sqrt{\\varepsilon},\\sqrt{M}\\}$} under different conditions.",
        "translated": "本文在用户级差分隐私(DP)的概念下研究了联邦线性上下文盗贼。我们首先介绍了一个统一的联邦土匪框架，该框架可以在序贯决策环境中容纳 DP 的各种定义。然后在联邦土匪模型中正式引入了用户级中央 DP (CDP)和本地 DP (LDP) ，研究了在联邦线性关联土匪模型中，学习遗憾与相应 DP 保证之间的基本权衡关系。对于 CDP，我们提出了一种称为 Robin 的联邦算法，当用户级 DP 满足时，通过导出接近匹配的上下后悔界限，证明了该算法在客户数 $M $和隐私预算 $varepsilon $方面接近最优。对于 LDP，我们得到了几个下界，表明在用户级 $(varepsilon，delta) $- LDP 下的学习必须在不同的条件下至少经历一个后悔爆破因子{ $min {1/varepsilon，M } $或 $min {1/sqrt { varepsilon } ，sqrt { M } $}。"
    },
    {
        "title": "Ownership Protection of Generative Adversarial Networks",
        "url": "http://arxiv.org/abs/2306.05233v1",
        "pub_date": "2023-06-08",
        "summary": "Generative adversarial networks (GANs) have shown remarkable success in image\nsynthesis, making GAN models themselves commercially valuable to legitimate\nmodel owners. Therefore, it is critical to technically protect the intellectual\nproperty of GANs. Prior works need to tamper with the training set or training\nprocess, and they are not robust to emerging model extraction attacks. In this\npaper, we propose a new ownership protection method based on the common\ncharacteristics of a target model and its stolen models. Our method can be\ndirectly applicable to all well-trained GANs as it does not require retraining\ntarget models. Extensive experimental results show that our new method can\nachieve the best protection performance, compared to the state-of-the-art\nmethods. Finally, we demonstrate the effectiveness of our method with respect\nto the number of generations of model extraction attacks, the number of\ngenerated samples, different datasets, as well as adaptive attacks.",
        "translated": "生成对抗网络(GAN)在图像合成方面取得了显著的成功，使得 GAN 模型本身对于合法的模型所有者来说具有商业价值。因此，从技术上保护 GAN 的知识产权至关重要。以前的工作需要篡改训练集或训练过程，它们对新出现的模型提取攻击不具有鲁棒性。本文基于目标模型及其窃取模型的共同特点，提出了一种新的所有权保护方法。我们的方法可以直接适用于所有训练有素的 GAN，因为它不需要再训练目标模型。大量的实验结果表明，与目前最先进的保护方法相比，我们的新方法可以获得最佳的保护性能。最后，从模型提取攻击的代数、生成的样本数、不同的数据集以及自适应攻击等方面证明了该方法的有效性。"
    },
    {
        "title": "Boosting Adversarial Transferability by Achieving Flat Local Maxima",
        "url": "http://arxiv.org/abs/2306.05225v1",
        "pub_date": "2023-06-08",
        "summary": "Transfer-based attack adopts the adversarial examples generated on the\nsurrogate model to attack various models, making it applicable in the physical\nworld and attracting increasing interest. Recently, various adversarial attacks\nhave emerged to boost adversarial transferability from different perspectives.\nIn this work, inspired by the fact that flat local minima are correlated with\ngood generalization, we assume and empirically validate that adversarial\nexamples at a flat local region tend to have good transferability by\nintroducing a penalized gradient norm to the original loss function. Since\ndirectly optimizing the gradient regularization norm is computationally\nexpensive and intractable for generating adversarial examples, we propose an\napproximation optimization method to simplify the gradient update of the\nobjective function. Specifically, we randomly sample an example and adopt the\nfirst-order gradient to approximate the second-order Hessian matrix, which\nmakes computing more efficient by interpolating two Jacobian matrices.\nMeanwhile, in order to obtain a more stable gradient direction, we randomly\nsample multiple examples and average the gradients of these examples to reduce\nthe variance due to random sampling during the iterative process. Extensive\nexperimental results on the ImageNet-compatible dataset show that the proposed\nmethod can generate adversarial examples at flat local regions, and\nsignificantly improve the adversarial transferability on either normally\ntrained models or adversarially trained models than the state-of-the-art\nattacks.",
        "translated": "基于转移的攻击采用在代理模型上生成的敌对实例对各种模型进行攻击，使其适用于物理世界，引起了人们越来越多的兴趣。最近，各种各样的对抗性攻击已经出现，从不同的角度提高了对抗性的可转移性。在本文中，受平坦局部极小与良好推广相关的启发，我们假设并实验验证了在平坦局部区域上的对立例子通过引入惩罚梯度范数对原始损失函数具有良好的可迁移性。由于直接优化梯度正则化范数的计算量很大，而且难以生成对立的例子，因此提出了一种近似优化方法来简化目标函数的梯度更新。具体地说，我们随机抽样一个例子，采用一阶梯度来逼近二阶 Hessian 矩阵，通过插值两个 Jacobian 矩阵来提高计算效率。同时，为了得到一个更稳定的梯度方向，在迭代过程中，我们随机抽样多个样本，并对这些样本的梯度进行平均，以减少随机抽样造成的方差。在与 ImageNet 兼容的数据集上的大量实验结果表明，该方法可以在平坦的局部区域生成对抗性示例，并且比最先进的攻击方法显著地提高了对抗性转移在正常训练模型或对抗性训练模型上的可转移性。"
    },
    {
        "title": "PriSampler: Mitigating Property Inference of Diffusion Models",
        "url": "http://arxiv.org/abs/2306.05208v1",
        "pub_date": "2023-06-08",
        "summary": "Diffusion models have been remarkably successful in data synthesis. Such\nsuccesses have also driven diffusion models to apply to sensitive data, such as\nhuman face data, but this might bring about severe privacy concerns. In this\nwork, we systematically present the first privacy study about property\ninference attacks against diffusion models, in which adversaries aim to extract\nsensitive global properties of the training set from a diffusion model, such as\nthe proportion of the training data for certain sensitive properties.\nSpecifically, we consider the most practical attack scenario: adversaries are\nonly allowed to obtain synthetic data. Under this realistic scenario, we\nevaluate the property inference attacks on different types of samplers and\ndiffusion models. A broad range of evaluations shows that various diffusion\nmodels and their samplers are all vulnerable to property inference attacks.\nFurthermore, one case study on off-the-shelf pre-trained diffusion models also\ndemonstrates the effectiveness of the attack in practice. Finally, we propose a\nnew model-agnostic plug-in method PriSampler to mitigate the property inference\nof diffusion models. PriSampler can be directly applied to well-trained\ndiffusion models and support both stochastic and deterministic sampling.\nExtensive experiments illustrate the effectiveness of our defense and it makes\nadversaries infer the proportion of properties as close as random guesses.\nPriSampler also shows its significantly superior performance to diffusion\nmodels trained with differential privacy on both model utility and defense\nperformance.",
        "translated": "扩散模型在数据综合方面取得了显著的成功。这些成功也促使扩散模型应用于敏感数据，如人脸数据，但这可能会带来严重的隐私问题。在这项工作中，我们系统地提出了第一个针对扩散模型的属性推理攻击的隐私研究，其中对手的目标是从扩散模型中提取训练集的敏感的全局属性，例如某些敏感属性的训练数据的比例。具体来说，我们考虑最实际的攻击场景: 只允许对手获取合成数据。在这种现实情况下，我们评估了属性推断攻击的不同类型的采样器和扩散模型。大量的评估表明，各种扩散模型及其采样器都容易受到属性推断攻击。此外，一个现成的预训练扩散模型的案例研究也证明了攻击在实践中的有效性。最后，我们提出了一种新的模型无关插件方法 PriSampler 来缓解扩散模型的性质推断。PriSampler 可以直接应用于训练有素的扩散模型，并支持随机和确定性采样。大量的实验证明了我们防御的有效性，它使对手推断出的属性比例就像随机猜测一样接近。PriSampler 还显示了其明显优于扩散模型的性能，扩散模型在模型实用性和防御性能方面都经过了差分隐私的训练。"
    },
    {
        "title": "Formalizing, Verifying and Applying ISA Security Guarantees as Universal\n  Contracts",
        "url": "http://arxiv.org/abs/2306.05128v1",
        "pub_date": "2023-06-08",
        "summary": "Progress has recently been made on specifying instruction set architectures\n(ISAs) in executable formalisms rather than through prose. However, to date,\nthose formal specifications are limited to the functional aspects of the ISA\nand do not cover its security guarantees. We present a novel, general method\nfor formally specifying an ISAs security guarantees to (1) balance the needs of\nISA implementations (hardware) and clients (software), (2) can be\nsemi-automatically verified to hold for the ISA operational semantics,\nproducing a high-assurance mechanically-verifiable proof, and (3) support\ninformal and formal reasoning about security-critical software in the presence\nof adversarial code. Our method leverages universal contracts: software\ncontracts that express bounds on the authority of arbitrary untrusted code.\nUniversal contracts can be kept agnostic of software abstractions, and strike\nthe right balance between requiring sufficient detail for reasoning about\nsoftware and preserving implementation freedom of ISA designers and CPU\nimplementers. We semi-automatically verify universal contracts against Sail\nimplementations of ISA semantics using our Katamaran tool; a semi-automatic\nseparation logic verifier for Sail which produces machine-checked proofs for\nsuccessfully verified contracts. We demonstrate the generality of our method by\napplying it to two ISAs that offer very different security primitives: (1)\nMinimalCaps: a custom-built capability machine ISA and (2) a (somewhat\nsimplified) version of RISC-V with PMP. We verify a femtokernel using the\nsecurity guarantee we have formalized for RISC-V with PMP.",
        "translated": "最近在指定可执行形式的指令集体系结构(ISA)方面取得了进展，而不是通过散文。然而，到目前为止，这些正式规范仅限于 ISA 的功能方面，并不包括其安全保障。我们提出了一个新颖的，通用的方法来正式指定一个 ISA 安全保证: (1)平衡 ISA 实施(硬件)和客户(软件)的需求; (2)可以半自动验证以支持 ISA 操作语义学，产生一个高保证的机械验证证据; (3)支持在对手代码存在的情况下关于安全关键软件的非正式和正式的推理。我们的方法利用了通用契约: 软件契约表达了任意不可信代码的权限。通用契约可以保持软件抽象的不可知性，并在需要足够详细的软件推理和保持 ISA 设计者和 CPU 实现者的实现自由之间取得适当的平衡。我们使用我们的 Katamaran 工具半自动验证通用合同和 Sail 实现的 ISA 语义，这是一个半自动的 Sail 分离逻辑验证器，它为成功验证的合同生成机器检查证明。我们通过将其应用于提供非常不同的安全原语的两个 ISA 来展示我们的方法的通用性: (1) MinimalCaps: 一个定制构建的能力机 ISA 和(2)一个(稍微简化的)带有 PMP 的 RISC-V 版本。我们使用我们已经形式化的 RISC-V 和 PMP 的安全保证来验证飞机内核。"
    },
    {
        "title": "FheFL: Fully Homomorphic Encryption Friendly Privacy-Preserving\n  Federated Learning with Byzantine Users",
        "url": "http://arxiv.org/abs/2306.05112v1",
        "pub_date": "2023-06-08",
        "summary": "The federated learning (FL) technique was initially developed to mitigate\ndata privacy issues that can arise in the traditional machine learning\nparadigm. While FL ensures that a user's data always remain with the user, the\ngradients of the locally trained models must be communicated with the\ncentralized server to build the global model. This results in privacy leakage,\nwhere the server can infer private information of the users' data from the\nshared gradients. To mitigate this flaw, the next-generation FL architectures\nproposed encryption and anonymization techniques to protect the model updates\nfrom the server. However, this approach creates other challenges, such as a\nmalicious user might sabotage the global model by sharing false gradients.\nSince the gradients are encrypted, the server is unable to identify and\neliminate rogue users which would protect the global model. Therefore, to\nmitigate both attacks, this paper proposes a novel fully homomorphic encryption\n(FHE) based scheme suitable for FL. We modify the one-to-one single-key\nCheon-Kim-Kim-Song (CKKS)-based FHE scheme into a distributed multi-key\nadditive homomorphic encryption scheme that supports model aggregation in FL.\nWe employ a novel aggregation scheme within the encrypted domain, utilizing\nusers' non-poisoning rates, to effectively address data poisoning attacks while\nensuring privacy is preserved by the proposed encryption scheme. Rigorous\nsecurity, privacy, convergence, and experimental analyses have been provided to\nshow that FheFL is novel, secure, and private, and achieves comparable accuracy\nat reasonable computational cost.",
        "translated": "联邦学习(FL)技术最初是为了缓解传统机器学习范式中可能出现的数据隐私问题而开发的。虽然 FL 确保用户的数据始终与用户保持一致，但是本地训练模型的梯度必须与集中式服务器通信，以构建全局模型。这会导致隐私泄露，服务器可以从共享梯度推断用户数据的隐私信息。为了减轻这一缺陷，下一代 FL 架构提出了加密和匿名技术来保护模型更新不受服务器的影响。但是，这种方法会带来其他挑战，比如恶意用户可能会通过共享虚假渐变破坏全局模型。由于梯度是加密的，服务器无法识别和消除流氓用户，这将保护全局模型。因此，为了减轻这两种攻击，本文提出了一种新的基于全同态加密(fHE)的适合于 FL 的方案。我们修改了基于一对一单密钥 Cheon-Kim-Kim-Song (CKKS)的 FHE 方案为一个分布式多密钥加法同态加密方案，该方案支持 FL 中的模型聚合。我们在加密域内采用一种新的聚合方案，利用用户的非中毒率，有效地解决数据中毒攻击，同时确保所提出的加密方案能够保护隐私。严格的安全性、隐私性、收敛性和实验分析表明，FheFL 是新颖的、安全的和私有的，并且在合理的计算成本下达到可比较的精度。"
    },
    {
        "title": "On the Robustness of Topics API to a Re-Identification Attack",
        "url": "http://arxiv.org/abs/2306.05094v1",
        "pub_date": "2023-06-08",
        "summary": "Web tracking through third-party cookies is considered a threat to users'\nprivacy and is supposed to be abandoned in the near future. Recently, Google\nproposed the Topics API framework as a privacy-friendly alternative for\nbehavioural advertising. Using this approach, the browser builds a user profile\nbased on navigation history, which advertisers can access. The Topics API has\nthe possibility of becoming the new standard for behavioural advertising, thus\nit is necessary to fully understand its operation and find possible\nlimitations.\n  This paper evaluates the robustness of the Topics API to a re-identification\nattack where an attacker reconstructs the user profile by accumulating user's\nexposed topics over time to later re-identify the same user on a different\nwebsite. Using real traffic traces and realistic population models, we find\nthat the Topics API mitigates but cannot prevent re-identification to take\nplace, as there is a sizeable chance that a user's profile is unique within a\nwebsite's audience. Consequently, the probability of correct re-identification\ncan reach 15-17%, considering a pool of 1,000 users. We offer the code and data\nwe use in this work to stimulate further studies and the tuning of the Topic\nAPI parameters.",
        "translated": "通过第三方 cookie 进行的网络跟踪被认为是对用户隐私的威胁，应该会在不久的将来被废弃。最近，Google 提出将 Topics API 框架作为行为广告的一个隐私友好的替代方案。使用这种方法，浏览器建立一个基于导航历史的用户配置文件，广告商可以访问。主题 API 有可能成为行为广告的新标准，因此有必要充分了解其运作并找出可能的局限性。本文评估了 Topics API 对重新识别攻击的健壮性，攻击者通过累积用户暴露的主题来重建用户配置文件，然后在不同的网站上重新识别相同的用户。通过使用真实的流量跟踪和真实的人口模型，我们发现 Topics API 可以缓解但不能阻止重新识别的发生，因为一个用户的个人资料在一个网站的受众中有很大的可能性是独一无二的。因此，考虑到有1,000个用户，正确重新识别的可能性可以达到15-17% 。我们提供了在这项工作中使用的代码和数据，以促进进一步的研究和 Topic API 参数的调优。"
    },
    {
        "title": "Re-aligning Shadow Models can Improve White-box Membership Inference\n  Attacks",
        "url": "http://arxiv.org/abs/2306.05093v1",
        "pub_date": "2023-06-08",
        "summary": "Machine learning models have been shown to leak sensitive information about\ntheir training datasets. As models are being increasingly used, on devices, to\nautomate tasks and power new applications, there have been concerns that such\nwhite-box access to its parameters, as opposed to the black-box setting which\nonly provides query access to the model, increases the attack surface. Directly\nextending the shadow modelling technique from the black-box to the white-box\nsetting has been shown, in general, not to perform better than black-box only\nattacks. A key reason is misalignment, a known characteristic of deep neural\nnetworks. We here present the first systematic analysis of the causes of\nmisalignment in shadow models and show the use of a different weight\ninitialisation to be the main cause of shadow model misalignment. Second, we\nextend several re-alignment techniques, previously developed in the model\nfusion literature, to the shadow modelling context, where the goal is to\nre-align the layers of a shadow model to those of the target model.We show\nre-alignment techniques to significantly reduce the measured misalignment\nbetween the target and shadow models. Finally, we perform a comprehensive\nevaluation of white-box membership inference attacks (MIA). Our analysis\nreveals that (1) MIAs suffer from misalignment between shadow models, but that\n(2) re-aligning the shadow models improves, sometimes significantly, MIA\nperformance. On the CIFAR10 dataset with a false positive rate of 1\\%,\nwhite-box MIA using re-aligned shadow models improves the true positive rate by\n4.5\\%.Taken together, our results highlight that on-device deployment increase\nthe attack surface and that the newly available information can be used by an\nattacker.",
        "translated": "机器学习模型已被证明会泄漏关于其训练数据集的敏感信息。随着模型在设备上越来越多地被用于自动化任务和驱动新的应用程序，有人担心，这种白盒访问其参数，而不是黑盒设置，只提供对模型的查询访问，增加了攻击面。直接将阴影建模技术从黑盒扩展到白盒设置已经被证明，一般来说，不会比黑盒单独的攻击表现得更好。一个关键原因是错位，这是深度神经网络的一个已知特征。本文首次系统地分析了影子模型失调的原因，并指出使用不同的权重初始化是影子模型失调的主要原因。其次，我们将以前在模型融合文献中开发的几种重新对齐技术扩展到阴影建模背景，其中目标是将阴影模型的层重新对齐到目标模型的层。我们显示重新对齐技术以显着减少目标和阴影模型之间测量的不对齐。最后，我们对白盒成员推理攻击(MIA)进行了综合评估。我们的分析表明: (1)阴影模型之间存在失调，但是(2)重新调整阴影模型可以显著提高 MIA 性能。在 CIFAR10数据集中，假阳性率为1% ，使用重新对齐的阴影模型的白盒 MIA 使真阳性率提高了4.5% 。综上所述，我们的结果强调了设备上的部署增加了攻击面，并且新获得的信息可以被攻击者使用。"
    },
    {
        "title": "\"My sex-related data is more sensitive than my financial data and I want\n  the same level of security and privacy\": User Risk Perceptions and Protective\n  Actions in Female-oriented Technologies",
        "url": "http://arxiv.org/abs/2306.05956v1",
        "pub_date": "2023-06-09",
        "summary": "The digitalization of the reproductive body has engaged myriads of\ncutting-edge technologies in supporting people to know and tackle their\nintimate health. Generally understood as female technologies (aka\nfemale-oriented technologies or 'FemTech'), these products and systems collect\na wide range of intimate data which are processed, transferred, saved and\nshared with other parties. In this paper, we explore how the \"data-hungry\"\nnature of this industry and the lack of proper safeguarding mechanisms,\nstandards, and regulations for vulnerable data can lead to complex harms or\nfaint agentic potential. We adopted mixed methods in exploring users'\nunderstanding of the security and privacy (SP) of these technologies. Our\nfindings show that while users can speculate the range of harms and risks\nassociated with these technologies, they are not equipped and provided with the\ntechnological skills to protect themselves against such risks. We discuss a\nnumber of approaches, including participatory threat modelling and SP by\ndesign, in the context of this work and conclude that such approaches are\ncritical to protect users in these sensitive systems.",
        "translated": "生殖机构的数字化使用了无数尖端技术，支持人们了解和处理自己的亲密健康。这些产品和系统通常被理解为女性技术(又名面向女性的技术或“ FemTech”) ，它们收集各种各样的私密数据，这些数据被处理、传输、保存并与其他各方共享。在本文中，我们探讨了这个行业的“数据饥渴”性质和缺乏适当的保护机制，标准和法规的脆弱数据可能导致复杂的危害或微弱的代理潜力。我们采用混合的方法来探索用户对这些技术的安全性和隐私性(SP)的理解。我们的研究结果表明，虽然用户可以推测与这些技术相关的危害和风险的范围，但他们没有装备和提供技术技能来保护自己免受这些风险。在这项工作的背景下，我们讨论了一些方法，包括参与式威胁建模和按设计的 SP，并得出结论认为，这些方法对于保护这些敏感系统中的用户至关重要。"
    },
    {
        "title": "GAN-CAN: A Novel Attack to Behavior-Based Driver Authentication Systems",
        "url": "http://arxiv.org/abs/2306.05923v2",
        "pub_date": "2023-06-09",
        "summary": "For many years, car keys have been the sole mean of authentication in\nvehicles. Whether the access control process is physical or wireless,\nentrusting the ownership of a vehicle to a single token is prone to stealing\nattempts. For this reason, many researchers started developing behavior-based\nauthentication systems. By collecting data in a moving vehicle, Deep Learning\n(DL) models can recognize patterns in the data and identify drivers based on\ntheir driving behavior. This can be used as an anti-theft system, as a thief\nwould exhibit a different driving style compared to the vehicle owner's.\nHowever, the assumption that an attacker cannot replicate the legitimate driver\nbehavior falls under certain conditions.\n  In this paper, we propose GAN-CAN, the first attack capable of fooling\nstate-of-the-art behavior-based driver authentication systems in a vehicle.\nBased on the adversary's knowledge, we propose different GAN-CAN\nimplementations. Our attack leverages the lack of security in the Controller\nArea Network (CAN) to inject suitably designed time-series data to mimic the\nlegitimate driver. Our design of the malicious time series results from the\ncombination of different Generative Adversarial Networks (GANs) and our study\non the safety importance of the injected values during the attack. We tested\nGAN-CAN in an improved version of the most efficient driver behavior-based\nauthentication model in the literature. We prove that our attack can fool it\nwith an attack success rate of up to 0.99. We show how an attacker, without\nprior knowledge of the authentication system, can steal a car by deploying\nGAN-CAN in an off-the-shelf system in under 22 minutes.",
        "translated": "多年来，车钥匙一直是车辆认证的唯一手段。无论访问控制过程是物理的还是无线的，将车辆的所有权委托给单个令牌都容易导致窃取尝试。出于这个原因，许多研究人员开始开发基于行为的认证系统。深度学习(DL)模型通过在行驶中的车辆中收集数据，可以识别数据中的模式，并根据驾驶员的驾驶行为识别驾驶员。这可以作为一个防盗系统，因为小偷会表现出不同的驾驶风格相比，车主的。但是，攻击者不能复制合法的驱动程序行为的假设在某些情况下是不成立的。在本文中，我们提出的 GAN-CAN，第一次攻击能够欺骗国家的最先进的行为为基础的驾驶员认证系统在车辆。基于对手的知识，我们提出了不同的 GAN-CAN 实现方案。我们的攻击利用控制器局域网路(CAN)缺乏安全性的特点，注入适当设计的时间序列数据来模拟合法的驱动程序。我们对恶意时间序列的设计结合了不同的生成对抗网络(GAN)和我们对注入值在攻击过程中的安全重要性的研究。我们在文献中最有效的基于驾驶员行为的认证模型的改进版本中测试了 GAN-CAN。我们证明我们的攻击可以愚弄它，攻击成功率高达0.99。我们展示了攻击者如何在没有事先了解认证系统的情况下，通过在不到22分钟的时间内在现成的系统中部署 GAN-CAN 来窃取汽车。"
    },
    {
        "title": "Differentially Private All-Pairs Shortest Distances for Low Tree-Width\n  Graphs",
        "url": "http://arxiv.org/abs/2306.05916v1",
        "pub_date": "2023-06-09",
        "summary": "In this paper, we present a polynomial time algorithm for the problem of\ndifferentially private all pair shortest distances over the class of low\ntree-width graphs. Our result generalizes the result of Sealfon 2016 for the\ncase of trees to a much larger family of graphs. Furthermore, if we restrict to\nthe class of low tree-width graphs, the additive error of our algorithm is\nsignificantly smaller than that of the best known algorithm for this problem,\nproposed by Chen et. al. 2023.",
        "translated": "本文提出了一种求解低树宽图类上的全对最短距离的差分私有化问题的多项式时间算法。我们的结果将 Sealfon 2016的结果推广到一个更大的图族。此外，如果我们限制在低树宽度图的类别，我们的算法的加性误差明显小于最好的已知算法对这个问题，提出了陈等人。Al 2023."
    },
    {
        "title": "You Can Tell a Cybercriminal by the Company they Keep: A Framework to\n  Infer the Relevance of Underground Communities to the Threat Landscape",
        "url": "http://arxiv.org/abs/2306.05898v1",
        "pub_date": "2023-06-09",
        "summary": "The criminal underground is populated with forum marketplaces where,\nallegedly, cybercriminals share and trade knowledge, skills, and cybercrime\nproducts. However, it is still unclear whether all marketplaces matter the same\nin the overall threat landscape. To effectively support trade and avoid\ndegenerating into scams-for-scammers places, underground markets must address\nfundamental economic problems (such as moral hazard, adverse selection) that\nenable the exchange of actual technology and cybercrime products (as opposed to\nrepackaged malware or years-old password databases). From the relevant\nliterature and manual investigation, we identify several mechanisms that\nmarketplaces implement to mitigate these problems, and we condense them into a\nmarket evaluation framework based on the Business Model Canvas. We use this\nframework to evaluate which mechanisms `successful' marketplaces have in place,\nand whether these differ from those employed by `unsuccessful' marketplaces. We\ntest the framework on 23 underground forum markets by searching 836 aliases of\nindicted cybercriminals to identify `successful' marketplaces. We find evidence\nthat marketplaces whose administrators are impartial in trade, verify their\nsellers, and have the right economic incentives to keep the market functional\nare more likely to be credible sources of threat.",
        "translated": "地下犯罪集团充斥着论坛市场，据称，网络犯罪分子在那里分享和交易知识、技能和网络犯罪产品。然而，目前还不清楚是否所有的市场在整体的威胁情况下都是一样的。为了有效地支持贸易，避免沦为骗子的地盘，地下市场必须解决根本性的经济问题(如道德风险、逆向选择) ，以便能够交换实际的技术和网络犯罪产品(而不是重新包装的恶意软件或存在多年的密码数据库)。从相关文献和手工调查中，我们确定了市场实施的几种机制来缓解这些问题，并将它们浓缩成一个基于商业模式图的市场评估框架。我们使用这个框架来评估哪些机制“成功的”市场已经到位，以及这些机制是否不同于“不成功的”市场所使用的机制。我们在23个地下论坛市场上通过搜索836个被起诉的网络犯罪分子的化名来确定“成功的”市场来测试这个框架。我们发现有证据表明，那些管理者在贸易方面不偏不倚、核实卖家、并具有保持市场正常运转的正确经济激励的市场，更有可能成为可信的威胁来源。"
    },
    {
        "title": "Detecting Adversarial Directions in Deep Reinforcement Learning to Make\n  Robust Decisions",
        "url": "http://arxiv.org/abs/2306.05873v1",
        "pub_date": "2023-06-09",
        "summary": "Learning in MDPs with highly complex state representations is currently\npossible due to multiple advancements in reinforcement learning algorithm\ndesign. However, this incline in complexity, and furthermore the increase in\nthe dimensions of the observation came at the cost of volatility that can be\ntaken advantage of via adversarial attacks (i.e. moving along worst-case\ndirections in the observation space). To solve this policy instability problem\nwe propose a novel method to detect the presence of these non-robust directions\nvia local quadratic approximation of the deep neural policy loss. Our method\nprovides a theoretical basis for the fundamental cut-off between safe\nobservations and adversarial observations. Furthermore, our technique is\ncomputationally efficient, and does not depend on the methods used to produce\nthe worst-case directions. We conduct extensive experiments in the Arcade\nLearning Environment with several different adversarial attack techniques. Most\nsignificantly, we demonstrate the effectiveness of our approach even in the\nsetting where non-robust directions are explicitly optimized to circumvent our\nproposed method.",
        "translated": "由于强化学习算法设计的多项进步，目前可以在具有高度复杂状态表示的 MDP 中进行学习。然而，这种复杂性的增加以及观察维度的增加是以可以通过对抗性攻击(即沿着观察空间中的最坏情况方向移动)利用的波动性为代价的。为了解决这个策略不稳定性问题，我们提出了一种新的方法来检测这些非鲁棒方向的存在，通过局部二次逼近的深度神经策略损失。我们的方法为安全观察和对抗观察之间的基本分界提供了理论基础。此外，我们的技术是计算效率，并不依赖于方法用于产生最坏的情况下的方向。我们在拱廊学习环境中使用几种不同的对抗性攻击技术进行广泛的实验。最重要的是，我们证明了我们的方法的有效性，即使在非鲁棒的方向是明确优化的设置，以规避我们提出的方法。"
    },
    {
        "title": "Detecting Phishing Sites Using ChatGPT",
        "url": "http://arxiv.org/abs/2306.05816v1",
        "pub_date": "2023-06-09",
        "summary": "The rise of large language models (LLMs) has had a significant impact on\nvarious domains, including natural language processing and artificial\nintelligence. While LLMs such as ChatGPT have been extensively researched for\ntasks such as code generation and text synthesis, their application in\ndetecting malicious web content, particularly phishing sites, has been largely\nunexplored. To combat the rising tide of automated cyber attacks facilitated by\nLLMs, it is imperative to automate the detection of malicious web content,\nwhich requires approaches that leverage the power of LLMs to analyze and\nclassify phishing sites. In this paper, we propose a novel method that utilizes\nChatGPT to detect phishing sites. Our approach involves leveraging a web\ncrawler to gather information from websites and generate prompts based on this\ncollected data. This approach enables us to detect various phishing sites\nwithout the need for fine-tuning machine learning models and identify social\nengineering techniques from the context of entire websites and URLs. To\nevaluate the performance of our proposed method, we conducted experiments using\na dataset. The experimental results using GPT-4 demonstrated promising\nperformance, with a precision of 98.3% and a recall of 98.4%. Comparative\nanalysis between GPT-3.5 and GPT-4 revealed an enhancement in the latter's\ncapability to reduce false negatives. These findings not only highlight the\npotential of LLMs in efficiently identifying phishing sites but also have\nsignificant implications for enhancing cybersecurity measures and protecting\nusers from the dangers of online fraudulent activities.",
        "translated": "大语言模型(LLM)的兴起对自然语言处理和人工智能等领域产生了重大影响。虽然像 ChatGPT 这样的 LLM 已经被广泛研究用于代码生成和文本合成等任务，但它们在检测恶意网络内容(尤其是网络钓鱼网站)方面的应用在很大程度上还没有被探索。为了应对 LLM 推动的自动化网络攻击浪潮，必须自动检测恶意网络内容，这需要利用 LLM 的能力来分析和分类钓鱼网站。本文提出了一种利用 ChatGPT 检测网络钓鱼网站的新方法。我们的方法包括利用网络爬虫从网站收集信息，并根据收集到的数据生成提示。这种方法使我们能够检测各种钓鱼网站，而不需要对机器学习模型进行微调，并从整个网站和 URL 的上下文中识别社会工程技术。为了评估我们提出的方法的性能，我们使用一个数据集进行了实验。实验结果表明 GPT-4具有良好的性能，准确率为98.3% ，召回率为98.4% 。GPT-3.5和 GPT-4的比较分析显示后者减少假阴性的能力有所提高。这些调查结果不仅突出了 LLM 在有效识别钓鱼网站方面的潜力，而且对加强网络安全措施和保护用户免遭网上欺诈活动的危险具有重大影响。"
    },
    {
        "title": "DETECTA: Investigación de metodologías no intrusivas apoyadas en\n  tecnologías habilitadoras 4.0 para abordar un mantenimiento predictivo y\n  ciberseguro en pymes industriales",
        "url": "http://arxiv.org/abs/2306.05799v1",
        "pub_date": "2023-06-09",
        "summary": "This work presents the results of the DETECTA project, which addresses\nindustrial research activities for the generation of predictive knowledge aimed\nat detecting anomalies in machining-based manufacturing systems. It addresses\ndifferent technological challenges to simultaneously improve the availability\nof machinery and the protection against cyberthreats of industrial systems,\nwith the collaboration of knowledge centers and experts in industrial\nprocesses. Through the use of innovative technologies such as the digital twin\nand artificial intelligence, it implements process characterization\nmethodologies and anomaly detection in a non-intrusive way without limiting the\nproductivity of the industrial plant according to the maintenance and remote\naccess needs. The research has been supported by a general evaluation of\nconnected environments in small and medium-sized enterprises to identify if the\nbenefits of digitization outweigh the risks that cannot be eliminated. The\nresults obtained, through a process of supervision by process experts and\nmachine learning, have made it possible to discriminate anomalies between\npurely technical events and events related to cyber incidents or cyber attacks.",
        "translated": "这项工作介绍了 DETECTA 项目的结果，该项目涉及产生预测性知识的工业研究活动，这些预测性知识旨在检测基于机械加工的制造系统中的异常。它通过知识中心和工业过程专家的合作，应对不同的技术挑战，同时改善机械的可用性和防范工业系统的网络威胁。通过使用创新技术，例如数码双胞胎和人工智能，它以非侵入性的方式实施过程角色塑造方法和异常检测，而不会根据维修和远程访问的需要限制工业厂房的生产力。这项研究得到了对中小型企业联网环境的一般评价的支持，以确定数字化的好处是否大于无法消除的风险。通过过程专家和机器学习的监督过程所获得的结果，使我们能够区分纯技术事件和与网络事件或网络攻击有关的事件之间的异常。"
    },
    {
        "title": "Cross-Consensus Measurement of Individual-level Decentralization in\n  Blockchains",
        "url": "http://arxiv.org/abs/2306.05788v1",
        "pub_date": "2023-06-09",
        "summary": "Decentralization is widely recognized as a crucial characteristic of\nblockchains that enables them to resist malicious attacks such as the 51%\nattack and the takeover attack. Prior research has primarily examined\ndecentralization in blockchains employing the same consensus protocol or at the\nlevel of block producers. This paper presents the first individual-level\nmeasurement study comparing the decentralization of blockchains employing\ndifferent consensus protocols. To facilitate cross-consensus evaluation, we\npresent a two-level comparison framework and a new metric. We apply the\nproposed methods to Ethereum and Steem, two representative blockchains for\nwhich decentralization has garnered considerable interest. Our findings dive\ndeeper into the level of decentralization, suggest the existence of\ncentralization risk at the individual level in Steem, and provide novel\ninsights into the cross-consensus comparison of decentralization in\nblockchains.",
        "translated": "地方分权被广泛认为是区块链的一个关键特征，它使区块链能够抵御恶意攻击，如51% 攻击和接管攻击。先前的研究主要是在区块链中使用相同的协商一致方案或在区块生产者的水平上研究地方分权。本文介绍了第一个个体水平的测量研究，比较采用不同共识方案的区块链的地方分权。为了便于交叉共识评价，我们提出了一个两级比较框架和一个新的度量。我们将提议的方法应用于 Ethereum 和 Steem，这两个具有代表性的区块链地方分权已经引起了相当大的兴趣。我们的研究结果更深入地探讨了地方分权的水平，提出了在 Steem 个人层面存在的集中风险，并为区块链中地方分权的交叉共识比较提供了新颖的见解。"
    },
    {
        "title": "DP-HyPO: An Adaptive Private Hyperparameter Optimization Framework",
        "url": "http://arxiv.org/abs/2306.05734v1",
        "pub_date": "2023-06-09",
        "summary": "Hyperparameter optimization, also known as hyperparameter tuning, is a widely\nrecognized technique for improving model performance. Regrettably, when\ntraining private ML models, many practitioners often overlook the privacy risks\nassociated with hyperparameter optimization, which could potentially expose\nsensitive information about the underlying dataset. Currently, the sole\nexisting approach to allow privacy-preserving hyperparameter optimization is to\nuniformly and randomly select hyperparameters for a number of runs,\nsubsequently reporting the best-performing hyperparameter. In contrast, in\nnon-private settings, practitioners commonly utilize \"adaptive\" hyperparameter\noptimization methods such as Gaussian process-based optimization, which select\nthe next candidate based on information gathered from previous outputs. This\nsubstantial contrast between private and non-private hyperparameter\noptimization underscores a critical concern. In our paper, we introduce\nDP-HyPO, a pioneering framework for \"adaptive\" private hyperparameter\noptimization, aiming to bridge the gap between private and non-private\nhyperparameter optimization. To accomplish this, we provide a comprehensive\ndifferential privacy analysis of our framework. Furthermore, we empirically\ndemonstrate the effectiveness of DP-HyPO on a diverse set of real-world and\nsynthetic datasets.",
        "translated": "超参数优化，也称为超参数调整，是一种被广泛认可的改善模型性能的技术。遗憾的是，在训练私有机器学习模型时，许多从业者往往忽视了与超参数优化相关的隐私风险，这可能会暴露关于底层数据集的敏感信息。目前，允许保护隐私的超参数优化的唯一现有方法是为多次运行统一和随机选择超参数，随后报告性能最好的超参数。相比之下，在非私有设置，从业人员通常利用“自适应”超参数优化方法，如高斯过程为基础的优化，选择下一个候选人的基础上收集的信息从以前的输出。私有和非私有超参数优化之间的实质性对比强调了一个关键问题。在本文中，我们介绍了 DP-HyPO，一个“自适应”私有超参数优化的开创性框架，旨在弥补私有和非私有超参数优化之间的差距。为了达到这个目的，我们对我们的架构进行了全面的差分隐私分析。此外，我们实证证明了 DP-HyPO 在一组不同的真实世界和合成数据集上的有效性。"
    },
    {
        "title": "JABBERWOCK: A Tool for WebAssembly Dataset Generation and Its\n  Application to Malicious Website Detection",
        "url": "http://arxiv.org/abs/2306.05698v1",
        "pub_date": "2023-06-09",
        "summary": "Machine learning is often used for malicious website detection, but an\napproach incorporating WebAssembly as a feature has not been explored due to a\nlimited number of samples, to the best of our knowledge. In this paper, we\npropose JABBERWOCK (JAvascript-Based Binary EncodeR by WebAssembly Optimization\npaCKer), a tool to generate WebAssembly datasets in a pseudo fashion via\nJavaScript. Loosely speaking, JABBERWOCK automatically gathers JavaScript code\nin the real world, convert them into WebAssembly, and then outputs vectors of\nthe WebAssembly as samples for malicious website detection. We also conduct\nexperimental evaluations of JABBERWOCK in terms of the processing time for\ndataset generation, comparison of the generated samples with actual WebAssembly\nsamples gathered from the Internet, and an application for malicious website\ndetection. Regarding the processing time, we show that JABBERWOCK can construct\na dataset in 4.5 seconds per sample for any number of samples. Next, comparing\n10,000 samples output by JABBERWOCK with 168 gathered WebAssembly samples, we\nbelieve that the generated samples by JABBERWOCK are similar to those in the\nreal world. We then show that JABBERWOCK can provide malicious website\ndetection with 99\\% F1-score because JABBERWOCK makes a gap between benign and\nmalicious samples as the reason for the above high score. We also confirm that\nJABBERWOCK can be combined with an existing malicious website detection tool to\nimprove F1-scores. JABBERWOCK is publicly available via GitHub\n(https://github.com/c-chocolate/Jabberwock).",
        "translated": "机器学习通常用于恶意网站检测，但是据我们所知，由于样本数量有限，还没有探索将 WebAssembly 作为一个特性的方法。在本文中，我们提出了 JABBERWOCK (通过 WebAssembly 优化包工具基于 JavaScript 的二进制编码器) ，一个通过 JavaScript 以伪方式生成 WebAssembly 数据集的工具。粗略地说，JABBERWOCK 自动收集现实世界中的 JavaScript 代码，将它们转换为 WebAssembly，然后将 WebAssembly 的向量输出为用于恶意网站检测的示例。我们还对 JABBERWOCK 进行了实验性的评估，包括数据集生成的处理时间、生成的样本与从互联网收集的实际 WebAssembly 样本的比较，以及恶意网站检测应用程序。关于处理时间，我们展示了 JABBERWOCK 可以在4.5秒内为任意数量的样本构造一个数据集。接下来，将 JABBERWOCK 输出的10,000个样本与收集的168个 WebAssembly 样本进行比较，我们认为 JABBERWOCK 生成的样本与现实世界中的样本相似。然后我们证明了 JABBERWOCK 可以提供99% 的 F1分数的恶意网站检测，因为 JABBERWOCK 使良性和恶意样本之间的差距作为上述高分的原因。我们还证实，JABBERWOCK 可以结合现有的恶意网站检测工具，以提高 F1分数。JABBERWOCK 可以通过 gitHub ( https://GitHub.com/c-chocolate/JABBERWOCK )公开获得。"
    },
    {
        "title": "Gaussian Membership Inference Privacy",
        "url": "http://arxiv.org/abs/2306.07273v1",
        "pub_date": "2023-06-12",
        "summary": "We propose a new privacy notion called $f$-Membership Inference Privacy\n($f$-MIP), which explicitly considers the capabilities of realistic adversaries\nunder the membership inference attack threat model. By doing so $f$-MIP offers\ninterpretable privacy guarantees and improved utility (e.g., better\nclassification accuracy). Our novel theoretical analysis of likelihood\nratio-based membership inference attacks on noisy stochastic gradient descent\n(SGD) results in a parametric family of $f$-MIP guarantees that we refer to as\n$\\mu$-Gaussian Membership Inference Privacy ($\\mu$-GMIP). Our analysis\nadditionally yields an analytical membership inference attack that offers\ndistinct advantages over previous approaches. First, unlike existing methods,\nour attack does not require training hundreds of shadow models to approximate\nthe likelihood ratio. Second, our analytical attack enables straightforward\nauditing of our privacy notion $f$-MIP. Finally, our analysis emphasizes the\nimportance of various factors, such as hyperparameters (e.g., batch size,\nnumber of model parameters) and data specific characteristics in controlling an\nattacker's success in reliably inferring a given point's membership to the\ntraining set. We demonstrate the effectiveness of our method on models trained\nacross vision and tabular datasets.",
        "translated": "我们提出了一种新的隐私概念，称为 $f $- 成员推理隐私($f $- MIP) ，它明确考虑了成员推理攻击威胁模型下现实对手的能力。通过这样做 $f $- MIP 提供了可解释的隐私保障和改进的实用性(例如，更好的分类准确性)。我们对噪声随机梯度下降(SGD)的基于似然比的成员推理攻击的新颖理论分析导致了 $f $- MIP 保证的参数族，我们称之为 $mu $- Gaussian 成员推理隐私($mu $- gmIP)。我们的分析还产生了一种分析性成员推断攻击，它比以前的方法具有明显的优势。首先，与现有的方法不同，我们的攻击不需要训练数百个阴影模型来近似似然比。其次，我们的分析攻击可以直接审计我们的隐私概念 $f $- MIP。最后，我们的分析强调了各种因素的重要性，例如超参数(例如，批量大小，模型参数的数量)和数据特定特征在控制攻击者成功可靠地推断给定点的训练集成员。我们证明了我们的方法的有效性模型训练跨视觉和表格数据集。"
    },
    {
        "title": "Generic Attacks against Cryptographic Hardware through Long-Range Deep\n  Learning",
        "url": "http://arxiv.org/abs/2306.07249v1",
        "pub_date": "2023-06-12",
        "summary": "Hardware-based cryptographic implementations utilize countermeasures to\nresist side-channel attacks. In this paper, we propose a novel deep-learning\narchitecture for side-channel analysis called SCANET that generalizes across\nmultiple implementations and algorithms without manual tuning or trace\npre-processing. We achieve this by combining a novel input processing technique\nwith several advanced deep learning techniques including transformer blocks and\nmulti-task learning. We demonstrate the generality of our approach by\nsuccessfully attacking four hardware-accelerated countermeasures for elliptic\ncurve digital signatures in an end-to-end manner without human tuning.\nAdditionally, we showcase SCANET's ability to generalize across multiple\nalgorithms by successfully replicating state-of-the-art attacks against\nprotected AES without the need for trace preprocessing, hand-tuning, or model\narchitectural changes. These results offer promising prospects for generic and\nautomated side-channel leakage evaluation without manual effort.",
        "translated": "基于硬件的加密实现利用对策来抵抗侧信道攻击。在本文中，我们提出了一种新的边通道分析的深度学习体系结构，称为 SCANET，它可以在不需要手动调优或跟踪预处理的情况下跨多个实现和算法进行泛化。我们通过将一种新的输入处理技术与包括变压器块和多任务学习在内的几种先进的深度学习技术相结合来实现这一目标。我们证明了我们的方法的一般性，成功地攻击四个硬件加速的椭圆曲线数字签名在一个端到端的方式没有人工调整的对策。此外，我们还展示了 SCANET 通过成功复制针对受保护的 AES 的最先进的攻击而不需要跟踪预处理、手工调优或模型架构更改的多种算法进行泛化的能力。这些结果提供了通用和自动化的前景，侧渠泄漏评价无需人工努力。"
    },
    {
        "title": "Cybersecurity Training for Users of Remote Computing",
        "url": "http://arxiv.org/abs/2306.07192v1",
        "pub_date": "2023-06-12",
        "summary": "End users of remote computing systems are frequently not aware of basic ways\nin which they could enhance protection against cyber-threats and attacks. In\nthis paper, we discuss specific techniques to help and train users to improve\ncybersecurity when using such systems. To explain the rationale behind these\ntechniques, we go into some depth explaining possible threats in the context of\nusing remote, shared computing resources. Although some of the details of these\nprescriptions and recommendations apply to specific use cases when connecting\nto remote servers, such as a supercomputer, cluster, or Linux workstation, the\nmain concepts and ideas can be applied to a wider spectrum of cases.",
        "translated": "远程计算系统的最终用户常常不知道他们可以通过哪些基本方法加强对网络威胁和攻击的保护。在本文中，我们讨论了在使用这些系统时帮助和培训用户提高网络安全的具体技术。为了解释这些技术背后的基本原理，我们深入解释了在使用远程共享计算资源的上下文中可能存在的威胁。尽管这些规定和建议的一些细节适用于连接到远程服务器(如超级计算机、集群或 Linux 工作站)时的特定用例，但主要概念和思想可以应用于更广泛的用例。"
    },
    {
        "title": "Twitter Bots Influence on the Russo-Ukrainian War During the 2022\n  Italian General Elections",
        "url": "http://arxiv.org/abs/2306.07183v1",
        "pub_date": "2023-06-12",
        "summary": "In February 2022, Russia launched a full-scale invasion of Ukraine. This\nevent had global repercussions, especially on the political decisions of\nEuropean countries. As expected, the role of Italy in the conflict became a\nmajor campaign issue for the Italian General Election held on 25 September\n2022. Politicians frequently use Twitter to communicate during political\ncampaigns, but bots often interfere and attempt to manipulate elections. Hence,\nunderstanding whether bots influenced public opinion regarding the conflict\nand, therefore, the elections is essential.\n  In this work, we investigate how Italian politics responded to the\nRusso-Ukrainian conflict on Twitter and whether bots manipulated public opinion\nbefore the 2022 general election. We first analyze 39,611 tweets of six major\npolitical Italian parties to understand how they discussed the war during the\nperiod February-December 2022. Then, we focus on the 360,823 comments under the\nlast month's posts before the elections, discovering around 12% of the\ncommenters are bots. By examining their activities, it becomes clear they both\ndistorted how war topics were treated and influenced real users during the last\nmonth before the elections.",
        "translated": "2022年2月，俄罗斯对乌克兰发动了全面入侵。这一事件对全球产生了影响，尤其是对欧洲国家的政治决策产生了影响。正如所料，意大利在冲突中的作用成为2022年9月25日举行的意大利大选的一个主要竞选议题。政客们经常在政治竞选期间使用 Twitter 进行交流，但是机器人经常干扰并试图操纵选举。因此，了解机器人是否影响了关于冲突的公众舆论，因此，选举是至关重要的。在这项研究中，我们调查了意大利政界是如何在 Twitter 上回应俄罗斯与乌克兰的冲突，以及机器人是否在2022年大选前操纵了公众舆论。我们首先分析了意大利六个主要政党的39611条推文，以了解他们在2022年2月至12月期间是如何讨论这场战争的。然后，我们关注选举前一个月帖子下的360,823条评论，发现大约12% 的评论者是机器人。通过检查他们的活动，可以清楚地看到，在选举前的最后一个月，他们都歪曲了战争话题的处理方式，并对真正的用户产生了影响。"
    },
    {
        "title": "Frequency-Based Vulnerability Analysis of Deep Learning Models against\n  Image Corruptions",
        "url": "http://arxiv.org/abs/2306.07178v1",
        "pub_date": "2023-06-12",
        "summary": "Deep learning models often face challenges when handling real-world image\ncorruptions. In response, researchers have developed image corruption datasets\nto evaluate the performance of deep neural networks in handling such\ncorruptions. However, these datasets have a significant limitation: they do not\naccount for all corruptions encountered in real-life scenarios. To address this\ngap, we present MUFIA (Multiplicative Filter Attack), an algorithm designed to\nidentify the specific types of corruptions that can cause models to fail. Our\nalgorithm identifies the combination of image frequency components that render\na model susceptible to misclassification while preserving the semantic\nsimilarity to the original image. We find that even state-of-the-art models\ntrained to be robust against known common corruptions struggle against the low\nvisibility-based corruptions crafted by MUFIA. This highlights the need for\nmore comprehensive approaches to enhance model robustness against a wider range\nof real-world image corruptions.",
        "translated": "深度学习模型在处理真实世界的图像损坏时经常面临挑战。作为回应，研究人员已经开发了图像腐败数据集，以评估深层神经网络在处理此类腐败方面的性能。但是，这些数据集有一个显著的局限性: 它们不能解释在实际场景中遇到的所有损坏。为了弥补这一差距，我们提出了 MUFIA (乘法过滤攻击) ，一种用于识别可能导致模型失败的特定类型的损坏的算法。该算法在保持原始图像语义相似性的同时，识别出使模型容易错误分类的图像频率分量的组合。我们发现，即使是最先进的模型，训练有素的强大反对已知的普通腐败斗争低可见度为基础的腐败由 MUFIA 精心设计。这突出表明，需要采取更加全面的办法，提高模型对更广泛的现实世界图像损坏的稳健性。"
    },
    {
        "title": "On building machine learning pipelines for Android malware detection: a\n  procedural survey of practices, challenges and opportunities",
        "url": "http://arxiv.org/abs/2306.07118v1",
        "pub_date": "2023-06-12",
        "summary": "As the smartphone market leader, Android has been a prominent target for\nmalware attacks. The number of malicious applications (apps) identified for it\nhas increased continually over the past decade, creating an immense challenge\nfor all parties involved. For market holders and researchers, in particular,\nthe large number of samples has made manual malware detection unfeasible,\nleading to an influx of research that investigate Machine Learning (ML)\napproaches to automate this process. However, while some of the proposed\napproaches achieve high performance, rapidly evolving Android malware has made\nthem unable to maintain their accuracy over time. This has created a need in\nthe community to conduct further research, and build more flexible ML\npipelines. Doing so, however, is currently hindered by a lack of systematic\noverview of the existing literature, to learn from and improve upon the\nexisting solutions. Existing survey papers often focus only on parts of the ML\nprocess (e.g., data collection or model deployment), while omitting other\nimportant stages, such as model evaluation and explanation. In this paper, we\naddress this problem with a review of 42 highly-cited papers, spanning a decade\nof research (from 2011 to 2021). We introduce a novel procedural taxonomy of\nthe published literature, covering how they have used ML algorithms, what\nfeatures they have engineered, which dimensionality reduction techniques they\nhave employed, what datasets they have employed for training, and what their\nevaluation and explanation strategies are. Drawing from this taxonomy, we also\nidentify gaps in knowledge and provide ideas for improvement and future work.",
        "translated": "作为智能手机市场的领导者，Android 一直是恶意软件攻击的主要目标。在过去的十年中，恶意应用程序的数量不断增加，给相关各方带来了巨大的挑战。特别是对于市场持有者和研究人员来说，大量的样本使得人工检测恶意软件变得不可行，导致了大量研究的涌入，这些研究调查机器学习(ML)方法来自动化这一过程。然而，虽然一些提议的方法取得了很高的性能，快速发展的安卓恶意软件已经使他们无法随着时间的推移保持他们的准确性。这就需要社区进行进一步的研究，建立更加灵活的机器学习管道。然而，由于缺乏对现有文献的系统性概述，以便从现有解决方案中学习和改进，目前这样做受到了阻碍。现有的调查论文往往只关注机器学习过程的一部分(如数据收集或模型部署) ，而忽略了其他重要阶段，如模型评估和解释。在这篇论文中，我们回顾了42篇被高度引用的论文，跨越了十年的研究(从2011年到2021年)来解决这个问题。我们介绍了一个新的已发表文献的程序分类，包括他们如何使用机器学习算法，他们设计了哪些特性，他们使用了哪些降维技术，他们使用了哪些数据集进行培训，以及他们的评估和解释策略是什么。从这个分类法中，我们还确定了知识中的差距，并提供了改进和未来工作的想法。"
    },
    {
        "title": "Residual-Based Detection of Attacks in Cyber-Physical Inverter-Based\n  Microgrids",
        "url": "http://arxiv.org/abs/2306.07082v1",
        "pub_date": "2023-06-12",
        "summary": "This paper discusses the challenges faced by cyber-physical microgrids (MGs)\ndue to the inclusion of information and communication technologies in their\nalready complex, multi-layered systems. The work identifies a research gap in\nmodeling and analyzing stealthy intermittent integrity attacks in MGs, which\nare designed to maximize damage and cancel secondary control objectives. To\naddress this, the paper proposes a nonlinear residual-based observer approach\nto detect and mitigate such attacks. In order to ensure a stable operation of\nthe MG, the formulation then incorporates stability constraints along with the\ndetection observer. The proposed design is validated through case studies on a\nMG benchmark with four distributed generators, demonstrating its effectiveness\nin detecting attacks while satisfying network and stability constraints.",
        "translated": "本文讨论了网络物理微网(MG)所面临的挑战，这些挑战是由于将信息和通信技术纳入其已经复杂的多层次系统。这项工作确定了在建模和分析隐形间歇完整性攻击的研究差距，这是设计最大限度地损害和取消二次控制目标。为了解决这个问题，本文提出了一种基于非线性残差的观测器方法来检测和减轻这种攻击。为了确保 MG 的稳定运行，该公式随后将稳定性约束与检测观测器结合在一起。通过对一个具有四个分布式发生器的 MG 基准的实例研究，验证了该设计在满足网络和稳定性约束的情况下，在检测攻击方面的有效性。"
    },
    {
        "title": "When Vision Fails: Text Attacks Against ViT and OCR",
        "url": "http://arxiv.org/abs/2306.07033v1",
        "pub_date": "2023-06-12",
        "summary": "While text-based machine learning models that operate on visual inputs of\nrendered text have become robust against a wide range of existing attacks, we\nshow that they are still vulnerable to visual adversarial examples encoded as\ntext. We use the Unicode functionality of combining diacritical marks to\nmanipulate encoded text so that small visual perturbations appear when the text\nis rendered. We show how a genetic algorithm can be used to generate visual\nadversarial examples in a black-box setting, and conduct a user study to\nestablish that the model-fooling adversarial examples do not affect human\ncomprehension. We demonstrate the effectiveness of these attacks in the real\nworld by creating adversarial examples against production models published by\nFacebook, Microsoft, IBM, and Google.",
        "translated": "虽然基于文本的机器学习模型对渲染文本的视觉输入进行操作，已经变得对现有的广泛的攻击具有鲁棒性，但是我们表明，它们仍然容易受到编码为文本的视觉对手示例的攻击。我们使用 Unicode 功能组合发音标记来操作编码文本，以便在呈现文本时出现小的视觉扰动。我们展示了如何使用遗传算法在黑盒设置中生成可视化对手例子，并进行了用户研究，以确定欺骗模型的对手例子不影响人类的理解。我们通过创建针对 Facebook、微软、 IBM 和谷歌发布的生产模型的对抗性示例，来展示这些攻击在现实世界中的有效性。"
    },
    {
        "title": "AI-Generated Image Detection using a Cross-Attention Enhanced\n  Dual-Stream Network",
        "url": "http://arxiv.org/abs/2306.07005v1",
        "pub_date": "2023-06-12",
        "summary": "With the rapid evolution of AI Generated Content (AIGC), forged images\nproduced through this technology are inherently more deceptive and require less\nhuman intervention compared to traditional Computer-generated Graphics (CG).\nHowever, owing to the disparities between CG and AIGC, conventional CG\ndetection methods tend to be inadequate in identifying AIGC-produced images. To\naddress this issue, our research concentrates on the text-to-image generation\nprocess in AIGC. Initially, we first assemble two text-to-image databases\nutilizing two distinct AI systems, DALLE2 and DreamStudio. Aiming to\nholistically capture the inherent anomalies produced by AIGC, we develope a\nrobust dual-stream network comprised of a residual stream and a content stream.\nThe former employs the Spatial Rich Model (SRM) to meticulously extract various\ntexture information from images, while the latter seeks to capture additional\nforged traces in low frequency, thereby extracting complementary information\nthat the residual stream may overlook. To enhance the information exchange\nbetween these two streams, we incorporate a cross multi-head attention\nmechanism. Numerous comparative experiments are performed on both databases,\nand the results show that our detection method consistently outperforms\ntraditional CG detection techniques across a range of image resolutions.\nMoreover, our method exhibits superior performance through a series of\nrobustness tests and cross-database experiments. When applied to widely\nrecognized traditional CG benchmarks such as SPL2018 and DsTok, our approach\nsignificantly exceeds the capabilities of other existing methods in the field\nof CG detection.",
        "translated": "随着人工智能生成内容(AIGC)的快速发展，与传统的计算机生成图形(CG)相比，通过这种技术生成的伪造图像本质上更具有欺骗性，需要的人工干预也更少。然而，由于 CG 和 AIGC 之间的差异，传统的 CG 检测方法往往不足以识别 AIGC 产生的图像。为了解决这个问题，我们的研究集中在 AIGC 中的文本到图像的生成过程。最初，我们首先利用两个不同的 AI 系统 DALLE2和 DreamStudio 组装两个文本到图像的数据库。为了全面地捕获 AIGC 产生的固有异常，我们开发了一个由剩余流和内容流组成的鲁棒双流网络。前者采用空间富集模型(SRM)精细地从图像中提取各种纹理信息，后者则寻求在低频下捕获额外的伪迹，从而提取残留流可能忽略的互补信息。为了加强这两个流之间的信息交换，我们引入了一个交叉的多头注意机制。在这两个数据库上进行了大量的对比实验，结果表明我们的检测方法在一定的图像分辨率范围内始终优于传统的 CG 检测技术。此外，通过一系列的鲁棒性测试和跨数据库实验，我们的方法表现出了优越的性能。当应用于广泛公认的传统 CG 基准，如 SPL2018和 DsTok，我们的方法显着超过了其他现有的方法在 CG 检测领域的能力。"
    },
    {
        "title": "SecOComp: A Fast and Secure Simultaneous Compression and Encryption\n  Scheme",
        "url": "http://arxiv.org/abs/2306.06949v1",
        "pub_date": "2023-06-12",
        "summary": "We live in a data-driven era that involves the generation, collection and\nprocessing of a massive amount of data. This data often contains valuable\nintellectual property and sensitive user information that must be safeguarded.\nThere is a need to both encrypt and compress the data at line speed and\nsometimes with added power constraints. The majority of the currently available\nsimultaneous compression and encryption (SCE) schemes are tailored for a\nspecific type of data such as images for instance. This reduces their generic\napplicability. In this paper, we tackle this issue and propose a generic,\nefficient, and secure simultaneous compression and encryption scheme where the\ndata is simultaneously encrypted using chaotic maps and compressed using a fast\nlossless compression algorithm. We claim that employing multiple chaotic maps\nand a lossless compression method can help us create not only an efficient\nencryption scheme but also compress the data efficiently in a hardware-friendly\nmanner. We avoid all the known pitfalls of chaos theory based encryption that\nhave prevented its widespread usage. Our algorithm passes all the NIST tests\nfor nine different types of popular datasets. The proposed implementation uses\n1.51x less storage as compared to the nearest computing work.",
        "translated": "我们生活在一个数据驱动的时代，其中涉及到大量数据的生成、收集和处理。这些数据通常包含有价值的知识产权和必须加以保护的敏感用户信息。需要以线速对数据进行加密和压缩，有时还需要附加功耗约束。当前可用的大多数同时压缩和加密(SCE)方案都是为特定类型的数据(例如图像)量身定制的。这降低了它们的通用适用性。在本文中，我们解决了这个问题，并提出了一个通用的，高效的，安全的同时压缩和加密方案，其中数据同时加密使用混沌映射和压缩使用快速无损数据压缩算法。我们声称，使用多个混沌映射和一个无损数据压缩方法，不仅可以帮助我们创建一个有效的加密方案，而且还可以有效地压缩数据，以一种硬件友好的方式。我们避免了所有已知的基于混沌理论的加密陷阱，这些陷阱阻碍了它的广泛应用。我们的算法通过了九种不同类型的流行数据集的所有 NIST 测试。与最接近的计算工作相比，建议的实现使用的存储空间少了1.51倍。"
    },
    {
        "title": "Continual Release of Differentially Private Synthetic Data",
        "url": "http://arxiv.org/abs/2306.07884v1",
        "pub_date": "2023-06-13",
        "summary": "Motivated by privacy concerns in long-term longitudinal studies in medical\nand social science research, we study the problem of continually releasing\ndifferentially private synthetic data. We introduce a model where, in every\ntime step, each individual reports a new data element, and the goal of the\nsynthesizer is to incrementally update a synthetic dataset to capture a rich\nclass of statistical properties. We give continual synthetic data generation\nalgorithms that preserve two basic types of queries: fixed time window queries\nand cumulative time queries. We show nearly tight upper bounds on the error\nrates of these algorithms and demonstrate their empirical performance on\nrealistically sized datasets from the U.S. Census Bureau's Survey of Income and\nProgram Participation.",
        "translated": "基于医学和社会科学研究中长期纵向研究中对隐私的关注，我们研究了不断发布差异化私人合成数据的问题。我们引入一个模型，在每个时间步骤中，每个个体报告一个新的数据元素，合成器的目标是增量更新合成数据集以捕获丰富的统计特性类。我们给出了连续的合成数据生成算法，它保留了两种基本类型的查询: 固定时间窗口查询和累积时间查询。我们在这些算法的错误率上显示了近乎严格的上限，并且在美国人口普查局的收入和项目参与调查的实际大小的数据集上展示了它们的经验性能。"
    },
    {
        "title": "Temporal Gradient Inversion Attacks with Robust Optimization",
        "url": "http://arxiv.org/abs/2306.07883v1",
        "pub_date": "2023-06-13",
        "summary": "Federated Learning (FL) has emerged as a promising approach for collaborative\nmodel training without sharing private data. However, privacy concerns\nregarding information exchanged during FL have received significant research\nattention. Gradient Inversion Attacks (GIAs) have been proposed to reconstruct\nthe private data retained by local clients from the exchanged gradients. While\nrecovering private data, the data dimensions and the model complexity increase,\nwhich thwart data reconstruction by GIAs. Existing methods adopt prior\nknowledge about private data to overcome those challenges. In this paper, we\nfirst observe that GIAs with gradients from a single iteration fail to\nreconstruct private data due to insufficient dimensions of leaked gradients,\ncomplex model architectures, and invalid gradient information. We investigate a\nTemporal Gradient Inversion Attack with a Robust Optimization framework, called\nTGIAs-RO, which recovers private data without any prior knowledge by leveraging\nmultiple temporal gradients. To eliminate the negative impacts of outliers,\ne.g., invalid gradients for collaborative optimization, robust statistics are\nproposed. Theoretical guarantees on the recovery performance and robustness of\nTGIAs-RO against invalid gradients are also provided. Extensive empirical\nresults on MNIST, CIFAR10, ImageNet and Reuters 21578 datasets show that the\nproposed TGIAs-RO with 10 temporal gradients improves reconstruction\nperformance compared to state-of-the-art methods, even for large batch sizes\n(up to 128), complex models like ResNet18, and large datasets like ImageNet\n(224*224 pixels). Furthermore, the proposed attack method inspires further\nexploration of privacy-preserving methods in the context of FL.",
        "translated": "联邦学习(FL)已经成为一种不需要共享私有数据的协同模型训练的有前途的方法。然而，关于外语学习期间信息交流的隐私问题已经引起了研究者的重视。梯度反转攻击(GIA)已被提出用于重构本地客户端从交换的梯度中保留的私有数据。在恢复私有数据时，数据维数和模型复杂度增加，阻碍了 GIA 的数据重构。现有方法采用对私有数据的事先了解来克服这些挑战。在本文中，我们首先观察到，由于泄漏的梯度维数不足，模型结构复杂，梯度信息无效，单次迭代的梯度 GIA 无法重建私有数据。我们研究了一种具有鲁棒优化框架的时间梯度反演攻击，称为 TGIA-RO，它通过利用多个时间梯度在没有任何先验知识的情况下恢复私有数据。为了消除离群值的负面影响，例如协作优化的无效梯度，提出了稳健统计。理论上保证了 TGIA-RO 对无效梯度的恢复性能和鲁棒性。对 MNIST，CIFAR10，ImageNet 和 Reuters 21578数据集的广泛实证结果表明，与最先进的方法相比，提出的具有10个时间梯度的 TGIA-RO 改善了重建性能，即使对于大批量(高达128) ，复杂模型如 ResNet18和大数据集如 ImageNet (224 * 224像素)。此外，本文提出的攻击方法对 FL 环境下的隐私保护方法的进一步研究具有启发意义。"
    },
    {
        "title": "Finite Gaussian Neurons: Defending against adversarial attacks by making\n  neural networks say \"I don't know\"",
        "url": "http://arxiv.org/abs/2306.07796v1",
        "pub_date": "2023-06-13",
        "summary": "Since 2014, artificial neural networks have been known to be vulnerable to\nadversarial attacks, which can fool the network into producing wrong or\nnonsensical outputs by making humanly imperceptible alterations to inputs.\nWhile defenses against adversarial attacks have been proposed, they usually\ninvolve retraining a new neural network from scratch, a costly task. In this\nwork, I introduce the Finite Gaussian Neuron (FGN), a novel neuron architecture\nfor artificial neural networks. My works aims to: - easily convert existing\nmodels to Finite Gaussian Neuron architecture, - while preserving the existing\nmodel's behavior on real data, - and offering resistance against adversarial\nattacks. I show that converted and retrained Finite Gaussian Neural Networks\n(FGNN) always have lower confidence (i.e., are not overconfident) in their\npredictions over randomized and Fast Gradient Sign Method adversarial images\nwhen compared to classical neural networks, while maintaining high accuracy and\nconfidence over real MNIST images. To further validate the capacity of Finite\nGaussian Neurons to protect from adversarial attacks, I compare the behavior of\nFGNs to that of Bayesian Neural Networks against both randomized and\nadversarial images, and show how the behavior of the two architectures differs.\nFinally I show some limitations of the FGN models by testing them on the more\ncomplex SPEECHCOMMANDS task, against the stronger Carlini-Wagner and Projected\nGradient Descent adversarial attacks.",
        "translated": "自2014年以来，人工神经网络已经被认为容易受到对抗性攻击，这种攻击可以通过对输入进行人类无法察觉的改变，欺骗网络产生错误或无意义的输出。虽然已经提出了对抗敌对攻击的防御措施，但它们通常涉及从零开始重新训练一个新的神经网络，这是一项代价高昂的任务。在这项工作中，我介绍了有限高斯神经元(FGN) ，一种新的神经元架构的人工神经网络。我的工作目标是:-轻松地将现有模型转换为有限高斯神经元结构,-同时保留现有模型在真实数据上的行为,-并提供对抗敌对攻击的能力。我指出，转换和再训练的有限高斯神经网络(FGNN)总是对随机和快速梯度符号方法对抗性图像的预测有较低的置信度(即，不过度自信) ，与经典的神经网络相比，同时保持高精度和置信度的真实 MNIST 图像。为了进一步验证有限高斯神经元抵御敌对攻击的能力，我比较了 FGNs 和贝叶斯神经网络对随机和敌对图像的行为，并展示了这两种结构的行为是如何不同的。最后，我通过在更复杂的 SPEECHCOMMANDS 任务上测试 FGN 模型，以对抗更强大的卡里尼-瓦格纳(Carlini-Wagner)和投影式梯度下降法对抗攻击，展示了它们的一些局限性。"
    },
    {
        "title": "SafeBet: Secure, Simple, and Fast Speculative Execution",
        "url": "http://arxiv.org/abs/2306.07785v1",
        "pub_date": "2023-06-13",
        "summary": "Spectre attacks exploit microprocessor speculative execution to read and\ntransmit forbidden data outside the attacker's trust domain and sandbox. Recent\nhardware schemes allow potentially-unsafe speculative accesses but prevent the\nsecret's transmission by delaying most access-dependent instructions even in\nthe predominantly-common, no-attack case, which incurs performance loss and\nhardware complexity. Instead, we propose SafeBet which allows only, and does\nnot delay most, safe accesses, achieving both security and high performance.\nSafeBet is based on the key observation that speculatively accessing a\ndestination location is safe if the location's access by the same static trust\ndomain has been committed previously; and potentially unsafe, otherwise. We\nextend this observation to handle inter trust-domain code and data\ninteractions. SafeBet employs the Speculative Memory Access Control Table\n(SMACT) to track non-speculative trust domain code region-destination pairs.\nDisallowed accesses wait until reaching commit to trigger well-known replay,\nwith virtually no change to the pipeline. Software simulations using SpecCPU\nbenchmarks show that SafeBet uses an 8.3-KB SMACT per core to perform within 6%\non average (63% at worst) of the unsafe baseline behind which NDA-restrictive,\na previous scheme of security and hardware complexity comparable to SafeBet's,\nlags by 83% on average.",
        "translated": "Spectre 攻击利用微处理器 Speculative_execution 在攻击者的信任域和沙盒之外读取和传输被禁止的数据。最近的硬件方案允许潜在的不安全的推测性访问，但是通过延迟大多数访问依赖的指令来阻止秘密的传输，即使在主要通用的、无攻击的情况下，这会导致性能损失和硬件复杂性。相反，我们建议安全打赌，只允许，而不延迟大多数，安全访问，实现安全和高性能。SafeBett 基于以下关键观察: 如果目标位置的访问以前已经通过相同的静态信任域提交，那么以投机方式访问该位置是安全的; 否则可能是不安全的。我们将这种观察扩展到处理信任域之间的代码和数据交互。SafeBet 使用推测内存访问控制表(SMACT)跟踪非推测信任域代码区域-目标对。不允许的访问要等到达到提交时才触发众所周知的重播，实际上不需要对管道进行任何更改。使用 SpecCPU 基准测试的软件模拟显示，SafeBet 每个核心使用8.3 KB 的 SMACT，平均在不安全基准的6% (最差为63%)之内执行。在这个基准之后，以前的安全和硬件复杂度与 SafeBet 相当的 NDA 限制方案平均落后83% 。"
    },
    {
        "title": "Area is all you need: repeatable elements make stronger adversarial\n  attacks",
        "url": "http://arxiv.org/abs/2306.07768v1",
        "pub_date": "2023-06-13",
        "summary": "Over the last decade, deep neural networks have achieved state of the art in\ncomputer vision tasks. These models, however, are susceptible to unusual\ninputs, known as adversarial examples, that cause them to misclassify or\notherwise fail to detect objects. Here, we provide evidence that the increasing\nsuccess of adversarial attacks is primarily due to increasing their size. We\nthen demonstrate a method for generating the largest possible adversarial patch\nby building a adversarial pattern out of repeatable elements. This approach\nachieves a new state of the art in evading detection by YOLOv2 and YOLOv3.\nFinally, we present an experiment that fails to replicate the prior success of\nseveral attacks published in this field, and end with some comments on testing\nand reproducibility.",
        "translated": "在过去的十年中，深度神经网络在计算机视觉任务中取得了最先进的技术。然而，这些模型容易受到不寻常的输入(称为对抗性示例)的影响，这些输入会导致模型错误分类或无法检测到对象。在这里，我们提供的证据表明，越来越成功的对抗性攻击主要是由于增加了他们的规模。然后，我们演示了一种方法，通过用可重复的元素构建一个对抗模式来生成尽可能大的对抗补丁。这种方法在躲避 YOLOv2和 YOLOv3的检测方面达到了一种新的技术水平。最后，我们提出了一个实验，未能复制在这个领域发表的几个攻击的先前的成功，并以一些测试和重现性的意见结束。"
    },
    {
        "title": "Generated Graph Detection",
        "url": "http://arxiv.org/abs/2306.07758v1",
        "pub_date": "2023-06-13",
        "summary": "Graph generative models become increasingly effective for data distribution\napproximation and data augmentation. While they have aroused public concerns\nabout their malicious misuses or misinformation broadcasts, just as what\nDeepfake visual and auditory media has been delivering to society. Hence it is\nessential to regulate the prevalence of generated graphs. To tackle this\nproblem, we pioneer the formulation of the generated graph detection problem to\ndistinguish generated graphs from real ones. We propose the first framework to\nsystematically investigate a set of sophisticated models and their performance\nin four classification scenarios. Each scenario switches between seen and\nunseen datasets/generators during testing to get closer to real-world settings\nand progressively challenge the classifiers. Extensive experiments evidence\nthat all the models are qualified for generated graph detection, with specific\nmodels having advantages in specific scenarios. Resulting from the validated\ngenerality and oblivion of the classifiers to unseen datasets/generators, we\ndraw a safe conclusion that our solution can sustain for a decent while to curb\ngenerated graph misuses.",
        "translated": "图生成模型在数据分布逼近和数据增强方面的应用越来越广泛。尽管它们引起了公众对其恶意滥用或错误信息广播的担忧，正如 Deepfalse 视听媒体向社会传递的信息一样。因此，规范生成图的流行是非常必要的。为了解决这个问题，我们首先提出了生成图检测问题的公式，以区分生成的图和真实的图。我们提出了第一个框架，系统地研究了一组复杂的模型及其在四种分类情景下的性能。每个场景在测试期间在可见和不可见的数据集/生成器之间切换，以更接近真实世界的设置并逐步挑战分类器。广泛的实验证明，所有的模型都有资格生成的图检测，具体的模型有优势，在特定的情况下。通过验证分类器对未知数据集/生成器的通用性和遗忘性，我们得出了一个安全的结论，即我们的解决方案可以维持一段时间，以遏制生成的图的误用。"
    },
    {
        "title": "Generative Watermarking Against Unauthorized Subject-Driven Image\n  Synthesis",
        "url": "http://arxiv.org/abs/2306.07754v1",
        "pub_date": "2023-06-13",
        "summary": "Large text-to-image models have shown remarkable performance in synthesizing\nhigh-quality images. In particular, the subject-driven model makes it possible\nto personalize the image synthesis for a specific subject, e.g., a human face\nor an artistic style, by fine-tuning the generic text-to-image model with a few\nimages from that subject. Nevertheless, misuse of subject-driven image\nsynthesis may violate the authority of subject owners. For example, malicious\nusers may use subject-driven synthesis to mimic specific artistic styles or to\ncreate fake facial images without authorization. To protect subject owners\nagainst such misuse, recent attempts have commonly relied on adversarial\nexamples to indiscriminately disrupt subject-driven image synthesis. However,\nthis essentially prevents any benign use of subject-driven synthesis based on\nprotected images.\n  In this paper, we take a different angle and aim at protection without\nsacrificing the utility of protected images for general synthesis purposes.\nSpecifically, we propose GenWatermark, a novel watermark system based on\njointly learning a watermark generator and a detector. In particular, to help\nthe watermark survive the subject-driven synthesis, we incorporate the\nsynthesis process in learning GenWatermark by fine-tuning the detector with\nsynthesized images for a specific subject. This operation is shown to largely\nimprove the watermark detection accuracy and also ensure the uniqueness of the\nwatermark for each individual subject. Extensive experiments validate the\neffectiveness of GenWatermark, especially in practical scenarios with unknown\nmodels and text prompts (74% Acc.), as well as partial data watermarking (80%\nAcc. for 1/4 watermarking). We also demonstrate the robustness of GenWatermark\nto two potential countermeasures that substantially degrade the synthesis\nquality.",
        "translated": "大型文本-图像模型在合成高质量图像方面表现出显著的性能。具体来说，主题驱动模型可以通过微调一般的文本到图像模型和一些来自该主题的图像，为特定主题(如人脸或艺术风格)进行个性化的图像合成。然而，误用主题驱动的图像合成可能会违反主题所有者的权威。例如，恶意用户可能使用主题驱动的合成来模仿特定的艺术风格或未经授权创建假面部图像。为了保护主题所有者免受这种滥用，最近的尝试通常依赖于对抗性的例子来不加区分地破坏主题驱动的图像合成。然而，这基本上阻止了任何良性使用主题驱动的合成基于受保护的图像。本文从不同的角度出发，在不牺牲图像保护效用的前提下，对图像进行综合保护。具体地说，我们提出了一种基于水印生成器和检测器联合学习的新型水印系统——基因水印。特别地，为了使水印在主题驱动的合成中存活下来，我们将合成过程融入到学习基因水印的过程中，通过对特定主题的合成图像的检测器进行微调来实现水印的学习。实验结果表明，该方法不仅大大提高了水印检测的准确性，而且保证了每个水印对象的唯一性。广泛的实验验证了基因水印的有效性，特别是在未知模型和文本提示的实际场景中(74% Acc。)，以及部分数据水印(80% 。1/4水印)。我们还证明了基因水印对两种潜在的对策的鲁棒性，这两种对策大大降低了合成质量。"
    },
    {
        "title": "An Inverse Approach to Windows' Resource-Based Permission Mechanism for\n  Access Permission Vulnerability Detection",
        "url": "http://arxiv.org/abs/2306.07734v1",
        "pub_date": "2023-06-13",
        "summary": "In organizations, employees work with information stored in files according\nto their duties and responsibilities. Windows uses resource-based access\npermissions that any permission for any user has to be set separately per\nresource. This approach gets complicated as the number of resources and users\nincrease, and causes oversights in assigning permissions. Therefore, a special\nmechanism is required to scrutinize what permissions any employee has on any\nset of resources. This requirement is circumvented by reversing the Windows\napproach in terms of user-accessible resources. This approach is implemented by\na program allowing quick and easy examination of any type of permissions\ngranted or denied to active directory users on any folder. In this way,\nadministrators can make sure there is no any missing or overlooked setting that\ncould cause a security vulnerability. This approach can easily be extended to\nscrutinize other resources, and for other local or active directory objects.",
        "translated": "在组织中，员工根据他们的职责和责任使用存储在文件中的信息工作。Windows 使用基于资源的访问权限，任何用户的任何权限都必须在每个资源中单独设置。随着资源和用户数量的增加，这种方法会变得复杂，并导致在分配权限时出现疏忽。因此，需要一种特殊的机制来仔细检查任何员工对任何资源集拥有什么权限。通过在用户可访问资源方面逆转 Windows 方法，规避了这一要求。这种方法是通过一个程序实现的，该程序允许快速、方便地检查授予或拒绝给任何文件夹上的活动目录用户的任何类型的权限。通过这种方式，管理员可以确保不会遗漏或忽略任何可能导致安全漏洞的设置。可以很容易地扩展这种方法来检查其他资源以及其他本地或活动目录对象。"
    },
    {
        "title": "Theoretical Foundations of Adversarially Robust Learning",
        "url": "http://arxiv.org/abs/2306.07723v1",
        "pub_date": "2023-06-13",
        "summary": "Despite extraordinary progress, current machine learning systems have been\nshown to be brittle against adversarial examples: seemingly innocuous but\ncarefully crafted perturbations of test examples that cause machine learning\npredictors to misclassify. Can we learn predictors robust to adversarial\nexamples? and how? There has been much empirical interest in this contemporary\nchallenge in machine learning, and in this thesis, we address it from a\ntheoretical perspective.\n  In this thesis, we explore what robustness properties can we hope to\nguarantee against adversarial examples and develop an understanding of how to\nalgorithmically guarantee them. We illustrate the need to go beyond traditional\napproaches and principles such as empirical risk minimization and uniform\nconvergence, and make contributions that can be categorized as follows: (1)\nintroducing problem formulations capturing aspects of emerging practical\nchallenges in robust learning, (2) designing new learning algorithms with\nprovable robustness guarantees, and (3) characterizing the complexity of robust\nlearning and fundamental limitations on the performance of any algorithm.",
        "translated": "尽管取得了非凡的进步，但是目前的机器学习系统已经被证明对于敌对的例子是脆弱的: 看似无害但是精心设计的测试例子扰动，导致机器学习预测错误分类。我们可以学习预测健壮的对抗性的例子？怎么做到的？当代机器学习面临的挑战已经引起了很多实证研究者的兴趣，在本文中，我们将从理论的角度来探讨这个问题。在这篇论文中，我们探讨了哪些鲁棒性特性可以在对抗性例子中得到保证，并且发展了如何通过算法来保证它们的理解。我们说明需要超越传统的方法和原则，如经验风险最小化和一致收敛，并作出贡献，可以分类如下: (1)引入问题公式捕捉方面的新兴实际挑战的鲁棒性学习，(2)设计新的学习算法与可证明的鲁棒性保证，(3)表征鲁棒性学习的复杂性和任何算法的性能的基本限制。"
    },
    {
        "title": "Public-Key Encryption with Quantum Keys",
        "url": "http://arxiv.org/abs/2306.07698v1",
        "pub_date": "2023-06-13",
        "summary": "In the framework of Impagliazzo's five worlds, a distinction is often made\nbetween two worlds, one where public-key encryption exists (Cryptomania), and\none in which only one-way functions exist (MiniCrypt). However, the boundaries\nbetween these worlds can change when quantum information is taken into account.\nRecent work has shown that quantum variants of oblivious transfer and\nmulti-party computation, both primitives that are classically in Cryptomania,\ncan be constructed from one-way functions, placing them in the realm of quantum\nMiniCrypt (the so-called MiniQCrypt). This naturally raises the following\nquestion: Is it possible to construct a quantum variant of public-key\nencryption, which is at the heart of Cryptomania, from one-way functions or\npotentially weaker assumptions?\n  In this work, we initiate the formal study of the notion of quantum\npublic-key encryption (qPKE), i.e., public-key encryption where keys are\nallowed to be quantum states. We propose new definitions of security and\nseveral constructions of qPKE based on the existence of one-way functions\n(OWF), or even weaker assumptions, such as pseudorandom function-like states\n(PRFS) and pseudorandom function-like states with proof of destruction\n(PRFSPD). Finally, to give a tight characterization of this primitive, we show\nthat computational assumptions are necessary to build quantum public-key\nencryption. That is, we give a self-contained proof that no quantum public-key\nencryption scheme can provide information-theoretic security.",
        "translated": "在 Impagliazzo 的五个世界的框架中，经常有两个世界之间的区别，一个是存在公钥加密的世界(Cryptomania) ，另一个是只存在单向函数的世界(MiniCrypt)。然而，当量子信息被考虑在内时，这些世界之间的界限会发生变化。最近的研究表明，无意识传输和多方计算的量子变体(这两种原语在 Cryptomania 都是经典的)可以由单向函数构造出来，将它们置于量子 MiniCrypt (即所谓的 MiniqCrypt)领域。这自然引发了以下问题: 有没有可能根据单向函数或潜在较弱的假设，构建一种处于 Cryptomania 核心的公钥加密的量子变体？在这项工作中，我们开始了量子公钥加密(qPKE)概念的正式研究，即公钥加密，其中允许密钥是量子态。基于单向函数(OWF)的存在性，或者更弱的假设，如伪随机类函数状态(PRFS)和伪随机类函数状态(PRFSPD) ，我们提出了安全性的新定义和 qPKE 的几种构造。最后，为了给这个原语一个严密的角色塑造，我们展示了构建量子公钥加密所需的计算假设。也就是说，我们给出了一个独立的证据，证明没有任何量子公钥加密方案能够提供资讯理论安全性。"
    },
    {
        "title": "Inroads into Autonomous Network Defence using Explained Reinforcement\n  Learning",
        "url": "http://arxiv.org/abs/2306.09318v1",
        "pub_date": "2023-06-15",
        "summary": "Computer network defence is a complicated task that has necessitated a high\ndegree of human involvement. However, with recent advancements in machine\nlearning, fully autonomous network defence is becoming increasingly plausible.\nThis paper introduces an end-to-end methodology for studying attack strategies,\ndesigning defence agents and explaining their operation. First, using state\ndiagrams, we visualise adversarial behaviour to gain insight about potential\npoints of intervention and inform the design of our defensive models. We opt to\nuse a set of deep reinforcement learning agents trained on different parts of\nthe task and organised in a shallow hierarchy. Our evaluation shows that the\nresulting design achieves a substantial performance improvement compared to\nprior work. Finally, to better investigate the decision-making process of our\nagents, we complete our analysis with a feature ablation and importance study.",
        "translated": "计算机网络防御是一项复杂的任务，需要人的高度参与。然而，随着机器学习的最新进展，完全自主的网络防御正变得越来越合理。本文介绍了一种端到端的攻击策略研究方法，设计了防御代理，并对它们的操作进行了说明。首先，利用状态图，我们可视化的对抗行为，以获得洞察潜在的干预点，并通知我们的防御模型的设计。我们选择使用一组深度强化学习的代理人，这些代理人接受过不同任务部分的培训，并按照肤浅的等级制度组织起来。我们的评估表明，与以前的工作相比，最终的设计实现了实质性的性能改进。最后，为了更好地研究我们的智能体的决策过程，我们用特征消融和重要性研究来完成我们的分析。"
    },
    {
        "title": "Matching Pairs: Attributing Fine-Tuned Models to their Pre-Trained Large\n  Language Models",
        "url": "http://arxiv.org/abs/2306.09308v1",
        "pub_date": "2023-06-15",
        "summary": "The wide applicability and adaptability of generative large language models\n(LLMs) has enabled their rapid adoption. While the pre-trained models can\nperform many tasks, such models are often fine-tuned to improve their\nperformance on various downstream applications. However, this leads to issues\nover violation of model licenses, model theft, and copyright infringement.\nMoreover, recent advances show that generative technology is capable of\nproducing harmful content which exacerbates the problems of accountability\nwithin model supply chains. Thus, we need a method to investigate how a model\nwas trained or a piece of text was generated and what their pre-trained base\nmodel was. In this paper we take the first step to address this open problem by\ntracing back the origin of a given fine-tuned LLM to its corresponding\npre-trained base model. We consider different knowledge levels and attribution\nstrategies, and find that we can correctly trace back 8 out of the 10 fine\ntuned models with our best method.",
        "translated": "生成式大型语言模型(LLM)的广泛适用性和适应性使它们得以迅速采用。虽然预先训练的模型可以执行许多任务，但这些模型通常经过微调，以提高它们在各种下游应用程序中的性能。然而，这导致了违反模型许可证、模型盗窃和盗版的问题。此外，最近的进展表明，生成技术能够产生有害的内容，这加剧了示范供应链中的问责问题。因此，我们需要一种方法来研究如何训练一个模型或一段文本生成，以及他们的预先训练的基础模型是什么。在本文中，我们采取的第一步，以解决这个开放的问题，追溯起源的一个给定的微调 LLM 相应的预训练的基础模型。我们考虑了不同的知识水平和归因策略，发现我们可以用我们最好的方法正确地追溯10个微调模型中的8个。"
    },
    {
        "title": "Your Room is not Private: Gradient Inversion Attack for Deep Q-Learning",
        "url": "http://arxiv.org/abs/2306.09273v1",
        "pub_date": "2023-06-15",
        "summary": "The prominence of embodied Artificial Intelligence (AI), which empowers\nrobots to navigate, perceive, and engage within virtual environments, has\nattracted significant attention, owing to the remarkable advancements in\ncomputer vision and large language models. Privacy emerges as a pivotal concern\nwithin the realm of embodied AI, as the robot access substantial personal\ninformation. However, the issue of privacy leakage in embodied AI tasks,\nparticularly in relation to decision-making algorithms, has not received\nadequate consideration in research. This paper aims to address this gap by\nproposing an attack on the Deep Q-Learning algorithm, utilizing gradient\ninversion to reconstruct states, actions, and Q-values. The choice of using\ngradients for the attack is motivated by the fact that commonly employed\nfederated learning techniques solely utilize gradients computed based on\nprivate user data to optimize models, without storing or transmitting the data\nto public servers. Nevertheless, these gradients contain sufficient information\nto potentially expose private data. To validate our approach, we conduct\nexperiments on the AI2THOR simulator and evaluate our algorithm on active\nperception, a prevalent task in embodied AI. The experimental results\nconvincingly demonstrate the effectiveness of our method in successfully\nrecovering all information from the data across all 120 room layouts.",
        "translated": "由于计算机视觉和大型语言模型的显著进步，具体化人工智能(AI)的突出地位，使机器人能够在虚拟环境中导航、感知和参与，引起了人们的广泛关注。隐私作为一个关键问题出现在具体的人工智能领域，因为机器人访问大量的个人信息。然而，具体人工智能任务中的隐私泄露问题，特别是决策算法方面的问题，在研究中还没有得到足够的重视。本文旨在通过对深度 Q 学习算法的攻击，利用梯度反演重建状态、动作和 Q 值来弥补这一缺陷。选择使用梯度进行攻击的动机是，通常采用的联邦学习技术只利用基于私有用户数据计算的梯度来优化模型，而不将数据存储或传输到公共服务器。不过，这些渐变包含足够的信息，可能会暴露私有数据。为了验证我们的方法，我们在 AI2THOR 模拟器上进行了实验，并且评估了我们的算法在主动感知上的效果。实验结果令人信服地证明了我们的方法在成功恢复所有120个房间布局数据的有效性。"
    },
    {
        "title": "Concealing CAN Message Sequences to Prevent Schedule-based Bus-off\n  Attacks",
        "url": "http://arxiv.org/abs/2306.09206v1",
        "pub_date": "2023-06-15",
        "summary": "This work focuses on eliminating timing-side channels in real-time\nsafety-critical cyber-physical network protocols like Controller Area Networks\n(CAN). Automotive Electronic Control Units (ECUs) implement predictable\nscheduling decisions based on task level response time estimation. Such levels\nof determinism exposes timing information about task executions and therefore\ncorresponding message transmissions via the network buses (that connect the\nECUs and actuators). With proper analysis, such timing side channels can be\nutilized to launch several schedule-based attacks that can lead to eventual\ndenial-of-service or man-in-the-middle-type attacks. To eliminate this\ndeterminism, we propose a novel schedule obfuscation strategy by skipping\ncertain control task executions and related data transmissions along with\nrandom shifting of the victim task instance. While doing this, our strategy\ncontemplates the performance of the control task as well by bounding the number\nof control execution skips. We analytically demonstrate how the attack success\nprobability (ASP) is reduced under this proposed attack-aware skipping and\nrandomization. We also demonstrate the efficacy and real-time applicability of\nour attack-aware schedule obfuscation strategy Hide-n-Seek by applying it to\nsynthesized automotive task sets in a real-time Hardware-in-loop (HIL) setup.",
        "translated": "这项工作的重点是消除实时安全性关键的网络物理协议，如控制器区域网络(CAN)的时间端通道。汽车电子控制单元(ECU)实现基于任务级响应时间估计的可预测调度决策。这种级别的确定性公开了关于任务执行的时间信息，因此通过网络总线(连接 ECU 和执行器)传输相应的消息。通过适当的分析，可以利用这种时间侧信道发动多种基于调度的攻击，最终导致拒绝服务或中间人攻击。为了消除这种确定性，我们提出了一种新的调度模糊策略，通过跳过某些控制任务的执行和相关的数据传输以及受害任务实例的随机移动。在这样做的同时，我们的策略还通过限制控制执行跳过的次数来考虑控制任务的性能。我们分析了攻击成功概率(ASP)是如何降低这种攻击感知跳跃和随机。通过将攻击感知调度模糊策略 Hide-n-Seek 应用于实时硬件在环(HIL)环境下的综合汽车任务集，验证了该策略的有效性和实时适用性。"
    },
    {
        "title": "High-Resolution Convolutional Neural Networks on Homomorphically\n  Encrypted Data via Sharding Ciphertexts",
        "url": "http://arxiv.org/abs/2306.09189v1",
        "pub_date": "2023-06-15",
        "summary": "Recently, Deep Convolutional Neural Networks (DCNNs) including the ResNet-20\narchitecture have been privately evaluated on encrypted, low-resolution data\nwith the Residue-Number-System Cheon-Kim-Kim-Song (RNS-CKKS) homomorphic\nencryption scheme. We extend methods for evaluating DCNNs on images with larger\ndimensions and many channels, beyond what can be stored in single ciphertexts.\nAdditionally, we simplify and improve the efficiency of the recently introduced\nmultiplexed image format, demonstrating that homomorphic evaluation can work\nwith standard, row-major matrix packing and results in encrypted inference time\nspeedups by $4.6-6.5\\times$. We also show how existing DCNN models can be\nregularized during the training process to further improve efficiency and\naccuracy. These techniques are applied to homomorphically evaluate a DCNN with\nhigh accuracy on the high-resolution ImageNet dataset for the first time,\nachieving $80.2\\%$ top-1 accuracy. We also achieve the highest reported\naccuracy of homomorphically evaluated CNNs on the CIFAR-10 dataset of $98.3\\%$.",
        "translated": "最近，包括 ResNet-20架构在内的深层卷积神经网络已经通过剩余数字系统 Cheon-Kim-Kim-Song (RNS-CKKS)同态加密方案在加密的低分辨率数据上进行了私人评估。我们扩展了评估大尺寸和多通道图像上的 DCNN 的方法，超出了能够存储在单个密文中的范围。此外，我们简化和提高了最近引入的多路复用图像格式的效率，证明了同态计算可以与标准的行主矩阵打包一起工作，并导致加密推理时间加速4.6 -6.5倍。我们还展示了如何在训练过程中规范现有的 DCNN 模型，以进一步提高效率和准确性。首次将这些技术应用于高分辨率 ImageNet 数据集上的 DCNN 同态评估，获得了80.2% 的最高精度。我们还在 CIFAR-10数据集上实现了同态评估的 CNN 的最高报告准确度为98.3% 。"
    },
    {
        "title": "DIFFender: Diffusion-Based Adversarial Defense against Patch Attacks in\n  the Physical World",
        "url": "http://arxiv.org/abs/2306.09124v1",
        "pub_date": "2023-06-15",
        "summary": "Adversarial attacks in the physical world, particularly patch attacks, pose\nsignificant threats to the robustness and reliability of deep learning models.\nDeveloping reliable defenses against patch attacks is crucial for real-world\napplications, yet current research in this area is severely lacking. In this\npaper, we propose DIFFender, a novel defense method that leverages the\npre-trained diffusion model to perform both localization and defense against\npotential adversarial patch attacks. DIFFender is designed as a pipeline\nconsisting of two main stages: patch localization and restoration. In the\nlocalization stage, we exploit the intriguing properties of a diffusion model\nto effectively identify the locations of adversarial patches. In the\nrestoration stage, we employ a text-guided diffusion model to eliminate\nadversarial regions in the image while preserving the integrity of the visual\ncontent. Additionally, we design a few-shot prompt-tuning algorithm to\nfacilitate simple and efficient tuning, enabling the learned representations to\neasily transfer to downstream tasks, which optimize two stages jointly. We\nconduct extensive experiments on image classification and face recognition to\ndemonstrate that DIFFender exhibits superior robustness under strong adaptive\nattacks and generalizes well across various scenarios, diverse classifiers, and\nmultiple attack methods.",
        "translated": "物理世界中的对抗性攻击，特别是补丁攻击，对深度学习模型的健壮性和可靠性构成了严重威胁。开发针对补丁攻击的可靠防御对于真实世界的应用程序是至关重要的，然而目前在这个领域的研究严重缺乏。在本文中，我们提出了 DIFFender，一种新的防御方法，利用预先训练的扩散模型来执行定位和防御潜在的对手补丁攻击。DIFFender 被设计成一个由两个主要阶段组成的流水线: 补丁定位和恢复。在局部化阶段，我们利用扩散模型的有趣特性来有效地识别对抗性斑块的位置。在图像恢复阶段，我们采用文本引导的扩散模型来消除图像中的敌对区域，同时保持视觉内容的完整性。此外，我们还设计了一个小镜头提示调优算法，以方便简单而有效的调优，使学习的表示方法能够轻松地转移到下游任务，这样就可以联合优化两个阶段。我们在图像分类和人脸识别方面进行了广泛的实验，证明了 DIFFender 在强自适应攻击下具有优越的鲁棒性，并且能够很好地推广到各种场景、不同的分类器和多种攻击方法。"
    },
    {
        "title": "On Strengthening and Defending Graph Reconstruction Attack with Markov\n  Chain Approximation",
        "url": "http://arxiv.org/abs/2306.09104v1",
        "pub_date": "2023-06-15",
        "summary": "Although powerful graph neural networks (GNNs) have boosted numerous\nreal-world applications, the potential privacy risk is still underexplored. To\nclose this gap, we perform the first comprehensive study of graph\nreconstruction attack that aims to reconstruct the adjacency of nodes. We show\nthat a range of factors in GNNs can lead to the surprising leakage of private\nlinks. Especially by taking GNNs as a Markov chain and attacking GNNs via a\nflexible chain approximation, we systematically explore the underneath\nprinciples of graph reconstruction attack, and propose two information\ntheory-guided mechanisms: (1) the chain-based attack method with adaptive\ndesigns for extracting more private information; (2) the chain-based defense\nmethod that sharply reduces the attack fidelity with moderate accuracy loss.\nSuch two objectives disclose a critical belief that to recover better in\nattack, you must extract more multi-aspect knowledge from the trained GNN;\nwhile to learn safer for defense, you must forget more link-sensitive\ninformation in training GNNs. Empirically, we achieve state-of-the-art results\non six datasets and three common GNNs. The code is publicly available at:\nhttps://github.com/tmlr-group/MC-GRA.",
        "translated": "尽管强大的图形神经网络(GNN)已经推动了许多现实世界的应用，但是潜在的隐私风险仍然没有得到充分的开发。为了弥补这一差距，我们首次对图重建攻击进行了全面的研究，目的是重建节点的邻接。我们表明，在 GNN 的一系列因素可以导致令人惊讶的泄漏私人链路。特别是将 GNN 作为一个马尔可夫链，通过柔性链近似对 GNN 进行攻击，系统地探讨了图重构攻击的基本原理，提出了两种信息理论指导机制: (1)基于链的攻击方法，采用自适应设计提取更多的私有信息; (2)基于链的防御方法，大大降低了攻击保真度，但精度损失不大。这两个目标揭示了一个关键的信念，即为了更好地恢复攻击，您必须从训练过的 GNN 中提取更多的多方面知识; 而为了学习更安全的防御，您必须在训练 GNN 中忘记更多的链接敏感信息。根据经验，我们在六个数据集和三个常见的 GNN 上获得了最先进的结果。该守则可于以下 https://github.com/tmlr-group/mc-gra 公开索取:。"
    },
    {
        "title": "A Learning Assisted Method for Uncovering Power Grid Generation and\n  Distribution System Vulnerabilities",
        "url": "http://arxiv.org/abs/2306.09057v1",
        "pub_date": "2023-06-15",
        "summary": "Intelligent attackers can suitably tamper sensor/actuator data at various\nSmart grid surfaces causing intentional power oscillations, which if left\nundetected, can lead to voltage disruptions. We develop a novel combination of\nformal methods and machine learning tools that learns power system dynamics\nwith the objective of generating unsafe yet stealthy false data based attack\nsequences. We enable the grid with anomaly detectors in a generalized manner so\nthat it is difficult for an attacker to remain undetected. Our methodology,\nwhen applied on an IEEE 14 bus power grid model, uncovers stealthy attack\nvectors even in presence of such detectors.",
        "translated": "智能攻击者可以适当地篡改各种智能电网表面的传感器/执行器数据，造成故意的电源振荡，如果不加以检测，可能导致电压中断。我们开发了一个新的组合的形式方法和机器学习工具，学习电力系统动态的目标是生成不安全但隐秘的虚假数据为基础的攻击序列。我们以一种通用的方式启用带有异常检测器的网格，这样攻击者很难保持不被检测。我们的方法，当应用于 IEEE 14总线电网模型，揭示了即使存在这样的检测器隐形攻击矢量。"
    },
    {
        "title": "Who Let the Smart Toaster Hack the House? An Investigation into the\n  Security Vulnerabilities of Consumer IoT Devices",
        "url": "http://arxiv.org/abs/2306.09017v1",
        "pub_date": "2023-06-15",
        "summary": "For smart homes to be safe homes, they must be designed with security in\nmind. Yet, despite the widespread proliferation of connected digital\ntechnologies in the home environment, there is a lack of research evaluating\nthe security vulnerabilities and potential risks present within these systems.\nOur research presents a comprehensive methodology for conducting systematic IoT\nsecurity attacks, intercepting network traffic and evaluating the security\nrisks of smart home devices. We perform thousands of automated experiments\nusing 11 popular commercial IoT devices when deployed in a testbed, exposed to\na series of real deployed attacks (flooding, port scanning and OS scanning).\nOur findings indicate that these devices are vulnerable to security attacks and\nour results are relevant to the security research community, device engineers\nand the users who rely on these technologies in their daily lives.",
        "translated": "为了使智能家居成为安全家居，在设计时必须考虑到安全问题。然而，尽管联网数字技术在家庭环境中广泛扩散，但缺乏对这些系统中存在的安全漏洞和潜在风险进行评估的研究。我们的研究提出了一个综合的方法来进行系统的物联网安全攻击，拦截网络流量和评估智能家居设备的安全风险。我们使用11种流行的商业物联网设备进行了成千上万的自动化实验，这些设备被部署在一个测试平台上，暴露在一系列实际部署的攻击(洪水、端口扫描和操作系统扫描)之下。我们的研究结果表明，这些设备很容易受到安全攻击，我们的研究结果与安全研究界、设备工程师和日常生活中依赖这些技术的用户有关。"
    },
    {
        "title": "An Efficient and Multi-private Key Secure Aggregation for Federated\n  Learning",
        "url": "http://arxiv.org/abs/2306.08970v1",
        "pub_date": "2023-06-15",
        "summary": "With the emergence of privacy leaks in federated learning, secure aggregation\nprotocols that mainly adopt either homomorphic encryption or threshold secret\nsharing have been widely developed for federated learning to protect the\nprivacy of the local training data of each client. However, these existing\nprotocols suffer from many shortcomings, such as the dependence on a trusted\nthird party, the vulnerability to clients being corrupted, low efficiency, the\ntrade-off between security and fault tolerance, etc. To solve these\ndisadvantages, we propose an efficient and multi-private key secure aggregation\nscheme for federated learning. Specifically, we skillfully modify the variant\nElGamal encryption technique to achieve homomorphic addition operation, which\nhas two important advantages: 1) The server and each client can freely select\npublic and private keys without introducing a trust third party and 2) Compared\nto the variant ElGamal encryption, the plaintext space is relatively large,\nwhich is more suitable for the deep model. Besides, for the high dimensional\ndeep model parameter, we introduce a super-increasing sequence to compress\nmulti-dimensional data into 1-D, which can greatly reduce encryption and\ndecryption times as well as communication for ciphertext transmission. Detailed\nsecurity analyses show that our proposed scheme achieves the semantic security\nof both individual local gradients and the aggregated result while achieving\noptimal robustness in tolerating both client collusion and dropped clients.\nExtensive simulations demonstrate that the accuracy of our scheme is almost the\nsame as the non-private approach, while the efficiency of our scheme is much\nbetter than the state-of-the-art homomorphic encryption-based secure\naggregation schemes. More importantly, the efficiency advantages of our scheme\nwill become increasingly prominent as the number of model parameters increases.",
        "translated": "随着联邦学习出现私隐泄漏的情况，为保障每个客户的本地训练资料的私隐，联邦学习已广泛发展主要采用同态加密或门槛秘密共享的安全聚合协议。然而，现有的协议存在许多缺陷，如对可信第三方的依赖性、易受客户端损坏、效率低下、安全性和容错性之间的权衡等。针对这些不足，本文提出了一种高效的多私钥安全聚合方案。具体地说，我们巧妙地修改了变型 ElGamal 加密技术，实现了同态加法运算，这种加法运算具有两个重要的优点: 1)服务器和每个客户端可以自由选择公钥和私钥，而不需要引入信任的第三方; 2)与变型 ElGamal 加密相比，明文空间相对较大，更适合于深度模型。此外，对于高维深度模型参数，我们引入了一个超增长序列将多维数据压缩成一维数据，这样可以大大减少加密和解密的时间以及密文传输的通信。详细的安全性分析表明，我们提出的方案既能达到个别局部梯度的语义安全，又能达到聚合结果的效果，同时在容忍客户端合谋和丢弃客户端方面达到最佳的稳健性。大量的仿真结果表明，该方案的精度与非私有方法相当，而效率远远高于目前最先进的基于同态加密的安全聚合方案。更重要的是，我们的方案的效率优势将日益突出的模型参数的数量增加。"
    },
    {
        "title": "CLIP2Protect: Protecting Facial Privacy using Text-Guided Makeup via\n  Adversarial Latent Search",
        "url": "http://arxiv.org/abs/2306.10008v1",
        "pub_date": "2023-06-16",
        "summary": "The success of deep learning based face recognition systems has given rise to\nserious privacy concerns due to their ability to enable unauthorized tracking\nof users in the digital world. Existing methods for enhancing privacy fail to\ngenerate naturalistic images that can protect facial privacy without\ncompromising user experience. We propose a novel two-step approach for facial\nprivacy protection that relies on finding adversarial latent codes in the\nlow-dimensional manifold of a pretrained generative model. The first step\ninverts the given face image into the latent space and finetunes the generative\nmodel to achieve an accurate reconstruction of the given image from its latent\ncode. This step produces a good initialization, aiding the generation of\nhigh-quality faces that resemble the given identity. Subsequently, user-defined\nmakeup text prompts and identity-preserving regularization are used to guide\nthe search for adversarial codes in the latent space. Extensive experiments\ndemonstrate that faces generated by our approach have stronger black-box\ntransferability with an absolute gain of 12.06% over the state-of-the-art\nfacial privacy protection approach under the face verification task. Finally,\nwe demonstrate the effectiveness of the proposed approach for commercial face\nrecognition systems. Our code is available at\nhttps://github.com/fahadshamshad/Clip2Protect.",
        "translated": "基于深度学习的人脸识别系统的成功引起了严重的隐私问题，因为它们能够在数字世界中对用户进行未经授权的跟踪。现有的增强隐私的方法无法生成自然的图像，从而在不损害用户体验的情况下保护面部隐私。我们提出了一种新颖的两步法来保护面部隐私，这种方法依赖于在预先训练好的生成模型的低维流形中发现对手的潜在代码。第一步将给定的人脸图像反转到潜在空间，并对生成模型进行微调，以便从潜在代码实现给定图像的准确重建。这一步产生了良好的初始化，有助于生成类似于给定身份的高质量面孔。然后，利用自定义化妆文本提示和身份保持正则化来引导在潜在空间中搜索对手代码。大量实验表明，该方法生成的人脸具有更强的黑盒可转移性，在人脸验证任务下比最先进的面部隐私保护方法的绝对增益为12.06% 。最后，我们证明了该方法在商业人脸识别系统中的有效性。我们的代码可以在 https://github.com/fahadshamshad/clip2protect 找到。"
    },
    {
        "title": "Evaluating Superhuman Models with Consistency Checks",
        "url": "http://arxiv.org/abs/2306.09983v1",
        "pub_date": "2023-06-16",
        "summary": "If machine learning models were to achieve superhuman abilities at various\nreasoning or decision-making tasks, how would we go about evaluating such\nmodels, given that humans would necessarily be poor proxies for ground truth?\nIn this paper, we propose a framework for evaluating superhuman models via\nconsistency checks. Our premise is that while the correctness of superhuman\ndecisions may be impossible to evaluate, we can still surface mistakes if the\nmodel's decisions fail to satisfy certain logical, human-interpretable rules.\nWe instantiate our framework on three tasks where correctness of decisions is\nhard to evaluate due to either superhuman model abilities, or to otherwise\nmissing ground truth: evaluating chess positions, forecasting future events,\nand making legal judgments. We show that regardless of a model's (possibly\nsuperhuman) performance on these tasks, we can discover logical inconsistencies\nin decision making. For example: a chess engine assigning opposing valuations\nto semantically identical boards; GPT-4 forecasting that sports records will\nevolve non-monotonically over time; or an AI judge assigning bail to a\ndefendant only after we add a felony to their criminal record.",
        "translated": "如果机器学习模型能够在各种推理或决策任务中获得超人的能力，那么我们如何评估这些模型，因为人类必然不能很好地代表基本事实？本文提出了一个通过一致性检验来评估超人模型的框架。我们的前提是，尽管超人类决策的正确性可能无法评估，但是如果模型的决策不能满足某些逻辑的、人类可以解释的规则，我们仍然可以表现出错误。我们在三个任务上实例化了我们的框架，其中决策的正确性很难评估，因为要么是超人的模型能力，要么是缺少基本真理: 评估国际象棋的位置，预测未来的事件，以及做出法律判断。我们表明，无论模型在这些任务上的表现如何(可能是超人的表现) ，我们都能发现决策过程中的逻辑不一致。例如: 一个国际象棋引擎为语义相同的棋盘分配相反的估值; GPT-4预测体育记录将随着时间的推移而非单调地发展; 或者一个人工智能法官只有在我们为被告的犯罪记录增加一项重罪之后才会为其分配保释金。"
    },
    {
        "title": "Data Protection for Data Privacy-A South African Problem?",
        "url": "http://arxiv.org/abs/2306.09934v1",
        "pub_date": "2023-06-16",
        "summary": "This study proposes a comprehensive framework for enhancing data security and\nprivacy within organizations through data protection awareness. It employs a\nquantitative method and survey research strategy to assess the level of data\nprotection awareness among employees of a public organization.",
        "translated": "本研究提出了一个通过提高数据保护意识加强组织内部数据安全和隐私的综合框架。采用定量研究方法和调查研究策略，对公共机构员工的数据保护意识水平进行评估。"
    },
    {
        "title": "Query-Free Evasion Attacks Against Machine Learning-Based Malware\n  Detectors with Generative Adversarial Networks",
        "url": "http://arxiv.org/abs/2306.09925v1",
        "pub_date": "2023-06-16",
        "summary": "Malware detectors based on machine learning (ML) have been shown to be\nsusceptible to adversarial malware examples. However, current methods to\ngenerate adversarial malware examples still have their limits. They either rely\non detailed model information (gradient-based attacks), or on detailed outputs\nof the model - such as class probabilities (score-based attacks), neither of\nwhich are available in real-world scenarios. Alternatively, adversarial\nexamples might be crafted using only the label assigned by the detector\n(label-based attack) to train a substitute network or an agent using\nreinforcement learning. Nonetheless, label-based attacks might require querying\na black-box system from a small number to thousands of times, depending on the\napproach, which might not be feasible against malware detectors. This work\npresents a novel query-free approach to craft adversarial malware examples to\nevade ML-based malware detectors. To this end, we have devised a GAN-based\nframework to generate adversarial malware examples that look similar to benign\nexecutables in the feature space. To demonstrate the suitability of our\napproach we have applied the GAN-based attack to three common types of features\nusually employed by static ML-based malware detectors: (1) Byte histogram\nfeatures, (2) API-based features, and (3) String-based features. Results show\nthat our model-agnostic approach performs on par with MalGAN, while generating\nmore realistic adversarial malware examples without requiring any query to the\nmalware detectors. Furthermore, we have tested the generated adversarial\nexamples against state-of-the-art multimodal and deep learning malware\ndetectors, showing a decrease in detection performance, as well as a decrease\nin the average number of detections by the anti-malware engines in VirusTotal.",
        "translated": "基于机器学习(ML)的恶意软件检测器已被证明易受敌对恶意软件例子的影响。然而，目前的方法生成对手恶意软件的例子仍然有其局限性。它们要么依赖于详细的模型信息(基于梯度的攻击) ，要么依赖于模型的详细输出——比如类概率(基于分数的攻击) ，这两种情况在现实世界的场景中都不可用。或者，可以使用检测器分配的标签(基于标签的攻击)来制作对抗性的例子，以训练替代网络或使用强化学习的代理。尽管如此，基于标签的攻击可能需要查询一个黑匣子系统，查询次数从少到数千次，这取决于不同的方法，这对于恶意软件检测器来说可能是不可行的。这项工作提出了一个新的查询免费的方法，以工艺敌对恶意软件的例子，以规避基于机器学习的恶意软件检测器。为此，我们设计了一个基于 GAN 的框架来生成与特性空间中的良性可执行程序类似的恶意软件示例。为了证明我们方法的适用性，我们将基于 GAN 的攻击应用于三种常见的特性，这些特性通常由静态基于 ML 的恶意软件检测器使用: (1)字节直方图特性，(2)基于 API 的特性，和(3)基于字符串的特性。结果表明，我们的模型无关的方法执行与 MalGAN 相当，同时产生更真实的对手恶意软件的例子，而不需要任何查询的恶意软件检测器。此外，我们已经测试了生成的敌对示例对最先进的多模式和深度学习恶意软件检测器，显示检测性能下降，以及 VirusTotal 中反恶意软件引擎的平均检测次数下降。"
    },
    {
        "title": "CroCoDai: A Stablecoin for Cross-Chain Commerce",
        "url": "http://arxiv.org/abs/2306.09754v1",
        "pub_date": "2023-06-16",
        "summary": "Decentralized Finance (DeFi), in which digital assets are exchanged without\ntrusted intermediaries, has grown rapidly in value in recent years. The global\nDeFi ecosystem is fragmented into multiple blockchains, fueling the demand for\ncross-chain commerce. Existing approaches for cross-chain transactions, e.g.,\nbridges and cross-chain deals, achieve atomicity by locking assets in escrow.\nHowever, locking up assets increases the financial risks for the participants,\nespecially due to price fluctuations and the long latency of cross-chain\ntransactions. Stablecoins, which are pegged to a non-volatile asset such as the\nUS dollar, help mitigate the risk associated with price fluctuations. However,\nexisting stablecoin designs are tied to individual blockchain platforms, and\ntrusted parties or complex protocols are needed to exchange stablecoin tokens\nbetween blockchains.\n  Our goal is to design a practical stablecoin for cross-chain commerce.\nRealizing this goal requires addressing two challenges. The first challenge is\nto support a large and growing number of blockchains efficiently. The second\nchallenge is to be resilient to price fluctuations and blockchain platform\nfailures. We present CroCoDai to address these challenges. We also present\nthree prototype implementations of our stablecoin system, and show that it\nincurs small execution overhead.",
        "translated": "分散金融(DeFi)是指在没有可信中介的情况下交换数字资产，近年来其价值迅速增长。全球的 DeFi 生态系统被分割成多个区块链，刺激了对跨链商务的需求。现有的跨链交易方法，例如桥梁和跨链交易，通过将资产锁定在第三方托管中来实现原子性。然而，锁定资产增加了参与者的财务风险，特别是由于价格波动和跨链交易的长期延迟。稳定币与美元等非波动性资产挂钩，有助于减轻与价格波动相关的风险。然而，现有的稳定币设计是与单个区块链平台相联系的，并且需要可信方或复杂的协议来在区块链之间交换稳定币令牌。我们的目标是设计一个实用的跨链商务稳定币。实现这一目标需要解决两个挑战。第一个挑战是有效地支持大量且不断增长的区块链。第二个挑战是对价格波动和区块链平台故障的弹性。我们介绍 CroCoDai 来应对这些挑战。我们还提出了我们的稳定币系统的三个原型实现，并表明它带来了小的执行开销。"
    },
    {
        "title": "PIEChain -- A Practical Blockchain Interoperability Framework",
        "url": "http://arxiv.org/abs/2306.09735v1",
        "pub_date": "2023-06-16",
        "summary": "A plethora of different blockchain platforms have emerged in recent years,\nbut many of them operate in silos. As such, there is a need for reliable\ncross-chain communication to enable blockchain interoperability. Blockchain\ninteroperability is challenging because transactions can typically not be\nreverted - as such, if one transaction is committed then the protocol must\nensure that all related transactions are committed as well. Existing\ninteroperability approaches, e.g., Cosmos and Polkadot, are limited in the\nsense that they only support interoperability between their own subchains, or\nrequire intrusive changes to existing blockchains. To overcome this limitation,\nwe propose PIEChain, a general, Kafka-based cross-chain communication\nframework. We utilize PIEChain for a practical case study: a cross-chain\nauction in which users who hold tokens on multiple chains bid for a ticket sold\non another chain. PIEChain is the first publicly available, practical\nimplementation of a general framework for cross-chain communication.",
        "translated": "近年来出现了大量不同的区块链平台，但其中许多是在竖井中运作的。因此，需要可靠的跨链通信来实现区块链互操作性。区块链互操作性是具有挑战性的，因为事务通常不能被恢复——因此，如果一个事务被提交，那么协议必须确保所有相关的事务也被提交。现有的互操作性方法，如 Cosmos 和 Polkadot，在某种意义上是有限的，它们只支持它们自己的子链之间的互操作性，或者需要对现有的区块链进行侵入性改变。为了克服这个限制，我们提出了 PIEChain，一个通用的，基于卡夫卡的跨链通信框架。我们利用 PIEChain 进行了一个实际案例研究: 一个跨链拍卖，其中持有多个链上的令牌的用户为另一个链上出售的门票出价。PIEChain 是第一个公开的、实用的跨链通信通用框架的实现。"
    },
    {
        "title": "Lost and not Found: An Investigation of Recovery Methods for\n  Multi-Factor Authentication",
        "url": "http://arxiv.org/abs/2306.09708v1",
        "pub_date": "2023-06-16",
        "summary": "Multi-Factor Authentication is intended to strengthen the security of\npassword-based authentication by adding another factor, such as hardware tokens\nor one-time passwords using mobile apps. However, this increased authentication\nsecurity comes with potential drawbacks that can lead to account and asset\nloss. If users lose access to their additional authentication factors for any\nreason, they will be locked out of their accounts. Consequently, services that\nprovide Multi-Factor Authentication should deploy procedures to allow their\nusers to recover from losing access to their additional factor that are both\nsecure and easy-to-use. To the best of our knowledge, we are the first to\nfirst-hand investigate the security and user experience of deployed\nMulti-Factor Authentication recovery procedures. We first evaluate the official\nhelp and support pages of 1,303 websites that provide Multi-Factor\nAuthentication and collect documented information about their recovery\nprocedures. Second, we select a subset of 71 websites, create accounts, set up\nMulti-Factor Authentication, and perform an in-depth investigation of their\nrecovery procedure security and user experience. We find that many websites\ndeploy insecure Multi-Factor Authentication recovery procedures and allowed us\nto circumvent and disable Multi-Factor Authentication when having access to the\naccounts' associated email addresses. Furthermore, we commonly observed\ndiscrepancies between our in-depth analysis and the official help and support\npages, implying that information meant to aid users is often either incorrect\nor outdated.",
        "translated": "双重身份验证的目的是增加另一项因素，例如硬件令牌或使用流动应用程式的一次性密码，以加强以密码为基础的认证的安全性。但是，这种增强的身份验证安全性带有潜在的缺点，可能导致帐户和资产损失。如果用户由于任何原因无法访问其附加身份验证因子，他们将被锁定在自己的帐户之外。因此，提供双重身份验证的服务应该部署程序，使用户能够从失去对其安全和易于使用的附加因素的访问中恢复过来。据我们所知，我们是第一个第一手调查已部署的双重身份验证恢复程序的安全性和用户体验。我们首先评估1,303个网站的官方帮助和支持页面，这些网站提供双重身份验证，并收集有关其恢复程序的文档信息。其次，我们从71个网站中选出一个子集，创建帐户，设置双重身份验证，并对其恢复过程的安全性和用户体验进行深入调查。我们发现很多网站采用不安全的双重身份验证恢复程序，让我们在进入有关帐户的电邮地址时可以规避和禁用双重身份验证。此外，我们经常观察到我们的深入分析与官方帮助和支持页面之间的差异，这意味着旨在帮助用户的信息往往不正确或过时。"
    },
    {
        "title": "A Smooth Binary Mechanism for Efficient Private Continual Observation",
        "url": "http://arxiv.org/abs/2306.09666v1",
        "pub_date": "2023-06-16",
        "summary": "In privacy under continual observation we study how to release differentially\nprivate estimates based on a dataset that evolves over time. The problem of\nreleasing private prefix sums of $x_1,x_2,x_3,\\dots \\in\\{0,1\\}$ (where the\nvalue of each $x_i$ is to be private) is particularly well-studied, and a\ngeneralized form is used in state-of-the-art methods for private stochastic\ngradient descent (SGD). The seminal binary mechanism privately releases the\nfirst $t$ prefix sums with noise of variance polylogarithmic in $t$. Recently,\nHenzinger et al. and Denisov et al. showed that it is possible to improve on\nthe binary mechanism in two ways: The variance of the noise can be reduced by a\n(large) constant factor, and also made more even across time steps. However,\ntheir algorithms for generating the noise distribution are not as efficient as\none would like in terms of computation time and (in particular) space. We\naddress the efficiency problem by presenting a simple alternative to the binary\nmechanism in which 1) generating the noise takes constant average time per\nvalue, 2) the variance is reduced by a factor about 4 compared to the binary\nmechanism, and 3) the noise distribution at each step is identical.\nEmpirically, a simple Python implementation of our approach outperforms the\nrunning time of the approach of Henzinger et al., as well as an attempt to\nimprove their algorithm using high-performance algorithms for multiplication\nwith Toeplitz matrices.",
        "translated": "在持续观察下，我们研究如何根据随时间演变的数据集发布不同的私人估计。发布 $x _ 1，x _ 2，x _ 3的私有前缀和的问题在{0,1} $中的点(其中每个 $x _ i $的值是私有的)被特别好地研究，并且在私有随机梯度下降的最先进的方法中使用了广义形式。开创性的二进制机制私下释放第一个 $t $前缀和，其噪声为 $t $中的方差多对数。最近，Henzinger 等人和 Denisov 等人表明，有可能通过两种方式改进二元机制: 噪声的方差可以通过一个(大的)常数因子来减小，并且在时间步长上更加均匀。然而，他们生成噪音分布的算法在计算时间和(特别是)空间方面并不像人们希望的那样有效。我们提出了一个简单的替代二元机制的效率问题，其中1)产生噪声的平均时间为每个值不变，2)方差比二元机制减少约4个因子，3)噪声分布在每个步骤是相同的。根据经验，我们方法的一个简单的 Python 实现比 Henzinger 等人的方法的运行时间更长，并且尝试使用高性能算法来改进他们的算法，用 Toeplitz 矩阵进行乘法。"
    },
    {
        "title": "Cybersecurity Career Requirements: A Literature Review",
        "url": "http://arxiv.org/abs/2306.09599v1",
        "pub_date": "2023-06-16",
        "summary": "This study employs a systematic literature review approach to identify the\nrequirements of a career as a cybersecurity professional. It aims to raise\npublic awareness regarding opportunities in the Information Security (IS)\nprofession. A total of 1,520 articles were identified from four academic\ndatabases by searching using the terms \"cybersecurity\" and \"skills\". After\nrigorous screening according to various criteria, 31 papers remained. The\nfindings of these studies were thematically analyzed to describe the knowledge\nand skills an IS professional should possess. The research found that a\nconsiderable investment in time is necessary for cybersecurity professionals to\nreach the required technical proficiency. It also identified female gender\nbarriers to cybersecurity careers due to the unique requirements of the field\nand suggests that females may successfully enter at lower levels and progress\nup the tiers as circumstances dictate.",
        "translated": "本研究采用系统的文献综述方法，以确定网络安全专业人员的职业要求。该计划旨在提高公众对资讯保安专业机会的认识。通过使用“网络安全”和“技能”两个术语搜索，从四个学术数据库中共识别出1,520篇文章。经过严格筛选，根据不同的标准，31篇论文仍然存在。这些研究的结果进行了专题分析，以描述知识和技能的信息系统专业人员应该具备的。研究发现，网络安全专业人员要达到所要求的技术熟练程度，需要在时间上进行大量投资。报告还指出，由于该领域的独特要求，女性在网络安全职业方面存在性别障碍，并建议女性可以根据情况的需要成功地进入较低的职等，并在职等上取得进展。"
    },
    {
        "title": "Fuzzy Feature Selection with Key-based Cryptographic Transformations",
        "url": "http://arxiv.org/abs/2306.09583v1",
        "pub_date": "2023-06-16",
        "summary": "In the field of cryptography, the selection of relevant features plays a\ncrucial role in enhancing the security and efficiency of cryptographic\nalgorithms. This paper presents a novel approach of applying fuzzy feature\nselection to key-based cryptographic transformations. The proposed fuzzy\nfeature selection leverages the power of fuzzy logic to identify and select\noptimal subsets of features that contribute most effectively to the\ncryptographic transformation process. By incorporating fuzzy feature selection\ninto key-based cryptographic transformations, this research aims to improve the\nresistance against attacks and enhance the overall performance of cryptographic\nsystems. Experimental evaluations may demonstrate the effectiveness of the\nproposed approach in selecting secure key features with minimal computational\noverhead. This paper highlights the potential of fuzzy feature selection as a\nvaluable tool in the design and optimization of key-based cryptographic\nalgorithms, contributing to the advancement of secure information exchange and\ncommunication in various domains.",
        "translated": "在密码学领域，相关特征的选择对提高密码算法的安全性和效率起着至关重要的作用。提出了一种将模糊特征选择应用于基于密钥的密码变换的新方法。提出的模糊特征选择利用模糊逻辑的能力来识别和选择最有效地促进密码转换过程的特征的最佳子集。将模糊特征选择技术引入到基于密钥的密码变换中，旨在提高密码系统的抗攻击能力，提高系统的整体性能。实验结果表明，该方法能够以最小的计算开销选择安全的关键特征。本文着重介绍了模糊特征选择技术作为密钥密码算法设计和优化的有效工具的潜力，为各领域的安全信息交换和通信的发展做出了贡献。"
    },
    {
        "title": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT\n  Models",
        "url": "http://arxiv.org/abs/2306.11698v1",
        "pub_date": "2023-06-20",
        "summary": "Generative Pre-trained Transformer (GPT) models have exhibited exciting\nprogress in capabilities, capturing the interest of practitioners and the\npublic alike. Yet, while the literature on the trustworthiness of GPT models\nremains limited, practitioners have proposed employing capable GPT models for\nsensitive applications to healthcare and finance - where mistakes can be\ncostly. To this end, this work proposes a comprehensive trustworthiness\nevaluation for large language models with a focus on GPT-4 and GPT-3.5,\nconsidering diverse perspectives - including toxicity, stereotype bias,\nadversarial robustness, out-of-distribution robustness, robustness on\nadversarial demonstrations, privacy, machine ethics, and fairness. Based on our\nevaluations, we discover previously unpublished vulnerabilities to\ntrustworthiness threats. For instance, we find that GPT models can be easily\nmisled to generate toxic and biased outputs and leak private information in\nboth training data and conversation history. We also find that although GPT-4\nis usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more\nvulnerable given jailbreaking system or user prompts, potentially due to the\nreason that GPT-4 follows the (misleading) instructions more precisely. Our\nwork illustrates a comprehensive trustworthiness evaluation of GPT models and\nsheds light on the trustworthiness gaps. Our benchmark is publicly available at\nhttps://decodingtrust.github.io/.",
        "translated": "生成式预训练变压器(GPT)模型在性能方面展示了令人兴奋的进步，吸引了从业者和公众的兴趣。然而，尽管关于 GPT 模型可信度的文献仍然有限，但从业人员已经提议，在医疗和金融领域的敏感应用中使用有能力的 GPT 模型——在这些领域，错误可能代价高昂。为此，这项工作提出了一个全面的大型语言模型的可信性评估，重点是 GPT-4和 GPT-3.5，考虑到不同的观点-包括毒性，刻板印象偏差，对抗性鲁棒性，分布外鲁棒性，对抗性示范的鲁棒性，隐私，机器伦理和公平。根据我们的评估，我们发现了以前未公布的可信度威胁的漏洞。例如，我们发现 GPT 模型很容易被误导，从而产生有毒和有偏见的输出，并在训练数据和对话历史中泄露私人信息。我们还发现，尽管 GPT-4在标准基准上通常比 GPT-3.5更值得信赖，但是在破解系统或用户提示下，GPT-4更容易受到攻击，这可能是由于 GPT-4更精确地遵循(误导性的)指令的原因。我们的工作阐述了 GPT 模型的全面可信性评估，并揭示了可信性差距。我们的基准 https://decodingtrust.github.io/已公开发售。"
    },
    {
        "title": "Pseudorandom unitaries are neither real nor sparse nor noise-robust",
        "url": "http://arxiv.org/abs/2306.11677v1",
        "pub_date": "2023-06-20",
        "summary": "Pseudorandom quantum states (PRSs) and pseudorandom unitaries (PRUs) possess\nthe dual nature of being efficiently constructible while appearing completely\nrandom to any efficient quantum algorithm. In this study, we establish\nfundamental bounds on pseudorandomness. We show that PRSs and PRUs exist only\nwhen the probability that an error occurs is negligible, ruling out their\ngeneration on noisy intermediate-scale and early fault-tolerant quantum\ncomputers. Additionally, we derive lower bounds on the imaginarity and\ncoherence of PRSs and PRUs, rule out the existence of sparse or real PRUs, and\nshow that PRUs are more difficult to generate than PRSs. Our work also\nestablishes rigorous bounds on the efficiency of property testing,\ndemonstrating the exponential complexity in distinguishing real quantum states\nfrom imaginary ones, in contrast to the efficient measurability of unitary\nimaginarity. Furthermore, we prove lower bounds on the testing of coherence.\nLastly, we show that the transformation from a complex to a real model of\nquantum computation is inefficient, in contrast to the reverse process, which\nis efficient. Overall, our results establish fundamental limits on property\ntesting and provide valuable insights into quantum pseudorandomness.",
        "translated": "伪随机量子态(PRS)和伪随机酉(PRU)具有有效构造的双重性质，同时对于任何有效的量子算法来说都是完全随机的。在这项研究中，我们建立了伪随机数的基本界限。我们证明了 PRS 和 PRU 只有在发生错误的概率可以忽略的情况下才存在，排除了它们在噪声中等规模和早期容错量子计算机上产生的可能性。此外，我们得到了 PRS 和 PRU 的想象性和一致性的下界，排除了稀疏或真实 PRU 的存在，并表明 PRU 比 PRS 更难产生。我们的工作也建立了严格的界限对性质测试的效率，证明了指数复杂性在区分真实的量子态和想象的，相对于有效的可测量的酉想象性。此外，我们还证明了一致性检验的下界。最后，我们发现量子计算从复杂模型到实际模型的转换是低效的，相比之下，反向过程是有效的。总的来说，我们的研究结果确立了物性测试的基本界限，并为量子伪随机数提供了有价值的见解。"
    },
    {
        "title": "On Cross-Layer Interactions of QUIC, Encrypted DNS and HTTP/3: Design,\n  Evaluation and Dataset",
        "url": "http://arxiv.org/abs/2306.11643v1",
        "pub_date": "2023-06-20",
        "summary": "Every Web session involves a DNS resolution. While, in the last decade, we\nwitnessed a promising trend towards an encrypted Web in general, DNS encryption\nhas only recently gained traction with the standardisation of DNS over TLS\n(DoT) and DNS over HTTPS (DoH). Meanwhile, the rapid rise of QUIC deployment\nhas now opened up an exciting opportunity to utilise the same protocol to not\nonly encrypt Web communications, but also DNS. In this paper, we evaluate this\nbenefit of using QUIC to coalesce name resolution via DNS over QUIC (DoQ), and\nWeb content delivery via HTTP/3 (H3) with 0-RTT. We compare this scenario using\nseveral possible combinations where H3 is used in conjunction with DoH and DoQ,\nas well as the unencrypted DNS over UDP (DoUDP). We observe, that when using H3\n1-RTT, page load times with DoH can get inflated by $&gt;$30\\% over fixed-line and\nby $&gt;$50\\% over mobile when compared to unencrypted DNS with DoUDP. However,\nthis cost of encryption can be drastically reduced when encrypted connections\nare coalesced (DoQ + H3 0-RTT), thereby reducing the page load times by 1/3\nover fixed-line and 1/2 over mobile, overall making connection coalescing with\nQUIC the best option for encrypted communication on the Internet.",
        "translated": "每个 Web 会话都涉及一个 DNS 解决方案。然而，在过去的十年中，我们目睹了一个向加密网络发展的大趋势，DNS 加密只是最近才随着 DNS over TLS (DoT)和 DNS over HTTPS (DoH)的标准化而获得关注。与此同时，QUIC 部署的快速增长为利用相同的协议不仅加密网络通信，而且加密域名解析系统(DNS)提供了一个令人兴奋的机会。在本文中，我们评估了使用 QUIC 通过 DNS over QUIC (DoQ)和0-RTT 通过 HTTP/3(H3)传输 Web 内容来合并名称解析的好处。我们使用几种可能的组合来比较这个场景，其中 H3与 DoH 和 DoQ 以及未加密的 DNS over UDP (DoUDP)一起使用。我们观察到，当使用 H31-RTT 时，与使用 DoUDP 的未加密 DNS 相比，使用 DoH 的页面加载时间可以在固定线路上增加30% 以上，在移动线路上增加50% 以上。然而，当加密连接合并(DoQ + H30-RTT)时，这种加密成本可以大大降低，从而使固定线路上的页面加载时间减少1/3，移动线路上的页面加载时间减少1/2，总体上使得与 QUIC 的连接合并成为互联网上加密通信的最佳选择。"
    },
    {
        "title": "SALSA VERDE: a machine learning attack on Learning With Errors with\n  sparse small secrets",
        "url": "http://arxiv.org/abs/2306.11641v1",
        "pub_date": "2023-06-20",
        "summary": "Learning with Errors (LWE) is a hard math problem used in post-quantum\ncryptography. Homomorphic Encryption (HE) schemes rely on the hardness of the\nLWE problem for their security, and two LWE-based cryptosystems were recently\nstandardized by NIST for digital signatures and key exchange (KEM). Thus, it is\ncritical to continue assessing the security of LWE and specific parameter\nchoices. For example, HE uses small secrets, and the HE community has\nconsidered standardizing small sparse secrets to improve efficiency and\nfunctionality. However, prior work, SALSA and PICANTE, showed that ML attacks\ncan recover sparse binary secrets. Building on these, we propose VERDE, an\nimproved ML attack that can recover sparse binary, ternary, and small Gaussian\nsecrets. Using improved preprocessing and secret recovery techniques, VERDE can\nattack LWE with larger dimensions ($n=512$) and smaller moduli ($\\log_2 q=12$\nfor $n=256$), using less time and power. We propose novel architectures for\nscaling. Finally, we develop a theory that explains the success of ML LWE\nattacks.",
        "translated": "带错误学习(LWE)是后量子密码学中的一个难题。高同态加密密码体制(HE)的安全性依赖于 LWE 问题的难度，而两个基于 LWE 的密码体制最近被 NIST 标准化，用于数字签名和密钥交换(KEM)。因此，继续评估 LWE 的安全性和特定的参数选择是至关重要的。例如，HE 使用小秘密，HE 社区已经考虑将小稀疏秘密标准化，以提高效率和功能。然而，先前的工作，SALSA 和 PICANTE，表明机器学习攻击可以恢复稀疏的二进制秘密。在此基础上，我们提出了 VERDE，一种改进的机器学习攻击，可以恢复稀疏的二进制，三进制和小高斯秘密。使用改进的预处理和秘密恢复技术，VERDE 可以用更大的维度($n = 512 $)和更小的模块($log _ 2 q = 12 $for $n = 256 $)攻击 LWE，使用更少的时间和功耗。我们提出了新颖的可伸缩架构。最后，我们发展了一个理论来解释 ML LWE 攻击的成功。"
    },
    {
        "title": "The Pricing And Hedging Of Constant Function Market Makers",
        "url": "http://arxiv.org/abs/2306.11580v1",
        "pub_date": "2023-06-20",
        "summary": "We investigate the most common type of blockchain-based decentralized\nexchange, which are known as constant function market makers (CFMMs). We\nexamine the the market microstructure around CFMMs and present a model for\nvaluing the liquidity provider (LP) mechanism and estimating the value of the\nassociated derivatives. We develop a model with two types of traders that have\ndifferent information and contribute methods for simulating the behavior of\neach trader and accounting for trade PnL. We also develop ideas around the\nequilibrium distribution of fair price conditional on the arrival of traders.\nFinally, we show how these findings might be used to think about parameters for\nalternative CFMMs.",
        "translated": "我们调查最常见的类型的区块链基于分散交易，这是众所周知的恒定函数做市商(CFMM)。我们考察了 CFMM 周围的市场微观结构，并提出了一个评估流动性提供者(LP)机制和估计相关衍生品价值的模型。我们发展了一个模型与两类交易者有不同的信息和贡献的方法来模拟每个交易者的行为和会计交易 PnL。我们还围绕以交易者到达为条件的公平价格的均衡分布提出了一些想法。最后，我们展示了如何利用这些发现来考虑替代 CFMM 的参数。"
    },
    {
        "title": "A Comparative Audit of Privacy Policies from Healthcare Organizations in\n  USA, UK and India",
        "url": "http://arxiv.org/abs/2306.11557v1",
        "pub_date": "2023-06-20",
        "summary": "Data privacy in healthcare is of paramount importance (and thus regulated\nusing laws like HIPAA) due to the highly sensitive nature of patient data. To\nthat end, healthcare organizations mention how they collect/process/store/share\nthis data (i.e., data practices) via their privacy policies. Thus there is a\nneed to audit these policies and check compliance with respective laws. This\npaper addresses this need and presents a large-scale data-driven study to audit\nprivacy policies from healthcare organizations in three countries -- USA, UK,\nand India.\n  We developed a three-stage novel \\textit{workflow} for our audit. First, we\ncollected the privacy policies of thousands of healthcare organizations in\nthese countries and cleaned this privacy policy data using a clustering-based\nmixed-method technique. We identified data practices regarding users' private\nmedical data (medical history) and site privacy (cookie, logs) in these\npolicies. Second, we adopted a summarization-based technique to uncover exact\nbroad data practices across countries and notice important differences.\nFinally, we evaluated the cross-country data practices using the lens of legal\ncompliance (with legal expert feedback) and grounded in the theory of\nContextual Integrity (CI). Alarmingly, we identified six themes of\nnon-alignment (observed in 21.8\\% of data practices studied in India) pointed\nout by our legal experts. Furthermore, there are four \\textit{potential\nviolations} according to case verdicts from Indian Courts as pointed out by our\nlegal experts. We conclude this paper by discussing the utility of our auditing\nworkflow and the implication of our findings for different stakeholders.",
        "translated": "由于患者数据的高度敏感性，医疗保健中的数据隐私至关重要(因此使用 HIPAA 等法律进行监管)。为此，医疗机构提到了他们如何通过隐私策略收集/处理/存储/共享这些数据(即数据实践)。因此，有必要对这些政策进行审计，并检查其是否符合相关法律。本文针对这一需求，提出了一个大规模的数据驱动的研究审计隐私政策来自三个国家的医疗机构-美国，英国和印度。我们开发了一个三阶段的小说文本{工作流程}为我们的审计。首先，我们收集了这些国家数千家医疗机构的隐私政策，并使用基于聚类的混合方法技术清理了这些隐私政策数据。在这些策略中，我们确定了有关用户私人医疗数据(病史)和站点隐私(cookie、日志)的数据实践。其次，我们采用了一种基于摘要的技术来揭示各国之间确切的广泛数据实践，并注意到重要的差异。最后，我们以法律遵从性为视角(法律专家反馈) ，以情境完整性(CI)理论为基础，对跨国数据实践进行了评价。令人担忧的是，我们确定了法律专家指出的六个不一致的主题(在印度研究的数据实践中有21.8% 观察到这一点)。此外，根据我国法律专家指出的印度法院的判决，有四个案文(潜在违法行为)。我们通过讨论我们的审计工作流程的效用和我们的发现对不同利益相关者的影响来结束本文。"
    },
    {
        "title": "Data Availability Sampling in Ethereum: Analysis of P2P Networking\n  Requirements",
        "url": "http://arxiv.org/abs/2306.11456v1",
        "pub_date": "2023-06-20",
        "summary": "Despite their increasing popularity, blockchains still suffer from severe\nscalability limitations. Recently, Ethereum proposed a novel approach to block\nvalidation based on Data Availability Sampling (DAS), that has the potential to\nimprove its transaction per second rate by more than two orders of magnitude.\nDAS should also significantly reduce per-transaction validation costs. At the\nsame time, DAS introduces new communication patterns in the Ethereum\nPeer-to-Peer (P2P) network. These drastically increase the amount of exchanged\ndata and impose stringent latency objectives. In this paper, we review the new\nrequirements for P2P networking associated with DAS, discuss open challenges,\nand identify new research directions.",
        "translated": "尽管区块链越来越受欢迎，但它们仍然受到严重的可伸缩性限制。最近，以太坊提出了一种基于数据可用性采样(DAS)的块验证方法，该方法有可能将其每秒的交易速度提高两个数量级以上。DAS 还应显著降低每笔交易的验证成本。同时，DAS 在以太对等(P2P)网络中引入了新的通信模式。这些极大地增加了交换数据的数量，并实现了严格的延迟目标。在本文中，我们回顾了与 DAS 相关的 P2P 网络的新需求，讨论了开放的挑战，并确定了新的研究方向。"
    },
    {
        "title": "Transparency in App Analytics: Analyzing the Collection of User\n  Interaction Data",
        "url": "http://arxiv.org/abs/2306.11447v1",
        "pub_date": "2023-06-20",
        "summary": "The rise of mobile apps has brought greater convenience and many options for\nusers. However, many apps use analytics services to collect a wide range of\nuser interaction data, with privacy policies often failing to reveal the types\nof interaction data collected or the extent of the data collection practices.\nThis lack of transparency potentially breaches data protection laws and also\nundermines user trust. We conducted an analysis of the top 20 analytic\nlibraries for Android apps to identify common practices of interaction data\ncollection and used this information to develop a standardized collection claim\ntemplate for summarizing an app's data collection practices wrt. user\ninteraction data. We selected the top 100 apps from popular categories on\nGoogle Play and used automatic static analysis to extract collection evidence\nfrom their data collection implementations. Our analysis found that a\nsignificant majority of these apps actively collected interaction data from UI\ntypes such as View (89%), Button (76%), and Textfield (63%), highlighting the\npervasiveness of user interaction data collection. By comparing the collection\nevidence to the claims derived from privacy policy analysis, we manually\nfact-checked the completeness and accuracy of these claims for the top 10 apps.\nWe found that, except for one app, they all failed to declare all types of\ninteraction data they collect and did not specify some of the collection\ntechniques used.",
        "translated": "移动应用的兴起给用户带来了更多的便利和选择。然而，许多应用程序使用分析服务来收集广泛的用户交互数据，隐私策略往往不能揭示收集的交互数据类型或数据收集实践的范围。这种缺乏透明度的情况有可能违反数据保护法，也有可能破坏用户的信任。我们对 Android 应用程序的前20个分析库进行了分析，以确定交互数据收集的常见做法，并利用这些信息开发了一个标准化的收集索赔模板，用于总结应用程序的数据收集实践，包括用户交互数据。我们从 Google Play 的热门分类中选出了前100名的应用程序，并使用自动静态分析从它们的数据收集实现中提取收集证据。我们的分析发现，绝大多数这些应用程序积极地从 UI 类型(如 View (89%) ，Button (76%)和 Textfield (63%))收集交互数据，突出了用户交互数据收集的普遍性。通过将收集的证据与隐私政策分析得出的声明进行比较，我们手动对前10个应用程序的这些声明的完整性和准确性进行事实核查。我们发现，除了一个应用程序之外，它们都未能声明它们收集的所有类型的交互数据，也没有指定所使用的一些收集技术。"
    },
    {
        "title": "A Survey of Multivariate Polynomial Commitment Schemes",
        "url": "http://arxiv.org/abs/2306.11383v2",
        "pub_date": "2023-06-20",
        "summary": "A commitment scheme is a cryptographic tool that allows one to commit to a\nhidden value, with the option to open it later at requested places without\nrevealing the secret itself. Commitment schemes have important applications in\nzero-knowledge proofs and secure multi-party computation, just to name a few.\nThis survey introduces a few multivariate polynomial commitment schemes that\nare built from a variety of mathematical structures. We study how Orion is\nconstructed using hash functions; Dory, Bulletproofs, and Vampire using the\ninner-product argument; Signatures of Correct Computation using polynomial\nfactoring; DARK and Dew using groups of unknown order; and Orion+ using a\nCP-SNARK. For each protocol, we prove its completeness and state its security\nassumptions.",
        "translated": "承诺方案是一种加密工具，允许用户提交隐藏值，并可以选择稍后在请求的位置打开它，而不会泄露秘密本身。承诺方案在零知识证明和安全多方计算方面有着重要的应用，仅举几例。本文介绍了几种基于多种数学结构的多元多项式承诺方案。我们研究 Orion 是如何使用散列函数构造的; 多利，防弹，吸血鬼使用内积参数; 签名正确计算使用多项式分解; DARK 和露使用未知的顺序组; 以及 Orion + 使用 CP-SNARK。对于每个协议，我们证明了它的完备性并陈述了它的安全性假设。"
    },
    {
        "title": "FDInet: Protecting against DNN Model Extraction via Feature Distortion\n  Index",
        "url": "http://arxiv.org/abs/2306.11338v1",
        "pub_date": "2023-06-20",
        "summary": "Machine Learning as a Service (MLaaS) platforms have gained popularity due to\ntheir accessibility, cost-efficiency, scalability, and rapid development\ncapabilities. However, recent research has highlighted the vulnerability of\ncloud-based models in MLaaS to model extraction attacks. In this paper, we\nintroduce FDINET, a novel defense mechanism that leverages the feature\ndistribution of deep neural network (DNN) models. Concretely, by analyzing the\nfeature distribution from the adversary's queries, we reveal that the feature\ndistribution of these queries deviates from that of the model's training set.\nBased on this key observation, we propose Feature Distortion Index (FDI), a\nmetric designed to quantitatively measure the feature distribution deviation of\nreceived queries. The proposed FDINET utilizes FDI to train a binary detector\nand exploits FDI similarity to identify colluding adversaries from distributed\nextraction attacks. We conduct extensive experiments to evaluate FDINET against\nsix state-of-the-art extraction attacks on four benchmark datasets and four\npopular model architectures. Empirical results demonstrate the following\nfindings FDINET proves to be highly effective in detecting model extraction,\nachieving a 100% detection accuracy on DFME and DaST. FDINET is highly\nefficient, using just 50 queries to raise an extraction alarm with an average\nconfidence of 96.08% for GTSRB. FDINET exhibits the capability to identify\ncolluding adversaries with an accuracy exceeding 91%. Additionally, it\ndemonstrates the ability to detect two types of adaptive attacks.",
        "translated": "机器学习即服务(MLaaS)平台因其易访问性、成本效率、可伸缩性和快速开发能力而广受欢迎。然而，最近的研究强调了 MLaaS 中基于云的模型在模型提取攻击下的脆弱性。本文介绍了一种利用深度神经网络(DNN)模型特征分布的新型防御机制 FDINET。具体地，通过分析对手查询的特征分布，我们发现这些查询的特征分布与模型训练集的特征分布存在偏差。在此基础上，我们提出了特征失真指数(FDI) ，这是一个定量度量接收查询的特征分布偏差的指标。提出的 FDINET 利用 FDI 对二进制检测器进行训练，并利用 FDI 的相似性来识别分布式抽取攻击中的合谋对手。我们进行了广泛的实验来评估 FDINET 对四个基准数据集和四个流行的模型体系结构的六种最先进的提取攻击。实验结果表明，FDINET 在检测模型提取方面具有很好的效果，对 DFME 和 DaST 的检测准确率达到100% 。FDINET 是非常高效的，只需要50个查询就可以提出一个提取警报，GTSRB 的平均置信度为96.08% 。FDINET 具有识别共谋对手的能力，准确率超过91% 。此外，它还演示了检测两种类型的自适应攻击的能力。"
    },
    {
        "title": "Geometric Algorithms for $k$-NN Poisoning",
        "url": "http://arxiv.org/abs/2306.12377v1",
        "pub_date": "2023-06-21",
        "summary": "We propose a label poisoning attack on geometric data sets against\n$k$-nearest neighbor classification. We provide an algorithm that can compute\nan $\\varepsilon n$-additive approximation of the optimal poisoning in $n\\cdot\n2^{2^{O(d+k/\\varepsilon)}}$ time for a given data set $X \\in \\mathbb{R}^d$,\nwhere $|X| = n$. Our algorithm achieves its objectives through the application\nof multi-scale random partitions.",
        "translated": "针对 $k $- 最近邻分类，提出了一种针对几何数据集的标签中毒攻击。我们提供了一个算法，可以计算给定数据集 $X 在 $n cdot 2 ^ {2 ^ { O (d + k/varepsilon)}} $time 中最佳中毒的 $varepsilon n $- 加法近似，其中 $| X | = n $。我们的算法通过应用多尺度随机分区来实现其目标。"
    },
    {
        "title": "Do you still need a manual smart contract audit?",
        "url": "http://arxiv.org/abs/2306.12338v2",
        "pub_date": "2023-06-21",
        "summary": "We investigate the feasibility of employing large language models (LLMs) for\nconducting the security audit of smart contracts, a traditionally\ntime-consuming and costly process. Our research focuses on the optimization of\nprompt engineering for enhanced security analysis, and we evaluate the\nperformance and accuracy of LLMs using a benchmark dataset comprising 52\nDecentralized Finance (DeFi) smart contracts that have previously been\ncompromised.\n  Our findings reveal that, when applied to vulnerable contracts, both GPT-4\nand Claude models correctly identify the vulnerability type in 40% of the\ncases. However, these models also demonstrate a high false positive rate,\nnecessitating continued involvement from manual auditors. The LLMs tested\noutperform a random model by 20% in terms of F1-score.\n  To ensure the integrity of our study, we conduct mutation testing on five\nnewly developed and ostensibly secure smart contracts, into which we manually\ninsert two and 15 vulnerabilities each. This testing yielded a remarkable\nbest-case 78.7% true positive rate for the GPT-4-32k model. We tested both,\nasking the models to perform a binary classification on whether a contract is\nvulnerable, and a non-binary prompt. We also examined the influence of model\ntemperature variations and context length on the LLM's performance.\n  Despite the potential for many further enhancements, this work lays the\ngroundwork for a more efficient and economical approach to smart contract\nsecurity audits.",
        "translated": "我们研究了使用大语言模型(LLM)进行智能合同安全审计的可行性，这是一个传统的耗时和昂贵的过程。我们的研究集中在优化快速工程，以增强安全分析，我们使用一个基准数据集评估 LLM 的性能和准确性，这个基准数据集包括52个先前已被破坏的分散金融(DeFi)智能合同。我们的研究结果表明，当应用于脆弱性合同时，GPT-4和 Claude 模型正确识别了40% 的情况下的脆弱性类型。然而，这些模型也表现出很高的假阳性率，需要继续从人工审计员的参与。LLM 测试的 F1分数比随机模型高出20% 。为了确保研究的完整性，我们对5份新开发的、表面上安全的智能合同进行了突变测试分析，每份合同中我们手动插入了2个和15个漏洞。这个测试为 GPT-4-32k 模型产生了显著的最佳情况下78.7% 的真阳性率。我们同时测试了这两种方法，要求模型对合同是否有漏洞执行二进制分类，以及非二进制提示。我们还研究了模型温度变化和上下文长度对 LLM 性能的影响。尽管有许多进一步增强的潜力，但这项工作为更有效和更经济的智能合同安全审计方法奠定了基础。"
    },
    {
        "title": "ICAR, a categorical framework to connect vulnerability, threat and asset\n  managements",
        "url": "http://arxiv.org/abs/2306.12240v1",
        "pub_date": "2023-06-21",
        "summary": "We present ICAR, a mathematical framework derived from category theory for\nrepresenting cybersecurity NIST and MITRE's ontologies. Designed for\ncybersecurity, ICAR is a category whose objects are cybersecurity knowledge\n(weakness, vulnerability, impacted product, attack technique, etc.) and whose\nmorphisms are relations between this knowledge, that make sense for\ncybersecurity. Within this rigorous and unified framework, we obtain a\nknowledge graph capable of identifying the attack and weakness structures of an\nIS, at the interface between description logics, database theory and\ncybersecurity. We then define ten cybersecurity queries to help understand the\nrisks incurred by IS and organise their defence.",
        "translated": "我们提出 ICAR，一个数学框架派生的范畴理论，表示网络安全 NIST 和 MITRE 的本体。ICAR 是为网络安全而设计的一个范畴，其对象是网络安全知识(弱点、脆弱性、受影响的产品、攻击技术等) ，其形态是这些知识之间的关系，对网络安全有意义。在这个严格统一的框架内，在描述逻辑、数据库理论和网络安全的接口处，我们得到了一个能够识别信息系统攻击和弱点结构的知识图。然后，我们定义了十个网络安全查询，以帮助理解信息系统所带来的风险并组织其防御。"
    },
    {
        "title": "Tailstorm: A Secure and Fair Blockchain for Cash Transactions",
        "url": "http://arxiv.org/abs/2306.12206v1",
        "pub_date": "2023-06-21",
        "summary": "Proof-of-work (PoW) cryptocurrencies rely on a balance of security and\nfairness in order to maintain a sustainable ecosystem of miners and users.\nUsers demand fast and consistent transaction confirmation, and in exchange\ndrive the adoption and valuation of the cryptocurrency. Miners provide the\nconfirmations, however, they primarily seek rewards. In unfair systems, miners\ncan amplify their rewards by consolidating mining power. Centralization\nhowever, undermines the security guarantees of the system and might discourage\nusers.\n  In this paper we present Tailstorm, a cryptocurrency that strikes this\nbalance. Tailstorm merges multiple recent protocol improvements addressing\nsecurity, confirmation latency, and throughput with a novel incentive mechanism\nimproving fairness. We implement a parallel proof-of-work consensus mechanism\nwith $k$ PoWs per block to obtain state-of-the-art consistency guarantees.\nInspired by Bobtail and Storm, we structure the individual PoWs in a tree\nwhich, by including a list of transactions with each PoW, reduces confirmation\nlatency and improves throughput. Our proposed incentive mechanism discounts\nrewards based on the depth of this tree. Thereby, it effectively punishes\ninformation withholding, the core attack strategy used to reap an unfair share\nof rewards.\n  We back our claims with a comprehensive analysis. We present a generic system\nmodel which allows us to specify Bitcoin, $B_k$, and Tailstorm from a joint set\nof assumptions. We provide an analytical bound for the fairness of Tailstorm\nand Bitcoin in honest networks and we confirm the results through simulation.\nWe evaluate the effectiveness of dishonest behaviour through reinforcement\nlearning. Our attack search reproduces known optimal strategies against\nBitcoin, uncovers new ones against $B_k$, and confirms that Tailstorm's reward\ndiscounting makes it more resilient to incentive layer attacks.",
        "translated": "工作证(Proof-of-work，PoW)加密货币依赖于安全性和公平性的平衡，以维持矿工和用户的可持续生态系统。用户需要快速和一致的交易确认，作为交换，推动加密货币的采用和估值。矿工提供证实，然而，他们主要寻求奖励。在不公平的体制下，矿商可以通过整合采矿权来放大自己的回报。然而，集中化会破坏系统的安全保障，并可能使用户泄气。在本文中，我们介绍了尾风暴，一种加密货币，以达到这种平衡。Tailstorm 融合了多个最新的协议改进，包括安全性、确认延迟和吞吐量，并采用新的激励机制提高公平性。我们实现了一个并行的工作证明共识机制，每个块具有 $k $PoWs，以获得最先进的一致性保证。灵感来自短尾和风暴，我们结构在一个树中的每个战俘，通过包括每个战俘的事务列表，减少了确认延迟和提高吞吐量。我们提出的激励机制基于此树的深度折扣奖励。因此，它有效地惩罚了隐瞒信息的核心攻击策略，用来获取不公平的奖励份额。我们以全面的分析来支持我们的主张。我们提出了一个通用的系统模型，允许我们指定比特币，$B _ k $，和尾风暴从一个联合的假设集。我们为 Tailstorm 和比特币在诚实网络中的公平性提供了一个分析界限，并通过模拟验证了结果。我们会透过强化学习评估不诚实行为的成效。我们的攻击搜索复制了已知的针对比特币的最优策略，发现了针对 $B _ k $的新策略，并证实了 Tailstorm 的奖励折扣使其更能抵御激励层攻击。"
    },
    {
        "title": "Probabilistic estimation of the algebraic degree of Boolean functions",
        "url": "http://arxiv.org/abs/2306.12196v1",
        "pub_date": "2023-06-21",
        "summary": "The algebraic degree is an important parameter of Boolean functions used in\ncryptography. When a function in a large number of variables is not given\nexplicitly in algebraic normal form, it might not be feasible to compute its\ndegree. Instead, one can try to estimate the degree using probabilistic tests.\n  We propose a probabilistic test for deciding whether the algebraic degree of\na Boolean function $f$ is below a certain value $k$. The test involves picking\nan affine space of dimension $k$ and testing whether the values on $f$ on that\nspace sum up to zero. If $deg(f)&lt;k$, then $f$ will always pass the test,\notherwise it will sometimes pass and sometimes fail the test, depending on\nwhich affine space was chosen. The probability of failing the proposed test is\nclosely related to the number of monomials of degree $k$ in a polynomial $g$,\naveraged over all the polynomials $g$ which are affine equivalent to $f$.\n  We initiate the study of the probability of failing the proposed\n``$deg(f)&lt;k$'' test. We show that in the particular case when the degree of $f$\nis actually equal to $k$, the probability will be in the interval $(0.288788,\n0.5]$, and therefore a small number of runs of the test is sufficient to give,\nwith very high probability, the correct answer. Exact values of this\nprobability for all the polynomials in 8 variables were computed using the\nrepresentatives listed by Hou and by Langevin and Leander.",
        "translated": "代数度是密码学中布尔函数的一个重要参数。当大量变量中的一个函数没有以代数规范形式显式给出时，计算它的次数可能是不可行的。相反，人们可以尝试使用概率测试来估计程度。我们提出了一个概率测试来判断布尔函数 f $的代数度是否低于某个值 $k $。测试包括挑选一个维度为 $k $的仿射空间，并测试该空间上 $f $上的值是否总和为零。如果 $deg (f) < k $，则 $f $将始终通过测试，否则它有时会通过测试，有时会失败，这取决于所选择的仿射空间。多项式 $g $中 k $度单项式的数目与拟合等价于 f $的所有多项式 $g $的平均值密切相关，因此，不通过拟合检验的概率与拟合多项式 $g $中 k $度单项式的数目密切相关，拟合多项式 $g $中 k $度单项式的平均值与拟合等价于 f $的多项式 $g $的平均值密切相关。我们开始研究不通过提议的“ $deg (f) < k $”测试的可能性。我们证明了在特殊情况下，当 $f $的度实际上等于 $k $时，概率将在区间 $(0.288788,0.5] $，因此少量的运行测试足以给出正确的答案，并且概率非常高。在8个变量中的所有多项式的这个概率的精确值是使用由侯和朗之万和利安德列出的代表计算出来的。"
    },
    {
        "title": "Decisions &amp; Disruptions 2: Decide Harder",
        "url": "http://arxiv.org/abs/2306.12168v1",
        "pub_date": "2023-06-21",
        "summary": "Cyber incident response is critical to business continuity -- we describe a\nnew exercise that challenges professionals to play the role of Chief\nInformation Security Officer (CISO) for a major financial organisation. Teams\nmust decide how organisational team and budget resources should be deployed\nacross Enterprise Architecture (EA) upgrades and cyber incidents. Every choice\nmade has an impact -- some prevent whilst others may trigger new or continue\ncurrent attacks. We explain how the underlying platform supports these\ninteractions through a reactionary event mechanism that introduces events based\non the current attack surface of the organisation. We explore how our platform\nmanages to introduce randomness on top of triggered events to ensure that the\nexercise is not deterministic and better matches incidents in the real world.\nWe conclude by describing next steps for the exercise and how we plan to use it\nin the future to better understand risk decision making.",
        "translated": "网上事故应变对业务的持续运作至为重要——我们描述了一项新的活动，挑战专业人士在一个主要金融机构担任首席资讯保安主任(CISO)的角色。团队必须决定如何在企业架构(Enterprise Architecture，EA)升级和网络事件中部署组织团队和预算资源。所做的每一个选择都会产生影响——有些可以预防，有些可以触发新的或者继续当前的攻击。我们解释了底层平台如何通过一个反应事件机制支持这些交互，该机制引入基于组织当前攻击面的事件。我们探索我们的平台如何设法在触发事件之上引入随机性，以确保练习不是确定性的，并更好地匹配现实世界中的事件。最后，我们描述了下一步的工作，以及我们计划如何在未来使用它，以更好地了解风险决策。"
    },
    {
        "title": "Adversarial Attacks Neutralization via Data Set Randomization",
        "url": "http://arxiv.org/abs/2306.12161v1",
        "pub_date": "2023-06-21",
        "summary": "Adversarial attacks on deep-learning models pose a serious threat to their\nreliability and security. Existing defense mechanisms are narrow addressing a\nspecific type of attack or being vulnerable to sophisticated attacks. We\npropose a new defense mechanism that, while being focused on image-based\nclassifiers, is general with respect to the cited category. It is rooted on\nhyperspace projection. In particular, our solution provides a pseudo-random\nprojection of the original dataset into a new dataset. The proposed defense\nmechanism creates a set of diverse projected datasets, where each projected\ndataset is used to train a specific classifier, resulting in different trained\nclassifiers with different decision boundaries. During testing, it randomly\nselects a classifier to test the input. Our approach does not sacrifice\naccuracy over legitimate input. Other than detailing and providing a thorough\ncharacterization of our defense mechanism, we also provide a proof of concept\nof using four optimization-based adversarial attacks (PGD, FGSM, IGSM, and\nC\\&amp;W) and a generative adversarial attack testing them on the MNIST dataset.\nOur experimental results show that our solution increases the robustness of\ndeep learning models against adversarial attacks and significantly reduces the\nattack success rate by at least 89% for optimization attacks and 78% for\ngenerative attacks. We also analyze the relationship between the number of used\nhyperspaces and the efficacy of the defense mechanism. As expected, the two are\npositively correlated, offering an easy-to-tune parameter to enforce the\ndesired level of security. The generality and scalability of our solution and\nadaptability to different attack scenarios, combined with the excellent\nachieved results, other than providing a robust defense against adversarial\nattacks on deep learning networks, also lay the groundwork for future research\nin the field.",
        "translated": "对深度学习模型的对抗性攻击对其可靠性和安全性构成严重威胁。现有的防御机制针对的是特定类型的攻击或易受复杂攻击的攻击。我们提出了一种新的防御机制，虽然重点是基于图像的分类器，是一般方面的被引范畴。它植根于超空间投影。特别是，我们的解决方案提供了原始数据集到新数据集的伪随机投影。提出的防御机制创建一组不同的投影数据集，其中每个投影数据集用于训练一个特定的分类器，产生具有不同决策边界的不同训练分类器。在测试期间，它随机选择一个分类器来测试输入。我们的方法不会牺牲精确性而牺牲合法的输入。除了详细介绍和提供我们的防御机制的全面角色塑造外，我们还提供了使用四种基于优化的对抗性攻击(PGD、 FGSM、 IGSM 和 C & W)的概念证明，以及在 MNIST 数据集上测试它们的生成性对抗性攻击。实验结果表明，该方案提高了深度学习模型对攻击的鲁棒性，使优化攻击和生成攻击的攻击成功率分别降低了89% 和78% 。我们还分析了使用多维空间的数量与防御机制的效能之间的关系。正如预期的那样，这两者是正相关的，提供了一个易于调整的参数来实现所需的安全级别。我们的解决方案的通用性和可扩展性以及对不同攻击场景的适应性，结合优秀的成果，除了提供对深度学习网络上的敌对攻击的强大防御外，还为未来该领域的研究奠定了基础。"
    },
    {
        "title": "PrivSketch: A Private Sketch-based Frequency Estimation Protocol for\n  Data Streams",
        "url": "http://arxiv.org/abs/2306.12144v1",
        "pub_date": "2023-06-21",
        "summary": "Local differential privacy (LDP) has recently become a popular\nprivacy-preserving data collection technique protecting users' privacy. The\nmain problem of data stream collection under LDP is the poor utility due to\nmulti-item collection from a very large domain. This paper proposes PrivSketch,\na high-utility frequency estimation protocol taking advantage of sketches,\nsuitable for private data stream collection. Combining the proposed background\ninformation and a decode-first collection-side workflow, PrivSketch improves\nthe utility by reducing the errors introduced by the sketching algorithm and\nthe privacy budget utilization when collecting multiple items. We analytically\nprove the superior accuracy and privacy characteristics of PrivSketch, and also\nevaluate them experimentally. Our evaluation, with several diverse synthetic\nand real datasets, demonstrates that PrivSketch is 1-3 orders of magnitude\nbetter than the competitors in terms of utility in both frequency estimation\nand frequent item estimation, while being up to ~100x faster.",
        "translated": "本地差分隐私(lDP)最近成为一种流行的保护用户隐私的数据收集技术。LDP 下数据流采集的主要问题是由于从一个非常大的域进行多项采集而造成的效用差。本文提出了一种利用草图的高效率频率估计协议 PrivSketch，适用于私有数据流的采集。PrivSketch 将提出的背景信息和解码优先的收集端工作流结合起来，通过减少草图算法引入的错误和收集多个项目时的隐私预算利用率，提高了工作效率。分析证明了 PrivSketch 具有较高的准确性和隐私性，并对其进行了实验评价。我们的评估，通过几个不同的合成和真实的数据集，表明 PrivSketch 在频率估计和频繁项目估计方面的效用比竞争对手高出1-3个数量级，同时比竞争对手快约100倍。"
    },
    {
        "title": "A new color image secret sharing protocol",
        "url": "http://arxiv.org/abs/2306.12107v1",
        "pub_date": "2023-06-21",
        "summary": "Visual cryptography aims to protect images against their possible\nillegitimate use. Thus, one can cipher, hash, or add watermarks for protecting\ncopyright, among others. In this paper we provide a new solution to the problem\nof secret sharing for the case when the secret is an image. Our method combines\nthe Shamir scheme for secret sharing using finite fields of characteristic 2\nwith the CBC mode of operation of a secure symmetric cryptographic scheme like\nAES, so that the security relies on that of the mentioned techniques. The\nresulting shares have the same resolution as that of the original image. The\nidea of the method could be generalized to other multimedia formats like audio\nor video, adapting the method to the corresponding encoded information.",
        "translated": "可视密码旨在保护图像免受非法使用。因此，可以加密、散列或添加水印以保护版权。本文针对秘密是图像的情况，提出了一种新的秘密共享问题的解决方案。该方法将利用特征2有限域的 Shamir 秘密共享方案与 AES 等安全对称密码方案的 CBC 操作模式相结合，使其安全性依赖于上述技术。产生的共享具有与原始图像相同的分辨率。该方法的思想可以推广到其他多媒体格式，如音频或视频，使该方法适应相应的编码信息。"
    },
    {
        "title": "Cryptographic ransomware encryption detection: Survey",
        "url": "http://arxiv.org/abs/2306.12008v1",
        "pub_date": "2023-06-21",
        "summary": "The ransomware threat has loomed over our digital life since 1989. Criminals\nuse this type of cyber attack to lock or encrypt victims' data, often coercing\nthem to pay exorbitant amounts in ransom. The damage ransomware causes ranges\nfrom monetary losses paid for ransom at best to endangering human lives.\nCryptographic ransomware, where attackers encrypt the victim's data, stands as\nthe predominant ransomware variant. The primary characteristics of these\nattacks have remained the same since the first ransomware attack. For this\nreason, we consider this a key factor differentiating ransomware from other\ncyber attacks, making it vital in tackling the threat of cryptographic\nransomware. This paper proposes a cyber kill chain that describes the modern\ncrypto-ransomware attack. The survey focuses on the Encryption phase as\ndescribed in our proposed cyber kill chain and its detection techniques. We\nidentify three main methods used in detecting encryption-related activities by\nransomware, namely API and System calls, I/O monitoring, and file system\nactivities monitoring. Machine learning (ML) is a tool used in all three\nidentified methodologies, and some of the issues within the ML domain related\nto this survey are also covered as part of their respective methodologies. The\nsurvey of selected proposals is conducted through the prism of those three\nmethodologies, showcasing the importance of detecting ransomware during\npre-encryption and encryption activities and the windows of opportunity to do\nso. We also examine commercial crypto-ransomware protection and detection\nofferings and show the gap between academic research and commercial\napplications.",
        "translated": "自1989年以来，勒索软件的威胁一直笼罩着我们的数字生活。犯罪分子利用这种类型的网络攻击锁定或加密受害者的数据，往往强迫他们支付过高的赎金。勒索软件造成的损害范围很广，从最多支付赎金的金钱损失到危及人类生命。加密勒索软件，攻击者加密受害者的数据，是主要的勒索软件变体。这些攻击的主要特征自第一次勒索软件攻击以来一直保持不变。出于这个原因，我们认为这是区分勒索软件和其他网络攻击的一个关键因素，使其在应对加密勒索软件的威胁时至关重要。提出了一种描述现代密码勒索软件攻击的网络杀伤链。该调查的重点是加密阶段所描述的，在我们提出的网络杀伤链及其检测技术。我们确定了用于通过勒索软件检测加密相关活动的三种主要方法，即 API 和系统调用、 I/O 监视和文件系统活动监视。机器学习(ML)是一个工具，用于所有三个确定的方法，和机器学习领域的一些问题相关的这项调查也涵盖了作为其各自方法的一部分。对选定提案的调查是通过这三种方法的棱镜进行的，显示了在加密前和加密活动期间发现勒索软件的重要性以及发现勒索软件的机会之窗。我们还研究了商业加密勒索软件的保护和检测服务，并展示了学术研究和商业应用之间的差距。"
    },
    {
        "title": "Evading Forensic Classifiers with Attribute-Conditioned Adversarial\n  Faces",
        "url": "http://arxiv.org/abs/2306.13091v1",
        "pub_date": "2023-06-22",
        "summary": "The ability of generative models to produce highly realistic synthetic face\nimages has raised security and ethical concerns. As a first line of defense\nagainst such fake faces, deep learning based forensic classifiers have been\ndeveloped. While these forensic models can detect whether a face image is\nsynthetic or real with high accuracy, they are also vulnerable to adversarial\nattacks. Although such attacks can be highly successful in evading detection by\nforensic classifiers, they introduce visible noise patterns that are detectable\nthrough careful human scrutiny. Additionally, these attacks assume access to\nthe target model(s) which may not always be true. Attempts have been made to\ndirectly perturb the latent space of GANs to produce adversarial fake faces\nthat can circumvent forensic classifiers. In this work, we go one step further\nand show that it is possible to successfully generate adversarial fake faces\nwith a specified set of attributes (e.g., hair color, eye size, race, gender,\netc.). To achieve this goal, we leverage the state-of-the-art generative model\nStyleGAN with disentangled representations, which enables a range of\nmodifications without leaving the manifold of natural images. We propose a\nframework to search for adversarial latent codes within the feature space of\nStyleGAN, where the search can be guided either by a text prompt or a reference\nimage. We also propose a meta-learning based optimization strategy to achieve\ntransferable performance on unknown target models. Extensive experiments\ndemonstrate that the proposed approach can produce semantically manipulated\nadversarial fake faces, which are true to the specified attribute set and can\nsuccessfully fool forensic face classifiers, while remaining undetectable by\nhumans. Code: https://github.com/koushiksrivats/face_attribute_attack.",
        "translated": "生成模型产生高度逼真的合成人脸图像的能力引起了安全和伦理方面的关注。作为抵御这种假面的第一道防线，基于深度学习的法医分类器已经被开发出来。虽然这些法医模型能够高精度地检测出人脸图像是合成的还是真实的，但它们也容易受到敌对攻击。虽然这种攻击可以非常成功地躲避法医分类器的检测，但是它们引入了可见的噪音模式，通过仔细的人类检查可以检测到。此外，这些攻击假设对目标模型的访问，这可能并不总是正确的。有人试图直接扰乱 GAN 的潜在空间，制造出可以绕过法医分类器的敌对假面孔。在这项工作中，我们更进一步，表明有可能成功地生成具有特定属性集(例如，头发颜色，眼睛大小，种族，性别等)的敌对假面孔。为了实现这个目标，我们利用了最先进的生成模型 StyleGAN，它可以进行一系列的修改，而不用离开自然图像的流形。提出了一种在 StyleGAN 特征空间内搜索对手潜码的框架，该框架可以通过文本提示或参考图像来指导搜索。我们还提出了一种基于元学习的优化策略，以实现对未知目标模型的可转移性能。大量的实验表明，该方法可以产生语义操纵的敌对假面孔，这是真实的特定属性集，可以成功地欺骗法医面孔分类器，而仍然无法被人类检测到。密码:  https://github.com/koushiksrivats/face_attribute_attack。"
    },
    {
        "title": "Unitary Complexity and the Uhlmann Transformation Problem",
        "url": "http://arxiv.org/abs/2306.13073v1",
        "pub_date": "2023-06-22",
        "summary": "State transformation problems such as compressing quantum information or\nbreaking quantum commitments are fundamental quantum tasks. However, their\ncomputational difficulty cannot easily be characterized using traditional\ncomplexity theory, which focuses on tasks with classical inputs and outputs.\n  To study the complexity of such state transformation tasks, we introduce a\nframework for unitary synthesis problems, including notions of reductions and\nunitary complexity classes. We use this framework to study the complexity of\ntransforming one entangled state into another via local operations. We\nformalize this as the Uhlmann Transformation Problem, an algorithmic version of\nUhlmann's theorem. Then, we prove structural results relating the complexity of\nthe Uhlmann Transformation Problem, polynomial space quantum computation, and\nzero knowledge protocols.\n  The Uhlmann Transformation Problem allows us to characterize the complexity\nof a variety of tasks in quantum information processing, including decoding\nnoisy quantum channels, breaking falsifiable quantum cryptographic assumptions,\nimplementing optimal prover strategies in quantum interactive proofs, and\ndecoding the Hawking radiation of black holes. Our framework for unitary\ncomplexity thus provides new avenues for studying the computational complexity\nof many natural quantum information processing tasks.",
        "translated": "状态转换问题，如压缩量子信息或打破量子承诺是基本的量子任务。然而，用传统的复杂性理论来描述它们的计算难度并不容易，因为传统的复杂性理论关注的是具有经典输入和输出的任务。为了研究这类状态转换任务的复杂性，我们引入了酉综合问题的一个框架，包括约化和酉复杂类的概念。我们使用这个框架来研究通过局域运算将一个纠缠态转化为另一个纠缠态的复杂性。我们将其形式化为乌尔曼变换问题，一个乌尔曼定理的算法版本。然后，我们证明了结构化结果与乌尔曼变换问题的复杂性、 PSPACE 量子计算和零知识协议有关。乌尔曼变换问题允许我们描述量子信息处理中各种任务的复杂性，包括解码嘈杂的量子通道，打破可证伪的量子密码假设，在量子交互证明中实现最佳证明策略，以及解码黑洞的霍金辐射。因此，我们的幺正复杂度框架为研究许多自然量子信息处理任务的计算复杂度提供了新的途径。"
    },
    {
        "title": "Quantum Pufferfish Privacy: A Flexible Privacy Framework for Quantum\n  Systems",
        "url": "http://arxiv.org/abs/2306.13054v1",
        "pub_date": "2023-06-22",
        "summary": "We propose a versatile privacy framework for quantum systems, termed quantum\npufferfish privacy (QPP). Inspired by classical pufferfish privacy, our\nformulation generalizes and addresses limitations of quantum differential\nprivacy by offering flexibility in specifying private information, feasible\nmeasurements, and domain knowledge. We show that QPP can be equivalently\nformulated in terms of the Datta-Leditzky information spectrum divergence, thus\nproviding the first operational interpretation thereof. We reformulate this\ndivergence as a semi-definite program and derive several properties of it,\nwhich are then used to prove convexity, composability, and post-processing of\nQPP mechanisms. Parameters that guarantee QPP of the depolarization mechanism\nare also derived. We analyze the privacy-utility tradeoff of general QPP\nmechanisms and, again, study the depolarization mechanism as an explicit\ninstance. The QPP framework is then applied to privacy auditing for identifying\nprivacy violations via a hypothesis testing pipeline that leverages quantum\nalgorithms. Connections to quantum fairness and other quantum divergences are\nalso explored and several variants of QPP are examined.",
        "translated": "我们提出了一个通用的量子系统隐私框架，称为量子河豚隐私(QPP)。受经典河豚隐私的启发，我们的公式通过在指定私人信息、可行测量和领域知识方面提供灵活性，概括并解决了量子差分隐私的局限性。我们证明了 QPP 可以等价地用 Datta-Leditzky 信息谱发散来表示，从而提供了它的第一个运算解释。我们把这种发散重新表述为一个半定规划，并导出了它的几个性质，然后用这些性质来证明 QPP 机构的凸性、可组合性和后处理。推导出了保证去极化机制 QPP 的参数。我们分析了一般 QPP 机制的隐私-效用权衡，并再次作为一个明确的实例研究了去极化机制。然后将 QPP 框架应用于隐私审计，通过利用量子算法的假设测试流水线来识别隐私侵犯。还探讨了与量子公平性和其他量子发散的联系，并研究了 QPP 的几种变体。"
    },
    {
        "title": "Impacts and Risk of Generative AI Technology on Cyber Defense",
        "url": "http://arxiv.org/abs/2306.13033v1",
        "pub_date": "2023-06-22",
        "summary": "Generative Artificial Intelligence (GenAI) has emerged as a powerful\ntechnology capable of autonomously producing highly realistic content in\nvarious domains, such as text, images, audio, and videos. With its potential\nfor positive applications in creative arts, content generation, virtual\nassistants, and data synthesis, GenAI has garnered significant attention and\nadoption. However, the increasing adoption of GenAI raises concerns about its\npotential misuse for crafting convincing phishing emails, generating\ndisinformation through deepfake videos, and spreading misinformation via\nauthentic-looking social media posts, posing a new set of challenges and risks\nin the realm of cybersecurity. To combat the threats posed by GenAI, we propose\nleveraging the Cyber Kill Chain (CKC) to understand the lifecycle of\ncyberattacks, as a foundational model for cyber defense. This paper aims to\nprovide a comprehensive analysis of the risk areas introduced by the offensive\nuse of GenAI techniques in each phase of the CKC framework. We also analyze the\nstrategies employed by threat actors and examine their utilization throughout\ndifferent phases of the CKC, highlighting the implications for cyber defense.\nAdditionally, we propose GenAI-enabled defense strategies that are both\nattack-aware and adaptive. These strategies encompass various techniques such\nas detection, deception, and adversarial training, among others, aiming to\neffectively mitigate the risks posed by GenAI-induced cyber threats.",
        "translated": "生成性人工智能(GenAI)已经成为一种强大的技术，能够自动生成各种领域的高度真实的内容，如文本、图像、音频和视频。凭借其在创意艺术、内容生成、虚拟助手和数据合成等方面的潜力，GenAI 获得了广泛的关注和采用。然而，GenAI 的日益普及引发了人们的担忧，即它可能被滥用，用于制作令人信服的钓鱼电子邮件、通过深度伪造视频制造虚假信息，以及通过看似真实的社交媒体帖子传播虚假信息，从而在网络安全领域构成一系列新的挑战和风险。为了应对 GenAI 带来的威胁，我们提出利用网络杀伤链(CKC)来理解网络攻击的生命周期，作为网络防御的基础模型。本文旨在全面分析在 CKC 框架的每个阶段进攻性地使用 GenAI 技术所引入的风险领域。我们还分析了威胁行为者所使用的策略，并检查了它们在 CKC 不同阶段的使用情况，强调了对网络防御的影响。此外，我们还提出了基于 GenAI 的攻击感知和自适应防御策略。这些策略包括各种技术，如侦测、欺骗和对抗性训练等，旨在有效地减轻由 GenAI 引起的网络威胁造成的风险。"
    },
    {
        "title": "Online Self-Supervised Learning in Machine Learning Intrusion Detection\n  for the Internet of Things",
        "url": "http://arxiv.org/abs/2306.13030v1",
        "pub_date": "2023-06-22",
        "summary": "This paper proposes a novel Self-Supervised Intrusion Detection (SSID)\nframework, which enables a fully online Machine Learning (ML) based Intrusion\nDetection System (IDS) that requires no human intervention or prior off-line\nlearning. The proposed framework analyzes and labels incoming traffic packets\nbased only on the decisions of the IDS itself using an Auto-Associative Deep\nRandom Neural Network, and on an online estimate of its statistically measured\ntrustworthiness. The SSID framework enables IDS to adapt rapidly to\ntime-varying characteristics of the network traffic, and eliminates the need\nfor offline data collection. This approach avoids human errors in data\nlabeling, and human labor and computational costs of model training and data\ncollection. The approach is experimentally evaluated on public datasets and\ncompared with well-known ML models, showing that this SSID framework is very\nuseful and advantageous as an accurate and online learning ML-based IDS for IoT\nsystems.",
        "translated": "本文提出了一个新的自我监督入侵检测(SSID)框架，它可以实现一个完全基于在线机器学习(ML)的入侵预防系统(IDS) ，不需要人工干预或事先离线学习。该框架仅根据入侵检测系统自身的决策，使用自联想深度随机神经网络对传入的流量数据包进行分析和标记，并对其统计测量的可信度进行在线估计。SSID 框架使 IDS 能够快速适应网络流量的时变特性，并消除了离线数据收集的需要。这种方法避免了数据标记中的人为错误，以及模型训练和数据收集的人力和计算成本。该方法在公共数据集上进行了实验评估，并与已知的机器学习模型进行了比较，结果表明该 SSID 框架对于物联网系统中基于机器学习的入侵检测系统是非常有用和有利的。"
    },
    {
        "title": "Decentralized Online Federated G-Network Learning for Lightweight\n  Intrusion Detection",
        "url": "http://arxiv.org/abs/2306.13029v1",
        "pub_date": "2023-06-22",
        "summary": "Cyberattacks are increasingly threatening networked systems, often with the\nemergence of new types of unknown (zero-day) attacks and the rise of vulnerable\ndevices. While Machine Learning (ML)-based Intrusion Detection Systems (IDSs)\nhave been shown to be extremely promising in detecting these attacks, the need\nto learn large amounts of labelled data often limits the applicability of\nML-based IDSs to cybersystems that only have access to private local data. To\naddress this issue, this paper proposes a novel Decentralized and Online\nFederated Learning Intrusion Detection (DOF-ID) architecture. DOF-ID is a\ncollaborative learning system that allows each IDS used for a cybersystem to\nlearn from experience gained in other cybersystems in addition to its own local\ndata without violating the data privacy of other systems. As the performance\nevaluation results using public Kitsune and Bot-IoT datasets show, DOF-ID\nsignificantly improves the intrusion detection performance in all collaborating\nnodes simultaneously with acceptable computation time for online learning.",
        "translated": "网络攻击对网络系统的威胁越来越大，通常伴随着新型未知(零日)攻击的出现和易受攻击设备的增加。基于机器学习(ML)的入侵检测系统(IDS)在检测这些攻击方面已经被证明是非常有前途的，但是需要学习大量的标记数据往往限制了基于 ML 的入侵检测系统对只能访问私有本地数据的网络系统的适用性。为了解决这一问题，本文提出了一种新的分布式在线联邦学习入侵检测(DOF-ID)体系结构。DOF-id 是一个合作学习系统，让每个用于网络系统的 IDS 除了学习自己的本地数据之外，还可以学习其他网络系统的经验，而不会侵犯其他系统的数据隐私。使用公共 Kitsune 和 bot-IoT 数据集进行的性能评估结果显示，DOF-ID 显著提高了所有协作节点的入侵检测性能，同时也为在线学习提供了可接受的计算时间。"
    },
    {
        "title": "Towards More Realistic Membership Inference Attacks on Large Diffusion\n  Models",
        "url": "http://arxiv.org/abs/2306.12983v1",
        "pub_date": "2023-06-22",
        "summary": "Generative diffusion models, including Stable Diffusion and Midjourney, can\ngenerate visually appealing, diverse, and high-resolution images for various\napplications. These models are trained on billions of internet-sourced images,\nraising significant concerns about the potential unauthorized use of\ncopyright-protected images. In this paper, we examine whether it is possible to\ndetermine if a specific image was used in the training set, a problem known in\nthe cybersecurity community and referred to as a membership inference attack.\nOur focus is on Stable Diffusion, and we address the challenge of designing a\nfair evaluation framework to answer this membership question. We propose a\nmethodology to establish a fair evaluation setup and apply it to Stable\nDiffusion, enabling potential extensions to other generative models. Utilizing\nthis evaluation setup, we execute membership attacks (both known and newly\nintroduced). Our research reveals that previously proposed evaluation setups do\nnot provide a full understanding of the effectiveness of membership inference\nattacks. We conclude that the membership inference attack remains a significant\nchallenge for large diffusion models (often deployed as black-box systems),\nindicating that related privacy and copyright issues will persist in the\nforeseeable future.",
        "translated": "生成扩散模型，包括稳定扩散和 Midjourney，可以产生视觉吸引力，多样化和高分辨率图像的各种应用。这些模型是在数十亿互联网来源的图像上训练出来的，引起了人们对未经授权使用受版权保护的图像的严重关切。在本文中，我们检查是否有可能确定一个特定的图像是否用于训练集，这个问题在网络安全界已知，称为成员推断攻击。我们的重点是稳定扩散，我们解决的挑战，设计一个公平的评估框架，以回答这个成员资格的问题。我们提出了一种方法来建立一个公平的评估设置，并应用到稳定扩散，使潜在的扩展到其他生成模型。利用这个评估设置，我们执行成员资格攻击(已知的和新引入的)。我们的研究表明，以前提出的评估设置并没有提供一个充分的理解成员推理攻击的有效性。我们的结论是，成员推理攻击仍然是一个重大的挑战，大型扩散模型(通常部署为黑盒系统) ，表明相关的隐私和版权问题将持续在可预见的未来。"
    },
    {
        "title": "On the Direct Construction of MDS and Near-MDS Matrices",
        "url": "http://arxiv.org/abs/2306.12848v1",
        "pub_date": "2023-06-22",
        "summary": "The optimal branch number of MDS matrices makes them a preferred choice for\ndesigning diffusion layers in many block ciphers and hash functions.\nConsequently, various methods have been proposed for designing MDS matrices,\nincluding search and direct methods. While exhaustive search is suitable for\nsmall order MDS matrices, direct constructions are preferred for larger orders\ndue to the vast search space involved. In the literature, there has been\nextensive research on the direct construction of MDS matrices using both\nrecursive and nonrecursive methods. On the other hand, in lightweight\ncryptography, Near-MDS (NMDS) matrices with sub-optimal branch numbers offer a\nbetter balance between security and efficiency as a diffusion layer compared to\nMDS matrices. However, no direct construction method is available in the\nliterature for constructing recursive NMDS matrices. This paper introduces some\ndirect constructions of NMDS matrices in both nonrecursive and recursive\nsettings. Additionally, it presents some direct constructions of nonrecursive\nMDS matrices from the generalized Vandermonde matrices. We propose a method for\nconstructing involutory MDS and NMDS matrices using generalized Vandermonde\nmatrices. Furthermore, we prove some folklore results that are used in the\nliterature related to the NMDS code.",
        "translated": "MDS 矩阵的最佳分支数使其成为许多分组密码和哈希函数扩散层设计的首选。因此，人们提出了各种设计 MDS 矩阵的方法，包括搜索法和直接法。尽管穷举搜索适用于小阶 MDS 矩阵，但由于所涉及的搜索空间很大，直接构造更适用于大阶 MDS 矩阵。在文献中，已经广泛地研究了用递归和非递归方法直接构造 MDS 矩阵。另一方面，在轻量级密码体制中，具有次优分支数目的近 MDS (NMDS)矩阵作为扩散层，比 MDS 矩阵能更好地平衡安全性和效率。然而，目前文献中还没有直接构造递归 NMDS 矩阵的方法。本文介绍了 NMDS 矩阵在非递归和递归环境下的一些直接构造。此外，还从广义 Vandermonde 矩阵出发，给出了一些非递归 MDS 矩阵的直接构造。提出了一种利用广义范德蒙矩阵构造对合 MDS 和 NMDS 矩阵的方法。此外，我们还证明了一些民间传说的结果，这些结果被用于与 NMDS 代码相关的文献中。"
    },
    {
        "title": "XACML Extension for Graphs: Flexible Authorization Policy Specification\n  and Datastore-independent Enforcement",
        "url": "http://arxiv.org/abs/2306.12819v1",
        "pub_date": "2023-06-22",
        "summary": "The increasing use of graph-structured data for business- and\nprivacy-critical applications requires sophisticated, flexible and fine-grained\nauthorization and access control. Currently, role-based access control is\nsupported in graph databases, where access to objects is restricted via roles.\nThis does not take special properties of graphs into account such as vertices\nand edges along the path between a given subject and resource. In previous\niterations of our research, we started to design an authorization policy\nlanguage and access control model, which considers the specification of graph\npaths and enforces them in the multi-model database ArangoDB. Since this\napproach is promising to consider graph characteristics in data protection, we\nimprove the language in this work to provide flexible path definitions and\nspecifying edges as protected resources. Furthermore, we introduce a method for\na datastore-independent policy enforcement. Besides discussing the latest work\nin our XACML4G model, which is an extension to the Extensible Access Control\nMarkup Language (XACML), we demonstrate our prototypical implementation with a\nreal case and give an outlook on performance.",
        "translated": "图形结构数据越来越多地用于业务和隐私关键型应用程序，需要复杂、灵活和细粒度的授权和访问控制。目前，图形数据库支持以角色为基础的存取控制，通过角色访问对象受到限制。这没有考虑到图的特殊属性，例如在给定主题和资源之间的路径上的顶点和边。在前面的研究中，我们开始设计一种授权策略语言和访问控制模型，该模型考虑了图路径的规范，并在多模型数据库 ArangoDB 中实施。由于该方法有望在数据保护中考虑图的特性，因此我们改进了该语言，提供了灵活的路径定义，并指定了边作为保护资源。此外，我们还介绍了一种独立于数据存储的策略实施方法。除了讨论可扩展访问控制标记语言(XACML)的扩展 XACML4G 模型中的最新工作之外，我们还用一个实际案例演示了我们的原型实现，并给出了性能展望。"
    },
    {
        "title": "On the Construction of Near-MDS Matrices",
        "url": "http://arxiv.org/abs/2306.12791v1",
        "pub_date": "2023-06-22",
        "summary": "The optimal branch number of MDS matrices makes them a preferred choice for\ndesigning diffusion layers in many block ciphers and hash functions. However,\nin lightweight cryptography, Near-MDS (NMDS) matrices with sub-optimal branch\nnumbers offer a better balance between security and efficiency as a diffusion\nlayer, compared to MDS matrices. In this paper, we study NMDS matrices,\nexploring their construction in both recursive and nonrecursive settings. We\nprovide several theoretical results and explore the hardware efficiency of the\nconstruction of NMDS matrices. Additionally, we make comparisons between the\nresults of NMDS and MDS matrices whenever possible. For the recursive approach,\nwe study the DLS matrices and provide some theoretical results on their use.\nSome of the results are used to restrict the search space of the DLS matrices.\nWe also show that over a field of characteristic 2, any sparse matrix of order\n$n\\geq 4$ with fixed XOR value of 1 cannot be an NMDS when raised to a power of\n$k\\leq n$. Following that, we use the generalized DLS (GDLS) matrices to\nprovide some lightweight recursive NMDS matrices of several orders that perform\nbetter than the existing matrices in terms of hardware cost or the number of\niterations. For the nonrecursive construction of NMDS matrices, we study\nvarious structures, such as circulant and left-circulant matrices, and their\ngeneralizations: Toeplitz and Hankel matrices. In addition, we prove that\nToeplitz matrices of order $n&gt;4$ cannot be simultaneously NMDS and involutory\nover a field of characteristic 2. Finally, we use GDLS matrices to provide some\nlightweight NMDS matrices that can be computed in one clock cycle. The proposed\nnonrecursive NMDS matrices of orders 4, 5, 6, 7, and 8 can be implemented with\n24, 50, 65, 96, and 108 XORs over $\\mathbb{F}_{2^4}$, respectively.",
        "translated": "MDS 矩阵的最佳分支数使其成为许多分组密码和哈希函数扩散层设计的首选。然而，在轻量级密码体制中，具有次优分支数的近 MDS (NMDS)矩阵作为扩散层，在安全性和效率之间提供了一个比 MDS 矩阵更好的平衡。本文研究了 NMDS 矩阵，探讨了它们在递归和非递归环境下的构造方法。我们提供了一些理论结果，并探讨了构造 NMDS 矩阵的硬件效率。此外，我们还尽可能对 NMDS 和 MDS 矩阵的结果进行了比较。对于递归方法，我们研究了 DLS 矩阵，并给出了它们的一些理论应用结果。其中一些结果被用来限制 DLS 矩阵的搜索空间。我们还证明了在特征值为2的域上，任何具有固定 XOR 值为1的 n 阶稀疏矩阵 $n-geq-4-当提高到 $k-leq-n-$的幂时都不能成为 NMDS。接着，我们利用广义 DLS (GDLS)矩阵来提供一些轻量级的递归 NMDS 矩阵，这些递归 NMDS 矩阵在硬件成本和迭代次数方面都优于现有矩阵。对于 NMDS 矩阵的非递归构造，我们研究了循环矩阵和左循环矩阵等各种结构及其推广: Toeplitz 矩阵和 Hankel 矩阵。另外，我们证明了在特征2域上，n > 4阶 Toeplitz 矩阵不能同时是 NMDS 和对合矩阵。最后，我们使用 GDLS 矩阵来提供一些可以在一个时钟周期内计算的轻量级 NMDS 矩阵。所提出的4,5,6,7和8阶非递归 NMDS 矩阵可以分别用 $mathbb { F } _ {2 ^ 4} $上的24,50,65,96和108个 XOR 实现。"
    },
    {
        "title": "Creating Valid Adversarial Examples of Malware",
        "url": "http://arxiv.org/abs/2306.13587v1",
        "pub_date": "2023-06-23",
        "summary": "Machine learning is becoming increasingly popular as a go-to approach for\nmany tasks due to its world-class results. As a result, antivirus developers\nare incorporating machine learning models into their products. While these\nmodels improve malware detection capabilities, they also carry the disadvantage\nof being susceptible to adversarial attacks. Although this vulnerability has\nbeen demonstrated for many models in white-box settings, a black-box attack is\nmore applicable in practice for the domain of malware detection. We present a\ngenerator of adversarial malware examples using reinforcement learning\nalgorithms. The reinforcement learning agents utilize a set of\nfunctionality-preserving modifications, thus creating valid adversarial\nexamples. Using the proximal policy optimization (PPO) algorithm, we achieved\nan evasion rate of 53.84% against the gradient-boosted decision tree (GBDT)\nmodel. The PPO agent previously trained against the GBDT classifier scored an\nevasion rate of 11.41% against the neural network-based classifier MalConv and\nan average evasion rate of 2.31% against top antivirus programs. Furthermore,\nwe discovered that random application of our functionality-preserving portable\nexecutable modifications successfully evades leading antivirus engines, with an\naverage evasion rate of 11.65%. These findings indicate that machine\nlearning-based models used in malware detection systems are vulnerable to\nadversarial attacks and that better safeguards need to be taken to protect\nthese systems.",
        "translated": "由于机器学习的世界级成果，它作为一种处理许多任务的首选方法正变得越来越流行。因此，防病毒开发人员正在将机器学习模型整合到他们的产品中。虽然这些模型提高了恶意软件的检测能力，但它们也带来了容易受到敌对攻击的缺点。尽管这种漏洞已经在许多白盒设置模型中得到了证明，但黑盒攻击在恶意软件检测领域的实践中更为适用。我们提出了一个使用强化学习算法生成敌对恶意软件的例子。强化学习代理人利用一系列保留功能的修改，从而创造出有效的对抗性例子。利用近似策略优化(PPO)算法，对梯度增强决策树(GBDT)模型的规避率达到了53.84% 。先前针对 GBDT 分类器训练的 PPO 代理针对基于神经网络的分类器 MalConv 的逃避率为11.41% ，针对顶级杀毒程序的平均逃避率为2.31% 。此外，我们发现，随机应用我们的功能保持 Portable Executable 修改成功地规避领先的防病毒引擎，平均规避率为11.65% 。这些发现表明，在恶意软件检测系统中使用的基于机器学习的模型很容易受到敌对攻击，需要采取更好的保障措施来保护这些系统。"
    },
    {
        "title": "The Landscape of Computing Symmetric $n$-Variable Functions with $2n$\n  Cards",
        "url": "http://arxiv.org/abs/2306.13551v1",
        "pub_date": "2023-06-23",
        "summary": "Secure multi-party computation using a physical deck of cards, often called\ncard-based cryptography, has been extensively studied during the past decade.\nMany card-based protocols to securely compute various Boolean functions have\nbeen developed. As each input bit is typically encoded by two cards, computing\nan $n$-variable Boolean function requires at least $2n$ cards. We are\ninterested in optimal protocols that use exactly $2n$ cards. In particular, we\nfocus on symmetric functions, where the output only depends on the number of 1s\nin the inputs. In this paper, we formulate the problem of developing $2n$-card\nprotocols to compute $n$-variable symmetric Boolean functions by classifying\nall such functions into several NPN-equivalence classes. We then summarize\nexisting protocols that can compute some representative functions from these\nclasses, and also solve some of the open problems by developing protocols to\ncompute particular functions in the cases $n=4$, $5$, $6$, and $7$.",
        "translated": "过去10年，人们广泛研究了使用一副物理卡片(通常称为基于卡片的密码学)的安全多方计算。许多基于卡片的安全计算各种布尔函数的协议已经被开发出来。由于每个输入位通常由两张卡编码，因此计算一个 $n $- 可变布尔函数需要至少200万美元的卡。我们感兴趣的是使用200万美元卡的最佳协议。我们特别关注对称函数，其中输出只取决于输入中的1s 个数。本文通过将 $n 变量对称布尔函数分类为几个 NPN 等价类，提出了开发 $2n 卡协议来计算 $n 变量对称布尔函数的问题。然后，我们总结了现有的协议，这些协议可以从这些类中计算出一些代表性的函数，并且通过开发协议来计算特定的函数，例如 $n = 4 $，$5 $，$6 $和 $7 $，从而解决了一些未解决的问题。"
    },
    {
        "title": "Fuzzification-based Feature Selection for Enhanced Website Content\n  Encryption",
        "url": "http://arxiv.org/abs/2306.13548v1",
        "pub_date": "2023-06-23",
        "summary": "We propose a novel approach that utilizes fuzzification theory to perform\nfeature selection on website content for encryption purposes. Our objective is\nto identify and select the most relevant features from the website by\nharnessing the principles of fuzzy logic. Fuzzification allows us to transform\nthe crisp website content into fuzzy representations, enabling a more nuanced\nanalysis of their characteristics. By considering the degree of membership of\neach feature in different fuzzy categories, we can evaluate their importance\nand relevance for encryption. This approach enables us to prioritize and focus\non the features that exhibit higher membership degrees, indicating their\nsignificance in the encryption process. By employing fuzzification-based\nfeature selection, we aim to enhance the effectiveness and efficiency of\nwebsite content encryption, ultimately improving the overall internet security.",
        "translated": "我们提出了一种新的方法，利用模糊化理论对网站内容进行特征选择，以达到加密的目的。我们的目标是通过利用模糊逻辑的原则，从网站中识别和选择最相关的特性。模糊化允许我们将清晰的网站内容转换为模糊表示，从而能够对其特征进行更细致入微的分析。通过考虑每个特征在不同模糊类别中的隶属度，我们可以评估它们对加密的重要性和相关性。这种方法使我们能够优先考虑和关注那些表现出更高的成员等级的特征，表明它们在加密过程中的重要性。采用基于模糊化的特征选择方法，提高网站内容加密的有效性和效率，最终提高网络的整体安全性。"
    },
    {
        "title": "Full Transparency in DBI frameworks",
        "url": "http://arxiv.org/abs/2306.13529v1",
        "pub_date": "2023-06-23",
        "summary": "Following the increasing trends of malicious applications or cyber threats in\ngeneral, program analysis has become a ubiquitous technique in extracting\nrelevant features. The current state-of-the-art solutions seem to fall behind\nnew techniques. For instance, dynamic binary instrumentation (DBI) provides\nsome promising results, but falls short when it comes to ease of use and\novercoming analysis evasion. In this regard, we propose a two-fold\ncontribution. First, we introduce COBAI (Complex Orchestrator for Binary\nAnalysis and Instrumentation), a DBI framework designed for malware analysis,\nprioritizing ease-of-use and analysis transparency, without imposing a\nsignificant overhead. Second, we introduce an aggregated test suite intended to\nstand as a benchmark in determining the quality of an analysis solution\nregarding the protection against evasion mechanisms. The efficiency of our\nsolution is validated by a careful evaluation taking into consideration other\nDBI frameworks, analysis environments, and the proposed benchmark.",
        "translated": "随着恶意应用程序或网络威胁日益增长的趋势，程序分析已经成为提取相关特征的普遍技术。目前最先进的解决方案似乎落后于新技术。例如，动态二进制检测(DBI)提供了一些有希望的结果，但在易用性和克服分析规避方面还有不足。在这方面，我们提出了双重贡献。首先，我们介绍 COBAI (用于二进制分析和仪器的复杂协调器) ，这是一个为恶意软件分析而设计的 DBI 框架，它优先考虑易用性和分析透明性，而不会带来大量的开销。其次，我们引入了一个聚合测试套件，目的是作为一个基准来确定关于防范逃避机制的分析解决方案的质量。考虑到其他 DBI 框架、分析环境和提出的基准，我们的解决方案的效率通过仔细的评估得到了验证。"
    },
    {
        "title": "Preventing EFail Attacks with Client-Side WebAssembly: The Case of Swiss\n  Post's IncaMail",
        "url": "http://arxiv.org/abs/2306.13388v1",
        "pub_date": "2023-06-23",
        "summary": "Traditional email encryption schemes are vulnerable to EFail attacks, which\nexploit the lack of message authentication by manipulating ciphertexts and\nexfiltrating plaintext via HTML backchannels. Swiss Post's IncaMail, a secure\nemail service for transmitting legally binding, encrypted, and verifiable\nemails, counters EFail attacks using an authenticated-encryption with\nassociated data (AEAD) encryption scheme to ensure message privacy and\nauthentication between servers. IncaMail relies on a trusted infrastructure\nbackend and encrypts messages per user policy. This paper presents a revised\nIncaMail architecture that offloads the majority of cryptographic operations to\nclients, offering benefits such as reduced computational load and energy\nfootprint, relaxed trust assumptions, and per-message encryption key policies.\nOur proof-of-concept prototype and benchmarks demonstrate the robustness of the\nproposed scheme, with client-side WebAssembly-based cryptographic operations\nyielding significant performance improvements (up to ~14x) over conventional\nJavaScript implementations.",
        "translated": "传统的邮件加密方案很容易受到 EFail 攻击，这种攻击通过操纵加密文本和通过 HTML 反向通道提取明文来利用信息认证的缺乏。瑞士邮政的 IncaMail 是一种安全的电子邮件服务，用于传输具有法律约束力的、加密的和可验证的电子邮件，它使用一种带有关联数据(AEAD)加密方案的认证加密来抵御 EFail 攻击，以确保信息隐私和服务器之间的认证。IncaMail 依赖于受信任的基础设施后端，并根据用户策略对消息进行加密。本文提出了一种改进的 IncaMail 体系结构，该体系结构将大部分加密操作卸载给客户端，提供了诸如减少计算负载和能量占用、放松信任假设和每消息加密密钥策略等优点。我们的概念验证原型和基准测试证明了提出的方案的健壮性，与传统的 JavaScript 实现相比，基于客户端 WebAssembly 的加密操作产生了显著的性能改进(最多14倍)。"
    },
    {
        "title": "Differentially Private Streaming Data Release under Temporal\n  Correlations via Post-processing",
        "url": "http://arxiv.org/abs/2306.13293v2",
        "pub_date": "2023-06-23",
        "summary": "The release of differentially private streaming data has been extensively\nstudied, yet striking a good balance between privacy and utility on temporally\ncorrelated data in the stream remains an open problem. Existing works focus on\nenhancing privacy when applying differential privacy to correlated data,\nhighlighting that differential privacy may suffer from additional privacy\nleakage under correlations; consequently, a small privacy budget has to be used\nwhich worsens the utility. In this work, we propose a post-processing framework\nto improve the utility of differential privacy data release under temporal\ncorrelations. We model the problem as a maximum posterior estimation given the\nreleased differentially private data and correlation model and transform it\ninto nonlinear constrained programming. Our experiments on synthetic datasets\nshow that the proposed approach significantly improves the utility and accuracy\nof differentially private data by nearly a hundred times in terms of mean\nsquare error when a strict privacy budget is given.",
        "translated": "差别私有流数据的发布已经得到了广泛的研究，然而在流中时间相关数据的隐私和效用之间取得良好的平衡仍然是一个悬而未决的问题。现有的工作着眼于在应用相关数据差分隐私时提高隐私保护，强调在相关性下，差分隐私可能会遭受额外的隐私泄露，因此，必须使用小额的隐私预算，这会使效用恶化。在这项工作中，我们提出了一个后处理框架，以改善差分隐私数据在时间相关性下发布的效用。我们将问题建模为给定发布的差分私有数据和相关模型的最大后验估计，并将其转化为非线性约束规划。我们在合成数据集上的实验表明，当给定严格的隐私预算时，该方法在均方误差方面显著提高了差分私有数据的实用性和准确性近百倍。"
    },
    {
        "title": "A First Order Meta Stackelberg Method for Robust Federated Learning\n  (Technical Report)",
        "url": "http://arxiv.org/abs/2306.13273v1",
        "pub_date": "2023-06-23",
        "summary": "Recent research efforts indicate that federated learning (FL) systems are\nvulnerable to a variety of security breaches. While numerous defense strategies\nhave been suggested, they are mainly designed to counter specific attack\npatterns and lack adaptability, rendering them less effective when facing\nuncertain or adaptive threats. This work models adversarial FL as a Bayesian\nStackelberg Markov game (BSMG) between the defender and the attacker to address\nthe lack of adaptability to uncertain adaptive attacks. We further devise an\neffective meta-learning technique to solve for the Stackelberg equilibrium,\nleading to a resilient and adaptable defense. The experiment results suggest\nthat our meta-Stackelberg learning approach excels in combating intense model\npoisoning and backdoor attacks of indeterminate types.",
        "translated": "最近的研究表明，联邦学习(FL)系统容易受到各种数字证书认证机构的影响。虽然提出了许多防御策略，但它们主要是针对特定的攻击模式而设计的，缺乏适应性，使其在面对不确定或适应性威胁时效果较差。本研究将防御者与攻击者之间的对抗性 FL 建模为一个贝叶斯 Stackelberg 马尔可夫博弈(BSMG) ，以解决对不确定性自适应攻击适应性不足的问题。我们进一步设计了一个有效的元学习技术来解决 Stackelberg 均衡，导致一个弹性和适应性的防御。实验结果表明，我们的元 Stackelberg 学习方法优于对强烈的模型中毒和不确定类型的后门攻击。"
    },
    {
        "title": "Visual Adversarial Examples Jailbreak Large Language Models",
        "url": "http://arxiv.org/abs/2306.13213v1",
        "pub_date": "2023-06-22",
        "summary": "Recently, there has been a surge of interest in introducing vision into Large\nLanguage Models (LLMs). The proliferation of large Visual Language Models\n(VLMs), such as Flamingo, BLIP-2, and GPT-4, signifies an exciting convergence\nof advancements in both visual and language foundation models. Yet, the risks\nassociated with this integrative approach are largely unexamined. In this\npaper, we shed light on the security and safety implications of this trend.\nFirst, we underscore that the continuous and high-dimensional nature of the\nadditional visual input space intrinsically makes it a fertile ground for\nadversarial attacks. This unavoidably expands the attack surfaces of LLMs.\nSecond, we highlight that the broad functionality of LLMs also presents visual\nattackers with a wider array of achievable adversarial objectives, extending\nthe implications of security failures beyond mere misclassification. To\nelucidate these risks, we study adversarial examples in the visual input space\nof a VLM. Specifically, against MiniGPT-4, which incorporates safety mechanisms\nthat can refuse harmful instructions, we present visual adversarial examples\nthat can circumvent the safety mechanisms and provoke harmful behaviors of the\nmodel. Remarkably, we discover that adversarial examples, even if optimized on\na narrow, manually curated derogatory corpus against specific social groups,\ncan universally jailbreak the model's safety mechanisms. A single such\nadversarial example can generally undermine MiniGPT-4's safety, enabling it to\nheed a wide range of harmful instructions and produce harmful content far\nbeyond simply imitating the derogatory corpus used in optimization. Unveiling\nthese risks, we accentuate the urgent need for comprehensive risk assessments,\nrobust defense strategies, and the implementation of responsible practices for\nthe secure and safe utilization of VLMs.",
        "translated": "最近，在大型语言模型(LLM)中引入视觉的兴趣激增。大型可视化语言模型(VLM)的增殖，例如 Flamingo、 BLIP-2和 GPT-4，标志着可视化模型和语言基础模型中令人兴奋的进步趋同。然而，与这种综合方法相关的风险在很大程度上没有得到检验。在本文中，我们阐明了这一趋势的安全和安全含义。首先，我们强调，额外的视觉输入空间的连续性和高维性本质上使其成为对抗性攻击的沃土。这不可避免地扩展了 LLM 的攻击面。其次，我们强调 LLM 的广泛功能也为视觉攻击者提供了一系列更广泛的可实现的敌对目标，扩展了安全失败的影响，而不仅仅是错误分类。为了阐明这些风险，我们研究了 VLM 视觉输入空间中的对抗性例子。具体来说，针对 MiniGPT-4，它包含了可以拒绝有害指令的安全机制，我们提供了可视化的对抗性例子，这些例子可以绕过安全机制并引发模型的有害行为。值得注意的是，我们发现，对抗性的例子，即使优化在一个狭窄的，手工策划的针对特定社会群体的贬损语料库，可以普遍越狱模型的安全机制。一个这样的对抗性例子通常会破坏 MiniGPT-4的安全性，使其能够听从范围广泛的有害指令并产生有害内容，而不仅仅是模仿优化中使用的贬义语料。揭示这些风险，我们强调迫切需要进行全面的风险评估、强有力的防御战略以及实施负责任的实践以确保 VLM 的安全使用。"
    },
    {
        "title": "Citadel: Side-Channel-Resistant Enclaves with Secure Shared Memory on a\n  Speculative Out-of-Order Processor",
        "url": "http://arxiv.org/abs/2306.14882v1",
        "pub_date": "2023-06-26",
        "summary": "We present Citadel, to our knowledge, the first side-channel-resistant\nenclave platform to run realistic secure programs on a speculative out-of-order\nmulticore processor. First, we develop a new hardware mechanism to enable\nsecure shared memory while defending against transient execution attacks. Then,\nwe develop an efficient dynamic cache partitioning scheme, improving both\nenclaves' and unprotected processes' performance. We conduct an in-depth\nsecurity analysis and a performance evaluation of our new mechanisms. Finally,\nwe build the hardware and software infrastructure required to run our secure\nenclaves. Our multicore processor runs on an FPGA and boots untrusted Linux\nfrom which users can securely launch and interact with enclaves. We open-source\nour end-to-end hardware and software infrastructure, hoping to spark more\nresearch and bridge the gap between conceptual proposals and FPGA prototypes.",
        "translated": "据我们所知，Citadel 是第一个能够在无序的多核处理器上运行现实的安全程序的抗侧通道飞地平台。首先，我们开发了一种新的硬件机制，使安全共享内存，同时防御短暂的执行攻击。然后，我们开发了一个有效的动态缓存分区方案，提高了包和非保护进程的性能。我们对我们的新机制进行深入的安全性分析和性能评估。最后，我们构建运行安全飞地所需的硬件和软件基础设施。我们的多核处理器在 FPGA 上运行，并引导不受信任的 Linux，用户可以从这个 Linux 安全地启动并与飞地交互。我们开源了我们的端到端硬件和软件基础设施，希望引发更多的研究，弥合概念方案和 FPGA 原型之间的差距。"
    },
    {
        "title": "Blockchain technology research and application: a systematic literature\n  review and future trends",
        "url": "http://arxiv.org/abs/2306.14802v2",
        "pub_date": "2023-06-26",
        "summary": "Blockchain, as the basis for cryptocurrencies, has received extensive\nattentions recently. Blockchain serves as an immutable distributed ledger\ntechnology which allows transactions to be carried out credibly in a\ndecentralized environment. Blockchain-based applications are springing up,\ncovering numerous fields including financial services, reputation system and\nInternet of Things (IoT), and so on. However, there are still many challenges\nof blockchain technology such as scalability, security and other issues waiting\nto be overcome. This article provides a comprehensive overview of blockchain\ntechnology and its applications. We begin with a summary of the development of\nblockchain, and then give an overview of the blockchain architecture and a\nsystematic review of the research and application of blockchain technology in\ndifferent fields from the perspective of academic research and industry\ntechnology. Furthermore, technical challenges and recent developments are also\nbriefly listed. We also looked at the possible future trends of blockchain.",
        "translated": "区块链作为加密货币的基础，近年来受到了广泛的关注。区块链作为一种不可变的分布式分类账技术，允许在分散的环境中可靠地执行交易。基于区块链的应用如雨后春笋般涌现，覆盖了金融服务、声誉系统和物联网等众多领域。然而，区块链技术仍然存在许多挑战，如可扩展性、安全性和其他问题有待克服。本文综述了区块链技术及其应用。我们首先对区块链的发展进行了总结，然后从学术研究和工业技术的角度对区块链体系结构进行了概述，并对区块链技术在不同领域的研究和应用进行了系统综述。此外，还简要介绍了技术挑战和最新发展。我们还研究了区块链未来可能的发展趋势。"
    },
    {
        "title": "Private Federated Learning in Gboard",
        "url": "http://arxiv.org/abs/2306.14793v1",
        "pub_date": "2023-06-26",
        "summary": "This white paper describes recent advances in Gboard(Google Keyboard)'s use\nof federated learning, DP-Follow-the-Regularized-Leader (DP-FTRL) algorithm,\nand secure aggregation techniques to train machine learning (ML) models for\nsuggestion, prediction and correction intelligence from many users' typing\ndata. Gboard's investment in those privacy technologies allows users' typing\ndata to be processed locally on device, to be aggregated as early as possible,\nand to have strong anonymization and differential privacy where possible.\nTechnical strategies and practices have been established to allow ML models to\nbe trained and deployed with meaningfully formal DP guarantees and high\nutility. The paper also looks ahead to how technologies such as trusted\nexecution environments may be used to further improve the privacy and security\nof Gboard's ML models.",
        "translated": "这份白皮书描述了 Gboard (Google Keyboard)的最新进展，它使用联邦学习、 DP-follow-the-Regulation ized-Leadership (DP-FTRL)算法和安全聚合技术来训练机器学习(ML)模型，从许多用户的输入数据中获取建议、预测和纠正智能。Gboard 在这些隐私技术上的投资使得用户的输入数据可以在设备上进行本地处理，尽可能早地进行聚合，并且在可能的情况下具有强大的匿名性和差分隐私。已经制定了技术战略和做法，使机器学习模型得到培训和部署，同时具有有意义的正式的动态数据保证和高效用。本文还展望了如何使用诸如可信执行环境等技术来进一步提高 Gboard 机器学习模型的隐私性和安全性。"
    },
    {
        "title": "On the Resilience of Machine Learning-Based IDS for Automotive Networks",
        "url": "http://arxiv.org/abs/2306.14782v1",
        "pub_date": "2023-06-26",
        "summary": "Modern automotive functions are controlled by a large number of small\ncomputers called electronic control units (ECUs). These functions span from\nsafety-critical autonomous driving to comfort and infotainment. ECUs\ncommunicate with one another over multiple internal networks using different\ntechnologies. Some, such as Controller Area Network (CAN), are very simple and\nprovide minimal or no security services. Machine learning techniques can be\nused to detect anomalous activities in such networks. However, it is necessary\nthat these machine learning techniques are not prone to adversarial attacks. In\nthis paper, we investigate adversarial sample vulnerabilities in four different\nmachine learning-based intrusion detection systems for automotive networks. We\nshow that adversarial samples negatively impact three of the four studied\nsolutions. Furthermore, we analyze transferability of adversarial samples\nbetween different systems. We also investigate detection performance and the\nattack success rate after using adversarial samples in the training. After\nanalyzing these results, we discuss whether current solutions are mature enough\nfor a use in modern vehicles.",
        "translated": "现代汽车的功能是由大量称为电子控制单元(ECU)的小型计算机控制的。这些功能的范围从安全至关重要的自主驾驶到舒适和信息娱乐。ECU 通过使用不同技术的多个内部网络相互通信。有些系统，如控制器局域网路(CAN) ，非常简单，只提供最低限度的安全服务，甚至不提供安全服务。机器学习技术可以用来检测这种网络中的异常活动。然而，这些机器学习技术不容易受到敌对攻击是必要的。本文研究了四种不同的基于机器学习的汽车网络入侵检测系统的对抗性样本漏洞。我们表明，对手样本负面影响四个研究的解决方案中的三个。此外，我们还分析了不同系统之间的对抗样本的可转移性。在训练中使用对手样本后，我们还研究了检测性能和攻击成功率。在分析了这些结果之后，我们讨论了目前的解决方案是否足够成熟，可以用于现代车辆。"
    },
    {
        "title": "Performance Analysis and Evaluation of Post Quantum Secure Blockchain\n  Federated Learning",
        "url": "http://arxiv.org/abs/2306.14772v1",
        "pub_date": "2023-06-26",
        "summary": "Post-quantum security is critical in the quantum era. Quantum computers,\nalong with quantum algorithms, make the standard cryptography based on RSA or\nECDSA over FL or Blockchain vulnerable. The implementation of post-quantum\ncryptography (PQC) over such systems is poorly understood as PQC is still in\nits standardization phase. In this work, we propose a hybrid approach to employ\nPQC over blockchain-based FL (BFL), where we combine a stateless signature\nscheme like Dilithium (or Falcon) with a stateful hash-based signature scheme\nlike the extended Merkle Signature Scheme (XMSS). We propose a linearbased\nformulaic approach to device role selection mechanisms based on multiple\nfactors to address the performance aspect. Our holistic approach of utilizing a\nverifiable random function (VRF) to assist in the blockchain consensus\nmechanism shows the practicality of the proposed approaches. The proposed\nmethod and extensive experimental results contribute to enhancing the security\nand performance aspects of BFL systems.",
        "translated": "在量子时代，后量子安全至关重要。量子计算机以及量子算法使得基于 RSA 或 ECDSA 的标准加密技术在 FL 或区块链上变得脆弱。后量子密码学(PQC)在这类系统上的实现知之甚少，因为 PQC 仍处于标准化阶段。在本文中，我们提出了一种基于区块链的 FL (BFL)上使用 PQC 的混合方法，将 Dilithium (或 Falcon)这样的无状态签名方案与扩展的 Merkle 签名方案(XMSS)这样的基于散列的有状态签名方案相结合。提出了一种基于多因素的设备角色选择机制的线性公式化方法，以解决性能方面的问题。利用可验证随机函数(VRF)协助区块链共识机制的整体方法表明了所提方法的实用性。提出的方法和广泛的实验结果有助于提高 BFL 系统的安全性和性能方面。"
    },
    {
        "title": "Ensemble of Random and Isolation Forests for Graph-Based Intrusion\n  Detection in Containers",
        "url": "http://arxiv.org/abs/2306.14750v1",
        "pub_date": "2023-06-26",
        "summary": "We propose a novel solution combining supervised and unsupervised machine\nlearning models for intrusion detection at kernel level in cloud containers. In\nparticular, the proposed solution is built over an ensemble of random and\nisolation forests trained on sequences of system calls that are collected at\nthe hosting machine's kernel level. The sequence of system calls are translated\ninto a weighted and directed graph to obtain a compact description of the\ncontainer behavior, which is given as input to the ensemble model. We executed\na set of experiments in a controlled environment in order to test our solution\nagainst the two most common threats that have been identified in cloud\ncontainers, and our results show that we can achieve high detection rates and\nlow false positives in the tested attacks.",
        "translated": "我们提出了一种新的解决方案，将监督模型和非监督式学习模型相结合，用于云容器内核级的入侵检测。特别是，所提出的解决方案是基于在宿主机内核级别收集的系统调用序列训练的随机和隔离森林集合构建的。将系统调用序列转换为加权有向图，以获得容器行为的简洁描述，并将其作为集成模型的输入。我们在一个受控的环境中执行了一系列的实验，以测试我们的解决方案对云容器中已经确定的两个最常见的威胁，我们的结果表明，我们可以在测试攻击中实现高检测率和低误报率。"
    },
    {
        "title": "MFDPG: Multi-Factor Authenticated Password Management With Zero Stored\n  Secrets",
        "url": "http://arxiv.org/abs/2306.14746v1",
        "pub_date": "2023-06-26",
        "summary": "While password managers are a vital tool for internet security, they can also\ncreate a massive central point of failure, as evidenced by several major recent\ndata breaches. For over 20 years, deterministic password generators (DPGs) have\nbeen proposed, and largely rejected, as a viable alternative to password\nmanagement tools. In this paper, we survey 45 existing DPGs to asses the main\nsecurity, privacy, and usability issues hindering their adoption. We then\npresent a new multi-factor deterministic password generator (MFDPG) design that\naims to address these shortcomings. The result not only achieves strong,\npractical password management with zero credential storage, but also\neffectively serves as a progressive client-side upgrade of weak password-only\nwebsites to strong multi-factor authentication.",
        "translated": "虽然密码管理器是互联网安全的重要工具，但它们也可能造成大规模的中心故障，最近几起重大数据泄露事件就证明了这一点。20多年来，确定性密码生成器(DPG)一直被提议作为密码管理工具的可行替代方案，但遭到了很大程度的拒绝。在本文中，我们调查了45个现有的 DPG，以评估阻碍其采用的主要安全性、隐私性和可用性问题。然后，我们提出了一种新的多因素确定性密码生成器(MFDPG)设计，旨在解决这些缺点。结果不仅实现了强大、实用的密码管理，没有任何凭证存储，而且有效地作为一个渐进的客户端升级软弱的密码网站到强大的双重身份验证。"
    },
    {
        "title": "ChatIDS: Explainable Cybersecurity Using Generative AI",
        "url": "http://arxiv.org/abs/2306.14504v1",
        "pub_date": "2023-06-26",
        "summary": "Intrusion Detection Systems (IDS) are a proven approach to secure networks.\nHowever, in a privately used network, it is difficult for users without\ncybersecurity expertise to understand IDS alerts, and to respond in time with\nadequate measures. This puts the security of home networks, smart home\ninstallations, home-office workers, etc. at risk, even if an IDS is correctly\ninstalled and configured. In this work, we propose ChatIDS, our approach to\nexplain IDS alerts to non-experts by using large language models. We evaluate\nthe feasibility of ChatIDS by using ChatGPT, and we identify open research\nissues with the help of interdisciplinary experts in artificial intelligence.\nOur results show that ChatIDS has the potential to increase network security by\nproposing meaningful security measures in an intuitive language from IDS\nalerts. Nevertheless, some potential issues in areas such as trust, privacy,\nethics, etc. need to be resolved, before ChatIDS might be put into practice.",
        "translated": "入侵检测系统(IDS)是一种经过验证的网络安全方法。然而，在私人使用的网络中，没有网络安全专业知识的用户很难理解 IDS 警报，并且很难及时采取适当的措施作出响应。这使得家庭网络、智能家庭安装、家庭办公室工作人员等的安全性处于危险之中，即使 IDS 安装和配置正确。在这项工作中，我们提出了 ChatIDS，这是我们通过使用大型语言模型向非专家解释 IDS 警报的方法。我们使用 ChatGPT 来评估 ChatIDS 的可行性，并在人工智能领域的跨学科专家的帮助下确定开放的研究问题。我们的研究结果表明，ChatIDS 可以通过根据 IDS 警报以直观的语言提出有意义的安全措施来提高网络安全性。然而，在 ChatIDS 可能付诸实施之前，在信任、隐私、道德等方面的一些潜在问题需要得到解决。"
    },
    {
        "title": "Practical Privacy-Preserving Gaussian Process Regression via Secret\n  Sharing",
        "url": "http://arxiv.org/abs/2306.14498v1",
        "pub_date": "2023-06-26",
        "summary": "Gaussian process regression (GPR) is a non-parametric model that has been\nused in many real-world applications that involve sensitive personal data\n(e.g., healthcare, finance, etc.) from multiple data owners. To fully and\nsecurely exploit the value of different data sources, this paper proposes a\nprivacy-preserving GPR method based on secret sharing (SS), a secure\nmulti-party computation (SMPC) technique. In contrast to existing studies that\nprotect the data privacy of GPR via homomorphic encryption, differential\nprivacy, or federated learning, our proposed method is more practical and can\nbe used to preserve the data privacy of both the model inputs and outputs for\nvarious data-sharing scenarios (e.g., horizontally/vertically-partitioned\ndata). However, it is non-trivial to directly apply SS on the conventional GPR\nalgorithm, as it includes some operations whose accuracy and/or efficiency have\nnot been well-enhanced in the current SMPC protocol. To address this issue, we\nderive a new SS-based exponentiation operation through the idea of\n'confusion-correction' and construct an SS-based matrix inversion algorithm\nbased on Cholesky decomposition. More importantly, we theoretically analyze the\ncommunication cost and the security of the proposed SS-based operations.\nEmpirical results show that our proposed method can achieve reasonable accuracy\nand efficiency under the premise of preserving data privacy.",
        "translated": "克里金模型是一种非参数模型，已经在许多涉及来自多个数据所有者的敏感个人数据(例如，医疗、金融等)的现实应用程序中使用。为了充分、安全地利用不同数据源的价值，本文提出了一种基于秘密共享(secret share，SS)的保护隐私的探地安全多方计算(gPR)方法。与现有通过同态加密、差分隐私或联合学习保护探地雷达数据隐私的研究相比，我们提出的方法更为实用，可用于保护各种数据共享场景(例如水平/垂直分区数据)的模型输入和输出的数据隐私。然而，在传统的探地雷达算法中直接应用 SS 并不简单，因为它包含了一些在现有的 SMPC 协议中未能很好地提高精度和/或效率的操作。为了解决这个问题，我们通过“混淆校正”的思想推导出一种新的基于 SS 的指数运算，并构造了一种基于乔列斯基分解的基于 SS 的矩阵反演算法。更重要的是，我们从理论上分析了所提出的基于 SS 的操作的通信成本和安全性。实验结果表明，在保护数据隐私的前提下，该方法能够达到合理的精度和效率。"
    },
    {
        "title": "Your Code is 0000: An Analysis of the Disposable Phone Numbers Ecosystem",
        "url": "http://arxiv.org/abs/2306.14497v1",
        "pub_date": "2023-06-26",
        "summary": "Short Message Service (SMS) is a popular channel for online service providers\nto verify accounts and authenticate users registered to a particular service.\nSpecialized applications, called Public SMS Gateways (PSGs), offer free\nDisposable Phone Numbers (DPNs) that can be used to receive SMS messages. DPNs\nallow users to protect their privacy when creating online accounts. However,\nthey can also be abused for fraudulent activities and to bypass security\nmechanisms like Two-Factor Authentication (2FA). In this paper, we perform a\nlarge-scale and longitudinal study of the DPN ecosystem by monitoring 17,141\nunique DPNs in 29 PSGs over the course of 12 months. Using a dataset of over\n70M messages, we provide an overview of the ecosystem and study the different\nservices that offer DPNs and their relationships. Next, we build a framework\nthat (i) identifies and classifies the purpose of an SMS; and (ii) accurately\nattributes every message to more than 200 popular Internet services that\nrequire SMS for creating registered accounts. Our results indicate that the DPN\necosystem is globally used to support fraudulent account creation and access,\nand that this issue is ubiquitous and affects all major Internet platforms and\nspecialized online services.",
        "translated": "短消息服务(SMS)是在线服务提供商验证帐户和认证注册到特定服务的用户的流行渠道。专门的应用程序，称为公共短信网关(PSG) ，提供可用于接收短信的免费一次性电话号码(DPN)。DPN 允许用户在创建在线帐户时保护自己的隐私。然而，他们也可以被滥用欺诈活动和绕过安全机制，如双因素认证(2FA)。在本文中，我们通过在12个月的时间里对29个 PSG 中的17,141个独特的 DPN 进行监测，对 DPN 生态系统进行了大规模的追踪研究研究。通过使用超过7000万条消息的数据集，我们提供了一个生态系统的概述，并研究了提供 DPN 的不同服务及其关系。接下来，我们构建一个框架，它(i)识别和分类短信的用途; (ii)准确地将每条消息归属于200多种流行的互联网服务，这些服务需要短信来创建注册账户。我们的研究结果表明，DPN 生态系统在全球范围内被用来支持欺诈性的账户创建和访问，这个问题无处不在，并影响到所有主要的互联网平台和专门的在线服务。"
    },
    {
        "title": "Automated Fuzzing Harness Generation for Library APIs and Binary\n  Protocol Parsers",
        "url": "http://arxiv.org/abs/2306.15596v1",
        "pub_date": "2023-06-27",
        "summary": "Fuzzing is a widely used software security testing technique that is designed\nto identify vulnerabilities in systems by providing invalid or unexpected\ninput. Continuous fuzzing systems like OSS-FUZZ have been successful in finding\nsecurity bugs in many different software systems. The typical process of\nfinding security bugs using fuzzing involves several steps: first, the\n\"fuzz-worthy\" functions that are likely to contain vulnerabilities must be\nidentified; second, the setup requirements for the API must be understood\nbefore it can be called; third, a fuzzing harness must be written and bound to\na coverage-guided fuzzer like LLVM's LibFuzzer; and finally, the security bugs\ndiscovered by the fuzzing harness must be triaged and checked for\nreproducibility. This project focuses on automating the first two steps in this\nprocess. In particular, we present an automated system that can generate\nfuzzing harnesses for library APIs and binary protocol parsers by analyzing\nunit tests. This allows for the scaling of the fuzzing infrastructure in\nproportion to the growth of the codebase, without the need for manual coding of\nharnesses. Additionally, we develop a metric to assess the \"fuzz-worthiness\" of\nan API, enabling us to prioritize the most promising targets for testing.",
        "translated": "Fuzzing 是一种广泛使用的软件安全测试技术，它通过提供无效或意外的输入来识别系统中的漏洞。像 OSS-FUZZ 这样的连续模糊系统已经成功地发现了许多不同软件系统中的安全漏洞。使用 fuzzing 发现安全漏洞的典型过程包括几个步骤: 首先，必须识别出可能包含漏洞的“模糊值”函数; 其次，在调用 API 之前必须了解 API 的设置要求; 第三，必须编写一个 fuzzing 装置并将其绑定到 LLVM 的 LibFuzzer 这样的覆盖率引导的 fuzzer 上; 最后，必须对 fuzzing 装置发现的安全漏洞进行分流并检查其可重复性。这个项目的重点是自动化这个过程的前两个步骤。特别是，我们提出了一个自动化系统，可以生成模糊综合库 API 和二进制协议解析器通过分析单元测试。这样就可以按照代码库的增长比例扩大模糊化基础设施的规模，而不需要手工编码控制线束。此外，我们开发了一个度量来评估 API 的“模糊价值”，使我们能够优先考虑最有前途的测试目标。"
    },
    {
        "title": "Developing and Deploying Security Applications for In-Vehicle Networks",
        "url": "http://arxiv.org/abs/2306.15588v1",
        "pub_date": "2023-06-27",
        "summary": "Radiological material transportation is primarily facilitated by heavy-duty\non-road vehicles. Modern vehicles have dozens of electronic control units or\nECUs, which are small, embedded computers that communicate with sensors and\neach other for vehicle functionality. ECUs use a standardized network\narchitecture--Controller Area Network or CAN--which presents grave security\nconcerns that have been exploited by researchers and hackers alike. For\ninstance, ECUs can be impersonated by adversaries who have infiltrated an\nautomotive CAN and disable or invoke unintended vehicle functions such as\nbrakes, acceleration, or safety mechanisms. Further, the quality of security\napproaches varies wildly between manufacturers. Thus, research and development\nof after-market security solutions have grown remarkably in recent years. Many\nresearchers are exploring deployable intrusion detection and prevention\nmechanisms using machine learning and data science techniques. However, there\nis a gap between developing security system algorithms and deploying prototype\nsecurity appliances in-vehicle. In this paper, we, a research team at Oak Ridge\nNational Laboratory working in this space, highlight challenges in the\ndevelopment pipeline, and provide techniques to standardize methodology and\novercome technological hurdles.",
        "translated": "放射性物质的运输主要由重型公路车辆提供便利。现代汽车有几十个电子控制单元或 ECU，这是一种小型的嵌入式计算机，可以与传感器相互通信，实现汽车功能。ECU 使用一种标准化的网络架构——控制器局域网路或 CAN ——这种架构表现出严重的安全问题，已被研究人员和黑客利用。例如，ECU 可以被渗透到汽车 CAN 网络中的敌人模仿，使得非预期的车辆功能(如刹车、加速或安全机制)失效或调用。此外，安全方法的质量在制造商之间差异很大。因此，近年来，售后安全解决方案的研究和开发显著增长。许多研究人员正在利用机器学习和数据科学技术探索可部署的入侵检测和预防机制。然而，在开发安全系统算法和部署车载原型安全设备之间存在差距。在这篇论文中，我们，一个在这个领域工作的橡树岭国家实验室研究团队，强调了开发过程中的挑战，并提供了标准化方法和克服技术障碍的技术。"
    },
    {
        "title": "MTFS: a Moving Target Defense-Enabled File System for Malware Mitigation",
        "url": "http://arxiv.org/abs/2306.15566v1",
        "pub_date": "2023-06-27",
        "summary": "Ransomware has remained one of the most notorious threats in the\ncybersecurity field. Moving Target Defense (MTD) has been proposed as a novel\nparadigm for proactive defense. Although various approaches leverage MTD, few\nof them rely on the operating system and, specifically, the file system,\nthereby making them dependent on other computing devices. Furthermore, existing\nransomware defense techniques merely replicate or detect attacks, without\npreventing them. Thus, this paper introduces the MTFS overlay file system and\nthe design and implementation of three novel MTD techniques implemented on top\nof it. One delaying attackers, one trapping recursive directory traversal, and\nanother one hiding file types. The effectiveness of the techniques are shown in\ntwo experiments. First, it is shown that the techniques can delay and mitigate\nransomware on real IoT devices. Secondly, in a broader scope, the solution was\nconfronted with 14 ransomware samples, highlighting that it can save 97% of the\nfiles.",
        "translated": "勒索软件一直是网络安全领域最臭名昭著的威胁之一。运动目标防御(MTD)是一种新型的主动防御模式。虽然各种方法都利用 MTD，但是很少有方法依赖于操作系统，特别是文件系统，因此它们依赖于其他计算设备。此外，现有的勒索软件防御技术只是复制或检测攻击，而没有防止它们。因此，本文介绍了 MTFS 覆盖文件系统以及在此基础上实现的三种新的 MTD 技术的设计与实现。一个延迟攻击，一个捕获递归目录遍历，另一个隐藏文件类型。两个实验表明了该技术的有效性。首先，研究表明该技术可以在实际物联网设备上延迟和缓解勒索软件。其次，在更广泛的范围内，解决方案遇到了14个勒索软件样本，突出显示它可以保存97% 的文件。"
    },
    {
        "title": "RansomAI: AI-powered Ransomware for Stealthy Encryption",
        "url": "http://arxiv.org/abs/2306.15559v1",
        "pub_date": "2023-06-27",
        "summary": "Cybersecurity solutions have shown promising performance when detecting\nransomware samples that use fixed algorithms and encryption rates. However, due\nto the current explosion of Artificial Intelligence (AI), sooner than later,\nransomware (and malware in general) will incorporate AI techniques to\nintelligently and dynamically adapt its encryption behavior to be undetected.\nIt might result in ineffective and obsolete cybersecurity solutions, but the\nliterature lacks AI-powered ransomware to verify it. Thus, this work proposes\nRansomAI, a Reinforcement Learning-based framework that can be integrated into\nexisting ransomware samples to adapt their encryption behavior and stay\nstealthy while encrypting files. RansomAI presents an agent that learns the\nbest encryption algorithm, rate, and duration that minimizes its detection\n(using a reward mechanism and a fingerprinting intelligent detection system)\nwhile maximizing its damage function. The proposed framework was validated in a\nransomware, Ransomware-PoC, that infected a Raspberry Pi 4, acting as a\ncrowdsensor. A pool of experiments with Deep Q-Learning and Isolation Forest\n(deployed on the agent and detection system, respectively) has demonstrated\nthat RansomAI evades the detection of Ransomware-PoC affecting the Raspberry Pi\n4 in a few minutes with &gt;90% accuracy.",
        "translated": "当检测使用固定算法和加密速率的勒索软件样本时，网络安全解决方案已经显示出有希望的性能。然而，由于目前人工智能(AI)的爆炸性增长，勒索软件(和一般的恶意软件)很快就会结合 AI 技术，智能地、动态地调整其加密行为，使其不被发现。它可能导致无效和过时的网络安全解决方案，但文献缺乏人工智能驱动的勒索软件来验证它。因此，这项工作提出了 RansomAI，一个基于强化学习的框架，可以集成到现有的勒索软件样本，以适应他们的加密行为，并在加密文件时保持隐形。RansomAI 提供了一个代理，它学习最佳的加密算法、速率和持续时间，以最小化其检测(使用奖励机制和指纹智能检测系统) ，同时最大化其损害功能。提出的框架在一个勒索软件 Ransomware-PoC 中得到了验证，该勒索软件感染了树莓 Pi 4，作为一个群体传感器。使用 Deep Q- 学习和隔离森林(分别部署在代理和检测系统上)的一组实验已经证明，RansomAI 在几分钟内以 > 90% 的准确性逃避影响 Raspberry Pi 4的 Ransomware-PoC 的检测。"
    },
    {
        "title": "PASNet: Polynomial Architecture Search Framework for Two-party\n  Computation-based Secure Neural Network Deployment",
        "url": "http://arxiv.org/abs/2306.15513v1",
        "pub_date": "2023-06-27",
        "summary": "Two-party computation (2PC) is promising to enable privacy-preserving deep\nlearning (DL). However, the 2PC-based privacy-preserving DL implementation\ncomes with high comparison protocol overhead from the non-linear operators.\nThis work presents PASNet, a novel systematic framework that enables low\nlatency, high energy efficiency &amp; accuracy, and security-guaranteed 2PC-DL by\nintegrating the hardware latency of the cryptographic building block into the\nneural architecture search loss function. We develop a cryptographic hardware\nscheduler and the corresponding performance model for Field Programmable Gate\nArrays (FPGA) as a case study. The experimental results demonstrate that our\nlight-weighted model PASNet-A and heavily-weighted model PASNet-B achieve 63 ms\nand 228 ms latency on private inference on ImageNet, which are 147 and 40 times\nfaster than the SOTA CryptGPU system, and achieve 70.54% &amp; 78.79% accuracy and\nmore than 1000 times higher energy efficiency.",
        "translated": "两方计算(2PC)有望实现隐私保护的深度学习(DL)。然而，基于2PC 的保护隐私的数字用户线实现与非线性运营商具有很高的比较协定开销。本文提出了一种新的系统结构 PASNet，它通过将密码构建块的硬件延迟集成到神经网络结构的搜索丢失函数中，实现了低延迟、高能量效率和准确性以及安全保证的2PC-DL。以现场可编程门阵列(FPGA)为例，开发了一个加密硬件调度器及相应的性能模型。实验结果表明，轻权模型 PASNet-A 和重权模型 PASNet-B 在 ImageNet 上实现了63ms 和228ms 的私有推理延迟，比 SOTA CryptGPU 系统分别快147和40倍，准确率分别为70.54% 和78.79% ，能量效率提高了1000多倍。"
    },
    {
        "title": "Identifying Practical Challenges in the Implementation of Technical\n  Measures for Data Privacy Compliance",
        "url": "http://arxiv.org/abs/2306.15497v1",
        "pub_date": "2023-06-27",
        "summary": "Modern privacy regulations provide a strict mandate for data processing\nentities to implement appropriate technical measures to demonstrate compliance.\nIn practice, determining what measures are indeed \"appropriate\" is not trivial,\nparticularly in light of vague guidelines provided by privacy regulations. To\nexacerbate the issue, challenges arise not only in the implementation of the\ntechnical measures themselves, but also in a variety of factors involving the\nroles, processes, decisions, and culture surrounding the pursuit of privacy\ncompliance. In this paper, we present 33 challenges faced in the implementation\nof technical measures for privacy compliance, derived from a qualitative\nanalysis of 16 interviews with privacy professionals. In addition, we evaluate\nthe interview findings in a survey study, which gives way to a discussion of\nthe identified challenges and their implications.",
        "translated": "现代隐私法规为数据处理实体提供了严格的授权，要求它们采取适当的技术措施来证明遵守了规定。在实践中，确定哪些措施确实是“适当的”并非易事，特别是考虑到隐私法规提供的模糊指导方针。为了加剧这个问题，挑战不仅出现在技术措施本身的实施中，而且出现在涉及角色、过程、决策和追求隐私遵从的文化的各种因素中。在这篇文章中，我们提出了33个面临的挑战，在实施技术措施的隐私遵守，来自定性分析的16个访谈的隐私专业人士。此外，我们评估调查研究中的访谈结果，这让位于对已确定的挑战及其影响的讨论。"
    },
    {
        "title": "Event-Triggered Islanding in Inverter-Based Grids",
        "url": "http://arxiv.org/abs/2306.15454v2",
        "pub_date": "2023-06-27",
        "summary": "The decentralization of modern power systems challenges the hierarchical\nstructure of the electric grid and requires the implementation of automated\nschemes that can overcome adverse conditions. This work proposes an adaptive\nisolation methodology that can segregate a grid topology in autonomous islands\nthat maintain stable and economic operation in the presence of deliberate\n(e.g., cyberattacks) or unintentional abnormal events. The adaptive isolation\nlogic is event-triggered to avoid false positives, improve detection accuracy,\nand reduce computational overheads. A measurement-based stable kernel\nrepresentation (SKR) triggering mechanism inspects distributed generation\ncontrollers for abnormal behavior. The SKR notifies a machine learning (ML)\nensemble classifier that detects whether the system behavior is within\nacceptable operational conditions. The event-triggered adaptive isolation\nframework is evaluated using IEEE RTS-24 bus system. Simulation results\ndemonstrate that the proposed framework detects anomalous behavior in real-time\nand identifies stable partitions minimizing operating costs faster than\ntraditional islanding detection techniques.",
        "translated": "现代电力系统的地方分权挑战了电网的等级结构，需要实施能够克服不利条件的自动化方案。这项工作提出了一个自适应隔离方法，可以隔离网格拓扑在自治岛屿，维持稳定和经济运行的存在(例如，网络攻击)或无意的异常事件。自适应隔离逻辑是事件触发的，以避免误报，提高检测精度，并减少计算开销。基于测量的稳定内核表示(SKR)触发机制检查分散式发电控制器的异常行为。SKR 通知机器学习(ML)集成分类器，该分类器检测系统行为是否在可接受的操作条件下。利用 IEEE RTS-24总线系统对事件触发自适应隔离框架进行了评估。仿真结果表明，与传统的孤岛检测技术相比，该框架能够实时检测异常行为并识别出稳定的分区，从而使运行成本最小化。"
    },
    {
        "title": "A New Mathematical Optimization-Based Method for the m-invariance\n  Problem",
        "url": "http://arxiv.org/abs/2306.15371v1",
        "pub_date": "2023-06-27",
        "summary": "The issue of ensuring privacy for users who share their personal information\nhas been a growing priority in a business and scientific environment where the\nuse of different types of data and the laws that protect it have increased in\ntandem. Different technologies have been widely developed for static\npublications, i.e., where the information is published only once, such as\nk-anonymity and {\\epsilon}-differential privacy. In the case where microdata\ninformation is published dynamically, although established notions such as\nm-invariance and {\\tau}-safety already exist, developments for improving\nutility remain superficial. We propose a new heuristic approach for the NP-hard\ncombinatorial problem of m-invariance and {\\tau}-safety, which is based on a\nmathematical optimization column generation scheme. The quality of a solution\nto m-invariance and {\\tau}-safety can be measured by the Information Loss (IL),\na value in [0,100], the closer to 0 the better. We show that our approach\nimproves by far current heuristics, providing in some instances solutions with\nILs of 1.87, 8.5 and 1.93, while the state-of-the art methods reported ILs of\n39.03, 51.84 and 57.97, respectively.",
        "translated": "在商业和科学环境中，确保分享个人信息的用户的隐私问题日益成为一个优先事项，在这种环境中，不同类型数据的使用和保护数据的法律同步增加。不同的技术已经被广泛应用于静态出版物，例如，信息只发布一次，例如 k 匿名和{ epsilon }-差分隐私。在动态发布微数据信息的情况下，尽管已经存在 m 不变性和{ tau }-安全性等既定概念，但改善效用的发展仍然是肤浅的。我们提出了一种基于最优化列生成方案的新的启发式方法来解决 m- 不变性和{ tau }-安全性的 NP 难组合问题。M 不变性和{ tau }-安全性的解的质量可以用信息损失(IL)来衡量，一个值在[0,100]中，越接近0越好。我们表明，我们的方法通过目前的启发式改进，在某些情况下提供了 ILs 为1.87,8.5和1.93的解决方案，而最先进的方法报告的 ILs 分别为39.03,51.84和57.97。"
    },
    {
        "title": "Your Attack Is Too DUMB: Formalizing Attacker Scenarios for Adversarial\n  Transferability",
        "url": "http://arxiv.org/abs/2306.15363v1",
        "pub_date": "2023-06-27",
        "summary": "Evasion attacks are a threat to machine learning models, where adversaries\nattempt to affect classifiers by injecting malicious samples. An alarming\nside-effect of evasion attacks is their ability to transfer among different\nmodels: this property is called transferability. Therefore, an attacker can\nproduce adversarial samples on a custom model (surrogate) to conduct the attack\non a victim's organization later. Although literature widely discusses how\nadversaries can transfer their attacks, their experimental settings are limited\nand far from reality. For instance, many experiments consider both attacker and\ndefender sharing the same dataset, balance level (i.e., how the ground truth is\ndistributed), and model architecture.\n  In this work, we propose the DUMB attacker model. This framework allows\nanalyzing if evasion attacks fail to transfer when the training conditions of\nsurrogate and victim models differ. DUMB considers the following conditions:\nDataset soUrces, Model architecture, and the Balance of the ground truth. We\nthen propose a novel testbed to evaluate many state-of-the-art evasion attacks\nwith DUMB; the testbed consists of three computer vision tasks with two\ndistinct datasets each, four types of balance levels, and three model\narchitectures. Our analysis, which generated 13K tests over 14 distinct\nattacks, led to numerous novel findings in the scope of transferable attacks\nwith surrogate models. In particular, mismatches between attackers and victims\nin terms of dataset source, balance levels, and model architecture lead to\nnon-negligible loss of attack performance.",
        "translated": "规避攻击是对机器学习模型的一种威胁，其中对手试图通过注入恶意样本来影响分类器。规避攻击的一个令人担忧的副作用是它们能够在不同模式之间转移: 这种特性被称为可转移性。因此，攻击者可以在自定义模型(代理)上生成对抗样本，以便以后对受害者的组织进行攻击。尽管文学作品广泛讨论了敌人如何转移攻击，但是他们的实验设置是有限的，而且远离现实。例如，许多实验考虑攻击者和防御者共享相同的数据集、平衡级(即如何分布地面真相)和模型架构。在这项工作中，我们提出了 DUMB 攻击模型。该框架允许分析当代理模型和受害模型的训练条件不同时，规避攻击是否无法转移。DUMB 考虑以下条件: 数据集源、模型体系结构和基础真理的平衡。然后，我们提出了一个新的测试平台来评估许多国家的最先进的逃避攻击与 DUMB; 测试平台由三个计算机视觉任务与两个不同的数据集，四种类型的平衡水平，和三个模型架构。我们的分析在14种不同的攻击中产生了13K 的测试，在使用替代模型的可转移攻击范围中导致了许多新的发现。特别是，攻击者和受害者之间在数据集源、平衡级别和模型体系结构方面的不匹配导致攻击性能的不可忽视的损失。"
    },
    {
        "title": "A Highly Accurate Query-Recovery Attack against Searchable Encryption\n  using Non-Indexed Documents",
        "url": "http://arxiv.org/abs/2306.15302v1",
        "pub_date": "2023-06-27",
        "summary": "Cloud data storage solutions offer customers cost-effective and reduced data\nmanagement. While attractive, data security issues remain to be a core concern.\nTraditional encryption protects stored documents, but hinders simple\nfunctionalities such as keyword search. Therefore, searchable encryption\nschemes have been proposed to allow for the search on encrypted data. Efficient\nschemes leak at least the access pattern (the accessed documents per keyword\nsearch), which is known to be exploitable in query recovery attacks assuming\nthe attacker has a significant amount of background knowledge on the stored\ndocuments. Existing attacks can only achieve decent results with strong\nadversary models (e.g. at least 20% of previously known documents or require\nadditional knowledge such as on query frequencies) and they give no metric to\nevaluate the certainty of recovered queries. This hampers their practical\nutility and questions their relevance in the real-world.\n  We propose a refined score attack which achieves query recovery rates of\naround 85% without requiring exact background knowledge on stored documents; a\ndistributionally similar, but otherwise different (i.e., non-indexed), dataset\nsuffices. The attack starts with very few known queries (around 10 known\nqueries in our experiments over different datasets of varying size) and then\niteratively recovers further queries with confidence scores by adding\npreviously recovered queries that had high confidence scores to the set of\nknown queries. Additional to high recovery rates, our approach yields\ninterpretable results in terms of confidence scores.",
        "translated": "云数据存储解决方案为客户提供具有成本效益和简化数据管理的解决方案。虽然具有吸引力，但数据安全问题仍然是一个核心问题。传统的加密保护存储的文档，但阻碍简单的功能，如关键字搜索。因此，提出了可搜索的加密方案，以便对加密数据进行搜索。高效的模式至少会泄漏访问模式(每次关键字搜索所访问的文档) ，假设攻击者对存储的文档有大量的背景知识，那么在查询恢复攻击中这些模式是可以利用的。现有的攻击只能通过强大的对手模型(例如至少20% 以前已知的文档或者需要额外的知识，比如查询频率)获得像样的结果，而且它们没有给出评估恢复查询确定性的度量。这阻碍了它们的实用性，并质疑了它们在现实世界中的相关性。我们提出了一种改进的分数攻击，它可以实现约85% 的查询恢复率，而不需要对存储的文档有精确的背景知识; 一个分布相似，但在其他方面不同(即，非索引)的数据集就足够了。这种攻击从非常少的已知查询开始(在我们的实验中，大约有10个已知查询，针对不同大小的数据集) ，然后通过在已知查询集中添加先前恢复的具有高置信度的查询，迭代地恢复具有置信度分数的进一步查询。除了高恢复率之外，我们的方法在置信度得分方面产生了可解释的结果。"
    },
    {
        "title": "On Practical Aspects of Aggregation Defenses against Data Poisoning\n  Attacks",
        "url": "http://arxiv.org/abs/2306.16415v1",
        "pub_date": "2023-06-28",
        "summary": "The increasing access to data poses both opportunities and risks in deep\nlearning, as one can manipulate the behaviors of deep learning models with\nmalicious training samples. Such attacks are known as data poisoning. Recent\nadvances in defense strategies against data poisoning have highlighted the\neffectiveness of aggregation schemes in achieving state-of-the-art results in\ncertified poisoning robustness. However, the practical implications of these\napproaches remain unclear. Here we focus on Deep Partition Aggregation, a\nrepresentative aggregation defense, and assess its practical aspects, including\nefficiency, performance, and robustness. For evaluations, we use ImageNet\nresized to a resolution of 64 by 64 to enable evaluations at a larger scale\nthan previous ones. Firstly, we demonstrate a simple yet practical approach to\nscaling base models, which improves the efficiency of training and inference\nfor aggregation defenses. Secondly, we provide empirical evidence supporting\nthe data-to-complexity ratio, i.e. the ratio between the data set size and\nsample complexity, as a practical estimation of the maximum number of base\nmodels that can be deployed while preserving accuracy. Last but not least, we\npoint out how aggregation defenses boost poisoning robustness empirically\nthrough the poisoning overfitting phenomenon, which is the key underlying\nmechanism for the empirical poisoning robustness of aggregations. Overall, our\nfindings provide valuable insights for practical implementations of aggregation\ndefenses to mitigate the threat of data poisoning.",
        "translated": ""
    },
    {
        "title": "The Power of Telemetry: Uncovering Software-Based Side-Channel Attacks\n  on Apple M1/M2 Systems",
        "url": "http://arxiv.org/abs/2306.16391v1",
        "pub_date": "2023-06-28",
        "summary": "Power analysis is a class of side-channel attacks, where power consumption\ndata is used to infer sensitive information and extract secrets from a system.\nTraditionally, such attacks required physical access to the target, as well as\nspecialized devices to measure the power consumption with enough precision. The\nPLATYPUS attack has shown that on-chip power meter capabilities exposed to a\nsoftware interface might form a new class of power side-channel attacks. This\npaper presents a software-based power side-channel attack on Apple Silicon\nM1/M2 platforms, exploiting the System Management Controller (SMC) and its\npower-related keys, which provides access to the on-chip power meters through a\nsoftware interface to user space software. We observed data-dependent power\nconsumption reporting from such keys and analyzed the correlations between the\npower consumption and the processed data. Our work also demonstrated how an\nunprivileged user mode application successfully recovers bytes from an AES\nencryption key from a cryptographic service supported by a kernel mode driver\nin macOS. Furthermore, we discuss the impact of software-based power\nside-channels in the industry, possible countermeasures, and the overall\nimplications of software interfaces for modern on-chip power management\nsystems.",
        "translated": ""
    },
    {
        "title": "Seeing is Believing: Detecting Sybil Attack in FANET by Matching Visual\n  and Auditory Domains",
        "url": "http://arxiv.org/abs/2306.16339v1",
        "pub_date": "2023-06-28",
        "summary": "The flying ad hoc network (FANET) will play a crucial role in the B5G/6G era\nsince it provides wide coverage and on-demand deployment services in a\ndistributed manner. The detection of Sybil attacks is essential to ensure\ntrusted communication in FANET. Nevertheless, the conventional methods only\nutilize the untrusted information that UAV nodes passively ``heard'' from the\n``auditory\" domain (AD), resulting in severe communication disruptions and even\ncollision accidents. In this paper, we present a novel VA-matching solution\nthat matches the neighbors observed from both the AD and the ``visual'' domain\n(VD), which is the first solution that enables UAVs to accurately correlate\nwhat they ``see'' from VD and ``hear'' from AD to detect the Sybil attacks.\nRelative entropy is utilized to describe the similarity of observed\ncharacteristics from dual domains. The dynamic weight algorithm is proposed to\ndistinguish neighbors according to the characteristics' popularity. The\nmatching model of neighbors observed from AD and VD is established and solved\nby the vampire bat optimizer. Experiment results show that the proposed\nVA-matching solution removes the unreliability of individual characteristics\nand single domains. It significantly outperforms the conventional RSSI-based\nmethod in detecting Sybil attacks. Furthermore, it has strong robustness and\nachieves high precision and recall rates.",
        "translated": ""
    },
    {
        "title": "Boost: Effective Caching in Differentially-Private Databases",
        "url": "http://arxiv.org/abs/2306.16163v1",
        "pub_date": "2023-06-28",
        "summary": "Differentially private (DP) databases can enable privacy-preserving analytics\nover datasets or data streams containing sensitive personal records. In such\nsystems, user privacy is a very limited resource that is consumed by every new\nquery, and hence must be aggressively conserved. We propose Boost, the most\neffective caching component for linear query workloads over DP databases. Boost\nbuilds upon private multiplicative weights (PMW), a DP mechanism that is\npowerful in theory but very ineffective in practice, and transforms it into a\nhighly effective caching object, PMW-Bypass, which uses prior-query results\nobtained through an external DP mechanism to train a PMW to answer arbitrary\nfuture linear queries accurately and \"for free\" from a privacy perspective. We\nshow that Boost with PMW-Bypass conserves significantly more budget compared to\nvanilla PMW and simpler cache designs: at least 1.51 - 14.25x improvement in\nexperiments on public Covid19 and CitiBike datasets. Moreover, Boost\nincorporates support for range-query workloads, such as timeseries or streaming\nworkloads, where opportunities exist to further conserve privacy budget through\nDP parallel composition and warm-starting of PMW state. Our work thus\nestablishes both a coherent system design and the theoretical underpinnings for\neffective caching in DP databases.",
        "translated": ""
    },
    {
        "title": "VERTICES: Efficient Two-Party Vertical Federated Linear Model with\n  TTP-aided Secret Sharing",
        "url": "http://arxiv.org/abs/2306.16139v1",
        "pub_date": "2023-06-28",
        "summary": "Vertical Federated Learning (VFL) has emerged as one of the most predominant\napproaches for secure collaborative machine learning where the training data is\npartitioned by features among multiple parties. Most VFL algorithms primarily\nrely on two fundamental privacy-preserving techniques: Homomorphic Encryption\n(HE) and secure Multi-Party Computation (MPC). Though generally considered with\nstronger privacy guarantees, existing general-purpose MPC frameworks suffer\nfrom expensive computation and communication overhead and are inefficient\nespecially under VFL settings. This study centers around MPC-based VFL\nalgorithms and presents a novel approach for two-party vertical federated\nlinear models via an efficient secret sharing (SS) scheme with a trusted\ncoordinator. Our approach can achieve significant acceleration of the training\nprocedure in vertical federated linear models of between 2.5x and 6.6x than\nother existing MPC frameworks under the same security setting.",
        "translated": ""
    },
    {
        "title": "MLSMM: Machine Learning Security Maturity Model",
        "url": "http://arxiv.org/abs/2306.16127v1",
        "pub_date": "2023-06-28",
        "summary": "Assessing the maturity of security practices during the development of\nMachine Learning (ML) based software components has not gotten as much\nattention as traditional software development. In this Blue Sky idea paper, we\npropose an initial Machine Learning Security Maturity Model (MLSMM) which\norganizes security practices along the ML-development lifecycle and, for each,\nestablishes three levels of maturity. We envision MLSMM as a step towards\ncloser collaboration between industry and academia.",
        "translated": ""
    },
    {
        "title": "Retrospective: Flipping Bits in Memory Without Accessing Them: An\n  Experimental Study of DRAM Disturbance Errors",
        "url": "http://arxiv.org/abs/2306.16093v1",
        "pub_date": "2023-06-28",
        "summary": "Our ISCA 2014 paper provided the first scientific and detailed\ncharacterization, analysis, and real-system demonstration of what is now\npopularly known as the RowHammer phenomenon (or vulnerability) in modern\ncommodity DRAM chips, which are used as main memory in almost all modern\ncomputing systems. It experimentally demonstrated that more than 80% of all\nDRAM modules we tested from the three major DRAM vendors were vulnerable to the\nRowHammer read disturbance phenomenon: one can predictably induce bitflips\n(i.e., data corruption) in real DRAM modules by repeatedly accessing a DRAM row\nand thus causing electrical disturbance to physically nearby rows. We showed\nthat a simple unprivileged user-level program induced RowHammer bitflips in\nmultiple real systems and suggested that a security attack can be built using\nthis proof-of-concept to hijack control of the system or cause other harm. To\nsolve the RowHammer problem, our paper examined seven different approaches\n(including a novel probabilistic approach that has very low cost), some of\nwhich influenced or were adopted in different industrial products.\n  Many later works from various research communities examined RowHammer,\nbuilding real security attacks, proposing new defenses, further analyzing the\nproblem at various (e.g., device/circuit, architecture, and system) levels, and\nexploiting RowHammer for various purposes (e.g., to reverse-engineer DRAM\nchips). Industry has worked to mitigate the problem, changing both memory\ncontrollers and DRAM standards/chips. Two major DRAM vendors finally wrote\npapers on the topic in 2023, describing their current approaches to mitigate\nRowHammer. Research &amp; development on RowHammer in both academia &amp; industry\ncontinues to be very active and fascinating.\n  This short retrospective provides a brief analysis of our ISCA 2014 paper and\nits impact.",
        "translated": ""
    },
    {
        "title": "Can Twitter be used to Acquire Reliable Alerts against Novel Cyber\n  Attacks?",
        "url": "http://arxiv.org/abs/2306.16087v1",
        "pub_date": "2023-06-28",
        "summary": "Time-relevant and accurate threat information from public domains are\nessential for cyber security. In a constantly evolving threat landscape, such\ninformation assists security researchers in thwarting attack strategies. In\nthis work, we collect and analyze threat-related information from Twitter to\nextract intelligence for proactive security. We first use a convolutional\nneural network to classify the tweets as containing or not valuable threat\nindicators. In particular, to gather threat intelligence from social media, the\nproposed approach collects pertinent Indicators of Compromise (IoCs) from\ntweets, such as IP addresses, URLs, File hashes, domain addresses, and CVE IDs.\nThen, we analyze the IoCs to confirm whether they are reliable and valuable for\nthreat intelligence using performance indicators, such as correctness,\ntimeliness, and overlap. We also evaluate how fast Twitter shares IoCs compared\nto existing threat intelligence services. Furthermore, through machine learning\nmodels, we classify Twitter accounts as either automated or human-operated and\ndelve into the role of bot accounts in disseminating cyber threat information\non social media. Our results demonstrate that Twitter is growing into a\npowerful platform for gathering precise and pertinent malware IoCs and a\nreliable source for mining threat intelligence.",
        "translated": ""
    },
    {
        "title": "Fast and Frobenius: Rational Isogeny Evaluation over Finite Fields",
        "url": "http://arxiv.org/abs/2306.16072v1",
        "pub_date": "2023-06-28",
        "summary": "Consider the problem of efficiently evaluating isogenies $\\phi: E \\to E/H$ of\nelliptic curves over a finite field $\\mathbb{F}_q$, where the kernel $H =\n\\langle G\\rangle$ is a cyclic group of odd (prime) order: given $E$, $G$, and a\npoint (or several points) $P$ on $E$, we want to compute $\\phi(P)$. This\nproblem is at the heart of efficient implementations of group-action- and\nisogeny-based post-quantum cryptosystems such as CSIDH. Algorithms based on\nV{\\'e}lu's formulae give an efficient solution to this problem when the kernel\ngenerator $G$ is defined over $\\mathbb{F}_q$. However, for general isogenies,\n$G$ is only defined over some extension $\\mathbb{F}_{q^k}$, even though\n$\\langle G\\rangle$ as a whole (and thus $\\phi$) is defined over the base field\n$\\mathbb{F}_q$; and the performance of V{\\'e}lu-style algorithms degrades\nrapidly as $k$ grows. In this article we revisit the isogeny-evaluation problem\nwith a special focus on the case where $1 \\le k \\le 12$. We improve\nV{\\'e}lu-style isogeny evaluation for many cases where $k = 1$ using special\naddition chains, and combine this with the action of Galois to give greater\nimprovements when $k &gt; 1$.",
        "translated": ""
    },
    {
        "title": "Enrollment-stage Backdoor Attacks on Speaker Recognition Systems via\n  Adversarial Ultrasound",
        "url": "http://arxiv.org/abs/2306.16022v1",
        "pub_date": "2023-06-28",
        "summary": "Automatic Speaker Recognition Systems (SRSs) have been widely used in voice\napplications for personal identification and access control. A typical SRS\nconsists of three stages, i.e., training, enrollment, and recognition. Previous\nwork has revealed that SRSs can be bypassed by backdoor attacks at the training\nstage or by adversarial example attacks at the recognition stage. In this\npaper, we propose TUNER, a new type of backdoor attack against the enrollment\nstage of SRS via adversarial ultrasound modulation, which is inaudible,\nsynchronization-free, content-independent, and black-box. Our key idea is to\nfirst inject the backdoor into the SRS with modulated ultrasound when a\nlegitimate user initiates the enrollment, and afterward, the polluted SRS will\ngrant access to both the legitimate user and the adversary with high\nconfidence. Our attack faces a major challenge of unpredictable user\narticulation at the enrollment stage. To overcome this challenge, we generate\nthe ultrasonic backdoor by augmenting the optimization process with random\nspeech content, vocalizing time, and volume of the user. Furthermore, to\nachieve real-world robustness, we improve the ultrasonic signal over\ntraditional methods using sparse frequency points, pre-compensation, and\nsingle-sideband (SSB) modulation. We extensively evaluate TUNER on two common\ndatasets and seven representative SRS models. Results show that our attack can\nsuccessfully bypass speaker recognition systems while remaining robust to\nvarious speakers, speech content, et",
        "translated": ""
    },
    {
        "title": "ItyFuzz: Snapshot-Based Fuzzer for Smart Contract",
        "url": "http://arxiv.org/abs/2306.17135v1",
        "pub_date": "2023-06-29",
        "summary": "Smart contracts are critical financial instruments, and their security is of\nutmost importance. However, smart contract programs are difficult to fuzz due\nto the persistent blockchain state behind all transactions. Mutating sequences\nof transactions are complex and often lead to a suboptimal exploration for both\ninput and program spaces. In this paper, we introduce a novel snapshot-based\nfuzzer ItyFuzz for testing smart contracts. In ItyFuzz, instead of storing\nsequences of transactions and mutating from them, we snapshot states and\nsingleton transactions. To explore interesting states, ItyFuzz introduces a\ndataflow waypoint mechanism to identify states with more potential momentum.\nItyFuzz also incorporates comparison waypoints to prune the space of states. By\nmaintaining snapshots of the states, ItyFuzz can synthesize concrete exploits\nlike reentrancy attacks quickly. Because ItyFuzz has second-level response time\nto test a smart contract, it can be used for on-chain testing, which has many\nbenefits compared to local development testing. Finally, we evaluate ItyFuzz on\nreal-world smart contracts and some hacked on-chain DeFi projects. ItyFuzz\noutperforms existing fuzzers in terms of instructional coverage and can find\nand generate realistic exploits for on-chain projects quickly.",
        "translated": ""
    },
    {
        "title": "Honesty is the Best Policy: On the Accuracy of Apple Privacy Labels\n  Compared to Apps' Privacy Policies",
        "url": "http://arxiv.org/abs/2306.17063v1",
        "pub_date": "2023-06-29",
        "summary": "Apple introduced \\textit{privacy labels} in Dec. 2020 as a way for developers\nto report the privacy behaviors of their apps. While Apple does not validate\nlabels, they do also require developers to provide a privacy policy, which\noffers an important comparison point. In this paper, we applied the NLP\nframework of Polisis to extract features of the privacy policy for 515,920 apps\non the iOS App Store comparing the output to the privacy labels. We identify\ndiscrepancies between the policies and the labels, particularly as it relates\nto data collected that is linked to users. We find that 287$\\pm196$K apps'\nprivacy policies may indicate data collection that is linked to users than what\nis reported in the privacy labels. More alarming, a large number of\n(97$\\pm30$\\%) of the apps that have {\\em Data Not Collected} privacy label have\na privacy policy that indicates otherwise. We provide insights into potential\nsources for discrepancies, including the use of templates and confusion around\nApple's definitions and requirements. These results suggest that there is still\nsignificant work to be done to help developers more accurately labeling their\napps. Incorporating a Polisis-like system as a first-order check can help\nimprove the current state and better inform developers when there are possible\nmisapplication of privacy labels.",
        "translated": ""
    },
    {
        "title": "RowPress: Amplifying Read Disturbance in Modern DRAM Chips",
        "url": "http://arxiv.org/abs/2306.17061v1",
        "pub_date": "2023-06-29",
        "summary": "Memory isolation is critical for system reliability, security, and safety.\nUnfortunately, read disturbance can break memory isolation in modern DRAM\nchips. For example, RowHammer is a well-studied read-disturb phenomenon where\nrepeatedly opening and closing (i.e., hammering) a DRAM row many times causes\nbitflips in physically nearby rows.\n  This paper experimentally demonstrates and analyzes another widespread\nread-disturb phenomenon, RowPress, in real DDR4 DRAM chips. RowPress breaks\nmemory isolation by keeping a DRAM row open for a long period of time, which\ndisturbs physically nearby rows enough to cause bitflips. We show that RowPress\namplifies DRAM's vulnerability to read-disturb attacks by significantly\nreducing the number of row activations needed to induce a bitflip by one to two\norders of magnitude under realistic conditions. In extreme cases, RowPress\ninduces bitflips in a DRAM row when an adjacent row is activated only once. Our\ndetailed characterization of 164 real DDR4 DRAM chips shows that RowPress 1)\naffects chips from all three major DRAM manufacturers, 2) gets worse as DRAM\ntechnology scales down to smaller node sizes, and 3) affects a different set of\nDRAM cells from RowHammer and behaves differently from RowHammer as temperature\nand access pattern changes.\n  We demonstrate in a real DDR4-based system with RowHammer protection that 1)\na user-level program induces bitflips by leveraging RowPress while conventional\nRowHammer cannot do so, and 2) a memory controller that adaptively keeps the\nDRAM row open for a longer period of time based on access pattern can\nfacilitate RowPress-based attacks. To prevent bitflips due to RowPress, we\ndescribe and evaluate a new methodology that adapts existing RowHammer\nmitigation techniques to also mitigate RowPress with low additional performance\noverhead. We open source all our code and data to facilitate future research on\nRowPress.",
        "translated": ""
    },
    {
        "title": "Would Friedman Burn your Tokens?",
        "url": "http://arxiv.org/abs/2306.17025v1",
        "pub_date": "2023-06-29",
        "summary": "Cryptocurrencies come with a variety of tokenomic policies as well as\naspirations of desirable monetary characteristics that have been described by\nproponents as 'sound money' or even 'ultra sound money.' These propositions are\ntypically devoid of economic analysis so it is a pertinent question how such\naspirations fit in the wider context of monetary economic theory. In this work,\nwe develop a framework that determines the optimal token supply policy of a\ncryptocurrency, as well as investigate how such policy may be algorithmically\nimplemented. Our findings suggest that the optimal policy complies with the\nFriedman rule and it is dependent on the risk free rate, as well as the growth\nof the cryptocurrency platform. Furthermore, we demonstrate a wide set of\nconditions under which such policy can be implemented via contractions and\nexpansions of token supply that can be realized algorithmically with block\nrewards, taxation of consumption and burning the proceeds, and blockchain\noracles.",
        "translated": ""
    },
    {
        "title": "VibHead: An Authentication Scheme for Smart Headsets through Vibration",
        "url": "http://arxiv.org/abs/2306.17002v1",
        "pub_date": "2023-06-29",
        "summary": "Recent years have witnessed the fast penetration of Virtual Reality (VR) and\nAugmented Reality (AR) systems into our daily life, the security and privacy\nissues of the VR/AR applications have been attracting considerable attention.\nMost VR/AR systems adopt head-mounted devices (i.e., smart headsets) to\ninteract with users and the devices usually store the users' private data.\nHence, authentication schemes are desired for the head-mounted devices.\nTraditional knowledge-based authentication schemes for general personal devices\nhave been proved vulnerable to shoulder-surfing attacks, especially considering\nthe headsets may block the sight of the users. Although the robustness of the\nknowledge-based authentication can be improved by designing complicated secret\ncodes in virtual space, this approach induces a compromise of usability.\nAnother choice is to leverage the users' biometrics; however, it either relies\non highly advanced equipments which may not always be available in commercial\nheadsets or introduce heavy cognitive load to users.\n  In this paper, we propose a vibration-based authentication scheme, VibHead,\nfor smart headsets. Since the propagation of vibration signals through human\nheads presents unique patterns for different individuals, VibHead employs a\nCNN-based model to classify registered legitimate users based the features\nextracted from the vibration signals. We also design a two-step authentication\nscheme where the above user classifiers are utilized to distinguish the\nlegitimate user from illegitimate ones. We implement VibHead on a Microsoft\nHoloLens equipped with a linear motor and an IMU sensor which are commonly used\nin off-the-shelf personal smart devices. According to the results of our\nextensive experiments, with short vibration signals ($\\leq 1s$), VibHead has an\noutstanding authentication accuracy; both FAR and FRR are around 5%.",
        "translated": ""
    },
    {
        "title": "Defending Black-box Classifiers by Bayesian Boundary Correction",
        "url": "http://arxiv.org/abs/2306.16979v1",
        "pub_date": "2023-06-29",
        "summary": "Classifiers based on deep neural networks have been recently challenged by\nAdversarial Attack, where the widely existing vulnerability has invoked the\nresearch in defending them from potential threats. Given a vulnerable\nclassifier, existing defense methods are mostly white-box and often require\nre-training the victim under modified loss functions/training regimes. While\nthe model/data/training specifics of the victim are usually unavailable to the\nuser, re-training is unappealing, if not impossible for reasons such as limited\ncomputational resources. To this end, we propose a new black-box defense\nframework. It can turn any pre-trained classifier into a resilient one with\nlittle knowledge of the model specifics. This is achieved by new joint Bayesian\ntreatments on the clean data, the adversarial examples and the classifier, for\nmaximizing their joint probability. It is further equipped with a new\npost-train strategy which keeps the victim intact. We name our framework\nBayesian Boundary Correction (BBC). BBC is a general and flexible framework\nthat can easily adapt to different data types. We instantiate BBC for image\nclassification and skeleton-based human activity recognition, for both static\nand dynamic data. Exhaustive evaluation shows that BBC has superior robustness\nand can enhance robustness without severely hurting the clean accuracy,\ncompared with existing defense methods.",
        "translated": ""
    },
    {
        "title": "SWAT: A System-Wide Approach to Tunable Leakage Mitigation in Encrypted\n  Data Stores",
        "url": "http://arxiv.org/abs/2306.16851v1",
        "pub_date": "2023-06-29",
        "summary": "Numerous studies have underscored the significant privacy risks associated\nwith various leakage patterns in encrypted data stores. Most existing systems\nthat conceal leakage either (1) incur substantial overheads, (2) focus on\nspecific subsets of leakage patterns, or (3) apply the same security notion\nacross various workloads, thereby impeding the attainment of fine-tuned\nprivacy-efficiency trade-offs. In light of various detrimental leakage\npatterns, this paper starts with an investigation into which specific leakage\npatterns require our focus respectively in the contexts of key-value,\nrange-query, and dynamic workloads. Subsequently, we introduce new security\nnotions tailored to the specific privacy requirements of these workloads.\nAccordingly, we present, SWAT, an efficient construction that progressively\nenables these workloads, while provably mitigating system-wide leakage via a\nsuite of algorithms with tunable privacy-efficiency trade-offs. We conducted\nextensive experiments and compiled a detailed result analysis, showing the\nefficiency of our solution. SWAT is about $10.6\\times$ slower than an\nencryption-only data store that reveals various leakage patterns and is\n$31.6\\times$ faster than a trivially zero-leakage solution. Meanwhile, the\nperformance of SWAT remains highly competitive compared to other designs that\nmitigate specific types of leakage.",
        "translated": ""
    },
    {
        "title": "Towards Optimal Randomized Strategies in Adversarial Example Game",
        "url": "http://arxiv.org/abs/2306.16738v1",
        "pub_date": "2023-06-29",
        "summary": "The vulnerability of deep neural network models to adversarial example\nattacks is a practical challenge in many artificial intelligence applications.\nA recent line of work shows that the use of randomization in adversarial\ntraining is the key to find optimal strategies against adversarial example\nattacks. However, in a fully randomized setting where both the defender and the\nattacker can use randomized strategies, there are no efficient algorithm for\nfinding such an optimal strategy. To fill the gap, we propose the first\nalgorithm of its kind, called FRAT, which models the problem with a new\ninfinite-dimensional continuous-time flow on probability distribution spaces.\nFRAT maintains a lightweight mixture of models for the defender, with\nflexibility to efficiently update mixing weights and model parameters at each\niteration. Furthermore, FRAT utilizes lightweight sampling subroutines to\nconstruct a random strategy for the attacker. We prove that the continuous-time\nlimit of FRAT converges to a mixed Nash equilibria in a zero-sum game formed by\na defender and an attacker. Experimental results also demonstrate the\nefficiency of FRAT on CIFAR-10 and CIFAR-100 datasets.",
        "translated": ""
    },
    {
        "title": "TrojanNet: Detecting Trojans in Quantum Circuits using Machine Learning",
        "url": "http://arxiv.org/abs/2306.16701v1",
        "pub_date": "2023-06-29",
        "summary": "Quantum computing holds tremendous potential for various applications, but\nits security remains a crucial concern. Quantum circuits need high-quality\ncompilers to optimize the depth and gate count to boost the success probability\non current noisy quantum computers. There is a rise of efficient but\nunreliable/untrusted compilers; however, they present a risk of tampering such\nas Trojan insertion. We propose TrojanNet, a novel approach to enhance the\nsecurity of quantum circuits by detecting and classifying Trojan-inserted\ncircuits. In particular, we focus on the Quantum Approximate Optimization\nAlgorithm (QAOA) circuit that is popular in solving a wide range of\noptimization problems. We investigate the impact of Trojan insertion on QAOA\ncircuits and develop a Convolutional Neural Network (CNN) model, referred to as\nTrojanNet, to identify their presence accurately. Using the Qiskit framework,\nwe generate 12 diverse datasets by introducing variations in Trojan gate types,\nthe number of gates, insertion locations, and compiler backends. These datasets\nconsist of both original Trojan-free QAOA circuits and their corresponding\nTrojan-inserted counterparts. The generated datasets are then utilized for\ntraining and evaluating the TrojanNet model. Experimental results showcase an\naverage accuracy of 98.80% and an average F1-score of 98.53% in effectively\ndetecting and classifying Trojan-inserted QAOA circuits. Finally, we conduct a\nperformance comparison between TrojanNet and existing machine learning-based\nTrojan detection methods specifically designed for conventional netlists.",
        "translated": ""
    },
    {
        "title": "Neural Polarizer: A Lightweight and Effective Backdoor Defense via\n  Purifying Poisoned Features",
        "url": "http://arxiv.org/abs/2306.16697v1",
        "pub_date": "2023-06-29",
        "summary": "Recent studies have demonstrated the susceptibility of deep neural networks\nto backdoor attacks. Given a backdoored model, its prediction of a poisoned\nsample with trigger will be dominated by the trigger information, though\ntrigger information and benign information coexist. Inspired by the mechanism\nof the optical polarizer that a polarizer could pass light waves with\nparticular polarizations while filtering light waves with other polarizations,\nwe propose a novel backdoor defense method by inserting a learnable neural\npolarizer into the backdoored model as an intermediate layer, in order to\npurify the poisoned sample via filtering trigger information while maintaining\nbenign information. The neural polarizer is instantiated as one lightweight\nlinear transformation layer, which is learned through solving a well designed\nbi-level optimization problem, based on a limited clean dataset. Compared to\nother fine-tuning-based defense methods which often adjust all parameters of\nthe backdoored model, the proposed method only needs to learn one additional\nlayer, such that it is more efficient and requires less clean data. Extensive\nexperiments demonstrate the effectiveness and efficiency of our method in\nremoving backdoors across various neural network architectures and datasets,\nespecially in the case of very limited clean data.",
        "translated": ""
    },
    {
        "title": "Vision Through the Veil: Differential Privacy in Federated Learning for\n  Medical Image Classification",
        "url": "http://arxiv.org/abs/2306.17794v1",
        "pub_date": "2023-06-30",
        "summary": "The proliferation of deep learning applications in healthcare calls for data\naggregation across various institutions, a practice often associated with\nsignificant privacy concerns. This concern intensifies in medical image\nanalysis, where privacy-preserving mechanisms are paramount due to the data\nbeing sensitive in nature. Federated learning, which enables cooperative model\ntraining without direct data exchange, presents a promising solution.\nNevertheless, the inherent vulnerabilities of federated learning necessitate\nfurther privacy safeguards. This study addresses this need by integrating\ndifferential privacy, a leading privacy-preserving technique, into a federated\nlearning framework for medical image classification. We introduce a novel\ndifferentially private federated learning model and meticulously examine its\nimpacts on privacy preservation and model performance. Our research confirms\nthe existence of a trade-off between model accuracy and privacy settings.\nHowever, we demonstrate that strategic calibration of the privacy budget in\ndifferential privacy can uphold robust image classification performance while\nproviding substantial privacy protection.",
        "translated": ""
    },
    {
        "title": "Beyond Neural-on-Neural Approaches to Speaker Gender Protection",
        "url": "http://arxiv.org/abs/2306.17700v1",
        "pub_date": "2023-06-30",
        "summary": "Recent research has proposed approaches that modify speech to defend against\ngender inference attacks. The goal of these protection algorithms is to control\nthe availability of information about a speaker's gender, a privacy-sensitive\nattribute. Currently, the common practice for developing and testing gender\nprotection algorithms is \"neural-on-neural\", i.e., perturbations are generated\nand tested with a neural network. In this paper, we propose to go beyond this\npractice to strengthen the study of gender protection. First, we demonstrate\nthe importance of testing gender inference attacks that are based on speech\nfeatures historically developed by speech scientists, alongside the\nconventionally used neural classifiers. Next, we argue that researchers should\nuse speech features to gain insight into how protective modifications change\nthe speech signal. Finally, we point out that gender-protection algorithms\nshould be compared with novel \"vocal adversaries\", human-executed voice\nadaptations, in order to improve interpretability and enable before-the-mic\nprotection.",
        "translated": ""
    },
    {
        "title": "A Quic(k) Security Overview: A Literature Research on Implemented\n  Security Recommendations",
        "url": "http://arxiv.org/abs/2306.17568v1",
        "pub_date": "2023-06-30",
        "summary": "Built on top of UDP, the relatively new QUIC protocol serves as the baseline\nfor modern web protocol stacks. Equipped with a rich feature set, the protocol\nis defined by a 151 pages strong IETF standard complemented by several\nadditional documents. Enabling fast updates and feature iteration, most QUIC\nimplementations are implemented as user space libraries leading to a large and\nfragmented ecosystem. This work addresses the research question, \"if a complex\nstandard with a large number of different implementations leads to an insecure\necosystem?\". The relevant RFC documents were studied and \"Security\nConsideration\" items describing conceptional problems were extracted. During\nthe research, 13 popular production ready QUIC implementations were compared by\nevaluating 10 security considerations from RFC9000. While related studies\nmostly focused on the functional part of QUIC, this study confirms that\navailable QUIC implementations are not yet mature enough from a security point\nof view.",
        "translated": ""
    },
    {
        "title": "Research on Virus Cyberattack-Defense Based on Electromagnetic Radiation",
        "url": "http://arxiv.org/abs/2306.17508v1",
        "pub_date": "2023-06-30",
        "summary": "Information technology and telecommunications have rapidly permeated various\ndomains, resulting in a significant influx of data traversing the networks\nbetween computers. Consequently, research of cyberattacks in computer systems\nhas become crucial for many organizations. Accordingly, recent cybersecurity\nincidents have underscored the rapidly evolving nature of future threats and\nattack methods, particularly those involving computer viruses wireless\ninjection. This paper aims to study and demonstrate the feasibility of remote\ncomputer virus radiation injection. To achieve this objective, digital signal\nprocessing (DSP) plays a vital role. By studying the principles and models of\nradiation attacks and computer virus propagation, the modulation of the binary\ndata stream of the simulated virus into a terahertz radar carrier signal by\nPhase-Shift Keying (PSK) is simulated, enabling the implementation of an attack\nthrough the \"field to line\" coupling of electromagnetic signals. Finally, the\ndefense and countermeasures based on signal recognition are discussed for such\nattacks. Additionally, an idea of establishing a virus library for cyberattack\nsignals and employing artificial intelligence (AI) algorithms for automated\nintrusion detection is proposed as a means to achieve cybersecurity situation\nawareness.",
        "translated": ""
    },
    {
        "title": "An ontological approach to compliance verification of the NIS 2\n  directive",
        "url": "http://arxiv.org/abs/2306.17494v1",
        "pub_date": "2023-06-30",
        "summary": "Cybersecurity, which notoriously concerns both human and technological\naspects, is becoming more and more regulated by a number of textual documents\nspanning several pages, such as the European GDPR Regulation and the NIS\nDirective. This paper introduces an approach that leverages techniques of\nsemantic representation and reasoning, hence an ontological approach, towards\nthe compliance check with the security measures that textual documents\nprescribe. We choose the ontology instrument to achieve two fundamental\nobjectives: domain modelling and resource interrogation. The formalisation of\nentities and relations from the directive, and the consequent improved\nstructuring with respect to sheer prose is dramatically helpful for any\norganisation through the hard task of compliance verification. The semantic\napproach is demonstrated with two articles of the new European NIS 2 directive.",
        "translated": ""
    },
    {
        "title": "Differential Privacy May Have a Potential Optimization Effect on Some\n  Swarm Intelligence Algorithms besides Privacy-preserving",
        "url": "http://arxiv.org/abs/2306.17370v1",
        "pub_date": "2023-06-30",
        "summary": "Differential privacy (DP), as a promising privacy-preserving model, has\nattracted great interest from researchers in recent years. Currently, the study\non combination of machine learning and DP is vibrant. In contrast, another\nwidely used artificial intelligence technique, the swarm intelligence (SI)\nalgorithm, has received little attention in the context of DP even though it\nalso triggers privacy concerns. For this reason, this paper attempts to combine\nDP and SI for the first time, and proposes a general differentially private\nswarm intelligence algorithm framework (DPSIAF). Based on the exponential\nmechanism, this framework can easily develop existing SI algorithms into the\nprivate versions. As examples, we apply the proposed DPSIAF to four popular SI\nalgorithms, and corresponding analyses demonstrate its effectiveness. More\ninterestingly, the experimental results show that, for our private algorithms,\ntheir performance is not strictly affected by the privacy budget, and one of\nthe private algorithms even owns better performance than its non-private\nversion in some cases. These findings are different from the conventional\ncognition, which indicates the uniqueness of SI with DP. Our study may provide\na new perspective on DP, and promote the synergy between metaheuristic\noptimization community and privacy computing community.",
        "translated": ""
    },
    {
        "title": "Secret-Free Device Pairing in the mmWave Band",
        "url": "http://arxiv.org/abs/2306.17330v1",
        "pub_date": "2023-06-29",
        "summary": "Many Next Generation (NextG) applications feature devices that are capable of\ncommunicating and sensing in the Millimeter-Wave (mmWave) bands. Trust\nestablishment is an important first step to bootstrap secure mmWave\ncommunication links, which is challenging due to the lack of prior secrets and\nthe fact that traditional cryptographic authentication methods cannot bind\ndigital trust with physical properties. Previously, context-based device\npairing approaches were proposed to extract shared secrets from common context,\nusing various sensing modalities. However, they suffer from various limitations\nin practicality and security.\n  In this work, we propose the first secret-free device pairing scheme in the\nmmWave band that explores the unique physical-layer properties of mmWave\ncommunications. Our basic idea is to let Alice and Bob derive common randomness\nby sampling physical activity in the surrounding environment that disturbs\ntheir wireless channel. They construct reliable fingerprints of the activity by\nextracting event timing information from the channel state. We further propose\nan uncoordinated path hopping mechanism to resolve the challenges of beam\nalignment for activity sensing without prior trust. A key novelty of our\nprotocol is that it remains secure against both co-located passive adversaries\nand active Man-in-the-Middle attacks, which is not possible with existing\ncontext-based pairing approaches. We implement our protocol in a 28GHz mmWave\ntestbed, and experimentally evaluate its security in realistic indoor\nenvironments. Results show that our protocol can effectively thwart several\ndifferent types of adversaries.",
        "translated": ""
    },
    {
        "title": "Patient-centric health data sovereignty: an approach using Proxy\n  re-encryption",
        "url": "http://arxiv.org/abs/2307.01175v1",
        "pub_date": "2023-07-03",
        "summary": "The exponential growth in the digitisation of services implies the handling\nand storage of large volumes of data. Businesses and services see data sharing\nand crossing as an opportunity to improve and produce new business\nopportunities. The health sector is one area where this proves to be true,\nenabling better and more innovative treatments. Notwithstanding, this raises\nconcerns regarding personal data being treated and processed. In this paper, we\npresent a patient-centric platform for the secure sharing of health records by\nshifting the control over the data to the patient, therefore, providing a step\nfurther towards data sovereignty. Data sharing is performed only with the\nconsent of the patient, allowing it to revoke access at any given time.\nFurthermore, we also provide a break-glass approach, resorting to Proxy\nRe-encryption (PRE) and the concept of a centralised trusted entity that\npossesses instant access to patients' medical records. Lastly, an analysis is\nmade to assess the performance of the platform's key operations, and the impact\nthat a PRE scheme has on those operations.",
        "translated": ""
    },
    {
        "title": "Passive Query-Recovery Attack Against Secure Conjunctive Keyword Search\n  Schemes",
        "url": "http://arxiv.org/abs/2307.01131v1",
        "pub_date": "2023-07-03",
        "summary": "While storing documents on the cloud can be attractive, the question remains\nwhether cloud providers can be trusted with storing private documents. Even if\ntrusted, data breaches are ubiquitous. To prevent information leakage one can\nstore documents encrypted. If encrypted under traditional schemes, one loses\nthe ability to perform simple operations over the documents, such as searching\nthrough them. Searchable encryption schemes were proposed allowing some search\nfunctionality while documents remain encrypted. Orthogonally, research is done\nto find attacks that exploit search and access pattern leakage that most\nefficient schemes have. One type of such an attack is the ability to recover\nplaintext queries. Passive query-recovery attacks on single-keyword search\nschemes have been proposed in literature, however, conjunctive keyword search\nhas not been considered, although keyword searches with two or three keywords\nappear more frequently in online searches.\n  We introduce a generic extension strategy for existing passive query-recovery\nattacks against single-keyword search schemes and explore its applicability for\nthe attack presented by Damie et al. (USENIX Security '21). While the original\nattack achieves up to a recovery rate of 85% against single-keyword search\nschemes for an attacker without exact background knowledge, our experiments\nshow that the generic extension to conjunctive queries comes with a significant\nperformance decrease achieving recovery rates of at most 32%. Assuming a\nstronger attacker with partial knowledge of the indexed document set boosts the\nrecovery rate to 85% for conjunctive keyword queries with two keywords and\nachieves similar recovery rates as previous attacks by Cash et al. (CCS '15)\nand Islam et al. (NDSS '12) in the same setting for single-keyword search\nschemes.",
        "translated": ""
    },
    {
        "title": "When Can Linear Learners be Robust to Indiscriminate Poisoning Attacks?",
        "url": "http://arxiv.org/abs/2307.01073v1",
        "pub_date": "2023-07-03",
        "summary": "We study indiscriminate poisoning for linear learners where an adversary\ninjects a few crafted examples into the training data with the goal of forcing\nthe induced model to incur higher test error. Inspired by the observation that\nlinear learners on some datasets are able to resist the best known attacks even\nwithout any defenses, we further investigate whether datasets can be inherently\nrobust to indiscriminate poisoning attacks for linear learners. For theoretical\nGaussian distributions, we rigorously characterize the behavior of an optimal\npoisoning attack, defined as the poisoning strategy that attains the maximum\nrisk of the induced model at a given poisoning budget. Our results prove that\nlinear learners can indeed be robust to indiscriminate poisoning if the\nclass-wise data distributions are well-separated with low variance and the size\nof the constraint set containing all permissible poisoning points is also\nsmall. These findings largely explain the drastic variation in empirical attack\nperformance of the state-of-the-art poisoning attacks on linear learners across\nbenchmark datasets, making an important initial step towards understanding the\nunderlying reasons some learning tasks are vulnerable to data poisoning\nattacks.",
        "translated": ""
    },
    {
        "title": "Practical Non-Invasive Probing Attacks Against Novel\n  Carbon-Nanotube-Based Physical Unclonable Functions",
        "url": "http://arxiv.org/abs/2307.01041v1",
        "pub_date": "2023-07-03",
        "summary": "As the number of devices being interconnected increases, so does also the\ndemand for (lightweight) security. To this end, Physical Unclonable Functions\n(PUFs) have been proposed as hardware primitives that can act as roots of trust\nand security. Recently, a new type of PUF based on Carbon NanoTubes (CNTs) has\nbeen proposed. At the same time, attacks and testing based on direct electrical\nprobing appear to be moving towards non-invasive techniques. In this context,\nthis work attempts to examine the potential for practical non-invasive probing\nattacks against the CNT-PUF, a novel PUF based on CNTs. Our results indicate\nthat direct probing might potentially compromise the security of this PUF.\nNevertheless, we note that this holds true only in the case that the attacker\ncan directly probe the wire corresponding to the secret value of each CNT-PUF\ncell. Thus, we can conclude that the examined CNT-PUFs are rather resilient to\ndirect probing attacks, that non-invasive probing methods appear to be\npromising for testing such PUFs, and that, in order for the attacker to gain\nthe full-length value of the secret, all the relevant channels would need to be\nprobed. Nevertheless, as our work proves, practical non-invasive attacks\nagainst the CNT-PUF are feasible and adequate countermeasures need to be\nemployed in order to address this issue.",
        "translated": ""
    },
    {
        "title": "Enhancing the Robustness of QMIX against State-adversarial Attacks",
        "url": "http://arxiv.org/abs/2307.00907v1",
        "pub_date": "2023-07-03",
        "summary": "Deep reinforcement learning (DRL) performance is generally impacted by\nstate-adversarial attacks, a perturbation applied to an agent's observation.\nMost recent research has concentrated on robust single-agent reinforcement\nlearning (SARL) algorithms against state-adversarial attacks. Still, there has\nyet to be much work on robust multi-agent reinforcement learning. Using QMIX,\none of the popular cooperative multi-agent reinforcement algorithms, as an\nexample, we discuss four techniques to improve the robustness of SARL\nalgorithms and extend them to multi-agent scenarios. To increase the robustness\nof multi-agent reinforcement learning (MARL) algorithms, we train models using\na variety of attacks in this research. We then test the models taught using the\nother attacks by subjecting them to the corresponding attacks throughout the\ntraining phase. In this way, we organize and summarize techniques for enhancing\nrobustness when used with MARL.",
        "translated": ""
    },
    {
        "title": "Tales from the Git: Automating the detection of secrets on code and\n  assessing developers' passwords choices",
        "url": "http://arxiv.org/abs/2307.00892v1",
        "pub_date": "2023-07-03",
        "summary": "Typical users are known to use and reuse weak passwords. Yet, as\ncybersecurity concerns continue to rise, understanding the password practices\nof software developers becomes increasingly important. In this work, we examine\ndevelopers' passwords on public repositories. Our dedicated crawler collected\nmillions of passwords from public GitHub repositories; however, our focus is on\ntheir unique characteristics. To this end, this is the first study\ninvestigating the developer traits in password selection across different\nprogramming languages and contexts, e.g. email and database. Despite the fact\nthat developers may have carelessly leaked their code on public repositories,\nour findings indicate that they tend to use significantly more secure\npasswords, regardless of the underlying programming language and context.\nNevertheless, when the context allows, they often resort to similar password\nselection criteria as typical users. The public availability of such\ninformation in a cleartext format indicates that there is still much room for\nimprovement and that further targeted awareness campaigns are necessary.",
        "translated": ""
    },
    {
        "title": "Cryptography and Key Management Schemes for Wireless Sensor Networks",
        "url": "http://arxiv.org/abs/2307.00872v1",
        "pub_date": "2023-07-03",
        "summary": "Wireless sensor networks (WSNs) are made up of a large number of tiny\nsensors, which can sense, analyze, and communicate information about the\noutside world. These networks play a significant role in a broad range of\nfields, from crucial military surveillance applications to monitoring building\nsecurity. Key management in WSNs is a critical task. While the security and\nintegrity of messages communicated through these networks and the authenticity\nof the nodes are dependent on the robustness of the key management schemes,\ndesigning an efficient key generation, distribution, and revocation scheme is\nquite challenging. While resource-constrained sensor nodes should not be\nexposed to computationally demanding asymmetric key algorithms, the use of\nsymmetric key-based systems leaves the entire network vulnerable to several\nattacks. This chapter provides a comprehensive survey of several well-known\ncryptographic mechanisms and key management schemes for WSNs.",
        "translated": ""
    },
    {
        "title": "Thompson Sampling under Bernoulli Rewards with Local Differential\n  Privacy",
        "url": "http://arxiv.org/abs/2307.00863v1",
        "pub_date": "2023-07-03",
        "summary": "This paper investigates the problem of regret minimization for multi-armed\nbandit (MAB) problems with local differential privacy (LDP) guarantee. Given a\nfixed privacy budget $\\epsilon$, we consider three privatizing mechanisms under\nBernoulli scenario: linear, quadratic and exponential mechanisms. Under each\nmechanism, we derive stochastic regret bound for Thompson Sampling algorithm.\nFinally, we simulate to illustrate the convergence of different mechanisms\nunder different privacy budgets.",
        "translated": ""
    },
    {
        "title": "A Comparative Study of Software Secrets Reporting by Secret Detection\n  Tools",
        "url": "http://arxiv.org/abs/2307.00714v1",
        "pub_date": "2023-07-03",
        "summary": "Background: According to GitGuardian's monitoring of public GitHub\nrepositories, secrets sprawl continued accelerating in 2022 by 67% compared to\n2021, exposing over 10 million secrets (API keys and other credentials). Though\nmany open-source and proprietary secret detection tools are available, these\ntools output many false positives, making it difficult for developers to take\naction and teams to choose one tool out of many. To our knowledge, the secret\ndetection tools are not yet compared and evaluated. Aims: The goal of our study\nis to aid developers in choosing a secret detection tool to reduce the exposure\nof secrets through an empirical investigation of existing secret detection\ntools. Method: We present an evaluation of five open-source and four\nproprietary tools against a benchmark dataset. Results: The top three tools\nbased on precision are: GitHub Secret Scanner (75%), Gitleaks (46%), and\nCommercial X (25%), and based on recall are: Gitleaks (88%), SpectralOps (67%)\nand TruffleHog (52%). Our manual analysis of reported secrets reveals that\nfalse positives are due to employing generic regular expressions and\nineffective entropy calculation. In contrast, false negatives are due to faulty\nregular expressions, skipping specific file types, and insufficient rulesets.\nConclusions: We recommend developers choose tools based on secret types present\nin their projects to prevent missing secrets. In addition, we recommend tool\nvendors update detection rules periodically and correctly employ secret\nverification mechanisms by collaborating with API vendors to improve accuracy.",
        "translated": ""
    },
    {
        "title": "From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and\n  Privacy",
        "url": "http://arxiv.org/abs/2307.00691v1",
        "pub_date": "2023-07-03",
        "summary": "Undoubtedly, the evolution of Generative AI (GenAI) models has been the\nhighlight of digital transformation in the year 2022. As the different GenAI\nmodels like ChatGPT and Google Bard continue to foster their complexity and\ncapability, it's critical to understand its consequences from a cybersecurity\nperspective. Several instances recently have demonstrated the use of GenAI\ntools in both the defensive and offensive side of cybersecurity, and focusing\non the social, ethical and privacy implications this technology possesses. This\nresearch paper highlights the limitations, challenges, potential risks, and\nopportunities of GenAI in the domain of cybersecurity and privacy. The work\npresents the vulnerabilities of ChatGPT, which can be exploited by malicious\nusers to exfiltrate malicious information bypassing the ethical constraints on\nthe model. This paper demonstrates successful example attacks like Jailbreaks,\nreverse psychology, and prompt injection attacks on the ChatGPT. The paper also\ninvestigates how cyber offenders can use the GenAI tools in developing cyber\nattacks, and explore the scenarios where ChatGPT can be used by adversaries to\ncreate social engineering attacks, phishing attacks, automated hacking, attack\npayload generation, malware creation, and polymorphic malware. This paper then\nexamines defense techniques and uses GenAI tools to improve security measures,\nincluding cyber defense automation, reporting, threat intelligence, secure code\ngeneration and detection, attack identification, developing ethical guidelines,\nincidence response plans, and malware detection. We will also discuss the\nsocial, legal, and ethical implications of ChatGPT. In conclusion, the paper\nhighlights open challenges and future directions to make this GenAI secure,\nsafe, trustworthy, and ethical as the community understands its cybersecurity\nimpacts.",
        "translated": ""
    },
    {
        "title": "Jailbroken: How Does LLM Safety Training Fail?",
        "url": "http://arxiv.org/abs/2307.02483v1",
        "pub_date": "2023-07-05",
        "summary": "Large language models trained for safety and harmlessness remain susceptible\nto adversarial misuse, as evidenced by the prevalence of \"jailbreak\" attacks on\nearly releases of ChatGPT that elicit undesired behavior. Going beyond\nrecognition of the issue, we investigate why such attacks succeed and how they\ncan be created. We hypothesize two failure modes of safety training: competing\nobjectives and mismatched generalization. Competing objectives arise when a\nmodel's capabilities and safety goals conflict, while mismatched generalization\noccurs when safety training fails to generalize to a domain for which\ncapabilities exist. We use these failure modes to guide jailbreak design and\nthen evaluate state-of-the-art models, including OpenAI's GPT-4 and Anthropic's\nClaude v1.3, against both existing and newly designed attacks. We find that\nvulnerabilities persist despite the extensive red-teaming and safety-training\nefforts behind these models. Notably, new attacks utilizing our failure modes\nsucceed on every prompt in a collection of unsafe requests from the models'\nred-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our\nanalysis emphasizes the need for safety-capability parity -- that safety\nmechanisms should be as sophisticated as the underlying model -- and argues\nagainst the idea that scaling alone can resolve these safety failure modes.",
        "translated": ""
    },
    {
        "title": "Vulnerable Source Code Detection using SonarCloud Code Analysis",
        "url": "http://arxiv.org/abs/2307.02446v1",
        "pub_date": "2023-07-05",
        "summary": "In Software Development Life Cycle (SDLC), security vulnerabilities are one\nof the points introduced during the construction stage. Failure to detect\nsoftware defects earlier after releasing the product to the market causes\nhigher repair costs for the company. So, it decreases the company's reputation,\nviolates user privacy, and causes an unrepairable issue for the application.\nThe introduction of vulnerability detection enables reducing the number of\nfalse alerts to focus the limited testing efforts on potentially vulnerable\nfiles. UMKM Masa Kini (UMI) is a Point of Sales application to sell any Micro,\nSmall, and Medium Enterprises Product (UMKM). Therefore, in the current work,\nwe analyze the suitability of these metrics to create Machine Learning based\nsoftware vulnerability detectors for UMI applications. Code is generated using\na commercial tool, SonarCloud. Experimental result shows that there are 3,285\nvulnerable rules detected.",
        "translated": ""
    },
    {
        "title": "DarkHorse: A UDP-based Framework to Improve the Latency of Tor Onion\n  Services",
        "url": "http://arxiv.org/abs/2307.02429v1",
        "pub_date": "2023-07-05",
        "summary": "Tor is the most popular anonymous communication overlay network which hides\nclients' identities from servers by passing packets through multiple relays. To\nprovide anonymity to both clients and servers, Tor onion services were\nintroduced by increasing the number of relays between a client and a server.\nBecause of the limited bandwidth of Tor relays, large numbers of users, and\nmultiple layers of encryption at relays, onion services suffer from high\nend-to-end latency and low data transfer rates, which degrade user experiences,\nmaking onion services unsuitable for latency-sensitive applications. In this\npaper, we present a UDP-based framework, called DarkHorse, that improves the\nend-to-end latency and the data transfer overhead of Tor onion services by\nexploiting the connectionless nature of UDP. Our evaluation results demonstrate\nthat DarkHorse is up to 3.62x faster than regular TCP-based Tor onion services\nand reduces the Tor network overhead by up to 47%.",
        "translated": ""
    },
    {
        "title": "Detecting Images Generated by Deep Diffusion Models using their Local\n  Intrinsic Dimensionality",
        "url": "http://arxiv.org/abs/2307.02347v1",
        "pub_date": "2023-07-05",
        "summary": "Diffusion models recently have been successfully applied for the visual\nsynthesis of strikingly realistic appearing images. This raises strong concerns\nabout their potential for malicious purposes. In this paper, we propose using\nthe lightweight multi Local Intrinsic Dimensionality (multiLID), which has been\noriginally developed in context of the detection of adversarial examples, for\nthe automatic detection of synthetic images and the identification of the\naccording generator networks. In contrast to many existing detection\napproaches, which often only work for GAN-generated images, the proposed method\nprovides close to perfect detection results in many realistic use cases.\nExtensive experiments on known and newly created datasets demonstrate that\nmultiLID exhibits superiority in diffusion detection and model identification.\nSince the empirical evaluations of recent publications on the detection of\ngenerated images is often too focused on the \"LSUN-Bedroom\" dataset, we further\nestablish a comprehensive benchmark for the detection of diffusion-generated\nimages, including samples from several diffusion models with different image\nsizes to evaluate the performance of their multiLID.\n  Code for our experiments is provided at\nhttps://github.com/deepfake-study/deepfake_multiLID.",
        "translated": ""
    },
    {
        "title": "Towards a Formal Verification of the Lightning Network with TLA+",
        "url": "http://arxiv.org/abs/2307.02342v1",
        "pub_date": "2023-07-05",
        "summary": "Payment channel networks are an approach to improve the scalability of\nblockchain-based cryptocurrencies. Because payment channel networks are used\nfor transfer of financial value, their security in the presence of adversarial\nparticipants should be verified formally. We formalize the protocol of the\nLightning Network, a payment channel network built for Bitcoin, and show that\nthe protocol fulfills the expected security properties. As the state space of a\nspecification consisting of multiple participants is too large for model\nchecking, we formalize intermediate specifications and use a chain of\nrefinements to validate the security properties where each refinement is\njustified either by model checking or by a pen-and-paper proof.",
        "translated": ""
    },
    {
        "title": "Fuzzing with Quantitative and Adaptive Hot-Bytes Identification",
        "url": "http://arxiv.org/abs/2307.02289v1",
        "pub_date": "2023-07-05",
        "summary": "Fuzzing has emerged as a powerful technique for finding security bugs in\ncomplicated real-world applications. American fuzzy lop (AFL), a leading\nfuzzing tool, has demonstrated its powerful bug finding ability through a vast\nnumber of reported CVEs. However, its random mutation strategy is unable to\ngenerate test inputs that satisfy complicated branching conditions (e.g.,\nmagic-byte comparisons, checksum tests, and nested if-statements), which are\ncommonly used in image decoders/encoders, XML parsers, and checksum tools.\nExisting approaches (such as Steelix and Neuzz) on addressing this problem\nassume unrealistic assumptions such as we can satisfy the branch condition\nbyte-to-byte or we can identify and focus on the important bytes in the input\n(called hot-bytes) once and for all. In this work, we propose an approach\ncalled \\tool~which is designed based on the following principles. First, there\nis a complicated relation between inputs and branching conditions and thus we\nneed not only an expressive model to capture such relationship but also an\ninformative measure so that we can learn such relationship effectively. Second,\ndifferent branching conditions demand different hot-bytes and we must adjust\nour fuzzing strategy adaptively depending on which branches are the current\nbottleneck. We implement our approach as an open source project and compare its\nefficiency with other state-of-the-art fuzzers. Our evaluation results on 10\nreal-world programs and LAVA-M dataset show that \\tool~achieves sustained\nincreases in branch coverage and discovers more bugs than other fuzzers.",
        "translated": ""
    },
    {
        "title": "Security Risk Analysis Methodologies for Automotive Systems",
        "url": "http://arxiv.org/abs/2307.02261v1",
        "pub_date": "2023-07-05",
        "summary": "Nowadays, systematic security risk analysis plays a vital role in the\nautomotive domain. The demand for advanced driver assistance systems and\nconnectivity of vehicles to the internet makes cyber-security a crucial\nrequirement for vehicle manufacturers. This paper summarizes the risk analysis\nmethod stated in the recently released automotive security standard ISO/SAE\n21434, which lays the high-level principles for threat analysis and risk\nassessment (TARA) methods. Following, we introduce a specific use case to\ncompare different security analysis approaches which OEMs can benefit from to\nachieve compliance with the standard.",
        "translated": ""
    },
    {
        "title": "On the Adversarial Robustness of Generative Autoencoders in the Latent\n  Space",
        "url": "http://arxiv.org/abs/2307.02202v1",
        "pub_date": "2023-07-05",
        "summary": "The generative autoencoders, such as the variational autoencoders or the\nadversarial autoencoders, have achieved great success in lots of real-world\napplications, including image generation, and signal communication.\n  However, little concern has been devoted to their robustness during practical\ndeployment.\n  Due to the probabilistic latent structure, variational autoencoders (VAEs)\nmay confront problems such as a mismatch between the posterior distribution of\nthe latent and real data manifold, or discontinuity in the posterior\ndistribution of the latent.\n  This leaves a back door for malicious attackers to collapse VAEs from the\nlatent space, especially in scenarios where the encoder and decoder are used\nseparately, such as communication and compressed sensing.\n  In this work, we provide the first study on the adversarial robustness of\ngenerative autoencoders in the latent space.\n  Specifically, we empirically demonstrate the latent vulnerability of popular\ngenerative autoencoders through attacks in the latent space.\n  We also evaluate the difference between variational autoencoders and their\ndeterministic variants and observe that the latter performs better in latent\nrobustness.\n  Meanwhile, we identify a potential trade-off between the adversarial\nrobustness and the degree of the disentanglement of the latent codes.\n  Additionally, we also verify the feasibility of improvement for the latent\nrobustness of VAEs through adversarial training.\n  In summary, we suggest concerning the adversarial latent robustness of the\ngenerative autoencoders, analyze several robustness-relative issues, and give\nsome insights into a series of key challenges.",
        "translated": ""
    },
    {
        "title": "Citation: A Key to Building Responsible and Accountable Large Language\n  Models",
        "url": "http://arxiv.org/abs/2307.02185v1",
        "pub_date": "2023-07-05",
        "summary": "Large Language Models (LLMs) bring transformative benefits alongside unique\nchallenges, including intellectual property (IP) and ethical concerns. This\nposition paper explores a novel angle to mitigate these risks, drawing\nparallels between LLMs and established web systems. We identify \"citation\" as a\ncrucial yet missing component in LLMs, which could enhance content transparency\nand verifiability while addressing IP and ethical dilemmas. We further propose\nthat a comprehensive citation mechanism for LLMs should account for both\nnon-parametric and parametric content. Despite the complexity of implementing\nsuch a citation mechanism, along with the inherent potential pitfalls, we\nadvocate for its development. Building on this foundation, we outline several\nresearch problems in this area, aiming to guide future explorations towards\nbuilding more responsible and accountable LLMs.",
        "translated": ""
    },
    {
        "title": "A Scheme to resist Fast Correlation Attack for Word Oriented LFSR based\n  Stream Cipher",
        "url": "http://arxiv.org/abs/2307.02182v1",
        "pub_date": "2023-07-05",
        "summary": "In LFSR-based stream ciphers, the knowledge of the feedback equation of the\nLFSR plays a critical role in most attacks. In word-based stream ciphers such\nas those in the SNOW series, even if the feedback configuration is hidden,\nknowing the characteristic polynomial of the state transition matrix of the\nLFSR enables the attacker to create a feedback equation over $GF(2)$. This, in\nturn, can be used to launch fast correlation attacks. In this work, we propose\na method for hiding both the feedback equation of a word-based LFSR and the\ncharacteristic polynomial of the state transition matrix. Here, we employ a\n$z$-primitive $\\sigma$-LFSR whose characteristic polynomial is randomly sampled\nfrom the distribution of primitive polynomials over $GF(2)$ of the appropriate\ndegree. We propose an algorithm for locating $z$-primitive $\\sigma$-LFSR\nconfigurations of a given degree. Further, an invertible matrix is generated\nfrom the key. This is then employed to generate a public parameter which is\nused to retrieve the feedback configuration using the key. If the key size is\n$n$- bits, the process of retrieving the feedback equation from the public\nparameter has a average time complexity $\\mathbb{O}(2^{n-1})$. The proposed\nmethod has been tested on SNOW 2.0 and SNOW 3G for resistance to fast\ncorrelation attacks. We have demonstrated that the security of SNOW 2.0 and\nSNOW 3G increases from 128 bits to 256 bits.",
        "translated": ""
    },
    {
        "title": "Quantum Solutions to the Privacy vs. Utility Tradeoff",
        "url": "http://arxiv.org/abs/2307.03118v1",
        "pub_date": "2023-07-06",
        "summary": "In this work, we propose a novel architecture (and several variants thereof)\nbased on quantum cryptographic primitives with provable privacy and security\nguarantees regarding membership inference attacks on generative models. Our\narchitecture can be used on top of any existing classical or quantum generative\nmodels. We argue that the use of quantum gates associated with unitary\noperators provides inherent advantages compared to standard Differential\nPrivacy based techniques for establishing guaranteed security from all\npolynomial-time adversaries.",
        "translated": ""
    },
    {
        "title": "How to Detect Unauthorized Data Usages in Text-to-image Diffusion Models",
        "url": "http://arxiv.org/abs/2307.03108v1",
        "pub_date": "2023-07-06",
        "summary": "Recent text-to-image diffusion models have shown surprising performance in\ngenerating high-quality images. However, concerns have arisen regarding the\nunauthorized usage of data during the training process. One example is when a\nmodel trainer collects a set of images created by a particular artist and\nattempts to train a model capable of generating similar images without\nobtaining permission from the artist. To address this issue, it becomes crucial\nto detect unauthorized data usage. In this paper, we propose a method for\ndetecting such unauthorized data usage by planting injected memorization into\nthe text-to-image diffusion models trained on the protected dataset.\nSpecifically, we modify the protected image dataset by adding unique contents\non the images such as stealthy image wrapping functions that are imperceptible\nto human vision but can be captured and memorized by diffusion models. By\nanalyzing whether the model has memorization for the injected content (i.e.,\nwhether the generated images are processed by the chosen post-processing\nfunction), we can detect models that had illegally utilized the unauthorized\ndata. Our experiments conducted on Stable Diffusion and LoRA model demonstrate\nthe effectiveness of the proposed method in detecting unauthorized data usages.",
        "translated": ""
    },
    {
        "title": "Quantum Complexity for Discrete Logarithms and Related Problems",
        "url": "http://arxiv.org/abs/2307.03065v1",
        "pub_date": "2023-07-06",
        "summary": "This paper studies the quantum computational complexity of the discrete\nlogarithm (DL) and related group-theoretic problems in the context of generic\nalgorithms -- that is, algorithms that do not exploit any properties of the\ngroup encoding.\n  We establish a generic model of quantum computation for group-theoretic\nproblems, which we call the quantum generic group model. Shor's algorithm for\nthe DL problem and related algorithms can be described in this model. We show\nthe quantum complexity lower bounds and almost matching algorithms of the DL\nand related problems in this model. More precisely, we prove the following\nresults for a cyclic group $G$ of prime order.\n  - Any generic quantum DL algorithm must make $\\Omega(\\log |G|)$ depth of\ngroup operations. This shows that Shor's algorithm is asymptotically optimal\namong the generic quantum algorithms, even considering parallel algorithms.\n  - We observe that variations of Shor's algorithm can take advantage of\nclassical computations to reduce the number of quantum group operations. We\nintroduce a model for generic hybrid quantum-classical algorithms and show that\nthese algorithms are almost optimal in this model. Any generic hybrid algorithm\nfor the DL problem with a total number of group operations $Q$ must make\n$\\Omega(\\log |G|/\\log Q)$ quantum group operations of depth $\\Omega(\\log\\log\n|G| - \\log\\log Q)$.\n  - When the quantum memory can only store $t$ group elements and use quantum\nrandom access memory of $r$ group elements, any generic hybrid algorithm must\nmake either $\\Omega(\\sqrt{|G|})$ group operations in total or $\\Omega(\\log\n|G|/\\log (tr))$ quantum group operations.\n  As a side contribution, we show a multiple DL problem admits a better\nalgorithm than solving each instance one by one, refuting a strong form of the\nquantum annoying property suggested in the context of password-authenticated\nkey exchange protocol.",
        "translated": ""
    },
    {
        "title": "DPM: Clustering Sensitive Data through Separation",
        "url": "http://arxiv.org/abs/2307.02969v1",
        "pub_date": "2023-07-06",
        "summary": "Privacy-preserving clustering groups data points in an unsupervised manner\nwhilst ensuring that sensitive information remains protected. Previous\nprivacy-preserving clustering focused on identifying concentration of point\nclouds. In this paper, we take another path and focus on identifying\nappropriate separators that split a data set. We introduce the novel\ndifferentially private clustering algorithm DPM that searches for accurate data\npoint separators in a differentially private manner. DPM addresses two key\nchallenges for finding accurate separators: identifying separators that are\nlarge gaps between clusters instead of small gaps within a cluster and, to\nefficiently spend the privacy budget, prioritising separators that split the\ndata into large subparts. Using the differentially private Exponential\nMechanism, DPM randomly chooses cluster separators with provably high utility:\nFor a data set $D$, if there is a wide low-density separator in the central\n$60\\%$ quantile, DPM finds that separator with probability $1 -\n\\exp(-\\sqrt{|D|})$. Our experimental evaluation demonstrates that DPM achieves\nsignificant improvements in terms of the clustering metric inertia. With the\ninertia results of the non-private KMeans++ as a baseline, for $\\varepsilon =\n1$ and $\\delta=10^{-5}$ DPM improves upon the difference to the baseline by up\nto $50\\%$ for a synthetic data set and by up to $62\\%$ for a real-world data\nset compared to a state-of-the-art clustering algorithm by Chang and Kamath.",
        "translated": ""
    },
    {
        "title": "Smartphones in a Microwave: Formal and Experimental Feasibility Study on\n  Fingerprinting the Corona-Warn-App",
        "url": "http://arxiv.org/abs/2307.02931v1",
        "pub_date": "2023-07-06",
        "summary": "Contact Tracing Apps (CTAs) have been developed to contain the coronavirus\ndisease 19 (COVID-19) spread. By design, such apps invade their users' privacy\nby recording data about their health, contacts, and partially location. Many\nCTAs frequently broadcast pseudorandom numbers via Bluetooth to detect\nencounters. These numbers are changed regularly to prevent individual\nsmartphones from being trivially trackable. However, the effectiveness of this\nprocedure has been little studied. We measured real smartphones and observed\nthat the German Corona-Warn-App (CWA) exhibits a device-specific latency\nbetween two subsequent broadcasts. These timing differences provide a potential\nattack vector for fingerprinting smartphones by passively recording Bluetooth\nmessages. This could conceivably lead to the tracking of users' trajectories\nand, ultimately, the re-identification of users.",
        "translated": ""
    },
    {
        "title": "It's more than just money: The real-world harms from ransomware attacks",
        "url": "http://arxiv.org/abs/2307.02855v1",
        "pub_date": "2023-07-06",
        "summary": "As cyber-attacks continue to increase in frequency and sophistication,\norganisations must be better prepared to face the reality of an incident. Any\norganisational plan that intends to be successful at managing security risks\nmust clearly understand the harm (i.e., negative impact) and the various\nparties affected in the aftermath of an attack. To this end, this article\nconducts a novel exploration into the multitude of real-world harms that can\narise from cyber-attacks, with a particular focus on ransomware incidents given\ntheir current prominence. This exploration also leads to the proposal of a new,\nrobust methodology for modelling harms from such incidents. We draw on\npublicly-available case data on high-profile ransomware incidents to examine\nthe types of harm that emerge at various stages after a ransomware attack and\nhow harms (e.g., an offline enterprise server) may trigger other negative,\npotentially more substantial impacts for stakeholders (e.g., the inability for\na customer to access their social welfare benefits or bank account). Prominent\nfindings from our analysis include the identification of a notable set of\nsocial/human harms beyond the business itself (and beyond the financial payment\nof a ransom) and a complex web of harms that emerge after attacks regardless of\nthe industry sector. We also observed that deciphering the full extent and\nsequence of harms can be a challenging undertaking because of the lack of\ncomplete data available. This paper consequently argues for more transparency\non ransomware harms, as it would lead to a better understanding of the\nrealities of these incidents to the benefit of organisations and society more\ngenerally.",
        "translated": ""
    },
    {
        "title": "Sampling-based Fast Gradient Rescaling Method for Highly Transferable\n  Adversarial Attacks",
        "url": "http://arxiv.org/abs/2307.02828v1",
        "pub_date": "2023-07-06",
        "summary": "Deep neural networks are known to be vulnerable to adversarial examples\ncrafted by adding human-imperceptible perturbations to the benign input. After\nachieving nearly 100% attack success rates in white-box setting, more focus is\nshifted to black-box attacks, of which the transferability of adversarial\nexamples has gained significant attention. In either case, the common\ngradient-based methods generally use the sign function to generate\nperturbations on the gradient update, that offers a roughly correct direction\nand has gained great success. But little work pays attention to its possible\nlimitation. In this work, we observe that the deviation between the original\ngradient and the generated noise may lead to inaccurate gradient update\nestimation and suboptimal solutions for adversarial transferability. To this\nend, we propose a Sampling-based Fast Gradient Rescaling Method (S-FGRM).\nSpecifically, we use data rescaling to substitute the sign function without\nextra computational cost. We further propose a Depth First Sampling method to\neliminate the fluctuation of rescaling and stabilize the gradient update. Our\nmethod could be used in any gradient-based attacks and is extensible to be\nintegrated with various input transformation or ensemble methods to further\nimprove the adversarial transferability. Extensive experiments on the standard\nImageNet dataset show that our method could significantly boost the\ntransferability of gradient-based attacks and outperform the state-of-the-art\nbaselines.",
        "translated": ""
    },
    {
        "title": "A Testbed To Study Adversarial Cyber-Attack Strategies in Enterprise\n  Networks",
        "url": "http://arxiv.org/abs/2307.02794v1",
        "pub_date": "2023-07-06",
        "summary": "In this work, we propose a testbed environment to capture the attack\nstrategies of an adversary carrying out a cyber-attack on an enterprise\nnetwork. The testbed contains nodes with known security vulnerabilities which\ncan be exploited by hackers. Participants can be invited to play the role of a\nhacker (e.g., black-hat, hacktivist) and attack the testbed. The testbed is\ndesigned such that there are multiple attack pathways available to hackers. We\ndescribe the working of the testbed components and discuss its implementation\non a VMware ESXi server. Finally, we subject our testbed implementation to a\nfew well-known cyber-attack strategies, collect data during the process and\npresent our analysis of the data.",
        "translated": ""
    },
    {
        "title": "DSARSR: Deep Stacked Auto-encoders Enhanced Robust Speaker Recognition",
        "url": "http://arxiv.org/abs/2307.02751v1",
        "pub_date": "2023-07-06",
        "summary": "Speaker recognition is a biometric modality that utilizes the speaker's\nspeech segments to recognize the identity, determining whether the test speaker\nbelongs to one of the enrolled speakers. In order to improve the robustness of\nthe i-vector framework on cross-channel conditions and explore the nova method\nfor applying deep learning to speaker recognition, the Stacked Auto-encoders\nare used to get the abstract extraction of the i-vector instead of applying\nPLDA. After pre-processing and feature extraction, the speaker and\nchannel-independent speeches are employed for UBM training. The UBM is then\nused to extract the i-vector of the enrollment and test speech. Unlike the\ntraditional i-vector framework, which uses linear discriminant analysis (LDA)\nto reduce dimension and increase the discrimination between speaker subspaces,\nthis research use stacked auto-encoders to reconstruct the i-vector with lower\ndimension and different classifiers can be chosen to achieve final\nclassification. The experimental results show that the proposed method achieves\nbetter performance than the state-of-the-art method.",
        "translated": ""
    },
    {
        "title": "Convergence of Communications, Control, and Machine Learning for Secure\n  and Autonomous Vehicle Navigation",
        "url": "http://arxiv.org/abs/2307.02663v1",
        "pub_date": "2023-07-05",
        "summary": "Connected and autonomous vehicles (CAVs) can reduce human errors in traffic\naccidents, increase road efficiency, and execute various tasks ranging from\ndelivery to smart city surveillance. Reaping these benefits requires CAVs to\nautonomously navigate to target destinations. To this end, each CAV's\nnavigation controller must leverage the information collected by sensors and\nwireless systems for decision-making on longitudinal and lateral movements.\nHowever, enabling autonomous navigation for CAVs requires a convergent\nintegration of communication, control, and learning systems. The goal of this\narticle is to explicitly expose the challenges related to this convergence and\npropose solutions to address them in two major use cases: Uncoordinated and\ncoordinated CAVs. In particular, challenges related to the navigation of\nuncoordinated CAVs include stable path tracking, robust control against\ncyber-physical attacks, and adaptive navigation controller design. Meanwhile,\nwhen multiple CAVs coordinate their movements during navigation, fundamental\nproblems such as stable formation, fast collaborative learning, and distributed\nintrusion detection are analyzed. For both cases, solutions using the\nconvergence of communication theory, control theory, and machine learning are\nproposed to enable effective and secure CAV navigation. Preliminary simulation\nresults are provided to show the merits of proposed solutions.",
        "translated": ""
    },
    {
        "title": "Scalable Membership Inference Attacks via Quantile Regression",
        "url": "http://arxiv.org/abs/2307.03694v1",
        "pub_date": "2023-07-07",
        "summary": "Membership inference attacks are designed to determine, using black box\naccess to trained models, whether a particular example was used in training or\nnot. Membership inference can be formalized as a hypothesis testing problem.\nThe most effective existing attacks estimate the distribution of some test\nstatistic (usually the model's confidence on the true label) on points that\nwere (and were not) used in training by training many \\emph{shadow models} --\ni.e. models of the same architecture as the model being attacked, trained on a\nrandom subsample of data. While effective, these attacks are extremely\ncomputationally expensive, especially when the model under attack is large.\n  We introduce a new class of attacks based on performing quantile regression\non the distribution of confidence scores induced by the model under attack on\npoints that are not used in training. We show that our method is competitive\nwith state-of-the-art shadow model attacks, while requiring substantially less\ncompute because our attack requires training only a single model. Moreover,\nunlike shadow model attacks, our proposed attack does not require any knowledge\nof the architecture of the model under attack and is therefore truly\n``black-box\". We show the efficacy of this approach in an extensive series of\nexperiments on various datasets and model architectures.",
        "translated": ""
    },
    {
        "title": "Random Number Generators and Seeding for Differential Privacy",
        "url": "http://arxiv.org/abs/2307.03543v1",
        "pub_date": "2023-07-07",
        "summary": "Differential Privacy (DP) relies on random numbers to preserve privacy,\ntypically utilising Pseudorandom Number Generators (PRNGs) as a source of\nrandomness. In order to allow for consistent reproducibility, testing and\nbug-fixing in DP algorithms and results, it is important to allow for the\nseeding of the PRNGs used therein. In this work, we examine the landscape of\nRandom Number Generators (RNGs), and the considerations software engineers\nshould make when choosing and seeding a PRNG for DP. We hope it serves as a\nsuitable guide for DP practitioners, and includes many lessons learned when\nimplementing seeding for diffprivlib.",
        "translated": ""
    },
    {
        "title": "Improving Bitswap Privacy with Forwarding and Source Obfuscation",
        "url": "http://arxiv.org/abs/2307.03480v1",
        "pub_date": "2023-07-07",
        "summary": "IPFS is a content-addressed decentralized peer-to-peer data network, using\nthe Bitswap protocol for exchanging data. The data exchange leaks the\ninformation to all neighbors, compromising a user's privacy. This paper\ninvestigates the suitability of forwarding with source obfuscation techniques\nfor improving the privacy of the Bitswap protocol. The usage of forwarding can\nadd plausible deniability and the source obfuscation provides additional\nprotection against passive observers. First results showed that through\ntrickle-spreading the source prediction could decrease to 40 %, at the cost of\nan increased content fetching time. However, assuming short distances between\ncontent provider and consumer the content fetching time can be faster even with\nthe additional source obfuscation.",
        "translated": ""
    },
    {
        "title": "Encrypted Dynamic Control exploiting Limited Number of Multiplications\n  and a Method using Ring-LWE based Cryptosystem",
        "url": "http://arxiv.org/abs/2307.03451v1",
        "pub_date": "2023-07-07",
        "summary": "In this paper, we present a method to encrypt dynamic controllers that can be\nimplemented through most homomorphic encryption schemes, including somewhat,\nleveled fully, and fully homomorphic encryption. To this end, we represent the\noutput of the given controller as a linear combination of a fixed number of\nprevious inputs and outputs. As a result, the encrypted controller involves\nonly a limited number of homomorphic multiplications on every encrypted data,\nassuming that the output is re-encrypted and transmitted back from the\nactuator. A guidance for parameter choice is also provided, ensuring that the\nencrypted controller achieves predefined performance for an infinite time\nhorizon. Furthermore, we propose a customization of the method for\nRing-Learning With Errors (Ring-LWE) based cryptosystems, where a vector of\nmessages can be encrypted into a single ciphertext and operated simultaneously,\nthus reducing computation and communication loads. Unlike previous results, the\nproposed customization does not require extra algorithms such as rotation,\nother than basic addition and multiplication. Simulation results demonstrate\nthe effectiveness of the proposed method.",
        "translated": ""
    },
    {
        "title": "Towards Deep Network Steganography: From Networks to Networks",
        "url": "http://arxiv.org/abs/2307.03444v1",
        "pub_date": "2023-07-07",
        "summary": "With the widespread applications of the deep neural network (DNN), how to\ncovertly transmit the DNN models in public channels brings us the attention,\nespecially for those trained for secret-learning tasks. In this paper, we\npropose deep network steganography for the covert communication of DNN models.\nUnlike the existing steganography schemes which focus on the subtle\nmodification of the cover data to accommodate the secrets, our scheme is\nlearning task oriented, where the learning task of the secret DNN model (termed\nas secret-learning task) is disguised into another ordinary learning task\nconducted in a stego DNN model (termed as stego-learning task). To this end, we\npropose a gradient-based filter insertion scheme to insert interference filters\ninto the important positions in the secret DNN model to form a stego DNN model.\nThese positions are then embedded into the stego DNN model using a key by side\ninformation hiding. Finally, we activate the interference filters by a partial\noptimization strategy, such that the generated stego DNN model works on the\nstego-learning task. We conduct the experiments on both the intra-task\nsteganography and inter-task steganography (i.e., the secret and stego-learning\ntasks belong to the same and different categories), both of which demonstrate\nthe effectiveness of our proposed method for covert communication of DNN\nmodels.",
        "translated": ""
    },
    {
        "title": "Differential Privacy for Clustering Under Continual Observation",
        "url": "http://arxiv.org/abs/2307.03430v1",
        "pub_date": "2023-07-07",
        "summary": "We consider the problem of clustering privately a dataset in $\\mathbb{R}^d$\nthat undergoes both insertion and deletion of points. Specifically, we give an\n$\\varepsilon$-differentially private clustering mechanism for the $k$-means\nobjective under continual observation. This is the first approximation\nalgorithm for that problem with an additive error that depends only\nlogarithmically in the number $T$ of updates. The multiplicative error is\nalmost the same as non privately. To do so we show how to perform dimension\nreduction under continual observation and combine it with a differentially\nprivate greedy approximation algorithm for $k$-means. We also partially extend\nour results to the $k$-median problem.",
        "translated": ""
    },
    {
        "title": "Exploring Encrypted Keyboards to Defeat Client-Side Scanning in\n  End-to-End Encryption Systems",
        "url": "http://arxiv.org/abs/2307.03426v1",
        "pub_date": "2023-07-07",
        "summary": "End-to-End Encryption (E2EE) aims to make all messages impossible to read by\nanyone except you and your intended recipient(s). Many well-known and widely\nused Instant-Messaging (IM) applications (such as Signal, WhatsApp, and Apple's\niMessage) claim to provide E2EE. However, a recent technique called client-side\nscanning (CSS) makes these E2EE claims grandiose and hollow promises. The CSS\nis a technology that scans all sending and receiving messages from one end to\nthe other. Some in industry and government now advocate this CSS technology to\ncombat the growth of malicious child pornography, terrorism, and other illicit\ncommunication. Even though combating the spread of illegal and morally\nobjectionable content is a laudable effort, it may open further backdoors that\nimpact the user's privacy and security. Therefore, it is not E2EE when there\nare censorship mechanisms and backdoors in end-to-end encrypted applications.\nIn this paper, we introduce an encrypted keyboard that functions as a system\nkeyboard, enabling users to employ it across all applications on their phones\nwhen entering data. By utilizing this encrypted keyboard, users can locally\nencrypt and decrypt messages, effectively bypassing the CSS system. We first\ndesign and implement our encrypted keyboard as a custom keyboard application,\nand then we evaluate the effectiveness and security of our encrypted keyboard.\nOur study results show that our encrypted keyboard can successfully encrypt and\ndecrypt all sending and receiving messages through IM applications, and\ntherefore, it can successfully defeat the CSS technology in end-to-end\nencrypted systems. We also show that our encrypted keyboard can be used to add\nanother layer of E2EE functionality on top of the existing E2EE functionality\nimplemented by many end-to-end encrypted applications.",
        "translated": ""
    },
    {
        "title": "A Cryptography Inspired Model for Non-local Correlations: Decrypting the\n  Enigmas",
        "url": "http://arxiv.org/abs/2307.03395v1",
        "pub_date": "2023-07-07",
        "summary": "We propose a cryptography-inspired model for nonlocal correlations. Following\nthe celebrated De Broglie-Bohm theory, we model nonlocal boxes as realistic\nsystems with instantaneous signalling at the hidden variable level. By\nintroducing randomness in the distribution of the hidden variable, the\nsuperluminal signalling model is made compatible with the operational\nno-signalling condition. As the design mimics the famous symmetric key\nencryption system called {\\it One Time Pads} (OTP), we call this the OTP model\nfor nonlocal boxes. We demonstrate utility of this model in several esoteric\nexamples related to the nonclassicality of nonlocal boxes. In particular, the\nbreakdown of communication complexity using nonlocal boxes can be better\nunderstood in this framework. Furthermore, we discuss the Van Dam protocol and\nshow its connection to homomorphic encryption in cryptography. We also discuss\npossible ways of encapsulating quantum realizable nonlocal correlations within\nthis framework and show that the principle of Information Causality imposes\nfurther constraints at the hidden variable level. Present work thus\norchestrates the results in classical cryptography to improve our understanding\nof nonlocal correlations and welcomes further research to this connection.",
        "translated": ""
    },
    {
        "title": "A Multi-Factor Homomorphic Encryption based Method for Authenticated\n  Access to IoT Devices",
        "url": "http://arxiv.org/abs/2307.03291v1",
        "pub_date": "2023-07-06",
        "summary": "Authentication is the first defence mechanism in many electronic systems,\nincluding Internet of Things (IoT) applications, as it is essential for other\nsecurity services such as intrusion detection. As existing authentication\nsolutions proposed for IoT environments do not provide multi-level\nauthentication assurance, particularly for device-to-device authentication\nscenarios, we recently proposed the M2I (Multi-Factor Multi-Level and\nInteraction based Authentication) framework to facilitate multi-factor\nauthentication of devices in device-to-device and device-to-multiDevice\ninteractions. In this paper, we extend the framework to address group\nauthentication. Two Many-to-One (M2O) protocols are proposed, the Hybrid Group\nAuthentication and Key Acquisition (HGAKA) protocol and the Hybrid Group Access\n(HGA) protocol. The protocols use a combination of symmetric and asymmetric\ncryptographic primitives to facilitate multifactor group authentication. The\ninformal analysis and formal security verification show that the protocols\nsatisfy the desirable security requirements and are secure against\nauthentication attacks.",
        "translated": ""
    },
    {
        "title": "Pretty Good Strategies for Benaloh Challenge",
        "url": "http://arxiv.org/abs/2307.03258v1",
        "pub_date": "2023-07-06",
        "summary": "Benaloh challenge allows the voter to audit the encryption of her vote, and\nin particular to check whether the vote has been represented correctly. An\ninteresting analysis of the mechanism has been presented by Culnane and Teague.\nThe authors propose a natural game-theoretic model of the interaction between\nthe voter and a corrupt, malicious encryption device. Then, they claim that\nthere is no \"natural\" rational strategy for the voter to play the game. In\nconsequence, the authorities cannot provide the voter with a sensible auditing\nstrategy, which undermines the whole idea.\n  Here, we claim the contrary, i.e., that there exist simple rational\nstrategies that justify the usefulness of Benaloh challenge.",
        "translated": ""
    },
    {
        "title": "A unifying framework for differentially private quantum algorithms",
        "url": "http://arxiv.org/abs/2307.04733v1",
        "pub_date": "2023-07-10",
        "summary": "Differential privacy is a widely used notion of security that enables the\nprocessing of sensitive information. In short, differentially private\nalgorithms map \"neighbouring\" inputs to close output distributions. Prior work\nproposed several quantum extensions of differential privacy, each of them built\non substantially different notions of neighbouring quantum states. In this\npaper, we propose a novel and general definition of neighbouring quantum\nstates. We demonstrate that this definition captures the underlying structure\nof quantum encodings and can be used to provide exponentially tighter privacy\nguarantees for quantum measurements. Our approach combines the addition of\nclassical and quantum noise and is motivated by the noisy nature of near-term\nquantum devices. Moreover, we also investigate an alternative setting where we\nare provided with multiple copies of the input state. In this case,\ndifferential privacy can be ensured with little loss in accuracy combining\nconcentration of measure and noise-adding mechanisms. En route, we prove the\nadvanced joint convexity of the quantum hockey-stick divergence and we\ndemonstrate how this result can be applied to quantum differential privacy.\nFinally, we complement our theoretical findings with an empirical estimation of\nthe certified adversarial robustness ensured by differentially private\nmeasurements.",
        "translated": ""
    },
    {
        "title": "Deceptive Information Retrieval",
        "url": "http://arxiv.org/abs/2307.04727v1",
        "pub_date": "2023-07-10",
        "summary": "We introduce the problem of deceptive information retrieval (DIR), in which a\nuser wishes to download a required file out of multiple independent files\nstored in a system of databases while \\emph{deceiving} the databases by making\nthe databases' predictions on the user-required file index incorrect with high\nprobability. Conceptually, DIR is an extension of private information retrieval\n(PIR). In PIR, a user downloads a required file without revealing its index to\nany of the databases. The metric of deception is defined as the probability of\nerror of databases' prediction on the user-required file, minus the\ncorresponding probability of error in PIR. The problem is defined on\ntime-sensitive data that keeps updating from time to time. In the proposed\nscheme, the user deceives the databases by sending \\emph{real} queries to\ndownload the required file at the time of the requirement and \\emph{dummy}\nqueries at multiple distinct future time instances to manipulate the\nprobabilities of sending each query for each file requirement, using which the\ndatabases' make the predictions on the user-required file index. The proposed\nDIR scheme is based on a capacity achieving probabilistic PIR scheme, and\nachieves rates lower than the PIR capacity due to the additional downloads made\nto deceive the databases. When the required level of deception is zero, the\nproposed scheme achieves the PIR capacity.",
        "translated": ""
    },
    {
        "title": "Performance comparison of timing-based anomaly detectors for Controller\n  Area Network: a reproducible study",
        "url": "http://arxiv.org/abs/2307.04561v1",
        "pub_date": "2023-07-10",
        "summary": "This work presents an experimental evaluation of the detection performance of\neight different algorithms for anomaly detection on the Controller Area Network\n(CAN) bus of modern vehicles based on the analysis of the timing or frequency\nof CAN messages. This work solves the current limitations of related scientific\nliterature, that is based on private dataset, lacks of open implementations,\nand detailed description of the detection algorithms. These drawback prevent\nthe reproducibility of published results, and makes it impossible to compare a\nnovel proposal against related work, thus hindering the advancement of science.\nThis paper solves these issues by publicly releasing implementations, labeled\ndatasets and by describing an unbiased experimental comparisons.",
        "translated": ""
    },
    {
        "title": "A Privacy-Preserving and Accountable Billing Protocol for Peer-to-Peer\n  Energy Trading Markets",
        "url": "http://arxiv.org/abs/2307.04501v1",
        "pub_date": "2023-07-10",
        "summary": "This paper proposes a privacy-preserving and accountable billing (PA-Bill)\nprotocol for trading in peer-to-peer energy markets, addressing situations\nwhere there may be discrepancies between the volume of energy committed and\ndelivered. Such discrepancies can lead to challenges in providing both privacy\nand accountability while maintaining accurate billing. To overcome these\nchallenges, a universal cost splitting mechanism is proposed that prioritises\nprivacy and accountability. It leverages a homomorphic encryption cryptosystem\nto provide privacy and employs blockchain technology to establish\naccountability. A dispute resolution mechanism is also introduced to minimise\nthe occurrence of erroneous bill calculations while ensuring accountability and\nnon-repudiation throughout the billing process. Our evaluation demonstrates\nthat PA-Bill offers an effective billing mechanism that maintains privacy and\naccountability in peer-to-peer energy markets utilising a semi-decentralised\napproach.",
        "translated": ""
    },
    {
        "title": "Towards Automated Cyber Range Design: Characterizing and Matching\n  Demands to Supplies",
        "url": "http://arxiv.org/abs/2307.04416v1",
        "pub_date": "2023-07-10",
        "summary": "Cyber ranges mimic real-world cyber environments and are in high demand.\nBefore building their own cyber ranges, organizations need to deeply understand\nwhat construction supplies are available to them. A fundamental supply is the\ncyber range architecture, which prompts an important research question: Which\ncyber range architecture is most appropriate for an organization's\nrequirements? To answer this question, we propose an innovative framework to\nspecify cyber range requirements, characterize cyber range architectures (based\non our analysis of 45 cyber range architectures), and match cyber range\narchitectures to cyber range requirements.",
        "translated": ""
    },
    {
        "title": "Towards Runtime Customizable Trusted Execution Environment on FPGA-SoC",
        "url": "http://arxiv.org/abs/2307.04375v1",
        "pub_date": "2023-07-10",
        "summary": "Processing sensitive data and deploying well-designed Intellectual Property\n(IP) cores on remote Field Programmable Gate Array (FPGA) are prone to private\ndata leakage and IP theft. One effective solution is constructing Trusted\nExecution Environment (TEE) on FPGA-SoCs (FPGA System on Chips). Researchers\nhave integrated this type TEE with Trusted Platform Module (TPM)-based trusted\nboot, denoted as FPGA-SoC tbTEE. But there is no effort on secure and trusted\nruntime customization of FPGA-SoC TEE. This paper extends FPGA-SoC tbTEE to\nbuild Runtime Customizable TEE (RCTEE) on FPGA-SoC by additive three major\ncomponents (our work): 1) CrloadIP, which can load an IP core at runtime such\nthat RCTEE can be adjusted dynamically and securely; 2) CexecIP, which can not\nonly execute an IP core without modifying the operating system of FPGA-SoC TEE,\nbut also prevent insider attacks from executing IPs deployed in RCTEE; 3)\nCremoAT, which can provide the newly measured RCTEE state and establish a\nsecure and trusted communication path between remote verifiers and RCTEE. We\nconduct a security analysis of RCTEE and its performance evaluation on Xilinx\nZynq UltraScale+ XCZU15EG 2FFVB1156 MPSoC.",
        "translated": ""
    },
    {
        "title": "False Sense of Security: Leveraging XAI to Analyze the Reasoning and\n  True Performance of Context-less DGA Classifiers",
        "url": "http://arxiv.org/abs/2307.04358v1",
        "pub_date": "2023-07-10",
        "summary": "The problem of revealing botnet activity through Domain Generation Algorithm\n(DGA) detection seems to be solved, considering that available deep learning\nclassifiers achieve accuracies of over 99.9%. However, these classifiers\nprovide a false sense of security as they are heavily biased and allow for\ntrivial detection bypass. In this work, we leverage explainable artificial\nintelligence (XAI) methods to analyze the reasoning of deep learning\nclassifiers and to systematically reveal such biases. We show that eliminating\nthese biases from DGA classifiers considerably deteriorates their performance.\nNevertheless we are able to design a context-aware detection system that is\nfree of the identified biases and maintains the detection rate of state-of-the\nart deep learning classifiers. In this context, we propose a visual analysis\nsystem that helps to better understand a classifier's reasoning, thereby\nincreasing trust in and transparency of detection methods and facilitating\ndecision-making.",
        "translated": ""
    },
    {
        "title": "ASCH-PUF: A \"Zero\" Bit Error Rate CMOS Physically Unclonable Function\n  with Dual-Mode Low-Cost Stabilization",
        "url": "http://arxiv.org/abs/2307.04344v2",
        "pub_date": "2023-07-10",
        "summary": "Physically unclonable functions (PUFs) are increasingly adopted for low-cost\nand secure secret key and chip ID generations for embedded and IoT devices.\nAchieving 100% reproducible keys across wide temperature and voltage variations\nover the lifetime of a device is critical and conventionally requires large\nmasking or Error Correction Code (ECC) overhead to guarantee. This paper\npresents an Automatic Self Checking and Healing (ASCH) stabilization technique\nfor a state-of-the-art PUF cell design based on sub-threshold inverter chains.\nThe ASCH system successfully removes all unstable PUF cells without the need\nfor expensive temperature sweeps during unstable bit detection. By accurately\nfinding all unstable bits without expensive temperature sweeps to find all\nunstable bits, ASCH achieves ultra-low bit error rate (BER), thus significantly\nreducing the costs of using ECC and enrollment. Our ASCH can operate in two\nmodes, a static mode (S-ASCH) with a conventional pre-enrolled unstable bit\nmask and a dynamic mode (D-ASCH) that further eliminates the need for\nnon-volatile memories (NVMs) for storing masks. The proposed ASCH-PUF is\nfabricated and evaluated in 65nm CMOS. The ASCH system achieves \"0\" Bit Error\nRate (BER, &lt; 1.77E-9) across temperature variations of -20{\\deg}C to\n125{\\deg}C, and voltage variations of 0.7V to 1.4V, by masking 31% and 35% of\nall fabricated PUF bits in S-ASCH and D-ASCH mode respectively. The prototype\nachieves a measured throughput of 11.4 Gbps with 0.057 fJ/b core energy\nefficiency at 1.2V, 25{\\deg}C.",
        "translated": ""
    },
    {
        "title": "Privacy-Preserving Graph Machine Learning from Data to Computation: A\n  Survey",
        "url": "http://arxiv.org/abs/2307.04338v1",
        "pub_date": "2023-07-10",
        "summary": "In graph machine learning, data collection, sharing, and analysis often\ninvolve multiple parties, each of which may require varying levels of data\nsecurity and privacy. To this end, preserving privacy is of great importance in\nprotecting sensitive information. In the era of big data, the relationships\namong data entities have become unprecedentedly complex, and more applications\nutilize advanced data structures (i.e., graphs) that can support network\nstructures and relevant attribute information. To date, many graph-based AI\nmodels have been proposed (e.g., graph neural networks) for various domain\ntasks, like computer vision and natural language processing. In this paper, we\nfocus on reviewing privacy-preserving techniques of graph machine learning. We\nsystematically review related works from the data to the computational aspects.\nWe first review methods for generating privacy-preserving graph data. Then we\ndescribe methods for transmitting privacy-preserved information (e.g., graph\nmodel parameters) to realize the optimization-based computation when data\nsharing among multiple parties is risky or impossible. In addition to\ndiscussing relevant theoretical methodology and software tools, we also discuss\ncurrent challenges and highlight several possible future research opportunities\nfor privacy-preserving graph machine learning. Finally, we envision a unified\nand comprehensive secure graph machine learning system.",
        "translated": ""
    },
    {
        "title": "Enhancing Adversarial Robustness via Score-Based Optimization",
        "url": "http://arxiv.org/abs/2307.04333v1",
        "pub_date": "2023-07-10",
        "summary": "Adversarial attacks have the potential to mislead deep neural network\nclassifiers by introducing slight perturbations. Developing algorithms that can\nmitigate the effects of these attacks is crucial for ensuring the safe use of\nartificial intelligence. Recent studies have suggested that score-based\ndiffusion models are effective in adversarial defenses. However, existing\ndiffusion-based defenses rely on the sequential simulation of the reversed\nstochastic differential equations of diffusion models, which are\ncomputationally inefficient and yield suboptimal results. In this paper, we\nintroduce a novel adversarial defense scheme named ScoreOpt, which optimizes\nadversarial samples at test-time, towards original clean data in the direction\nguided by score-based priors. We conduct comprehensive experiments on multiple\ndatasets, including CIFAR10, CIFAR100 and ImageNet. Our experimental results\ndemonstrate that our approach outperforms existing adversarial defenses in\nterms of both robustness performance and inference speed.",
        "translated": ""
    },
    {
        "title": "Improving the Security of Smartwatch Payment with Deep Learning",
        "url": "http://arxiv.org/abs/2307.05437v1",
        "pub_date": "2023-07-11",
        "summary": "Making contactless payments using a smartwatch is increasingly popular, but\nthis payment medium lacks traditional biometric security measures such as\nfacial or fingerprint recognition. In 2022, Sturgess et al. proposed WatchAuth,\na system for authenticating smartwatch payments using the physical gesture of\nreaching towards a payment terminal. While effective, the system requires the\nuser to undergo a burdensome enrolment period to achieve acceptable error\nlevels. In this dissertation, we explore whether applications of deep learning\ncan reduce the number of gestures a user must provide to enrol into an\nauthentication system for smartwatch payment. We firstly construct a\ndeep-learned authentication system that outperforms the current\nstate-of-the-art, including in a scenario where the target user has provided a\nlimited number of gestures. We then develop a regularised autoencoder model for\ngenerating synthetic user-specific gestures. We show that using these gestures\nin training improves classification ability for an authentication system.\nThrough this technique we can reduce the number of gestures required to enrol a\nuser into a WatchAuth-like system without negatively impacting its error rates.",
        "translated": ""
    },
    {
        "title": "Let's shake on it: Extracting secure shared keys from Wi-Fi CSI",
        "url": "http://arxiv.org/abs/2307.05423v1",
        "pub_date": "2023-07-11",
        "summary": "A shared secret key is necessary for encrypted communications. Since Wi-Fi\nrelies on OFDM, we suggest a method to generate such a key by utilizing Wi-Fi's\nchannel state information (CSI). CSI is typically reciprocal but very sensitive\nto location: While the legitimate Alice and Bob observe the same CSI, an\neavesdropper Eve observes an uncorrelated CSI when positioned over 0.5\nwavelength away. We show that if endpoint Bob is shaken, sufficient diversity\nis induced in the CSI so that it can serve as a source of true randomness. Then\nwe show that the CSI among neighboring sub-carriers is correlated, so we select\na small set of judiciously-spaced sub-carriers, and use a majority rule around\neach. We demonstrate that Alice and Bob observe a 5-15\\% bit mismatch rate\n(BMR) in the extracted bitstream while Eve observes a BMR of around 50\\% even\nwhen placed within 10cm of Alice. We employ the cryptography-oriented\ndefinition of min-entropy to estimate the number of secure bits within the\nbitstream, and use the Cascade algorithm of quantum-key-distribution to\nreconcile Alice and Bob's bitstreams, while quantifying the number of bits\nleaked by the algorithm. Accounting for both the min-entropy and the cascade\nleakage we quantify the Secured Bit Generation Rate of our method.\n  We conducted extensive tests in an indoor environment. Our system exhibits a\nsecure bit generation rate of 1.2--1.6 %secure bits per packet, at distances\nranging from 0.5m--9m, and can generate a secure shared 128-bit key with 20sec\nof device shaking.",
        "translated": ""
    },
    {
        "title": "Differential Analysis of Triggers and Benign Features for Black-Box DNN\n  Backdoor Detection",
        "url": "http://arxiv.org/abs/2307.05422v1",
        "pub_date": "2023-07-11",
        "summary": "This paper proposes a data-efficient detection method for deep neural\nnetworks against backdoor attacks under a black-box scenario. The proposed\napproach is motivated by the intuition that features corresponding to triggers\nhave a higher influence in determining the backdoored network output than any\nother benign features. To quantitatively measure the effects of triggers and\nbenign features on determining the backdoored network output, we introduce five\nmetrics. To calculate the five-metric values for a given input, we first\ngenerate several synthetic samples by injecting the input's partial contents\ninto clean validation samples. Then, the five metrics are computed by using the\noutput labels of the corresponding synthetic samples. One contribution of this\nwork is the use of a tiny clean validation dataset. Having the computed five\nmetrics, five novelty detectors are trained from the validation dataset. A meta\nnovelty detector fuses the output of the five trained novelty detectors to\ngenerate a meta confidence score. During online testing, our method determines\nif online samples are poisoned or not via assessing their meta confidence\nscores output by the meta novelty detector. We show the efficacy of our\nmethodology through a broad range of backdoor attacks, including ablation\nstudies and comparison to existing approaches. Our methodology is promising\nsince the proposed five metrics quantify the inherent differences between clean\nand poisoned samples. Additionally, our detection method can be incrementally\nimproved by appending more metrics that may be proposed to address future\nadvanced attacks.",
        "translated": ""
    },
    {
        "title": "Smart Environment for Adaptive Learning of Cybersecurity Skills",
        "url": "http://arxiv.org/abs/2307.05281v1",
        "pub_date": "2023-07-11",
        "summary": "Hands-on computing education requires a realistic learning environment that\nenables students to gain and deepen their skills. Available learning\nenvironments, including virtual and physical labs, provide students with\nreal-world computer systems but rarely adapt the learning environment to\nindividual students of various proficiency and background. We designed a unique\nand novel smart environment for adaptive training of cybersecurity skills. The\nenvironment collects a variety of student data to assign a suitable learning\npath through the training. To enable such adaptiveness, we proposed, developed,\nand deployed a new tutor model and a training format. We evaluated the learning\nenvironment using two different adaptive trainings attended by 114 students of\nvarious proficiency. The results show students were assigned tasks with a more\nappropriate difficulty, which enabled them to successfully complete the\ntraining. Students reported that they enjoyed the training, felt the training\ndifficulty was appropriately designed, and would attend more training sessions\nlike these. Instructors can use the environment for teaching any topic\ninvolving real-world computer networks and systems because it is not tailored\nto particular training. We freely released the software along with exemplary\ntraining so that other instructors can adopt the innovations in their teaching\npractice.",
        "translated": ""
    },
    {
        "title": "Application-aware Energy Attack Mitigation in the Battery-less Internet\n  of Things",
        "url": "http://arxiv.org/abs/2307.05206v1",
        "pub_date": "2023-07-11",
        "summary": "We study how to mitigate the effects of energy attacks in the batteryless\nInternet of Things (IoT). Battery-less IoT devices live and die with ambient\nenergy, as they use energy harvesting to power their operation. They are\nemployed in a multitude of applications, including safety-critical ones such as\nbiomedical implants. Due to scarce energy intakes and limited energy buffers,\ntheir executions become intermittent, alternating periods of active operation\nwith periods of recharging their energy buffers. Experimental evidence exists\nthat shows how controlling ambient energy allows an attacker to steer a device\nexecution in unintended ways: energy provisioning effectively becomes an attack\nvector. We design, implement, and evaluate a mitigation system for energy\nattacks. By taking into account the specific application requirements and the\noutput of an attack detection module, we tune task execution rates and optimize\nenergy management. This ensures continued application execution in the event of\nan energy attack. When a device is under attack, our solution ensures the\nexecution of 23.3% additional application cycles compared to the baselines we\nconsider and increases task schedulability by at least 21%, while enabling a\n34% higher peripheral availability.",
        "translated": ""
    },
    {
        "title": "Differentially Private Statistical Inference through $β$-Divergence\n  One Posterior Sampling",
        "url": "http://arxiv.org/abs/2307.05194v1",
        "pub_date": "2023-07-11",
        "summary": "Differential privacy guarantees allow the results of a statistical analysis\ninvolving sensitive data to be released without compromising the privacy of any\nindividual taking part. Achieving such guarantees generally requires the\ninjection of noise, either directly into parameter estimates or into the\nestimation process. Instead of artificially introducing perturbations, sampling\nfrom Bayesian posterior distributions has been shown to be a special case of\nthe exponential mechanism, producing consistent, and efficient private\nestimates without altering the data generative process. The application of\ncurrent approaches has, however, been limited by their strong bounding\nassumptions which do not hold for basic models, such as simple linear\nregressors. To ameliorate this, we propose $\\beta$D-Bayes, a posterior sampling\nscheme from a generalised posterior targeting the minimisation of the\n$\\beta$-divergence between the model and the data generating process. This\nprovides private estimation that is generally applicable without requiring\nchanges to the underlying model and consistently learns the data generating\nparameter. We show that $\\beta$D-Bayes produces more precise inference\nestimation for the same privacy guarantees, and further facilitates\ndifferentially private estimation via posterior sampling for complex\nclassifiers and continuous regression models such as neural networks for the\nfirst time.",
        "translated": ""
    },
    {
        "title": "Membership Inference Attacks on DNNs using Adversarial Perturbations",
        "url": "http://arxiv.org/abs/2307.05193v1",
        "pub_date": "2023-07-11",
        "summary": "Several membership inference (MI) attacks have been proposed to audit a\ntarget DNN. Given a set of subjects, MI attacks tell which subjects the target\nDNN has seen during training. This work focuses on the post-training MI attacks\nemphasizing high confidence membership detection -- True Positive Rates (TPR)\nat low False Positive Rates (FPR). Current works in this category -- likelihood\nratio attack (LiRA) and enhanced MI attack (EMIA) -- only perform well on\ncomplex datasets (e.g., CIFAR-10 and Imagenet) where the target DNN overfits\nits train set, but perform poorly on simpler datasets (0% TPR by both attacks\non Fashion-MNIST, 2% and 0% TPR respectively by LiRA and EMIA on MNIST at 1%\nFPR). To address this, firstly, we unify current MI attacks by presenting a\nframework divided into three stages -- preparation, indication and decision.\nSecondly, we utilize the framework to propose two novel attacks: (1)\nAdversarial Membership Inference Attack (AMIA) efficiently utilizes the\nmembership and the non-membership information of the subjects while\nadversarially minimizing a novel loss function, achieving 6% TPR on both\nFashion-MNIST and MNIST datasets; and (2) Enhanced AMIA (E-AMIA) combines EMIA\nand AMIA to achieve 8% and 4% TPRs on Fashion-MNIST and MNIST datasets\nrespectively, at 1% FPR. Thirdly, we introduce two novel augmented indicators\nthat positively leverage the loss information in the Gaussian neighborhood of a\nsubject. This improves TPR of all four attacks on average by 2.5% and 0.25%\nrespectively on Fashion-MNIST and MNIST datasets at 1% FPR. Finally, we propose\nsimple, yet novel, evaluation metric, the running TPR average (RTA) at a given\nFPR, that better distinguishes different MI attacks in the low FPR region. We\nalso show that AMIA and E-AMIA are more transferable to the unknown DNNs (other\nthan the target DNN) and are more robust to DP-SGD training as compared to LiRA\nand EMIA.",
        "translated": ""
    },
    {
        "title": "SecFlow: Adaptive Security-Aware Workflow Management System in\n  Multi-Cloud Environments",
        "url": "http://arxiv.org/abs/2307.05137v1",
        "pub_date": "2023-07-11",
        "summary": "In this paper, we propose an architecture for a security-aware workflow\nmanagement system (WfMS) we call SecFlow in answer to the recent developments\nof combining workflow management systems with Cloud environments and the still\nlacking abilities of such systems to ensure the security and privacy of\ncloud-based workflows. The SecFlow architecture focuses on full workflow life\ncycle coverage as, in addition to the existing approaches to design\nsecurity-aware processes, there is a need to fill in the gap of maintaining\nsecurity properties of workflows during their execution phase. To address this\ngap, we derive the requirements for such a security-aware WfMS and design a\nsystem architecture that meets these requirements. SecFlow integrates key\nfunctional components such as secure model construction, security-aware service\nselection, security violation detection, and adaptive response mechanisms while\nconsidering all potential malicious parties in multi-tenant and cloud-based\nWfMS.",
        "translated": ""
    },
    {
        "title": "ATWM: Defense against adversarial malware based on adversarial training",
        "url": "http://arxiv.org/abs/2307.05095v1",
        "pub_date": "2023-07-11",
        "summary": "Deep learning technology has made great achievements in the field of image.\nIn order to defend against malware attacks, researchers have proposed many\nWindows malware detection models based on deep learning. However, deep learning\nmodels are vulnerable to adversarial example attacks. Malware can generate\nadversarial malware with the same malicious function to attack the malware\ndetection model and evade detection of the model. Currently, many adversarial\ndefense studies have been proposed, but existing adversarial defense studies\nare based on image sample and cannot be directly applied to malware sample.\nTherefore, this paper proposes an adversarial malware defense method based on\nadversarial training. This method uses preprocessing to defend simple\nadversarial examples to reduce the difficulty of adversarial training.\nMoreover, this method improves the adversarial defense capability of the model\nthrough adversarial training. We experimented with three attack methods in two\nsets of datasets, and the results show that the method in this paper can\nimprove the adversarial defense capability of the model without reducing the\naccuracy of the model.",
        "translated": ""
    },
    {
        "title": "A Blockchain-based two Factor Honeytoken Authentication System",
        "url": "http://arxiv.org/abs/2307.05047v2",
        "pub_date": "2023-07-11",
        "summary": "This paper extends and advances our recently introduced two-factor Honeytoken\nauthentication method by incorporating blockchain technology. This novel\napproach strengthens the authentication method to prevent many attacks\nincluding tampering attacks. Evaluation results show that integrating\nblockchain into the Honeytoken method could improve performance and operational\nefficiency.",
        "translated": ""
    },
    {
        "title": "Information-Theoretically Private Federated Submodel Learning with\n  Storage Constrained Databases",
        "url": "http://arxiv.org/abs/2307.06323v1",
        "pub_date": "2023-07-12",
        "summary": "In federated submodel learning (FSL), a machine learning model is divided\ninto multiple submodels based on different types of data used for training.\nEach user involved in the training process only downloads and updates the\nsubmodel relevant to the user's local data, which significantly reduces the\ncommunication cost compared to classical federated learning (FL). However, the\nindex of the submodel updated by the user and the values of the updates reveal\ninformation about the user's private data. In order to guarantee\ninformation-theoretic privacy in FSL, the model is stored at multiple\nnon-colluding databases, and the user sends queries and updates to each\ndatabase in such a way that no information is revealed on the updating submodel\nindex or the values of the updates. In this work, we consider the practical\nscenario where the multiple non-colluding databases are allowed to have\narbitrary storage constraints. The goal of this work is to develop read-write\nschemes and storage mechanisms for FSL that efficiently utilize the available\nstorage in each database to store the submodel parameters in such a way that\nthe total communication cost is minimized while guaranteeing\ninformation-theoretic privacy of the updating submodel index and the values of\nthe updates. As the main result, we consider both heterogeneous and homogeneous\nstorage constrained databases, and propose private read-write and storage\nschemes for the two cases.",
        "translated": ""
    },
    {
        "title": "Exposing the Fake: Effective Diffusion-Generated Images Detection",
        "url": "http://arxiv.org/abs/2307.06272v1",
        "pub_date": "2023-07-12",
        "summary": "Image synthesis has seen significant advancements with the advent of\ndiffusion-based generative models like Denoising Diffusion Probabilistic Models\n(DDPM) and text-to-image diffusion models. Despite their efficacy, there is a\ndearth of research dedicated to detecting diffusion-generated images, which\ncould pose potential security and privacy risks. This paper addresses this gap\nby proposing a novel detection method called Stepwise Error for\nDiffusion-generated Image Detection (SeDID). Comprising statistical-based\n$\\text{SeDID}_{\\text{Stat}}$ and neural network-based\n$\\text{SeDID}_{\\text{NNs}}$, SeDID exploits the unique attributes of diffusion\nmodels, namely deterministic reverse and deterministic denoising computation\nerrors. Our evaluations demonstrate SeDID's superior performance over existing\nmethods when applied to diffusion models. Thus, our work makes a pivotal\ncontribution to distinguishing diffusion model-generated images, marking a\nsignificant step in the domain of artificial intelligence security.",
        "translated": ""
    },
    {
        "title": "SoK: Comparing Different Membership Inference Attacks with a\n  Comprehensive Benchmark",
        "url": "http://arxiv.org/abs/2307.06123v1",
        "pub_date": "2023-07-12",
        "summary": "Membership inference (MI) attacks threaten user privacy through determining\nif a given data example has been used to train a target model. However, it has\nbeen increasingly recognized that the \"comparing different MI attacks\"\nmethodology used in the existing works has serious limitations. Due to these\nlimitations, we found (through the experiments in this work) that some\ncomparison results reported in the literature are quite misleading. In this\npaper, we seek to develop a comprehensive benchmark for comparing different MI\nattacks, called MIBench, which consists not only the evaluation metrics, but\nalso the evaluation scenarios. And we design the evaluation scenarios from four\nperspectives: the distance distribution of data samples in the target dataset,\nthe distance between data samples of the target dataset, the differential\ndistance between two datasets (i.e., the target dataset and a generated dataset\nwith only nonmembers), and the ratio of the samples that are made no inferences\nby an MI attack. The evaluation metrics consist of ten typical evaluation\nmetrics. We have identified three principles for the proposed \"comparing\ndifferent MI attacks\" methodology, and we have designed and implemented the\nMIBench benchmark with 84 evaluation scenarios for each dataset. In total, we\nhave used our benchmark to fairly and systematically compare 15\nstate-of-the-art MI attack algorithms across 588 evaluation scenarios, and\nthese evaluation scenarios cover 7 widely used datasets and 7 representative\ntypes of models. All codes and evaluations of MIBench are publicly available at\nhttps://github.com/MIBench/MIBench.github.io/blob/main/README.md.",
        "translated": ""
    },
    {
        "title": "Security in Online Freelance Software Development: A case for\n  Distributed Security Responsibility",
        "url": "http://arxiv.org/abs/2307.06066v1",
        "pub_date": "2023-07-12",
        "summary": "Secure software is a cornerstone to safe and resilient digital ecosystems. It\noffers strong foundation to protect users' sensitive data and guard against\ncyber-threats. The rapidly increasing landscape of digital economy has\nencouraged developers from different socio-technical and socio-economic\nbackgrounds to join online freelance marketplaces. While, secure software\npractices facilitate software developers in developing secure software, there\nis paucity of research on how freelance developers adhere to security practices\nand how they can be facilitated to improve their security behavior in\nunder-resourced environments. Moreover, freelance developers are often held\nresponsible for producing insecure code. In this position paper, we review\nexisting literature and argue for the case of distributed security\nresponsibilities in online freelance environment. We propose a research agenda\naimed at offering an organized and systematic effort by researchers to address\nsecurity needs and challenges of online freelance marketplaces. These include:\ncharacterising software security and defining separation of responsibilities,\nbuilding trust in online freelance development communities, leveraging the\npotential of online freelancing platforms in the promotion of secure software\ndevelopment and building adaptive security interventions for online freelance\nsoftware development. The research has the potential to bring forth existing\nsecurity solutions to wider developer community and deliver substantial\nbenefits to the broader security ecosystem.",
        "translated": ""
    },
    {
        "title": "Robbed withdrawal",
        "url": "http://arxiv.org/abs/2307.05992v1",
        "pub_date": "2023-07-12",
        "summary": "In this article we show that Theorem 2 in Lie et al. (2023) is incorrect.\nSince Wombat Exchange, a decentralized exchange, is built upon Lie et al.\n(2023) and Theorem 2 is fundamental to Wombat Finance, we show that an\nundesirable phenomenon, which we call the robbed withdrawal, can happen as a\nconsequence.",
        "translated": ""
    },
    {
        "title": "Introducing Packet-Level Analysis in Programmable Data Planes to Advance\n  Network Intrusion Detection",
        "url": "http://arxiv.org/abs/2307.05936v1",
        "pub_date": "2023-07-12",
        "summary": "Programmable data planes offer precise control over the low-level processing\nsteps applied to network packets, serving as a valuable tool for analysing\nmalicious flows in the field of intrusion detection. Albeit with limitations on\nphysical resources and capabilities, they allow for the efficient extraction of\ndetailed traffic information, which can then be utilised by Machine Learning\n(ML) algorithms responsible for identifying security threats. In addressing\nresource constraints, existing solutions in the literature rely on compressing\nnetwork data through the collection of statistical traffic features in the data\nplane. While this compression saves memory resources in switches and minimises\nthe burden on the control channel between the data and the control plane, it\nalso results in a loss of information available to the Network Intrusion\nDetection System (NIDS), limiting access to packet payload, categorical\nfeatures, and the semantic understanding of network communications, such as the\nbehaviour of packets within traffic flows. This paper proposes P4DDLe, a\nframework that exploits the flexibility of P4-based programmable data planes\nfor packet-level feature extraction and pre-processing. P4DDLe leverages the\nprogrammable data plane to extract raw packet features from the network\ntraffic, categorical features included, and to organise them in a way that the\nsemantics of traffic flows is preserved. To minimise memory and control channel\noverheads, P4DDLe selectively processes and filters packet-level data, so that\nall and only the relevant features required by the NIDS are collected. The\nexperimental evaluation with recent Distributed Denial of Service (DDoS) attack\ndata demonstrates that the proposed approach is very efficient in collecting\ncompact and high-quality representations of network flows, ensuring precise\ndetection of DDoS attacks.",
        "translated": ""
    },
    {
        "title": "Sublinear Message Bounds of Authenticated Implicit Byzantine Agreement",
        "url": "http://arxiv.org/abs/2307.05922v1",
        "pub_date": "2023-07-12",
        "summary": "This paper studies the message complexity of authenticated Byzantine\nagreement (BA) in synchronous, fully-connected distributed networks under an\nhonest majority. We focus on the so-called {\\em implicit} Byzantine agreement\nproblem where each node starts with an input value and at the end a non-empty\nsubset of the honest nodes should agree on a common input value by satisfying\nthe BA properties (i.e., there can be undecided nodes). We show that a\nsublinear (in $n$, number of nodes) message complexity BA protocol under honest\nmajority is possible in the standard PKI model when the nodes have access to an\nunbiased global coin and hash function. In particular, we present a randomized\nByzantine agreement algorithm which, with high probability achieves implicit\nagreement, uses $\\tilde{O}(\\sqrt{n})$ messages, and runs in $\\tilde{O}(1)$\nrounds while tolerating $(1/2 - \\epsilon)n$ Byzantine nodes for any fixed\n$\\epsilon &gt; 0$, the notation $\\Tilde{O}$ hides a $O(\\polylog{n})$ factor. The\nalgorithm requires standard cryptographic setup PKI and hash function with a\nstatic Byzantine adversary. The algorithm works in the CONGEST model and each\nnode does not need to know the identity of its neighbors, i.e., works in the\n$KT_0$ model. The message complexity (and also the time complexity) of our\nalgorithm is optimal up to a $\\polylog n$ factor, as we show a\n$\\Omega(\\sqrt{n})$ lower bound on the message complexity.",
        "translated": ""
    },
    {
        "title": "ObNoCs: Protecting Network-on-Chip Fabrics Against Reverse-Engineering\n  Attacks",
        "url": "http://arxiv.org/abs/2307.05815v1",
        "pub_date": "2023-07-11",
        "summary": "Modern System-on-Chip designs typically use Network-on-Chip (NoC) fabrics to\nimplement coordination among integrated hardware blocks. An important class of\nsecurity vulnerabilities involves a rogue foundry reverse-engineering the NoC\ntopology and routing logic. In this paper, we develop an infrastructure,\n$\\obnocs$, for protecting NoC fabrics against such attacks. $\\obnocs$\nsystematically replaces router connections with switches that can be programmed\nafter fabrication to induce the desired topology. Our approach provides\nprovable redaction of NoC functionality: switch configurations induce a large\nnumber of legal topologies, only one of which corresponds to the intended\ntopology. We implement the $\\obnocs$ methodology on Intel\nQuartus\\texttrademark\\ Platform, and experimental results on realistic SoC\ndesigns show that the architecture incurs minimal overhead in power, resource\nutilization, and system latency.",
        "translated": ""
    },
    {
        "title": "Time Moves Faster When There is Nothing You Anticipate: The Role of Time\n  in MEV Rewards",
        "url": "http://arxiv.org/abs/2307.05814v1",
        "pub_date": "2023-07-11",
        "summary": "This study explores the intricacies of waiting games, a novel dynamic that\nemerged with Ethereum's transition to a Proof-of-Stake (PoS)-based block\nproposer selection protocol. Within this PoS framework, validators acquire a\ndistinct monopoly position during their assigned slots, given that block\nproposal rights are set deterministically, contrasting with Proof-of-Work (PoW)\nprotocols. Consequently, validators have the power to delay block proposals,\nstepping outside the honest validator specs, optimizing potential returns\nthrough MEV payments. Nonetheless, this strategic behaviour introduces the risk\nof orphaning if attestors fail to observe and vote on the block timely. Our\nquantitative analysis of this waiting phenomenon and its associated risks\nreveals an opportunity for enhanced MEV extraction, exceeding standard protocol\nrewards, and providing sufficient incentives for validators to play the game.\nNotably, our findings indicate that delayed proposals do not always result in\norphaning and orphaned blocks are not consistently proposed later than\nnon-orphaned ones. To further examine consensus stability under varying network\nconditions, we adopt an agent-based simulation model tailored for PoS-Ethereum,\nillustrating that consensus disruption will not be observed unless significant\ndelay strategies are adopted. Ultimately, this research offers valuable\ninsights into the advent of waiting games on Ethereum, providing a\ncomprehensive understanding of trade-offs and potential profits for validators\nwithin the blockchain ecosystem.",
        "translated": ""
    },
    {
        "title": "Verifi-Chain: A Credentials Verifier using Blockchain and IPFS",
        "url": "http://arxiv.org/abs/2307.05797v1",
        "pub_date": "2023-07-11",
        "summary": "Submitting fake certificates is a common problem in Southeast Asia, which\nprevents qualified candidates from getting the jobs they deserve. When applying\nfor a job, students must provide academic credentials as proof of their\nqualifications, acquired both inside and outside the classroom. Verifying\nacademic documents before hiring is crucial to prevent fraud. Employing\nblockchain technology has the potential to address this issue. Blockchain\nprovides an electronic certificate that is tamper-proof and non-repudiable,\nmaking it difficult for students to manipulate their academic credentials. This\npaper presents a prototype for an academic credential verification model that\nleverages the security features of blockchain and IPFS (Interplanetary File\nSystem). Certificates are temporarily stored in a database before being\ntransferred to IPFS, where a unique hash code is generated using a hashing\nalgorithm. This hash code serves as the certificate's unique identity and is\nstored in the blockchain nodes. Companies can verify an applicant's credentials\nby searching for the applicant and accessing their already verified\ncertificates. Utilizing IPFS as a middleman storage platform lowers the\nexpenses of directly storing massive data on the blockchain. To sum it up, the\nproposed solution would make the process of certificate verification more\nefficient, secure, and cost-effective. It would save time and resources that\nwould otherwise be used to manually verify certificates.",
        "translated": ""
    },
    {
        "title": "PHOENI2X -- A European Cyber Resilience Framework With\n  Artificial-Intelligence-Assisted Orchestration, Automation and Response\n  Capabilities for Business Continuity and Recovery, Incident Response, and\n  Information Exchange",
        "url": "http://arxiv.org/abs/2307.06932v1",
        "pub_date": "2023-07-13",
        "summary": "As digital technologies become more pervasive in society and the economy,\ncybersecurity incidents become more frequent and impactful. According to the\nNIS and NIS2 Directives, EU Member States and their Operators of Essential\nServices must establish a minimum baseline set of cybersecurity capabilities\nand engage in cross-border coordination and cooperation. However, this is only\na small step towards European cyber resilience. In this landscape,\npreparedness, shared situational awareness, and coordinated incident response\nare essential for effective cyber crisis management and resilience. Motivated\nby the above, this paper presents PHOENI2X, an EU-funded project aiming to\ndesign, develop, and deliver a Cyber Resilience Framework providing\nArtificial-Intelligence-assisted orchestration, automation and response\ncapabilities for business continuity and recovery, incident response, and\ninformation exchange, tailored to the needs of Operators of Essential Services\nand the EU Member State authorities entrusted with cybersecurity.",
        "translated": ""
    },
    {
        "title": "DAXiot: A Decentralized Authentication and Authorization Scheme for\n  Dynamic IoT Networks",
        "url": "http://arxiv.org/abs/2307.06919v1",
        "pub_date": "2023-07-13",
        "summary": "Federated and decentralized networks supporting frequently changing system\nparticipants are a requirement for future Internet of Things (IoT) use cases.\nIoT devices and networks often lack adequate authentication and authorization\nmechanisms, resulting in insufficient privacy for entities in such systems. In\nthis work we address both issues by designing a privacy preserving\nchallenge-response style authentication and authorization scheme based on\nDecentralized Identifiers and Verifiable Credentials. Our solution allows a\ndecentralized permission management of frequently changing network participants\nand supports authenticated encryption for data confidentiality. We demonstrate\nour solution in an MQTT 5.0 scenario and evaluate its security, privacy\nguarantees, and performance.",
        "translated": ""
    },
    {
        "title": "Data Behind the Walls An Advanced Architecture for Data Privacy\n  Management",
        "url": "http://arxiv.org/abs/2307.06779v1",
        "pub_date": "2023-07-13",
        "summary": "In today's highly connected society, we are constantly asked to provide\npersonal information to retailers, voter surveys, medical professionals, and\nother data collection efforts. The collected data is stored in large data\nwarehouses. Organisations and statistical agencies share and use this data to\nfacilitate research in public health, economics, sociology, etc. However, this\ndata contains sensitive information about individuals, which can result in\nidentity theft, financial loss, stress and depression, embarrassment, abuse,\netc. Therefore, one must ensure rigorous management of individuals' privacy. We\npropose, an advanced data privacy management architecture composed of three\nlayers. The data management layer consists of de-identification and\nanonymisation, the access management layer for re-enforcing data access based\non the concepts of Role-Based Access Control and the Chinese Wall Security\nPolicy, and the roles layer for regulating different users. The proposed system\narchitecture is validated on healthcare datasets.",
        "translated": ""
    },
    {
        "title": "Privacy-Utility Trade-offs in Neural Networks for Medical Population\n  Graphs: Insights from Differential Privacy and Graph Structure",
        "url": "http://arxiv.org/abs/2307.06760v1",
        "pub_date": "2023-07-13",
        "summary": "We initiate an empirical investigation into differentially private graph\nneural networks on population graphs from the medical domain by examining\nprivacy-utility trade-offs at different privacy levels on both real-world and\nsynthetic datasets and performing auditing through membership inference\nattacks. Our findings highlight the potential and the challenges of this\nspecific DP application area. Moreover, we find evidence that the underlying\ngraph structure constitutes a potential factor for larger performance gaps by\nshowing a correlation between the degree of graph homophily and the accuracy of\nthe trained model.",
        "translated": ""
    },
    {
        "title": "To share or not to share: What risks would laypeople accept to give\n  sensitive data to differentially-private NLP systems?",
        "url": "http://arxiv.org/abs/2307.06708v1",
        "pub_date": "2023-07-13",
        "summary": "Although the NLP community has adopted central differential privacy as a\ngo-to framework for privacy-preserving model training or data sharing, the\nchoice and interpretation of the key parameter, privacy budget $\\varepsilon$\nthat governs the strength of privacy protection, remains largely arbitrary. We\nargue that determining the $\\varepsilon$ value should not be solely in the\nhands of researchers or system developers, but must also take into account the\nactual people who share their potentially sensitive data. In other words: Would\nyou share your instant messages for $\\varepsilon$ of 10? We address this\nresearch gap by designing, implementing, and conducting a behavioral experiment\n(311 lay participants) to study the behavior of people in uncertain\ndecision-making situations with respect to privacy-threatening situations.\nFraming the risk perception in terms of two realistic NLP scenarios and using a\nvignette behavioral study help us determine what $\\varepsilon$ thresholds would\nlead lay people to be willing to share sensitive textual data - to our\nknowledge, the first study of its kind.",
        "translated": ""
    },
    {
        "title": "Towards Traitor Tracing in Black-and-White-Box DNN Watermarking with\n  Tardos-based Codes",
        "url": "http://arxiv.org/abs/2307.06695v1",
        "pub_date": "2023-07-13",
        "summary": "The growing popularity of Deep Neural Networks, which often require\ncomputationally expensive training and access to a vast amount of data, calls\nfor accurate authorship verification methods to deter unlawful dissemination of\nthe models and identify the source of the leak. In DNN watermarking the owner\nmay have access to the full network (white-box) or only be able to extract\ninformation from its output to queries (black-box), but a watermarked model may\ninclude both approaches in order to gather sufficient evidence to then gain\naccess to the network. Although there has been limited research in white-box\nwatermarking that considers traitor tracing, this problem is yet to be explored\nin the black-box scenario. In this paper, we propose a black-and-white-box\nwatermarking method that opens the door to collusion-resistant traitor tracing\nin black-box, exploiting the properties of Tardos codes, and making it possible\nto identify the source of the leak before access to the model is granted. While\nexperimental results show that the method can successfully identify traitors,\neven when further attacks have been performed, we also discuss its limitations\nand open problems for traitor tracing in black-box.",
        "translated": ""
    },
    {
        "title": "Uncovering the Deceptions: An Analysis on Audio Spoofing Detection and\n  Future Prospects",
        "url": "http://arxiv.org/abs/2307.06669v1",
        "pub_date": "2023-07-13",
        "summary": "Audio has become an increasingly crucial biometric modality due to its\nability to provide an intuitive way for humans to interact with machines. It is\ncurrently being used for a range of applications, including person\nauthentication to banking to virtual assistants. Research has shown that these\nsystems are also susceptible to spoofing and attacks. Therefore, protecting\naudio processing systems against fraudulent activities, such as identity theft,\nfinancial fraud, and spreading misinformation, is of paramount importance. This\npaper reviews the current state-of-the-art techniques for detecting audio\nspoofing and discusses the current challenges along with open research\nproblems. The paper further highlights the importance of considering the\nethical and privacy implications of audio spoofing detection systems. Lastly,\nthe work aims to accentuate the need for building more robust and generalizable\nmethods, the integration of automatic speaker verification and countermeasure\nsystems, and better evaluation protocols.",
        "translated": ""
    },
    {
        "title": "A Comprehensive Analysis of Blockchain Applications for Securing\n  Computer Vision Systems",
        "url": "http://arxiv.org/abs/2307.06659v1",
        "pub_date": "2023-07-13",
        "summary": "Blockchain (BC) and Computer Vision (CV) are the two emerging fields with the\npotential to transform various sectors.The ability of BC can help in offering\ndecentralized and secure data storage, while CV allows machines to learn and\nunderstand visual data. This integration of the two technologies holds massive\npromise for developing innovative applications that can provide solutions to\nthe challenges in various sectors such as supply chain management, healthcare,\nsmart cities, and defense. This review explores a comprehensive analysis of the\nintegration of BC and CV by examining their combination and potential\napplications. It also provides a detailed analysis of the fundamental concepts\nof both technologies, highlighting their strengths and limitations. This paper\nalso explores current research efforts that make use of the benefits offered by\nthis combination. The effort includes how BC can be used as an added layer of\nsecurity in CV systems and also ensure data integrity, enabling decentralized\nimage and video analytics using BC. The challenges and open issues associated\nwith this integration are also identified, and appropriate potential future\ndirections are also proposed.",
        "translated": ""
    },
    {
        "title": "SecureFalcon: The Next Cyber Reasoning System for Cyber Security",
        "url": "http://arxiv.org/abs/2307.06616v1",
        "pub_date": "2023-07-13",
        "summary": "Software vulnerabilities leading to various detriments such as crashes, data\nloss, and security breaches, significantly hinder the quality, affecting the\nmarket adoption of software applications and systems. Although traditional\nmethods such as automated software testing, fault localization, and repair have\nbeen intensively studied, static analysis tools are most commonly used and have\nan inherent false positives rate, posing a solid challenge to developer\nproductivity. Large Language Models (LLMs) offer a promising solution to these\npersistent issues. Among these, FalconLLM has shown substantial potential in\nidentifying intricate patterns and complex vulnerabilities, hence crucial in\nsoftware vulnerability detection. In this paper, for the first time, FalconLLM\nis being fine-tuned for cybersecurity applications, thus introducing\nSecureFalcon, an innovative model architecture built upon FalconLLM.\nSecureFalcon is trained to differentiate between vulnerable and non-vulnerable\nC code samples. We build a new training dataset, FormAI, constructed thanks to\nGenerative Artificial Intelligence (AI) and formal verification to evaluate its\nperformance. SecureFalcon achieved an impressive 94% accuracy rate in detecting\nsoftware vulnerabilities, emphasizing its significant potential to redefine\nsoftware vulnerability detection methods in cybersecurity.",
        "translated": ""
    },
    {
        "title": "Introducing Foundation Models as Surrogate Models: Advancing Towards\n  More Practical Adversarial Attacks",
        "url": "http://arxiv.org/abs/2307.06608v1",
        "pub_date": "2023-07-13",
        "summary": "Recently, the no-box adversarial attack, in which the attacker lacks access\nto the model's architecture, weights, and training data, become the most\npractical and challenging attack setup. However, there is an unawareness of the\npotential and flexibility inherent in the surrogate model selection process on\nno-box setting. Inspired by the burgeoning interest in utilizing foundational\nmodels to address downstream tasks, this paper adopts an innovative idea that\n1) recasting adversarial attack as a downstream task. Specifically, image noise\ngeneration to meet the emerging trend and 2) introducing foundational models as\nsurrogate models. Harnessing the concept of non-robust features, we elaborate\non two guiding principles for surrogate model selection to explain why the\nfoundational model is an optimal choice for this role. However, paradoxically,\nwe observe that these foundational models underperform. Analyzing this\nunexpected behavior within the feature space, we attribute the lackluster\nperformance of foundational models (e.g., CLIP) to their significant\nrepresentational capacity and, conversely, their lack of discriminative\nprowess. To mitigate this issue, we propose the use of a margin-based loss\nstrategy for the fine-tuning of foundational models on target images. The\nexperimental results verify that our approach, which employs the basic Fast\nGradient Sign Method (FGSM) attack algorithm, outstrips the performance of\nother, more convoluted algorithms. We conclude by advocating for the research\ncommunity to consider surrogate models as crucial determinants in the\neffectiveness of adversarial attacks in no-box settings. The implications of\nour work bear relevance for improving the efficacy of such adversarial attacks\nand the overall robustness of AI systems.",
        "translated": ""
    },
    {
        "title": "Exhaustive Generation of Linear Orthogonal Cellular Automata",
        "url": "http://arxiv.org/abs/2307.07505v1",
        "pub_date": "2023-07-14",
        "summary": "We consider the problem of exhaustively visiting all pairs of linear cellular\nautomata which give rise to orthogonal Latin squares, i.e., linear Orthogonal\nCellular Automata (OCA). The problem is equivalent to enumerating all pairs of\ncoprime polynomials over a finite field having the same degree and a nonzero\nconstant term. While previous research showed how to count all such pairs for a\ngiven degree and order of the finite field, no practical enumeration algorithms\nhave been proposed so far. Here, we start closing this gap by addressing the\ncase of polynomials defined over the field $\\F_2$, which corresponds to binary\nCA. In particular, we exploit Benjamin and Bennett's bijection between coprime\nand non-coprime pairs of polynomials, which enables us to organize our study\nalong three subproblems, namely the enumeration and count of: (1) sequences of\nconstant terms, (2) sequences of degrees, and (3) sequences of intermediate\nterms. In the course of this investigation, we unveil interesting connections\nwith algebraic language theory and combinatorics, obtaining an enumeration\nalgorithm and an alternative derivation of the counting formula for this\nproblem.",
        "translated": ""
    },
    {
        "title": "TUSH-Key: Transferable User Secrets on Hardware Key",
        "url": "http://arxiv.org/abs/2307.07484v1",
        "pub_date": "2023-07-14",
        "summary": "Passwordless authentication was first tested for seamless and secure merchant\npayments without the use of passwords or pins. It opened a whole new world of\nauthentications giving up the former reliance on traditional passwords. It\nrelied on the W3C Web Authentication (WebAuthn) and Client to Authenticator\nProtocol (CTAP) standards to use the public key cryptosystem to uniquely attest\na user's device and then their identity. These standards comprise of the FIDO\nauthentication standard. As the popularity of passwordless is increasing, more\nand more users and service providers are adopting to it. However, the concept\nof device attestation makes it device-specific for a user. It makes it\ndifficult for a user to switch devices. FIDO Passkeys were aimed at solving the\nsame, synchronizing the private cryptographic keys across multiple devices so\nthat the user can perform passwordless authentication even from devices not\nexplicitly enrolled with the service provider. However, passkeys have certain\ndrawbacks including that it uses proprietary end to end encryption algorithms,\nall keys pass through proprietary cloud provider, and it is usually not very\nseamless when dealing with cross-platform key synchronization. To deal with the\nproblems and drawbacks of FIDO Passkeys, the paper proposes a novel private key\nmanagement system for passwordless authentication called Transferable User\nSecret on Hardware Key (TUSH-Key). TUSH-Key allows cross-platform\nsynchronization of devices for seamless passwordless logins with FIDO2\nspecifications.",
        "translated": ""
    },
    {
        "title": "Population Expansion for Training Language Models with Private Federated\n  Learning",
        "url": "http://arxiv.org/abs/2307.07477v1",
        "pub_date": "2023-07-14",
        "summary": "Federated learning (FL) combined with differential privacy (DP) offers\nmachine learning (ML) training with distributed devices and with a formal\nprivacy guarantee. With a large population of devices, FL with DP produces a\nperformant model in a timely manner. However, for applications with a smaller\npopulation, not only does the model utility degrade as the DP noise is\ninversely proportional to population, but also the training latency increases\nsince waiting for enough clients to become available from a smaller pool is\nslower. In this work, we thus propose expanding the population based on domain\nadaptation techniques to speed up the training and improves the final model\nquality when training with small populations. We empirically demonstrate that\nour techniques can improve the utility by 13% to 30% on real-world language\nmodeling datasets.",
        "translated": ""
    },
    {
        "title": "Differentially Private Clustering in Data Streams",
        "url": "http://arxiv.org/abs/2307.07449v1",
        "pub_date": "2023-07-14",
        "summary": "The streaming model is an abstraction of computing over massive data streams,\nwhich is a popular way of dealing with large-scale modern data analysis. In\nthis model, there is a stream of data points, one after the other. A streaming\nalgorithm is only allowed one pass over the data stream, and the goal is to\nperform some analysis during the stream while using as small space as possible.\n  Clustering problems (such as $k$-means and $k$-median) are fundamental\nunsupervised machine learning primitives, and streaming clustering algorithms\nhave been extensively studied in the past. However, since data privacy becomes\na central concern in many real-world applications, non-private clustering\nalgorithms are not applicable in many scenarios.\n  In this work, we provide the first differentially private streaming\nalgorithms for $k$-means and $k$-median clustering of $d$-dimensional Euclidean\ndata points over a stream with length at most $T$ using $poly(k,d,\\log(T))$\nspace to achieve a {\\it constant} multiplicative error and a\n$poly(k,d,\\log(T))$ additive error. In particular, we present a differentially\nprivate streaming clustering framework which only requires an offline DP\ncoreset algorithm as a blackbox. By plugging in existing DP coreset results via\nGhazi, Kumar, Manurangsi 2020 and Kaplan, Stemmer 2018, we achieve (1) a\n$(1+\\gamma)$-multiplicative approximation with\n$\\tilde{O}_\\gamma(poly(k,d,\\log(T)))$ space for any $\\gamma&gt;0$, and the\nadditive error is $poly(k,d,\\log(T))$ or (2) an $O(1)$-multiplicative\napproximation with $\\tilde{O}(k \\cdot poly(d,\\log(T)))$ space and\n$poly(k,d,\\log(T))$ additive error.\n  In addition, our algorithmic framework is also differentially private under\nthe continual release setting, i.e., the union of outputs of our algorithms at\nevery timestamp is always differentially private.",
        "translated": ""
    },
    {
        "title": "Boosting Backdoor Attack with A Learnable Poisoning Sample Selection\n  Strategy",
        "url": "http://arxiv.org/abs/2307.07328v1",
        "pub_date": "2023-07-14",
        "summary": "Data-poisoning based backdoor attacks aim to insert backdoor into models by\nmanipulating training datasets without controlling the training process of the\ntarget model. Existing attack methods mainly focus on designing triggers or\nfusion strategies between triggers and benign samples. However, they often\nrandomly select samples to be poisoned, disregarding the varying importance of\neach poisoning sample in terms of backdoor injection. A recent selection\nstrategy filters a fixed-size poisoning sample pool by recording forgetting\nevents, but it fails to consider the remaining samples outside the pool from a\nglobal perspective. Moreover, computing forgetting events requires significant\nadditional computing resources. Therefore, how to efficiently and effectively\nselect poisoning samples from the entire dataset is an urgent problem in\nbackdoor attacks.To address it, firstly, we introduce a poisoning mask into the\nregular backdoor training loss. We suppose that a backdoored model training\nwith hard poisoning samples has a more backdoor effect on easy ones, which can\nbe implemented by hindering the normal training process (\\ie, maximizing loss\n\\wrt mask). To further integrate it with normal training process, we then\npropose a learnable poisoning sample selection strategy to learn the mask\ntogether with the model parameters through a min-max optimization.Specifically,\nthe outer loop aims to achieve the backdoor attack goal by minimizing the loss\nbased on the selected samples, while the inner loop selects hard poisoning\nsamples that impede this goal by maximizing the loss. After several rounds of\nadversarial training, we finally select effective poisoning samples with high\ncontribution. Extensive experiments on benchmark datasets demonstrate the\neffectiveness and efficiency of our approach in boosting backdoor attack\nperformance.",
        "translated": ""
    },
    {
        "title": "Evaluation Methodologies in Software Protection Research",
        "url": "http://arxiv.org/abs/2307.07300v1",
        "pub_date": "2023-07-14",
        "summary": "Man-at-the-end (MATE) attackers have full control over the system on which\nthe attacked software runs, and try to break the confidentiality or integrity\nof assets embedded in the software. Both companies and malware authors want to\nprevent such attacks. This has driven an arms race between attackers and\ndefenders, resulting in a plethora of different protection and analysis\nmethods. However, it remains difficult to measure the strength of protections\nbecause MATE attackers can reach their goals in many different ways and a\nuniversally accepted evaluation methodology does not exist. This survey\nsystematically reviews the evaluation methodologies of papers on obfuscation, a\nmajor class of protections against MATE attacks. For 572 papers, we collected\n113 aspects of their evaluation methodologies, ranging from sample set types\nand sizes, over sample treatment, to performed measurements. We provide\ndetailed insights into how the academic state of the art evaluates both the\nprotections and analyses thereon. In summary, there is a clear need for better\nevaluation methodologies. We identify nine challenges for software protection\nevaluations, which represent threats to the validity, reproducibility, and\ninterpretation of research results in the context of MATE attacks.",
        "translated": ""
    },
    {
        "title": "The Automation of the Extraction of Evidence masked by Steganographic\n  Techniques in WAV and MP3 Audio Files",
        "url": "http://arxiv.org/abs/2307.07293v1",
        "pub_date": "2023-07-14",
        "summary": "Antiforensics techniques and particularly steganography and cryptography have\nbecome increasingly pressing issues that affect the current digital forensics\npractice, both techniques are widely researched and developed as considered in\nthe heart of the modern digital era but remain double edged swords standing\nbetween the privacy conscious and the criminally malicious, dependent on the\nseverity of the methods deployed. This paper advances the automation of hidden\nevidence extraction in the context of audio files enabling the correlation\nbetween unprocessed evidence artefacts and extreme Steganographic and\nCryptographic techniques using the Least Significant Bits extraction method\n(LSB). The research generates an in-depth review of current digital forensic\ntoolkit and systems and formally address their capabilities in handling\nsteganography-related cases, we opted for experimental research methodology in\nthe form of quantitative analysis of the efficiency of detecting and extraction\nof hidden artefacts in WAV and MP3 audio files by comparing standard industry\nsoftware. This work establishes an environment for the practical implementation\nand testing of the proposed approach and the new toolkit for extracting\nevidence hidden by Cryptographic and Steganographic techniques during forensics\ninvestigations. The proposed multi-approach automation demonstrated a huge\npositive impact in terms of efficiency and accuracy and notably on large audio\nfiles (MP3 and WAV) which the forensics analysis is time-consuming and requires\nsignificant computational resources and memory. However, the proposed\nautomation may occasionally produce false positives (detecting steganography\nwhere none exists) or false negatives (failing to detect steganography that is\npresent) but overall achieve a balance between detecting hidden data accurately\nalong with minimising the false alarms.",
        "translated": ""
    },
    {
        "title": "Polarization-Based Security: Safeguarding Wireless Communications at the\n  Physical Layer",
        "url": "http://arxiv.org/abs/2307.07244v1",
        "pub_date": "2023-07-14",
        "summary": "Physical layer security is a field of study that continues to gain importance\nover time. It encompasses a range of algorithms applicable to various aspects\nof communication systems. While research in the physical layer has\npredominantly focused on secrecy capacity, which involves logical and digital\nmanipulations to achieve secure communication, there is limited exploration of\ndirectly manipulating electromagnetic fields to enhance security against\neavesdroppers. In this paper, we propose a novel system that utilizes the\nMueller calculation to establish a theoretical framework for manipulating\nelectromagnetic fields in the context of physical layer security. We develop\nfundamental expressions and introduce new metrics to analyze the system's\nperformance analytically. Additionally, we present three techniques that\nleverage polarization to enhance physical layer security.",
        "translated": ""
    },
    {
        "title": "Proof of Training (PoT): Harnessing Crypto Mining Power for Distributed\n  AI Training",
        "url": "http://arxiv.org/abs/2307.07066v1",
        "pub_date": "2023-07-13",
        "summary": "In the midst of the emerging trend of integrating artificial intelligence\n(AI) with crypto mining, we identify three major challenges that create a gap\nbetween these two fields. To bridge this gap, we introduce the\nproof-of-training (PoT) protocol, an approach that combines the strengths of\nboth AI and blockchain technology. The PoT protocol utilizes the practical\nByzantine fault tolerance (PBFT) consensus mechanism to synchronize global\nstates. To evaluate the performance of the protocol design, we present an\nimplementation of a decentralized training network (DTN) that adopts the PoT\nprotocol. Our results indicate that the protocol exhibits considerable\npotential in terms of task throughput, system robustness, and network security.",
        "translated": ""
    },
    {
        "title": "Information Leakage from Optical Emanations",
        "url": "http://arxiv.org/abs/2307.07043v1",
        "pub_date": "2023-07-13",
        "summary": "A previously unknown form of compromising emanations has been discovered. LED\nstatus indicators on data communication equipment, under certain conditions,\nare shown to carry a modulated optical signal that is significantly correlated\nwith information being processed by the device. Physical access is not\nrequired; the attacker gains access to all data going through the device,\nincluding plaintext in the case of data encryption systems. Experiments show\nthat it is possible to intercept data under realistic conditions at a\nconsiderable distance. Many different sorts of devices, including modems and\nInternet Protocol routers, were found to be vulnerable. A taxonomy of\ncompromising optical emanations is developed, and design changes are described\nthat will successfully block this kind of \"Optical TEMPEST\" attack.",
        "translated": ""
    },
    {
        "title": "Secure Composition of Robust and Optimising Compilers",
        "url": "http://arxiv.org/abs/2307.08681v1",
        "pub_date": "2023-07-17",
        "summary": "To ensure that secure applications do not leak their secrets, they are\nrequired to uphold several security properties such as spatial and temporal\nmemory safety as well as cryptographic constant time. Existing work shows how\nto enforce these properties individually, in an architecture-independent way,\nby using secure compiler passes that each focus on an individual property.\nUnfortunately, given two secure compiler passes that each preserve a possibly\ndifferent security property, it is unclear what kind of security property is\npreserved by the composition of those secure compiler passes. This paper is the\nfirst to study what security properties are preserved across the composition of\ndifferent secure compiler passes. Starting from a general theory of property\ncomposition for security-relevant properties (such as the aforementioned ones),\nthis paper formalises a theory of composition of secure compilers. Then, it\nshowcases this theory a secure multi-pass compiler that preserves the\naforementioned security-relevant properties. Crucially, this paper derives the\nsecurity of the multi-pass compiler from the composition of the security\nproperties preserved by its individual passes, which include\nsecurity-preserving as well as optimisation passes. From an engineering\nperspective, this is the desirable approach to building secure compilers.",
        "translated": ""
    },
    {
        "title": "MIRA: a Digital Signature Scheme based on the MinRank problem and the\n  MPC-in-the-Head paradigm",
        "url": "http://arxiv.org/abs/2307.08575v1",
        "pub_date": "2023-07-17",
        "summary": "We exploit the idea of [Fen22] which proposes to build an efficient signature\nscheme based on a zero-knowledge proof of knowledge of a solution of a MinRank\ninstance. The scheme uses the MPCitH paradigm, which is an efficient way to\nbuild ZK proofs. We combine this idea with another idea, the hypercube\ntechnique introduced in [AMGH+22], which leads to more efficient MPCitH-based\nscheme. This new approach is more efficient than classical MPCitH, as it allows\nto reduce the number of party computation. This gives us a first scheme called\nMIRA-Additive. We then present an other scheme, based on low-threshold secret\nsharings, called MIRA-Threshold, which is a faster scheme, at the price of\nlarger signatures. The construction of MPCitH using threshold secret sharing is\ndetailed in [FR22]. These two constructions allows us to be faster than\nclassical MPCitH, with a size of signature around 5.6kB with MIRA-Additive, and\n8.3kB with MIRA-Threshold. We detail here the constructions and optimizations\nof the schemes, as well as their security proofs.",
        "translated": ""
    },
    {
        "title": "TorMult: Introducing a Novel Tor Bandwidth Inflation Attack",
        "url": "http://arxiv.org/abs/2307.08550v1",
        "pub_date": "2023-07-17",
        "summary": "The Tor network is the most prominent system for providing anonymous\ncommunication to web users, with a daily user base of 2 million users. However,\nsince its inception, it has been constantly targeted by various traffic\nfingerprinting and correlation attacks aiming at deanonymizing its users. A\ncritical requirement for these attacks is to attract as much user traffic to\nadversarial relays as possible, which is typically accomplished by means of\nbandwidth inflation attacks. This paper proposes a new inflation attack vector\nin Tor, referred to as TorMult, which enables inflation of measured bandwidth.\nThe underlying attack technique exploits resource sharing among Tor relay nodes\nand employs a cluster of attacker-controlled relays with coordinated resource\nallocation within the cluster to deceive bandwidth measurers into believing\nthat each relay node in the cluster possesses ample resources. We propose two\nattack variants, C-TorMult and D-TorMult, and test both versions in a private\nTor test network. Our evaluation demonstrates that an attacker can inflate the\nmeasured bandwidth by a factor close to n using C-TorMult and nearly half n*N\nusing D-TorMult, where n is the size of the cluster hosted on one server and N\nis the number of servers. Furthermore, our theoretical analysis reveals that\ngaining control over half of the Tor network's traffic can be achieved by\nemploying just 10 dedicated servers with a cluster size of 109 relays running\nthe TorMult attack, each with a bandwidth of 100MB/s. The problem is further\nexacerbated by the fact that Tor not only allows resource sharing but,\naccording to recent reports, even promotes it.",
        "translated": ""
    },
    {
        "title": "G-Scan: Graph Neural Networks for Line-Level Vulnerability\n  Identification in Smart Contracts",
        "url": "http://arxiv.org/abs/2307.08549v1",
        "pub_date": "2023-07-17",
        "summary": "Due to the immutable and decentralized nature of Ethereum (ETH) platform,\nsmart contracts are prone to security risks that can result in financial loss.\nWhile existing machine learning-based vulnerability detection algorithms\nachieve high accuracy at the contract level, they require developers to\nmanually inspect source code to locate bugs. To this end, we present G-Scan,\nthe first end-to-end fine-grained line-level vulnerability detection system\nevaluated on the first-of-its-kind real world dataset. G-Scan first converts\nsmart contracts to code graphs in a dependency and hierarchy preserving manner.\nNext, we train a graph neural network to identify vulnerable nodes and assess\nsecurity risks. Finally, the code graphs with node vulnerability predictions\nare mapped back to the smart contracts for line-level localization. We train\nand evaluate G-Scan on a collected real world smart contracts dataset with\nline-level annotations on reentrancy vulnerability, one of the most common and\nsevere types of smart contract vulnerabilities. With the well-designed graph\nrepresentation and high-quality dataset, G-Scan achieves 93.02% F1-score in\ncontract-level vulnerability detection and 93.69% F1-score in line-level\nvulnerability localization. Additionally, the lightweight graph neural network\nenables G-Scan to localize vulnerabilities in 6.1k lines of code smart contract\nwithin 1.2 seconds.",
        "translated": ""
    },
    {
        "title": "Metadata-based Malware Detection on Android using Machine Learning",
        "url": "http://arxiv.org/abs/2307.08547v1",
        "pub_date": "2023-07-17",
        "summary": "In the digitized world, smartphones and their apps play an important role. To\nname just a few examples, some apps offer possibilities for entertainment,\nothers for online banking, and others offer support for two-factor\nauthentication. Therefore, with smartphones also, sensitive information is\nshared; thus, they are a desirable target for malware. The following technical\nreport gives an overview of how machine learning, especially neural networks,\ncan be employed to detect malicious Android apps based on their metadata.\nDetection based on the metadata is necessary since not all of an app's\ninformation is readable from another app due to the security layout of Android.\nTo do so, a comparable big dataset of metadata of apps has been collected for\nlearning and evaluation in this work. The first section, after the\nintroduction, presents the related work, followed by the description of the\nsources of the dataset and the selection of the features used for machine\nlearning, in this case, only the app permissions. Afterward, a free available\ndataset is used to find an efficient and effective neural network model for\nlearning and evaluation. Here, the fully connected network type consisting of\ndense layers is chosen. Then this model is trained and evaluated on the new,\nmore extensive dataset to obtain a representative result. It turns out that\nthis model detects malware with an accuracy of 92.93% based on an app's\npermissions.",
        "translated": ""
    },
    {
        "title": "A Privacy-Preserving Blockchain-based E-voting System",
        "url": "http://arxiv.org/abs/2307.08412v1",
        "pub_date": "2023-07-17",
        "summary": "Within a modern democratic nation, elections play a significant role in the\nnation's functioning. However, with the existing infrastructure for conducting\nelections using Electronic Voting Systems (EVMs), many loopholes exist, which\nillegitimate entities might leverage to cast false votes or even tamper with\nthe EVMs after the voting session is complete. The need of the hour is to\nintroduce a robust, auditable, transparent, and tamper-proof e-voting system,\nenabling a more reliable and fair election process. To address such concerns,\nwe propose a novel solution for blockchain-based e-voting, focusing on the\nsecurity and privacy aspects of the e-voting process. We consider the security\nrisks and loopholes and aim to preserve the anonymity of the voters while\nensuring that illegitimate votes are properly handled. Additionally, we develop\na prototype as a proof of concept using the Ethereum blockchain platform.\nFinally, we perform experiments to demonstrate the performance of the system.",
        "translated": ""
    },
    {
        "title": "Are we there yet? An Industrial Viewpoint on Provenance-based Endpoint\n  Detection and Response Tools",
        "url": "http://arxiv.org/abs/2307.08349v1",
        "pub_date": "2023-07-17",
        "summary": "Provenance-Based Endpoint Detection and Response (P-EDR) systems are deemed\ncrucial for future APT defenses. Despite the fact that numerous new techniques\nto improve P-EDR systems have been proposed in academia, it is still unclear\nwhether the industry will adopt P-EDR systems and what improvements the\nindustry desires for P-EDR systems. To this end, we conduct the first set of\nsystematic studies on the effectiveness and the limitations of P-EDR systems.\nOur study consists of four components: a one-to-one interview, an online\nquestionnaire study, a survey of the relevant literature, and a systematic\nmeasurement study. Our research indicates that all industry experts consider\nP-EDR systems to be more effective than conventional Endpoint Detection and\nResponse (EDR) systems. However, industry experts are concerned about the\noperating cost of P-EDR systems. In addition, our research reveals three\nsignificant gaps between academia and industry: (1) overlooking client-side\noverhead; (2) imbalanced alarm triage cost and interpretation cost; and (3)\nexcessive server-side memory consumption. This paper's findings provide\nobjective data on the effectiveness of P-EDR systems and how much improvements\nare needed to adopt P-EDR systems in industry.",
        "translated": ""
    },
    {
        "title": "A Secure Aggregation for Federated Learning on Long-Tailed Data",
        "url": "http://arxiv.org/abs/2307.08324v1",
        "pub_date": "2023-07-17",
        "summary": "As a distributed learning, Federated Learning (FL) faces two challenges: the\nunbalanced distribution of training data among participants, and the model\nattack by Byzantine nodes. In this paper, we consider the long-tailed\ndistribution with the presence of Byzantine nodes in the FL scenario. A novel\ntwo-layer aggregation method is proposed for the rejection of malicious models\nand the advisable selection of valuable models containing tail class data\ninformation. We introduce the concept of think tank to leverage the wisdom of\nall participants. Preliminary experiments validate that the think tank can make\neffective model selections for global aggregation.",
        "translated": ""
    },
    {
        "title": "LogPrécis: Unleashing Language Models for Automated Shell Log Analysis",
        "url": "http://arxiv.org/abs/2307.08309v1",
        "pub_date": "2023-07-17",
        "summary": "The collection of security-related logs holds the key to understanding attack\nbehaviors and diagnosing vulnerabilities. Still, their analysis remains a\ndaunting challenge. Recently, Language Models (LMs) have demonstrated unmatched\npotential in understanding natural and programming languages. The question\narises whether and how LMs could be also useful for security experts since\ntheir logs contain intrinsically confused and obfuscated information. In this\npaper, we systematically study how to benefit from the state-of-the-art in LM\nto automatically analyze text-like Unix shell attack logs. We present a\nthorough design methodology that leads to LogPr\\'ecis. It receives as input raw\nshell sessions and automatically identifies and assigns the attacker tactic to\neach portion of the session, i.e., unveiling the sequence of the attacker's\ngoals. We demonstrate LogPr\\'ecis capability to support the analysis of two\nlarge datasets containing about 400,000 unique Unix shell attacks. LogPr\\'ecis\nreduces them into about 3,000 fingerprints, each grouping sessions with the\nsame sequence of tactics. The abstraction it provides lets the analyst better\nunderstand attacks, identify fingerprints, detect novelty, link similar\nattacks, and track families and mutations. Overall, LogPr\\'ecis, released as\nopen source, paves the way for better and more responsive defense against\ncyberattacks.",
        "translated": ""
    },
    {
        "title": "Adversarial Attacks on Traffic Sign Recognition: A Survey",
        "url": "http://arxiv.org/abs/2307.08278v1",
        "pub_date": "2023-07-17",
        "summary": "Traffic sign recognition is an essential component of perception in\nautonomous vehicles, which is currently performed almost exclusively with deep\nneural networks (DNNs). However, DNNs are known to be vulnerable to adversarial\nattacks. Several previous works have demonstrated the feasibility of\nadversarial attacks on traffic sign recognition models. Traffic signs are\nparticularly promising for adversarial attack research due to the ease of\nperforming real-world attacks using printed signs or stickers. In this work, we\nsurvey existing works performing either digital or real-world attacks on\ntraffic sign detection and classification models. We provide an overview of the\nlatest advancements and highlight the existing research areas that require\nfurther investigation.",
        "translated": ""
    },
    {
        "title": "Balancing Privacy and Progress in Artificial Intelligence: Anonymization\n  in Histopathology for Biomedical Research and Education",
        "url": "http://arxiv.org/abs/2307.09426v1",
        "pub_date": "2023-07-18",
        "summary": "The advancement of biomedical research heavily relies on access to large\namounts of medical data. In the case of histopathology, Whole Slide Images\n(WSI) and clinicopathological information are valuable for developing\nArtificial Intelligence (AI) algorithms for Digital Pathology (DP).\nTransferring medical data \"as open as possible\" enhances the usability of the\ndata for secondary purposes but poses a risk to patient privacy. At the same\ntime, existing regulations push towards keeping medical data \"as closed as\nnecessary\" to avoid re-identification risks. Generally, these legal regulations\nrequire the removal of sensitive data but do not consider the possibility of\ndata linkage attacks due to modern image-matching algorithms. In addition, the\nlack of standardization in DP makes it harder to establish a single solution\nfor all formats of WSIs. These challenges raise problems for bio-informatics\nresearchers in balancing privacy and progress while developing AI algorithms.\nThis paper explores the legal regulations and terminologies for medical\ndata-sharing. We review existing approaches and highlight challenges from the\nhistopathological perspective. We also present a data-sharing guideline for\nhistological data to foster multidisciplinary research and education.",
        "translated": ""
    },
    {
        "title": "Trajectory Data Collection with Local Differential Privacy",
        "url": "http://arxiv.org/abs/2307.09339v1",
        "pub_date": "2023-07-18",
        "summary": "Trajectory data collection is a common task with many applications in our\ndaily lives. Analyzing trajectory data enables service providers to enhance\ntheir services, which ultimately benefits users. However, directly collecting\ntrajectory data may give rise to privacy-related issues that cannot be ignored.\nLocal differential privacy (LDP), as the de facto privacy protection standard\nin a decentralized setting, enables users to perturb their trajectories locally\nand provides a provable privacy guarantee. Existing approaches to private\ntrajectory data collection in a local setting typically use relaxed versions of\nLDP, which cannot provide a strict privacy guarantee, or require some external\nknowledge that is impractical to obtain and update in a timely manner. To\ntackle these problems, we propose a novel trajectory perturbation mechanism\nthat relies solely on an underlying location set and satisfies pure\n$\\epsilon$-LDP to provide a stringent privacy guarantee. In the proposed\nmechanism, each point's adjacent direction information in the trajectory is\nused in its perturbation process. Such information serves as an effective clue\nto connect neighboring points and can be used to restrict the possible region\nof a perturbed point in order to enhance utility. To the best of our knowledge,\nour study is the first to use direction information for trajectory perturbation\nunder LDP. Furthermore, based on this mechanism, we present an anchor-based\nmethod that adaptively restricts the region of each perturbed trajectory,\nthereby significantly boosting performance without violating the privacy\nconstraint. Extensive experiments on both real-world and synthetic datasets\ndemonstrate the effectiveness of the proposed mechanisms.",
        "translated": ""
    },
    {
        "title": "A New Hybrid Cryptosystem Involving DNA,Rabin, One Time Pad and Fiestel",
        "url": "http://arxiv.org/abs/2307.09322v1",
        "pub_date": "2023-07-18",
        "summary": "Information security is a crucial need in the modern world. Data security is\na real concern, and many customers and organizations need to protect their\nsensitive information from unauthorized parties and attackers. In previous\nyears, numerous cryptographic schemes have been proposed. DNA cryptography is a\nnew and developing field that combines the computational and biological worlds.\nDNA cryptography is intriguing due to its high storage capacity, secure data\ntransport, and massive parallel computing. In this paper, a new combination is\nproposed that offers good security by combining DNA, the Rabin algorithm, one\ntime pad, and a structure inspired by Fiestel. This algorithm employs two keys.\nThe first key is a DNA OTP key which is used for only one secure communication\nsession. The second key, which combines the public and private keys, is a Rabin\nkey. Additionally, by using a Feistel inspired scheme and randomness provided\nby DNA, the ciphertext is made harder to obtain without the private key.",
        "translated": ""
    },
    {
        "title": "Measuring the Leakage and Exploitability of Authentication Secrets in\n  Super-apps: The WeChat Case",
        "url": "http://arxiv.org/abs/2307.09317v1",
        "pub_date": "2023-07-18",
        "summary": "We conduct a large-scale measurement of developers' insecure practices\nleading to mini-app to super-app authentication bypass, among which hard-coding\ndeveloper secrets for such authentication is a major contributor. We also\nanalyze the exploitability and security consequences of developer secret\nleakage in mini-apps by examining individual super-app server-side APIs. We\ndevelop an analysis framework for measuring such secret leakage, and primarily\nanalyze 110,993 WeChat mini-apps, and 10,000 Baidu mini-apps (two of the most\nprominent super-app platforms), along with a few more datasets to test the\nevolution of developer practices and platform security enforcement over time.\nWe found a large number of WeChat mini-apps (36,425, 32.8%) and a few Baidu\nmini-apps (112) leak their developer secrets, which can cause severe security\nand privacy problems for the users and developers of mini-apps. A network\nattacker who does not even have an account on the super-app platform, can\neffectively take down a mini-app, send malicious and phishing links to users,\nand access sensitive information of the mini-app developer and its users. We\nresponsibly disclosed our findings and also put forward potential directions\nthat could be considered to alleviate/eliminate the root causes of developers\nhard-coding the app secrets in the mini-app's front-end code.",
        "translated": ""
    },
    {
        "title": "From Dragondoom to Dragonstar: Side-channel Attacks and Formally\n  Verified Implementation of WPA3 Dragonfly Handshake",
        "url": "http://arxiv.org/abs/2307.09243v1",
        "pub_date": "2023-07-18",
        "summary": "It is universally acknowledged that Wi-Fi communications are important to\nsecure. Thus, the Wi-Fi Alliance published WPA3 in 2018 with a distinctive\nsecurity feature: it leverages a Password-Authenticated Key Exchange (PAKE)\nprotocol to protect users' passwords from offline dictionary attacks.\nUnfortunately, soon after its release, several attacks were reported against\nits implementations, in response to which the protocol was updated in a\nbest-effort manner.\n  In this paper, we show that the proposed mitigations are not enough,\nespecially for a complex protocol to implement even for savvy developers.\nIndeed, we present **Dragondoom**, a collection of side-channel vulnerabilities\nof varying strength allowing attackers to recover users' passwords in widely\ndeployed Wi-Fi daemons, such as hostap in its default settings. Our findings\ntarget both password conversion methods, namely the default probabilistic\nhunting-and-pecking and its newly standardized deterministic alternative based\non SSWU. We successfully exploit our leakage in practice through\nmicroarchitectural mechanisms, and overcome the limited spatial resolution of\nFlush+Reload. Our attacks outperform previous works in terms of required\nmeasurements.\n  Then, driven by the need to end the spiral of patch-and-hack in Dragonfly\nimplementations, we propose **Dragonstar**, an implementation of Dragonfly\nleveraging a formally verified implementation of the underlying mathematical\noperations, thereby removing all the related leakage vector. Our implementation\nrelies on HACL*, a formally verified crypto library guaranteeing\nsecret-independence. We design Dragonstar, so that its integration within\nhostap requires minimal modifications to the existing project. Our experiments\nshow that the performance of HACL*-based hostap is comparable to OpenSSL-based,\nimplying that Dragonstar is both efficient and proved to be leakage-free.",
        "translated": ""
    },
    {
        "title": "The Hitchhiker's Guide to Malicious Third-Party Dependencies",
        "url": "http://arxiv.org/abs/2307.09087v1",
        "pub_date": "2023-07-18",
        "summary": "The increasing popularity of certain programming languages has spurred the\ncreation of ecosystem-specific package repositories and package managers. Such\nrepositories (e.g., NPM, PyPI) serve as public databases that users can query\nto retrieve packages for various functionalities, whereas package managers\nautomatically handle dependency resolution and package installation on the\nclient side. These mechanisms enhance software modularization and accelerate\nimplementation. However, they have become a target for malicious actors seeking\nto propagate malware on a large scale.\n  In this work, we show how attackers can leverage capabilities of popular\npackage managers and languages to achieve arbitrary code execution on victim\nmachines, thereby realizing open-source software supply chain attacks. Based on\nthe analysis of 7 ecosystems, we identify 3 install-time and 5 runtime\ntechniques, and we provide recommendations describing how to reduce the risk\nwhen consuming third-party dependencies. We will provide proof-of-concepts that\ndemonstrate the identified techniques. Furthermore, we describe evasion\nstrategies employed by attackers to circumvent detection mechanisms.",
        "translated": ""
    },
    {
        "title": "Mitigating Intersection Attacks in Anonymous Microblogging",
        "url": "http://arxiv.org/abs/2307.09069v1",
        "pub_date": "2023-07-18",
        "summary": "Anonymous microblogging systems are known to be vulnerable to intersection\nattacks due to network churn. An adversary that monitors all communications can\nleverage the churn to learn who is publishing what with increasing confidence\nover time. In this paper, we propose a protocol for mitigating intersection\nattacks in anonymous microblogging systems by grouping users into anonymity\nsets based on similarities in their publishing behavior. The protocol provides\na configurable communication schedule for users in each set to manage the\ninevitable trade-off between latency and bandwidth overhead. In our evaluation,\nwe use real-world datasets from two popular microblogging platforms, Twitter\nand Reddit, to simulate user publishing behavior. The results demonstrate that\nthe protocol can protect users against intersection attacks at low bandwidth\noverhead when the users adhere to communication schedules. In addition, the\nprotocol can sustain a slow degradation in the size of the anonymity set over\ntime under various churn rates.",
        "translated": ""
    },
    {
        "title": "FedDefender: Client-Side Attack-Tolerant Federated Learning",
        "url": "http://arxiv.org/abs/2307.09048v1",
        "pub_date": "2023-07-18",
        "summary": "Federated learning enables learning from decentralized data sources without\ncompromising privacy, which makes it a crucial technique. However, it is\nvulnerable to model poisoning attacks, where malicious clients interfere with\nthe training process. Previous defense mechanisms have focused on the\nserver-side by using careful model aggregation, but this may not be effective\nwhen the data is not identically distributed or when attackers can access the\ninformation of benign clients. In this paper, we propose a new defense\nmechanism that focuses on the client-side, called FedDefender, to help benign\nclients train robust local models and avoid the adverse impact of malicious\nmodel updates from attackers, even when a server-side defense cannot identify\nor remove adversaries. Our method consists of two main components: (1)\nattack-tolerant local meta update and (2) attack-tolerant global knowledge\ndistillation. These components are used to find noise-resilient model\nparameters while accurately extracting knowledge from a potentially corrupted\nglobal model. Our client-side defense strategy has a flexible structure and can\nwork in conjunction with any existing server-side strategies. Evaluations of\nreal-world scenarios across multiple datasets show that the proposed method\nenhances the robustness of federated learning against model poisoning attacks.",
        "translated": ""
    },
    {
        "title": "CBSeq: A Channel-level Behavior Sequence For Encrypted Malware Traffic\n  Detection",
        "url": "http://arxiv.org/abs/2307.09002v1",
        "pub_date": "2023-07-18",
        "summary": "Machine learning and neural networks have become increasingly popular\nsolutions for encrypted malware traffic detection. They mine and learn complex\ntraffic patterns, enabling detection by fitting boundaries between malware\ntraffic and benign traffic. Compared with signature-based methods, they have\nhigher scalability and flexibility. However, affected by the frequent variants\nand updates of malware, current methods suffer from a high false positive rate\nand do not work well for unknown malware traffic detection. It remains a\ncritical task to achieve effective malware traffic detection. In this paper, we\nintroduce CBSeq to address the above problems. CBSeq is a method that\nconstructs a stable traffic representation, behavior sequence, to characterize\nattacking intent and achieve malware traffic detection. We novelly propose the\nchannels with similar behavior as the detection object and extract side-channel\ncontent to construct behavior sequence. Unlike benign activities, the behavior\nsequences of malware and its variant's traffic exhibit solid internal\ncorrelations. Moreover, we design the MSFormer, a powerful Transformer-based\nmulti-sequence fusion classifier. It captures the internal similarity of\nbehavior sequence, thereby distinguishing malware traffic from benign traffic.\nOur evaluations demonstrate that CBSeq performs effectively in various known\nmalware traffic detection and exhibits superior performance in unknown malware\ntraffic detection, outperforming state-of-the-art methods.",
        "translated": ""
    },
    {
        "title": "On Borrowed Time -- Preventing Static Power Side-Channel Analysis",
        "url": "http://arxiv.org/abs/2307.09001v1",
        "pub_date": "2023-07-18",
        "summary": "In recent years, static power side-channel analysis attacks have emerged as a\nserious threat to cryptographic implementations, overcoming state-of-the-art\ncountermeasures against side-channel attacks. The continued down-scaling of\nsemiconductor process technology, which results in an increase of the relative\nweight of static power in the total power budget of circuits, will only improve\nthe viability of static power side-channel analysis attacks. Yet, despite the\nthreat posed, limited work has been invested into mitigating this class of\nattack. In this work we address this gap. We observe that static power\nside-channel analysis relies on stopping the target circuit's clock over a\nprolonged period, during which the circuit holds secret information in its\nregisters. We propose Borrowed Time, a countermeasure that hinders an\nattacker's ability to leverage such clock control. Borrowed Time detects a\nstopped clock and triggers a reset that wipes any registers containing\nsensitive intermediates, whose leakages would otherwise be exploitable. We\ndemonstrate the effectiveness of our countermeasure by performing practical\nCorrelation Power Analysis attacks under optimal conditions against an AES\nimplementation on an FPGA target with and without our countermeasure in place.\nIn the unprotected case, we can recover the entire secret key using traces from\n1,500 encryptions. Under the same conditions, the protected implementation\nsuccessfully prevents key recovery even with traces from 1,000,000 encryptions.",
        "translated": ""
    },
    {
        "title": "Rethinking Backdoor Attacks",
        "url": "http://arxiv.org/abs/2307.10163v1",
        "pub_date": "2023-07-19",
        "summary": "In a backdoor attack, an adversary inserts maliciously constructed backdoor\nexamples into a training set to make the resulting model vulnerable to\nmanipulation. Defending against such attacks typically involves viewing these\ninserted examples as outliers in the training set and using techniques from\nrobust statistics to detect and remove them.\n  In this work, we present a different approach to the backdoor attack problem.\nSpecifically, we show that without structural information about the training\ndata distribution, backdoor attacks are indistinguishable from\nnaturally-occurring features in the data--and thus impossible to \"detect\" in a\ngeneral sense. Then, guided by this observation, we revisit existing defenses\nagainst backdoor attacks and characterize the (often latent) assumptions they\nmake and on which they depend. Finally, we explore an alternative perspective\non backdoor attacks: one that assumes these attacks correspond to the strongest\nfeature in the training data. Under this assumption (which we make formal) we\ndevelop a new primitive for detecting backdoor attacks. Our primitive naturally\ngives rise to a detection algorithm that comes with theoretical guarantees and\nis effective in practice.",
        "translated": ""
    },
    {
        "title": "EPUF: A Novel Scheme Based on Entropy Features of Latency-based DRAM\n  PUFs Providing Lightweight Authentication in IoT Networks",
        "url": "http://arxiv.org/abs/2307.09968v1",
        "pub_date": "2023-07-19",
        "summary": "Physical unclonable functions (PUFs) are hardware-oriented primitives that\nexploit manufacturing variations to generate a unique identity for a physical\nsystem. Recent advancements showed how DRAM can be exploited to implement PUFs.\nDRAM PUFs require no additional circuits for PUF operations and can be used in\nmost of the applications with resource-constrained nodes such as Internet of\nThings (IoT) networks. However, the existing DRAM PUF solutions either require\nto interrupt other functions in the host system, or provide unreliable\nresponses due to their sensitiveness to the environmental conditions.\n  In this paper, we propose EPUF, a novel strategy to extract random and unique\nfeatures from DRAM cells to generate reliable PUF responses. In particular, we\nuse the bitmap images of the binary DRAM values and their entropy features. We\nshow via real device experiments that EPUF is approximately $1.7$ times faster\nthan other state of the art solutions, achieves $100\\%$ reliability, generates\nfeatures with $47.79\\%$ uniqueness, and supports a large set of CRP that leads\nto new potentials for DRAM PUF-based authentication. We also propose a\nlightweight authentication protocol based on EPUF, which not only provides far\nbetter security guarantees but also outperforms the state-of-the-art in terms\nof communication overhead and computational cost.",
        "translated": ""
    },
    {
        "title": "Reduction of the secret key length in the perfect cipher by data\n  compression and randomisation",
        "url": "http://arxiv.org/abs/2307.09735v1",
        "pub_date": "2023-07-19",
        "summary": "Perfect ciphers have been a very attractive cryptographic tool ever since C.\nShannon described them. Note that, by definition, if a perfect cipher is used,\nno one can get any information about the encrypted message without knowing the\nsecret key. We consider the problem of reducing the key length of perfect\nciphers, because in many applications the length of the secret key is a crucial\nparameter. This paper describes a simple method of key length reduction. This\nmethod gives a perfect cipher and is based on the use of data compression and\nrandomisation, and the average key length can be made close to Shannon entropy\n(which is the key length limit). It should be noted that the method can\neffectively use readily available data compressors (archivers).",
        "translated": ""
    },
    {
        "title": "VISER: A Tractable Solution Concept for Games with Information Asymmetry",
        "url": "http://arxiv.org/abs/2307.09652v1",
        "pub_date": "2023-07-18",
        "summary": "Many real-world games suffer from information asymmetry: one player is only\naware of their own payoffs while the other player has the full game\ninformation. Examples include the critical domain of security games and\nadversarial multi-agent reinforcement learning. Information asymmetry renders\ntraditional solution concepts such as Strong Stackelberg Equilibrium (SSE) and\nRobust-Optimization Equilibrium (ROE) inoperative. We propose a novel solution\nconcept called VISER (Victim Is Secure, Exploiter best-Responds). VISER enables\nan external observer to predict the outcome of such games. In particular, for\nsecurity applications, VISER allows the victim to better defend itself while\ncharacterizing the most damaging attacks available to the attacker. We show\nthat each player's VISER strategy can be computed independently in polynomial\ntime using linear programming (LP). We also extend VISER to its Markov-perfect\ncounterpart for Markov games, which can be solved efficiently using a series of\nLPs.",
        "translated": ""
    },
    {
        "title": "Application of BadNets in Spam Filters",
        "url": "http://arxiv.org/abs/2307.09649v1",
        "pub_date": "2023-07-18",
        "summary": "Spam filters are a crucial component of modern email systems, as they help to\nprotect users from unwanted and potentially harmful emails. However, the\neffectiveness of these filters is dependent on the quality of the machine\nlearning models that power them. In this paper, we design backdoor attacks in\nthe domain of spam filtering. By demonstrating the potential vulnerabilities in\nthe machine learning model supply chain, we highlight the need for careful\nconsideration and evaluation of the models used in spam filters. Our results\nshow that the backdoor attacks can be effectively used to identify\nvulnerabilities in spam filters and suggest the need for ongoing monitoring and\nimprovement in this area.",
        "translated": ""
    },
    {
        "title": "Dead Man's PLC: Towards Viable Cyber Extortion for Operational\n  Technology",
        "url": "http://arxiv.org/abs/2307.09549v1",
        "pub_date": "2023-07-18",
        "summary": "For decades, operational technology (OT) has enjoyed the luxury of being\nsuitably inaccessible so as to experience directly targeted cyber attacks from\nonly the most advanced and well-resourced adversaries. However, security via\nobscurity cannot last forever, and indeed a shift is happening whereby less\nadvanced adversaries are showing an appetite for targeting OT. With this shift\nin adversary demographics, there will likely also be a shift in attack goals,\nfrom clandestine process degradation and espionage to overt cyber extortion\n(Cy-X). The consensus from OT cyber security practitioners suggests that, even\nif encryption-based Cy-X techniques were launched against OT assets, typical\nrecovery practices designed for engineering processes would provide adequate\nresilience. In response, this paper introduces Dead Man's PLC (DM-PLC), a\npragmatic step towards viable OT Cy-X that acknowledges and weaponises the\nresilience processes typically encountered. Using only existing functionality,\nDM-PLC considers an entire environment as the entity under ransom, whereby all\nassets constantly poll one another to ensure the attack remains untampered,\ntreating any deviations as a detonation trigger akin to a Dead Man's switch. A\nproof of concept of DM-PLC is implemented and evaluated on an academically peer\nreviewed and industry validated OT testbed to demonstrate its malicious\nefficacy.",
        "translated": ""
    },
    {
        "title": "To What Extent Are Honeypots and Honeynets Autonomic Computing Systems?",
        "url": "http://arxiv.org/abs/2307.11038v1",
        "pub_date": "2023-07-20",
        "summary": "Cyber threats, such as advanced persistent threats (APTs), ransomware, and\nzero-day exploits, are rapidly evolving and demand improved security measures.\nHoneypots and honeynets, as deceptive systems, offer valuable insights into\nattacker behavior, helping researchers and practitioners develop innovative\ndefense strategies and enhance detection mechanisms. However, their deployment\ninvolves significant maintenance and overhead expenses. At the same time, the\ncomplexity of modern computing has prompted the rise of autonomic computing,\naiming for systems that can operate without human intervention. Recent honeypot\nand honeynet research claims to incorporate autonomic computing principles,\noften using terms like adaptive, dynamic, intelligent, and learning. This study\ninvestigates such claims by measuring the extent to which autonomic principles\nprinciples are expressed in honeypot and honeynet literature. The findings\nreveal that autonomic computing keywords are present in the literature sample,\nsuggesting an evolution from self-adaptation to autonomic computing\nimplementations. Yet, despite these findings, the analysis also shows low\nfrequencies of self-configuration, self-healing, and self-protection keywords.\nInterestingly, self-optimization appeared prominently in the literature. While\nthis study presents a foundation for the convergence of autonomic computing and\ndeceptive systems, future research could explore technical implementations in\nsample articles and test them for autonomic behavior. Additionally,\ninvestigations into the design and implementation of individual autonomic\ncomputing principles in honeypots and determining the necessary ratio of these\nprinciples for a system to exhibit autonomic behavior could provide valuable\ninsights for both researchers and practitioners.",
        "translated": ""
    },
    {
        "title": "DREAM: Domain-free Reverse Engineering Attributes of Black-box Model",
        "url": "http://arxiv.org/abs/2307.10997v1",
        "pub_date": "2023-07-20",
        "summary": "Deep learning models are usually black boxes when deployed on machine\nlearning platforms. Prior works have shown that the attributes ($e.g.$, the\nnumber of convolutional layers) of a target black-box neural network can be\nexposed through a sequence of queries. There is a crucial limitation: these\nworks assume the dataset used for training the target model to be known\nbeforehand and leverage this dataset for model attribute attack. However, it is\ndifficult to access the training dataset of the target black-box model in\nreality. Therefore, whether the attributes of a target black-box model could be\nstill revealed in this case is doubtful. In this paper, we investigate a new\nproblem of Domain-agnostic Reverse Engineering the Attributes of a black-box\ntarget Model, called DREAM, without requiring the availability of the target\nmodel's training dataset, and put forward a general and principled framework by\ncasting this problem as an out of distribution (OOD) generalization problem. In\nthis way, we can learn a domain-agnostic model to inversely infer the\nattributes of a target black-box model with unknown training data. This makes\nour method one of the kinds that can gracefully apply to an arbitrary domain\nfor model attribute reverse engineering with strong generalization ability.\nExtensive experimental studies are conducted and the results validate the\nsuperiority of our proposed method over the baselines.",
        "translated": ""
    },
    {
        "title": "PATROL: Privacy-Oriented Pruning for Collaborative Inference Against\n  Model Inversion Attacks",
        "url": "http://arxiv.org/abs/2307.10981v1",
        "pub_date": "2023-07-20",
        "summary": "Collaborative inference has been a promising solution to enable\nresource-constrained edge devices to perform inference using state-of-the-art\ndeep neural networks (DNNs). In collaborative inference, the edge device first\nfeeds the input to a partial DNN locally and then uploads the intermediate\nresult to the cloud to complete the inference. However, recent research\nindicates model inversion attacks (MIAs) can reconstruct input data from\nintermediate results, posing serious privacy concerns for collaborative\ninference. Existing perturbation and cryptography techniques are inefficient\nand unreliable in defending against MIAs while performing accurate inference.\nThis paper provides a viable solution, named PATROL, which develops\nprivacy-oriented pruning to balance privacy, efficiency, and utility of\ncollaborative inference. PATROL takes advantage of the fact that later layers\nin a DNN can extract more task-specific features. Given limited local resources\nfor collaborative inference, PATROL intends to deploy more layers at the edge\nbased on pruning techniques to enforce task-specific features for inference and\nreduce task-irrelevant but sensitive features for privacy preservation. To\nachieve privacy-oriented pruning, PATROL introduces two key components:\nLipschitz regularization and adversarial reconstruction training, which\nincrease the reconstruction errors by reducing the stability of MIAs and\nenhance the target inference model by adversarial training, respectively.",
        "translated": ""
    },
    {
        "title": "Adaptively Weighted Audits of Instant-Runoff Voting Elections: AWAIRE",
        "url": "http://arxiv.org/abs/2307.10972v1",
        "pub_date": "2023-07-20",
        "summary": "An election audit is risk-limiting if the audit limits (to a pre-specified\nthreshold) the chance that an erroneous electoral outcome will be certified.\nExtant methods for auditing instant-runoff voting (IRV) elections are either\nnot risk-limiting or require cast vote records (CVRs), the voting system's\nelectronic record of the votes on each ballot. CVRs are not always available,\nfor instance, in jurisdictions that tabulate IRV contests manually.\n  We develop an RLA method (AWAIRE) that uses adaptively weighted averages of\ntest supermartingales to efficiently audit IRV elections when CVRs are not\navailable. The adaptive weighting 'learns' an efficient set of hypotheses to\ntest to confirm the election outcome. When accurate CVRs are available, AWAIRE\ncan use them to increase the efficiency to match the performance of existing\nmethods that require CVRs.\n  We provide an open-source prototype implementation that can handle elections\nwith up to six candidates. Simulations using data from real elections show that\nAWAIRE is likely to be efficient in practice. We discuss how to extend the\ncomputational approach to handle elections with more candidates.\n  Adaptively weighted averages of test supermartingales are a general tool,\nuseful beyond election audits to test collections of hypotheses sequentially\nwhile rigorously controlling the familywise error rate.",
        "translated": ""
    },
    {
        "title": "ESASCF: Expertise Extraction, Generalization and Reply Framework for an\n  Optimized Automation of Network Security Compliance",
        "url": "http://arxiv.org/abs/2307.10967v1",
        "pub_date": "2023-07-20",
        "summary": "The Cyber threats exposure has created worldwide pressure on organizations to\ncomply with cyber security standards and policies for protecting their digital\nassets. Vulnerability assessment (VA) and Penetration Testing (PT) are widely\nadopted Security Compliance (SC) methods to identify security gaps and\nanticipate security breaches. In the computer networks context and despite the\nuse of autonomous tools and systems, security compliance remains highly\nrepetitive and resources consuming. In this paper, we proposed a novel method\nto tackle the ever-growing problem of efficiency and effectiveness in network\ninfrastructures security auditing by formally introducing, designing, and\ndeveloping an Expert-System Automated Security Compliance Framework (ESASCF)\nthat enables industrial and open-source VA and PT tools and systems to extract,\nprocess, store and re-use the expertise in a human-expert way to allow direct\napplication in similar scenarios or during the periodic re-testing. The\nimplemented model was then integrated within the ESASCF and tested on different\nsize networks and proved efficient in terms of time-efficiency and testing\neffectiveness allowing ESASCF to take over autonomously the SC in Re-testing\nand offloading Expert by automating repeated segments SC and thus enabling\nExperts to prioritize important tasks in Ad-Hoc compliance tests. The obtained\nresults validate the performance enhancement notably by cutting the time\nrequired for an expert to 50% in the context of typical corporate networks\nfirst SC and 20% in re-testing, representing a significant cost-cutting. In\naddition, the framework allows a long-term impact illustrated in the knowledge\nextraction, generalization, and re-utilization, which enables better SC\nconfidence independent of the human expert skills, coverage, and wrong\ndecisions resulting in impactful false negatives.",
        "translated": ""
    },
    {
        "title": "Threshold Encrypted Mempools: Limitations and Considerations",
        "url": "http://arxiv.org/abs/2307.10878v1",
        "pub_date": "2023-07-20",
        "summary": "Encrypted mempools are a class of solutions aimed at preventing or reducing\nnegative externalities of MEV extraction using cryptographic privacy. Mempool\nencryption aims to hide information related to pending transactions until a\nblock including the transactions is committed, targeting the prevention of\nfrontrunning and similar behaviour. Among the various methods of encryption,\nthreshold schemes are particularly interesting for the design of MEV mitigation\nmechanisms, as their distributed nature and minimal hardware requirements\nharmonize with a broader goal of decentralization.\n  This work looks beyond the formal and technical cryptographic aspects of\nthreshold encryption schemes to focus on the market and incentive implications\nof implementing encrypted mempools as MEV mitigation techniques. In particular,\nthis paper argues that the deployment of such protocols without proper\nconsideration and understanding of market impact invites several undesired\noutcomes, with the ultimate goal of stimulating further analysis of this class\nof solutions outside of pure cryptograhic considerations. Included in the paper\nis an overview of a series of problems, various candidate solutions in the form\nof mempool encryption techniques with a focus on threshold encryption,\npotential drawbacks to these solutions, and Osmosis as a case study. The paper\ntargets a broad audience and remains agnostic to blockchain design where\npossible while drawing from mostly financial examples.",
        "translated": ""
    },
    {
        "title": "Battle Ground: Data Collection and Labeling of CTF Games to Understand\n  Human Cyber Operators",
        "url": "http://arxiv.org/abs/2307.10877v1",
        "pub_date": "2023-07-20",
        "summary": "Industry standard frameworks are now widespread for labeling the high-level\nstages and granular actions of attacker and defender behavior in cyberspace.\nWhile these labels are used for atomic actions, and to some extent for\nsequences of actions, there remains a need for labeled data from realistic\nfull-scale attacks. This data is valuable for better understanding human\nactors' decisions, behaviors, and individual attributes. The analysis could\nlead to more effective attribution and disruption of attackers.\n  We present a methodological approach and exploratory case study for\nsystematically analyzing human behavior during a cyber offense/defense\ncapture-the-flag (CTF) game. We describe the data collection and analysis to\nderive a metric called keystroke accuracy. After collecting players' commands,\nwe label them using the MITRE ATT&amp;CK framework using a new tool called\nPathfinder. We present results from preliminary analysis of participants'\nkeystroke accuracy and its relation to score outcome in CTF games. We describe\nfrequency of action classification within the MITRE ATT&amp;CK framework and\ndiscuss some of the mathematical trends suggested by our observations. We\nconclude with a discussion of extensions for the methodology, including\nperformance evaluation during games and the potential use of this methodology\nfor training artificial intelligence.",
        "translated": ""
    },
    {
        "title": "Risk-optimized Outlier Removal for Robust Point Cloud Classification",
        "url": "http://arxiv.org/abs/2307.10875v1",
        "pub_date": "2023-07-20",
        "summary": "The popularity of point cloud deep models for safety-critical purposes has\nincreased, but the reliability and security of these models can be compromised\nby intentional or naturally occurring point cloud noise. To combat this issue,\nwe present a novel point cloud outlier removal method called PointCVaR, which\nempowers standard-trained models to eliminate additional outliers and restore\nthe data. Our approach begins by conducting attribution analysis to determine\nthe influence of each point on the model output, which we refer to as point\nrisk. We then optimize the process of filtering high-risk points using\nConditional Value at Risk (CVaR) as the objective. The rationale for this\napproach is based on the observation that noise points in point clouds tend to\ncluster in the tail of the risk distribution, with a low frequency but a high\nlevel of risk, resulting in significant interference with classification\nresults. Despite requiring no additional training effort, our method produces\nexceptional results in various removal-and-classification experiments for noisy\npoint clouds, which are corrupted by random noise, adversarial noise, and\nbackdoor trigger noise. Impressively, it achieves 87% accuracy in defense\nagainst the backdoor attack by removing triggers. Overall, the proposed\nPointCVaR effectively eliminates noise points and enhances point cloud\nclassification, making it a promising plug-in module for various models in\ndifferent scenarios.",
        "translated": ""
    },
    {
        "title": "A Blockchain-based Electronic Voting System: EtherVote",
        "url": "http://arxiv.org/abs/2307.10726v1",
        "pub_date": "2023-07-20",
        "summary": "The development of an electronic voting system that would replace traditional\nelection procedures is a research topic of great interest for many years.\nBlockchain technology could provide some guarantees and fulfill strong\nrequirements for electronic voting platforms, such as transparency,\nimmutability, and confidentiality. From time to time research is conducted to\naddress problems in voting systems. Many research works attempt to implement\nsecure and reliable voting systems, which address known security, anonymity,\nand fraud issues that might threaten such systems.\n  This paper presents a proposal of a secure electronic voting system, the\nEtherVote, using the Ethereum Blockchain network that focuses deeply on the\nfield of identification of eligible citizens. The proposed system will be\nentirely based on Blockchain without any central authority servers or\ndatabases, thus improving security, privacy, and election cost. Limitations,\nproblems, and solutions are discussed, in order to make the proposed electronic\nvoting system ideal and ready to use for national elections.",
        "translated": ""
    },
    {
        "title": "LLM Censorship: A Machine Learning Challenge or a Computer Security\n  Problem?",
        "url": "http://arxiv.org/abs/2307.10719v1",
        "pub_date": "2023-07-20",
        "summary": "Large language models (LLMs) have exhibited impressive capabilities in\ncomprehending complex instructions. However, their blind adherence to provided\ninstructions has led to concerns regarding risks of malicious use. Existing\ndefence mechanisms, such as model fine-tuning or output censorship using LLMs,\nhave proven to be fallible, as LLMs can still generate problematic responses.\nCommonly employed censorship approaches treat the issue as a machine learning\nproblem and rely on another LM to detect undesirable content in LLM outputs. In\nthis paper, we present the theoretical limitations of such semantic censorship\napproaches. Specifically, we demonstrate that semantic censorship can be\nperceived as an undecidable problem, highlighting the inherent challenges in\ncensorship that arise due to LLMs' programmatic and instruction-following\ncapabilities. Furthermore, we argue that the challenges extend beyond semantic\ncensorship, as knowledgeable attackers can reconstruct impermissible outputs\nfrom a collection of permissible ones. As a result, we propose that the problem\nof censorship needs to be reevaluated; it should be treated as a security\nproblem which warrants the adaptation of security-based approaches to mitigate\npotential risks.",
        "translated": ""
    },
    {
        "title": "Differentially Private Heavy Hitter Detection using Federated Analytics",
        "url": "http://arxiv.org/abs/2307.11749v1",
        "pub_date": "2023-07-21",
        "summary": "In this work, we study practical heuristics to improve the performance of\nprefix-tree based algorithms for differentially private heavy hitter detection.\nOur model assumes each user has multiple data points and the goal is to learn\nas many of the most frequent data points as possible across all users' data\nwith aggregate and local differential privacy. We propose an adaptive\nhyperparameter tuning algorithm that improves the performance of the algorithm\nwhile satisfying computational, communication and privacy constraints. We\nexplore the impact of different data-selection schemes as well as the impact of\nintroducing deny lists during multiple runs of the algorithm. We test these\nimprovements using extensive experimentation on the Reddit\ndataset~\\cite{caldas2018leaf} on the task of learning the most frequent words.",
        "translated": ""
    },
    {
        "title": "Mitigating Communications Threats in Decentralized Federated Learning\n  through Moving Target Defense",
        "url": "http://arxiv.org/abs/2307.11730v1",
        "pub_date": "2023-07-21",
        "summary": "The rise of Decentralized Federated Learning (DFL) has enabled the training\nof machine learning models across federated participants, fostering\ndecentralized model aggregation and reducing dependence on a server. However,\nthis approach introduces unique communication security challenges that have yet\nto be thoroughly addressed in the literature. These challenges primarily\noriginate from the decentralized nature of the aggregation process, the varied\nroles and responsibilities of the participants, and the absence of a central\nauthority to oversee and mitigate threats. Addressing these challenges, this\npaper first delineates a comprehensive threat model, highlighting the potential\nrisks of DFL communications. In response to these identified risks, this work\nintroduces a security module designed for DFL platforms to counter\ncommunication-based attacks. The module combines security techniques such as\nsymmetric and asymmetric encryption with Moving Target Defense (MTD)\ntechniques, including random neighbor selection and IP/port switching. The\nsecurity module is implemented in a DFL platform called Fedstellar, allowing\nthe deployment and monitoring of the federation. A DFL scenario has been\ndeployed, involving eight physical devices implementing three security\nconfigurations: (i) a baseline with no security, (ii) an encrypted\nconfiguration, and (iii) a configuration integrating both encryption and MTD\ntechniques. The effectiveness of the security module is validated through\nexperiments with the MNIST dataset and eclipse attacks. The results indicated\nan average F1 score of 95%, with moderate increases in CPU usage (up to 63.2%\n+-3.5%) and network traffic (230 MB +-15 MB) under the most secure\nconfiguration, mitigating the risks posed by eavesdropping or eclipse attacks.",
        "translated": ""
    },
    {
        "title": "Fast Adaptive Test-Time Defense with Robust Features",
        "url": "http://arxiv.org/abs/2307.11672v1",
        "pub_date": "2023-07-21",
        "summary": "Adaptive test-time defenses are used to improve the robustness of deep neural\nnetworks to adversarial examples. However, existing methods significantly\nincrease the inference time due to additional optimization on the model\nparameters or the input at test time. In this work, we propose a novel adaptive\ntest-time defense strategy that is easy to integrate with any existing (robust)\ntraining procedure without additional test-time computation. Based on the\nnotion of robustness of features that we present, the key idea is to project\nthe trained models to the most robust feature space, thereby reducing the\nvulnerability to adversarial attacks in non-robust directions. We theoretically\nshow that the top eigenspace of the feature matrix are more robust for a\ngeneralized additive model and support our argument for a large width neural\nnetwork with the Neural Tangent Kernel (NTK) equivalence. We conduct extensive\nexperiments on CIFAR-10 and CIFAR-100 datasets for several robustness\nbenchmarks, including the state-of-the-art methods in RobustBench, and observe\nthat the proposed method outperforms existing adaptive test-time defenses at\nmuch lower computation costs.",
        "translated": ""
    },
    {
        "title": "WM-NET: Robust Deep 3D Watermarking with Limited Data",
        "url": "http://arxiv.org/abs/2307.11628v1",
        "pub_date": "2023-07-21",
        "summary": "The goal of 3D mesh watermarking is to embed the message in 3D meshes that\ncan withstand various attacks imperceptibly and reconstruct the message\naccurately from watermarked meshes. Traditional methods are less robust against\nattacks. Recent DNN-based methods either introduce excessive distortions or\nfail to embed the watermark without the help of texture information. However,\nembedding the watermark in textures is insecure because replacing the texture\nimage can completely remove the watermark. In this paper, we propose a robust\ndeep 3D mesh watermarking WM-NET, which leverages attention-based convolutions\nin watermarking tasks to embed binary messages in vertex distributions without\ntexture assistance. Furthermore, our WM-NET exploits the property that\nsimplified meshes inherit similar relations from the original ones, where the\nrelation is the offset vector directed from one vertex to its neighbor. By\ndoing so, our method can be trained on simplified meshes(limited data) but\nremains effective on large-sized meshes (size adaptable) and unseen categories\nof meshes (geometry adaptable). Extensive experiments demonstrate our method\nbrings 50% fewer distortions and 10% higher bit accuracy compared to previous\nwork. Our watermark WM-NET is robust against various mesh attacks, e.g. Gauss,\nrotation, translation, scaling, and cropping.",
        "translated": ""
    },
    {
        "title": "Dissecting Code Vulnerabilities: Insights from C++ and Java\n  Vulnerability Analysis with ReVeal Model",
        "url": "http://arxiv.org/abs/2307.11454v1",
        "pub_date": "2023-07-21",
        "summary": "This study presents an analysis conducted on a real-world dataset of Java\nvulnerability-fixing commits. The dataset consists of commits with varying\nnumbers of modified methods, leading to a natural partitioning based on the\nnumber of changed functions. The research aims to address several key\nquestions. Firstly, the study investigates the optimal parameter selection for\nReVeal, a state-of-the-art model, in order to achieve its best performance.\nSecondly, it explores the contributions of different parts of the Java dataset\ntowards vulnerability detection. Lastly, the study evaluates the model's\nperformance in separating close-to-vulnerable methods (vulnerable methods and\ntheir fixed versions) from randomly selected safe code, as well as the finer\nseparation of vulnerable methods from their fixed versions within the set of\nclose-to-vulnerable methods. The research employs a series of experiments to\nanswer these questions and derive meaningful insights.",
        "translated": ""
    },
    {
        "title": "Improving Transferability of Adversarial Examples via Bayesian Attacks",
        "url": "http://arxiv.org/abs/2307.11334v1",
        "pub_date": "2023-07-21",
        "summary": "This paper presents a substantial extension of our work published at ICLR.\nOur ICLR work advocated for enhancing transferability in adversarial examples\nby incorporating a Bayesian formulation into model parameters, which\neffectively emulates the ensemble of infinitely many deep neural networks,\nwhile, in this paper, we introduce a novel extension by incorporating the\nBayesian formulation into the model input as well, enabling the joint\ndiversification of both the model input and model parameters. Our empirical\nfindings demonstrate that: 1) the combination of Bayesian formulations for both\nthe model input and model parameters yields significant improvements in\ntransferability; 2) by introducing advanced approximations of the posterior\ndistribution over the model input, adversarial transferability achieves further\nenhancement, surpassing all state-of-the-arts when attacking without model\nfine-tuning. Moreover, we propose a principled approach to fine-tune model\nparameters in such an extended Bayesian formulation. The derived optimization\nobjective inherently encourages flat minima in the parameter space and input\nspace. Extensive experiments demonstrate that our method achieves a new\nstate-of-the-art on transfer-based attacks, improving the average success rate\non ImageNet and CIFAR-10 by 19.14% and 2.08%, respectively, when comparing with\nour ICLR basic Bayesian method. We will make our code publicly available.",
        "translated": ""
    },
    {
        "title": "Epsilon*: Privacy Metric for Machine Learning Models",
        "url": "http://arxiv.org/abs/2307.11280v1",
        "pub_date": "2023-07-21",
        "summary": "We introduce Epsilon*, a new privacy metric for measuring the privacy risk of\na single model instance prior to, during, or after deployment of privacy\nmitigation strategies. The metric does not require access to the training data\nsampling or model training algorithm. Epsilon* is a function of true positive\nand false positive rates in a hypothesis test used by an adversary in a\nmembership inference attack. We distinguish between quantifying the privacy\nloss of a trained model instance and quantifying the privacy loss of the\ntraining mechanism which produces this model instance. Existing approaches in\nthe privacy auditing literature provide lower bounds for the latter, while our\nmetric provides a lower bound for the former by relying on an\n(${\\epsilon}$,${\\delta}$)-type of quantification of the privacy of the trained\nmodel instance. We establish a relationship between these lower bounds and show\nhow to implement Epsilon* to avoid numerical and noise amplification\ninstability. We further show in experiments on benchmark public data sets that\nEpsilon* is sensitive to privacy risk mitigation by training with differential\nprivacy (DP), where the value of Epsilon* is reduced by up to 800% compared to\nthe Epsilon* values of non-DP trained baseline models. This metric allows\nprivacy auditors to be independent of model owners, and enables all\ndecision-makers to visualize the privacy-utility landscape to make informed\ndecisions regarding the trade-offs between model privacy and utility.",
        "translated": ""
    },
    {
        "title": "Formal-Guided Fuzz Testing: Targeting Security Assurance from\n  Specification to Implementation for 5G and Beyond",
        "url": "http://arxiv.org/abs/2307.11247v1",
        "pub_date": "2023-07-20",
        "summary": "Softwarization and virtualization in 5G and beyond necessitate thorough\ntesting to ensure the security of critical infrastructure and networks,\nrequiring the identification of vulnerabilities and unintended emergent\nbehaviors from protocol designs to their software stack implementation. To\nprovide an efficient and comprehensive solution, we propose a novel and\nfirst-of-its-kind approach that connects the strengths and coverage of formal\nand fuzzing methods to efficiently detect vulnerabilities across protocol logic\nand implementation stacks in a hierarchical manner. We design and implement\nformal verification to detect attack traces in critical protocols, which are\nused to guide subsequent fuzz testing and incorporate feedback from fuzz\ntesting to broaden the scope of formal verification. This innovative approach\nsignificantly improves efficiency and enables the auto-discovery of\nvulnerabilities and unintended emergent behaviors from the 3GPP protocols to\nsoftware stacks. Following this approach, we discover one identifier leakage\nmodel, one DoS attack model, and two eavesdrop attack models due to the absence\nof rudimentary MITM protection within the protocol, despite the existence of a\nTransport Layer Security (TLS) solution to this issue for over a decade. More\nremarkably, guided by the identified formal analysis and attack models, we\nexploit 61 vulnerabilities using fuzz testing demonstrated on srsRAN platforms.\nThese identified vulnerabilities contribute to fortifying protocol-level\nassumptions and refining the search space. Compared to state-of-the-art fuzz\ntesting, our united formal and fuzzing methodology enables auto-assurance by\nsystematically discovering vulnerabilities. It significantly reduces\ncomputational complexity, transforming the non-practical exponential growth in\ncomputational cost into linear growth.",
        "translated": ""
    },
    {
        "title": "RCVaR: an Economic Approach to Estimate Cyberattacks Costs using Data\n  from Industry Reports",
        "url": "http://arxiv.org/abs/2307.11140v1",
        "pub_date": "2023-07-20",
        "summary": "Digitization increases business opportunities and the risk of companies being\nvictims of devastating cyberattacks. Therefore, managing risk exposure and\ncybersecurity strategies is essential for digitized companies that want to\nsurvive in competitive markets. However, understanding company-specific risks\nand quantifying their associated costs is not trivial. Current approaches fail\nto provide individualized and quantitative monetary estimations of\ncybersecurity impacts. Due to limited resources and technical expertise, SMEs\nand even large companies are affected and struggle to quantify their\ncyberattack exposure. Therefore, novel approaches must be placed to support the\nunderstanding of the financial loss due to cyberattacks. This article\nintroduces the Real Cyber Value at Risk (RCVaR), an economical approach for\nestimating cybersecurity costs using real-world information from public\ncybersecurity reports. RCVaR identifies the most significant cyber risk factors\nfrom various sources and combines their quantitative results to estimate\nspecific cyberattacks costs for companies. Furthermore, RCVaR extends current\nmethods to achieve cost and risk estimations based on historical real-world\ndata instead of only probability-based simulations. The evaluation of the\napproach on unseen data shows the accuracy and efficiency of the RCVaR in\npredicting and managing cyber risks. Thus, it shows that the RCVaR is a\nvaluable addition to cybersecurity planning and risk management processes.",
        "translated": ""
    },
    {
        "title": "SoK: Design, Vulnerabilities and Defense of Cryptocurrency Wallets",
        "url": "http://arxiv.org/abs/2307.12874v2",
        "pub_date": "2023-07-24",
        "summary": "The rapid growth of decentralized digital currencies, enabled by blockchain\ntechnology, has ushered in a new era of peer-to-peer transactions,\nrevolutionizing the global economy. Cryptocurrency wallets, serving as crucial\nendpoints for these transactions, have become increasingly prevalent. However,\nthe escalating value and usage of these wallets also expose them to significant\nsecurity risks and challenges. This research aims to comprehensively explore\nthe security aspects of cryptocurrency wallets. It provides a taxonomy of\nwallet types, analyzes their design and implementation, identifies common\nvulnerabilities and attacks, and discusses defense mechanisms and mitigation\nstrategies. The taxonomy covers custodial, non-custodial, hot, and cold\nwallets, highlighting their unique characteristics and associated security\nconsiderations. The security analysis scrutinizes the theoretical and practical\naspects of wallet design, while assessing the efficacy of existing security\nmeasures and protocols. Notable wallet attacks, such as Binance, Mt. Gox are\nexamined to understand their causes and consequences. Furthermore, the paper\nsurveys defense mechanisms, transaction monitoring, evaluating their\neffectiveness in mitigating threats.",
        "translated": ""
    },
    {
        "title": "Data-free Black-box Attack based on Diffusion Model",
        "url": "http://arxiv.org/abs/2307.12872v1",
        "pub_date": "2023-07-24",
        "summary": "Since the training data for the target model in a data-free black-box attack\nis not available, most recent schemes utilize GANs to generate data for\ntraining substitute model. However, these GANs-based schemes suffer from low\ntraining efficiency as the generator needs to be retrained for each target\nmodel during the substitute training process, as well as low generation\nquality. To overcome these limitations, we consider utilizing the diffusion\nmodel to generate data, and propose a data-free black-box attack scheme based\non diffusion model to improve the efficiency and accuracy of substitute\ntraining. Despite the data generated by the diffusion model exhibits high\nquality, it presents diverse domain distributions and contains many samples\nthat do not meet the discriminative criteria of the target model. To further\nfacilitate the diffusion model to generate data suitable for the target model,\nwe propose a Latent Code Augmentation (LCA) method to guide the diffusion model\nin generating data. With the guidance of LCA, the data generated by the\ndiffusion model not only meets the discriminative criteria of the target model\nbut also exhibits high diversity. By utilizing this data, it is possible to\ntrain substitute model that closely resemble the target model more efficiently.\nExtensive experiments demonstrate that our LCA achieves higher attack success\nrates and requires fewer query budgets compared to GANs-based schemes for\ndifferent target models.",
        "translated": ""
    },
    {
        "title": "Securing Bystander Privacy in Mixed Reality While Protecting the User\n  Experience",
        "url": "http://arxiv.org/abs/2307.12847v1",
        "pub_date": "2023-07-24",
        "summary": "The modern Mixed Reality devices that make the Metaverse viable can also\nrequire vast information about the physical world. These devices can also\nviolate the privacy of unsuspecting or unwilling bystanders in their vicinity.\nIn this article, we explore the problem, existing solutions, and avenues for\nfuture research.",
        "translated": ""
    },
    {
        "title": "Execution at RISC: Stealth JOP Attacks on RISC-V Applications",
        "url": "http://arxiv.org/abs/2307.12648v1",
        "pub_date": "2023-07-24",
        "summary": "RISC-V is a recently developed open instruction set architecture gaining a\nlot of attention. To achieve a lasting security on these systems and design\nefficient countermeasures, a better understanding of vulnerabilities to novel\nand potential future attacks is mandatory. This paper demonstrates that RISC-V\nis sensible to Jump-Oriented Programming, a class of complex code-reuse\nattacks. We provide an analysis of new dispatcher gadgets we discovered, and\nshow how they can be used together in order to build a stealth attack,\nbypassing existing protections. A proof-of-concept attack is implemented on an\nembedded web server compiled for RISC-V, in which we introduced a\nvulnerability, allowing an attacker to remotely read an arbitrary file from the\nhost machine.",
        "translated": ""
    },
    {
        "title": "A Degree Bound For The c-Boomerang Uniformity Of Permutation Monomials",
        "url": "http://arxiv.org/abs/2307.12621v1",
        "pub_date": "2023-07-24",
        "summary": "Let $\\mathbb{F}_q$ be a finite field of characteristic $p$. In this paper we\nprove that the $c$-Boomerang Uniformity, $c \\neq 0$, for all permutation\nmonomials $x^d$, where $d &gt; 1$ and $p \\nmid d$, is bounded by $d^2$. Further,\nwe utilize this bound to estimate the $c$-boomerang uniformity of a large class\nof Generalized Triangular Dynamical Systems, a polynomial-based approach to\ndescribe cryptographic permutations, including the well-known\nSubstitution-Permutation Network.",
        "translated": ""
    },
    {
        "title": "PUMA: Secure Inference of LLaMA-7B in Five Minutes",
        "url": "http://arxiv.org/abs/2307.12533v1",
        "pub_date": "2023-07-24",
        "summary": "With ChatGPT as a representative, tons of companies have began to provide\nservices based on large Transformers models. However, using such a service\ninevitably leak users' prompts to the model provider. Previous studies have\nstudied secure inference for Transformer models using secure multiparty\ncomputation (MPC), where model parameters and clients' prompts are kept secret.\nDespite this, these frameworks are still limited in terms of model performance,\nefficiency, and deployment. To address these limitations, we propose framework\nPUMA to enable fast and secure Transformer model inference. Our framework\ndesigns high quality approximations for expensive functions, such as GeLU and\nSoftmax, which significantly reduce the cost of secure inference while\npreserving the model performance. Additionally, we design secure Embedding and\nLayerNorm procedures that faithfully implement the desired functionality\nwithout undermining the Transformer architecture. PUMA is about 2x faster than\nthe state-of-the-art MPC framework MPCFORMER(ICLR 2023) and has similar\naccuracy as plaintext models without fine-tuning (which the previous works\nfailed to achieve).\n  One more thing, PUMA can evaluate LLaMA-7B in around 5 minutes to generate 1\ntoken. To our best knowledge, this is the first time that a model with such a\nparameter size is able to be evaluated under MPC. PUMA has been open-sourced in\nthe Github repository of SecretFlow-SPU.",
        "translated": ""
    },
    {
        "title": "Maximal Quantum Information Leakage",
        "url": "http://arxiv.org/abs/2307.12529v1",
        "pub_date": "2023-07-24",
        "summary": "A new measure of information leakage for quantum encoding of classical data\nis defined. An adversary can access a single copy of the state of a quantum\nsystem that encodes some classical data and is interested in correctly guessing\na general randomized or deterministic function of the data (e.g., a specific\nfeature or attribute of the data in quantum machine learning) that is unknown\nto the security analyst. The resulting measure of information leakage, referred\nto as maximal quantum leakage, is the multiplicative increase of the\nprobability of correctly guessing any function of the data upon observing\nmeasurements of the quantum state. Maximal quantum leakage is shown to satisfy\npost-processing inequality (i.e., applying a quantum channel reduces\ninformation leakage) and independence property (i.e., leakage is zero if the\nquantum state is independent of the classical data), which are fundamental\nproperties required for privacy and security analysis. It also bounds\naccessible information. Effects of global and local depolarizing noise models\non the maximal quantum leakage are established.",
        "translated": ""
    },
    {
        "title": "Investigating the Existence of \"Secret Language'' in Language Models",
        "url": "http://arxiv.org/abs/2307.12507v1",
        "pub_date": "2023-07-24",
        "summary": "In this paper, we study the problem of secret language in NLP, where current\nlanguage models (LMs) seem to have a hidden vocabulary that allows them to\ninterpret absurd inputs as meaningful concepts. We investigate two research\nquestions: ``Does the secret language phenomenon exist in different language\nmodels?'' and ``Does secret language depend on specific context?'' To answer\nthese questions, we introduce a novel method named \\textit{SecretFinding}, a\ngradient-based approach that can automatically discover secret languages in\nLMs. We conduct experiments on five representative models (Electra, ALBERT,\nRoberta, DistillBERT, and CLIP) finetuned on four NLP benchmarks (SST-2, MRPC,\nSNLI, and SQuAD) and a language-grounding benchmark (MSCOCO). Our experimental\nresults show that even when we replace the most important words with others\nthat are semantically dissimilar to the original words in a sentence, LMs do\nnot consider the new sentence semantically dissimilar to the original, as the\noutput does not change with a high probability. This phenomenon holds true\nacross the five models and five tasks and gives a positive answer to the first\nresearch question. As for the second research question, we find that the secret\nlanguage discovered by \\textit{SecretFinding} is quite general and could even\nbe transferred to other models in the black-box settings, such as GPT-3 and\nChatGPT. Finally, we discuss the causes of secret language, how to eliminate\nit, the potential connection to memorization, and ethical implications.\nExamples of secret language found by SecretFinding are available on\nhttps://huggingface.co/spaces/anonymousauthors/ACL23_SecretLanguage.",
        "translated": ""
    },
    {
        "title": "A Coefficient-Embedding Ideal Lattice can be Embedded into Infinitely\n  Many Polynomial Rings",
        "url": "http://arxiv.org/abs/2307.12497v1",
        "pub_date": "2023-07-24",
        "summary": "Many lattice-based crypstosystems employ ideal lattices for high efficiency.\nHowever, the additional algebraic structure of ideal lattices usually makes us\nworry about the security, and it is widely believed that the algebraic\nstructure will help us solve the hard problems in ideal lattices more\nefficiently. In this paper, we study the additional algebraic structure of\nideal lattices further and find that a given ideal lattice in some fixed\npolynomial ring can be embedded as an ideal in infinitely many different\npolynomial rings. We explicitly present all these polynomial rings for any\ngiven ideal lattice. The interesting phenomenon tells us that a single ideal\nlattice may have more abundant algebraic structures than we imagine, which will\nimpact the security of corresponding crypstosystems. For example, it increases\nthe difficulties to evaluate the security of crypstosystems based on ideal\nlattices, since it seems that we need consider all the polynomial rings that\nthe given ideal lattices can be embedded into if we believe that the algebraic\nstructure will contribute to solve the corresponding hard problem. It also\ninspires us a new method to solve the ideal lattice problems by embedding the\ngiven ideal lattice into another well-studied polynomial ring. As a by-product,\nwe also introduce an efficient algorithm to identify if a given lattice is an\nideal lattice or not.",
        "translated": ""
    },
    {
        "title": "ChatGPT for Software Security: Exploring the Strengths and Limitations\n  of ChatGPT in the Security Applications",
        "url": "http://arxiv.org/abs/2307.12488v1",
        "pub_date": "2023-07-24",
        "summary": "ChatGPT, as a versatile large language model, has demonstrated remarkable\npotential in addressing inquiries across various domains. Its ability to\nanalyze, comprehend, and synthesize information from both online sources and\nuser inputs has garnered significant attention. Previous research has explored\nChatGPT's competence in code generation and code reviews. In this paper, we\ndelve into ChatGPT's capabilities in security-oriented program analysis,\nfocusing on perspectives from both attackers and security analysts. We present\na case study involving several security-oriented program analysis tasks while\ndeliberately introducing challenges to assess ChatGPT's responses. Through an\nexamination of the quality of answers provided by ChatGPT, we gain a clearer\nunderstanding of its strengths and limitations in the realm of\nsecurity-oriented program analysis.",
        "translated": ""
    },
    {
        "title": "Node Injection Link Stealing Attack",
        "url": "http://arxiv.org/abs/2307.13548v1",
        "pub_date": "2023-07-25",
        "summary": "In this paper, we present a stealthy and effective attack that exposes\nprivacy vulnerabilities in Graph Neural Networks (GNNs) by inferring private\nlinks within graph-structured data. Focusing on the inductive setting where new\nnodes join the graph and an API is used to query predictions, we investigate\nthe potential leakage of private edge information. We also propose methods to\npreserve privacy while maintaining model utility. Our attack demonstrates\nsuperior performance in inferring the links compared to the state of the art.\nFurthermore, we examine the application of differential privacy (DP) mechanisms\nto mitigate the impact of our proposed attack, we analyze the trade-off between\nprivacy preservation and model utility. Our work highlights the privacy\nvulnerabilities inherent in GNNs, underscoring the importance of developing\nrobust privacy-preserving mechanisms for their application.",
        "translated": ""
    },
    {
        "title": "Federated Heavy Hitter Recovery under Linear Sketching",
        "url": "http://arxiv.org/abs/2307.13347v1",
        "pub_date": "2023-07-25",
        "summary": "Motivated by real-life deployments of multi-round federated analytics with\nsecure aggregation, we investigate the fundamental communication-accuracy\ntradeoffs of the heavy hitter discovery and approximate (open-domain) histogram\nproblems under a linear sketching constraint. We propose efficient algorithms\nbased on local subsampling and invertible bloom look-up tables (IBLTs). We also\nshow that our algorithms are information-theoretically optimal for a broad\nclass of interactive schemes. The results show that the linear sketching\nconstraint does increase the communication cost for both tasks by introducing\nan extra linear dependence on the number of users in a round. Moreover, our\nresults also establish a separation between the communication cost for heavy\nhitter discovery and approximate histogram in the multi-round setting. The\ndependence on the number of rounds $R$ is at most logarithmic for heavy hitter\ndiscovery whereas that of approximate histogram is $\\Theta(\\sqrt{R})$. We also\nempirically demonstrate our findings.",
        "translated": ""
    },
    {
        "title": "On-Device Speaker Anonymization of Acoustic Embeddings for ASR based\n  onFlexible Location Gradient Reversal Layer",
        "url": "http://arxiv.org/abs/2307.13343v1",
        "pub_date": "2023-07-25",
        "summary": "Smart devices serviced by large-scale AI models necessitates user data\ntransfer to the cloud for inference. For speech applications, this means\ntransferring private user information, e.g., speaker identity. Our paper\nproposes a privacy-enhancing framework that targets speaker identity\nanonymization while preserving speech recognition accuracy for our downstream\ntask~-~Automatic Speech Recognition (ASR). The proposed framework attaches\nflexible gradient reversal based speaker adversarial layers to target layers\nwithin an ASR model, where speaker adversarial training anonymizes acoustic\nembeddings generated by the targeted layers to remove speaker identity. We\npropose on-device deployment by execution of initial layers of the ASR model,\nand transmitting anonymized embeddings to the cloud, where the rest of the\nmodel is executed while preserving privacy. Experimental results show that our\nmethod efficiently reduces speaker recognition relative accuracy by 33%, and\nimproves ASR performance by achieving 6.2% relative Word Error Rate (WER)\nreduction.",
        "translated": ""
    },
    {
        "title": "Spectral-DP: Differentially Private Deep Learning through Spectral\n  Perturbation and Filtering",
        "url": "http://arxiv.org/abs/2307.13231v1",
        "pub_date": "2023-07-25",
        "summary": "Differential privacy is a widely accepted measure of privacy in the context\nof deep learning algorithms, and achieving it relies on a noisy training\napproach known as differentially private stochastic gradient descent (DP-SGD).\nDP-SGD requires direct noise addition to every gradient in a dense neural\nnetwork, the privacy is achieved at a significant utility cost. In this work,\nwe present Spectral-DP, a new differentially private learning approach which\ncombines gradient perturbation in the spectral domain with spectral filtering\nto achieve a desired privacy guarantee with a lower noise scale and thus better\nutility. We develop differentially private deep learning methods based on\nSpectral-DP for architectures that contain both convolution and fully connected\nlayers. In particular, for fully connected layers, we combine a block-circulant\nbased spatial restructuring with Spectral-DP to achieve better utility. Through\ncomprehensive experiments, we study and provide guidelines to implement\nSpectral-DP deep learning on benchmark datasets. In comparison with\nstate-of-the-art DP-SGD based approaches, Spectral-DP is shown to have\nuniformly better utility performance in both training from scratch and transfer\nlearning settings.",
        "translated": ""
    },
    {
        "title": "Sensor selection for fine-grained behavior verification that respects\n  privacy",
        "url": "http://arxiv.org/abs/2307.13203v1",
        "pub_date": "2023-07-25",
        "summary": "A useful capability is that of classifying some agent's behavior using data\nfrom a sequence, or trace, of sensor measurements. The sensor selection problem\ninvolves choosing a subset of available sensors to ensure that, when generated,\nobservation traces will contain enough information to determine whether the\nagent's activities match some pattern. In generalizing prior work, this paper\nstudies a formulation in which multiple behavioral itineraries may be supplied,\nwith sensors selected to distinguish between behaviors. This allows one to pose\nfine grained questions, e.g., to position the agent's activity on a spectrum.\nIn addition, with multiple itineraries, one can also ask about choices of\nsensors where some behavior is always plausibly concealed by (or mistaken for,\nor conflated with) another. Using sensor ambiguity to limit the acquisition of\nknowledge is a strong privacy guarantee, and one which some earlier work has\nexamined. By concretely formulating privacy requirements for sensor selection,\nthis paper connects both lines of work: privacy -- where there is a bound from\nabove, and behavior verification -- where sensors are bounded from below. We\nexamine the worst case computational complexity that results from both types of\nbounds, proving that upper bounds are more challenging under standard\ncomputational complexity assumptions. The problem is intractable in general,\nbut we give a novel approach to solving this problem that can exploit\ninterrelationships between constraints, and we see opportunities for a few\noptimizations. Case studies are presented to demonstrate the usefulness and\nscalability of our proposed solution, and to assess the impact of the\noptimizations.",
        "translated": ""
    },
    {
        "title": "Malware Resistant Data Protection in Hyper-connected Networks: A survey",
        "url": "http://arxiv.org/abs/2307.13164v1",
        "pub_date": "2023-07-24",
        "summary": "Data protection is the process of securing sensitive information from being\ncorrupted, compromised, or lost. A hyperconnected network, on the other hand,\nis a computer networking trend in which communication occurs over a network.\nHowever, what about malware. Malware is malicious software meant to penetrate\nprivate data, threaten a computer system, or gain unauthorised network access\nwithout the users consent. Due to the increasing applications of computers and\ndependency on electronically saved private data, malware attacks on sensitive\ninformation have become a dangerous issue for individuals and organizations\nacross the world. Hence, malware defense is critical for keeping our computer\nsystems and data protected. Many recent survey articles have focused on either\nmalware detection systems or single attacking strategies variously. To the best\nof our knowledge, no survey paper demonstrates malware attack patterns and\ndefense strategies combinedly. Through this survey, this paper aims to address\nthis issue by merging diverse malicious attack patterns and machine learning\n(ML) based detection models for modern and sophisticated malware. In doing so,\nwe focus on the taxonomy of malware attack patterns based on four fundamental\ndimensions the primary goal of the attack, method of attack, targeted exposure\nand execution process, and types of malware that perform each attack. Detailed\ninformation on malware analysis approaches is also investigated. In addition,\nexisting malware detection techniques employing feature extraction and ML\nalgorithms are discussed extensively. Finally, it discusses research\ndifficulties and unsolved problems, including future research directions.",
        "translated": ""
    },
    {
        "title": "Attacks on Dynamic DeFi Interest Rate Curves",
        "url": "http://arxiv.org/abs/2307.13139v1",
        "pub_date": "2023-07-24",
        "summary": "As decentralized money market protocols continue to grow in value locked,\nthere have been a number of optimizations proposed for improving capital\nefficiency. One set of proposals from Euler Finance and Mars Protocol is to\nhave an interest rate curve that is a proportional-integral-derivative (PID)\ncontroller. In this paper, we demonstrate attacks on proportional and\nproportional-integral controlled interest rate curves. The attack allows one to\nmanipulate the interest rate curve to take a higher proportion of the earned\nyield than their pro-rata share of the lending pool. We conclude with an\nargument that PID interest rate curves can actually \\emph{reduce} capital\nefficiency (due to attack mitigations) unless supply and demand elasticity to\nrate changes are sufficiently high.",
        "translated": ""
    },
    {
        "title": "Why Don't You Clean Your Glasses? Perception Attacks with Dynamic\n  Optical Perturbations",
        "url": "http://arxiv.org/abs/2307.13131v1",
        "pub_date": "2023-07-24",
        "summary": "Camera-based autonomous systems that emulate human perception are\nincreasingly being integrated into safety-critical platforms. Consequently, an\nestablished body of literature has emerged that explores adversarial attacks\ntargeting the underlying machine learning models. Adapting adversarial attacks\nto the physical world is desirable for the attacker, as this removes the need\nto compromise digital systems. However, the real world poses challenges related\nto the \"survivability\" of adversarial manipulations given environmental noise\nin perception pipelines and the dynamicity of autonomous systems. In this\npaper, we take a sensor-first approach. We present EvilEye, a man-in-the-middle\nperception attack that leverages transparent displays to generate dynamic\nphysical adversarial examples. EvilEye exploits the camera's optics to induce\nmisclassifications under a variety of illumination conditions. To generate\ndynamic perturbations, we formalize the projection of a digital attack into the\nphysical domain by modeling the transformation function of the captured image\nthrough the optical pipeline. Our extensive experiments show that EvilEye's\ngenerated adversarial perturbations are much more robust across varying\nenvironmental light conditions relative to existing physical perturbation\nframeworks, achieving a high attack success rate (ASR) while bypassing\nstate-of-the-art physical adversarial detection frameworks. We demonstrate that\nthe dynamic nature of EvilEye enables attackers to adapt adversarial examples\nacross a variety of objects with a significantly higher ASR compared to\nstate-of-the-art physical world attack frameworks. Finally, we discuss\nmitigation strategies against the EvilEye attack.",
        "translated": ""
    },
    {
        "title": "Quantum key distribution for data center security -- a feasibility study",
        "url": "http://arxiv.org/abs/2307.13098v1",
        "pub_date": "2023-07-24",
        "summary": "Data centers are nowadays referred to as the digital world's cornerstone.\nQuantum key distribution (QKD) is a method that solves the problem of\ndistributing cryptographic keys between two entities, with the security rooted\nin the laws of quantum physics. This document provides an assessment of the\nneed and opportunity for ushering QKD in data centers. Together with technical\nexamples and inputs on how QKD has and could be integrated into data-center\nlike environments, the document also discusses the creation of value through\nfuture-proof data security as well as the market potential that QKD brings on\nthe table through e.g., crypto-agility. While primarily addressed to data\ncenter owners/operators, the document also offers a knowledge base to QKD\nvendors planning to diversify to the data center market segment.",
        "translated": ""
    },
    {
        "title": "Unveiling Security, Privacy, and Ethical Concerns of ChatGPT",
        "url": "http://arxiv.org/abs/2307.14192v1",
        "pub_date": "2023-07-26",
        "summary": "This paper delves into the realm of ChatGPT, an AI-powered chatbot that\nutilizes topic modeling and reinforcement learning to generate natural\nresponses. Although ChatGPT holds immense promise across various industries,\nsuch as customer service, education, mental health treatment, personal\nproductivity, and content creation, it is essential to address its security,\nprivacy, and ethical implications. By exploring the upgrade path from GPT-1 to\nGPT-4, discussing the model's features, limitations, and potential\napplications, this study aims to shed light on the potential risks of\nintegrating ChatGPT into our daily lives. Focusing on security, privacy, and\nethics issues, we highlight the challenges these concerns pose for widespread\nadoption. Finally, we analyze the open problems in these areas, calling for\nconcerted efforts to ensure the development of secure and ethically sound large\nlanguage models.",
        "translated": ""
    },
    {
        "title": "ICCPS: Impact discovery using causal inference for cyber attacks in CPSs",
        "url": "http://arxiv.org/abs/2307.14161v1",
        "pub_date": "2023-07-26",
        "summary": "We propose a new method to quantify the impact of cyber attacks in Cyber\nPhysical Systems (CPSs). In particular, our method allows to identify the\nDesign Parameter (DPs) affected due to a cyber attack launched on a different\nset of DPs in the same CPS. To achieve this, we adopt causal graphs to causally\nlink DPs with each other and quantify the impact of one DP on another. Using\nSWaT, a real world testbed of a water treatment system, we demonstrate that\ncausal graphs can be build in two ways: i) using domain knowledge of the\ncontrol logic and the physical connectivity structure of the DPs, we call these\ncausal domain graphs and ii) learning from operational data logs, we call these\ncausal learnt graphs. We then compare these graphs when a same set of DPs is\nused. Our analysis shows a common set of edges between the causal domain graphs\nand the causal learnt graphs exists, which helps validate the causal learnt\ngraphs. Additionally, we show that the learnt graphs can discover new causal\nrelations, not initially considered in the domain graphs, that help\nsignificantly characterising the impact of the attack. We use causal domain\ngraphs to estimate the parameters of the graphs, and the causal learnt graphs\nfor causal inference. To learn the structure of the causal learnt graphs in all\nthe six-stages of SWaT, we experiment with three learning algorithms: Peter\nClarke (PC), Hill Climb (HC) search and Chow-Lie (CH). Finally, we demonstrate\nhow causal graphs can be used to analyse the impact of cyber attacks by\nanalysing nine well known cyber attacks on the SWaT test bed. We find that by\nusing causal learnt graphs the DPs impacted by the attacks are correctly\ndiscovered with a probability greater than 0.9.",
        "translated": ""
    },
    {
        "title": "Risk Assessment Graphs: Utilizing Attack Graphs for Risk Assessment",
        "url": "http://arxiv.org/abs/2307.14114v1",
        "pub_date": "2023-07-26",
        "summary": "Risk assessment plays a crucial role in ensuring the security and resilience\nof modern computer systems. Existing methods for conducting risk assessments\noften suffer from tedious and time-consuming processes, making it challenging\nto maintain a comprehensive overview of potential security issues. In this\npaper, we propose a novel approach that leverages attack graphs to enhance the\nefficiency and effectiveness of risk assessment. Attack graphs visually\nrepresent the various attack paths that adversaries can exploit within a\nsystem, enabling a systematic exploration of potential vulnerabilities. By\nextending attack graphs with capabilities to include countermeasures and\nconsequences, they can be leveraged to constitute the complete risk assessment\nprocess. Our method offers a more streamlined and comprehensive analysis of\nsystem vulnerabilities, where system changes, or environment changes can easily\nbe adapted and the issues exposing the highest risk can easily be identified.\nWe demonstrate the effectiveness of our approach through a case study, as well\nas the applicability by combining existing risk assessment standards with our\nmethod. Our work aims to bridge the gap between risk assessment practices and\nevolving threat landscapes, offering an improved methodology for managing and\nmitigating risks in modern computer systems.",
        "translated": ""
    },
    {
        "title": "Open Image Content Disarm And Reconstruction",
        "url": "http://arxiv.org/abs/2307.14057v1",
        "pub_date": "2023-07-26",
        "summary": "With the advance in malware technology, attackers create new ways to hide\ntheir malicious code from antivirus services. One way to obfuscate an attack is\nto use common files as cover to hide the malicious scripts, so the malware will\nlook like a legitimate file. Although cutting-edge Artificial Intelligence and\ncontent signature exist, evasive malware successfully bypasses next-generation\nmalware detection using advanced methods like steganography. Some of the files\ncommonly used to hide malware are image files (e.g., JPEG). In addition, some\nmalware use steganography to hide malicious scripts or sensitive data in\nimages. Steganography in images is difficult to detect even with specialized\ntools. Image-based attacks try to attack the user's device using malicious\npayloads or utilize image steganography to hide sensitive data inside\nlegitimate images and leak it outside the user's device. Therefore in this\npaper, we present a novel Image Content Disarm and Reconstruction (ICDR). Our\nICDR system removes potential malware, with a zero trust approach, while\nmaintaining high image quality and file usability. By extracting the image\ndata, removing it from the rest of the file, and manipulating the image pixels,\nit is possible to disable or remove the hidden malware inside the file.",
        "translated": ""
    },
    {
        "title": "GovernR: Provenance and Confidentiality Guarantees In Research Data\n  Repositories",
        "url": "http://arxiv.org/abs/2307.14041v1",
        "pub_date": "2023-07-26",
        "summary": "We propose cryptographic protocols to incorporate time provenance guarantees\nwhile meeting confidentiality and controlled sharing needs for research data.\nWe demonstrate the efficacy of these mechanisms by developing and benchmarking\na practical tool, GovernR, which furthermore takes into usability issues and is\ncompatible with a popular open-sourced research data storage platform,\nDataverse. In doing so, we identify and provide a solution addressing an\nimportant gap (though applicable to only niche use cases) in practical research\ndata management.",
        "translated": ""
    },
    {
        "title": "Enhanced Security against Adversarial Examples Using a Random Ensemble\n  of Encrypted Vision Transformer Models",
        "url": "http://arxiv.org/abs/2307.13985v1",
        "pub_date": "2023-07-26",
        "summary": "Deep neural networks (DNNs) are well known to be vulnerable to adversarial\nexamples (AEs). In addition, AEs have adversarial transferability, which means\nAEs generated for a source model can fool another black-box model (target\nmodel) with a non-trivial probability. In previous studies, it was confirmed\nthat the vision transformer (ViT) is more robust against the property of\nadversarial transferability than convolutional neural network (CNN) models such\nas ConvMixer, and moreover encrypted ViT is more robust than ViT without any\nencryption. In this article, we propose a random ensemble of encrypted ViT\nmodels to achieve much more robust models. In experiments, the proposed scheme\nis verified to be more robust against not only black-box attacks but also\nwhite-box ones than convention methods.",
        "translated": ""
    },
    {
        "title": "Security Weaknesses in IoT Management Platforms",
        "url": "http://arxiv.org/abs/2307.13952v1",
        "pub_date": "2023-07-26",
        "summary": "A diverse set of Internet of Things (IoT) devices are becoming an integrated\npart of daily lives, and playing an increasingly vital role in various\nindustry, enterprise and agricultural settings. The current IoT ecosystem\nrelies on several IoT management platforms to manage and operate a large number\nof IoT devices, their data, and their connectivity. Considering their key role,\nthese platforms must be properly secured against cyber attacks. In this work,\nwe first explore the core operations/features of leading platforms to design a\nframework to perform a systematic security evaluation of these platforms.\nSubsequently, we use our framework to analyze a representative set of 52 IoT\nmanagement platforms, including 42 web-hosted and 10 locally-deployable\nplatforms. We discover a number of high severity unauthorized access\nvulnerabilities in 9/52 evaluated IoT management platforms, which could be\nabused to perform attacks such as remote IoT SIM deactivation, IoT SIM\novercharging and IoT device data forgery. More seriously, we also uncover\ninstances of broken authentication in 13/52 platforms, including complete\naccount takeover on 8/52 platforms along with remote code execution on 2/52\nplatforms. In effect, 17/52 platforms were affected by vulnerabilities that\ncould lead to platform-wide attacks. Overall, vulnerabilities were uncovered in\n33 platforms, out of which 28 platforms responded to our responsible\ndisclosure. We were also assigned 11 CVEs and awarded bounty for our findings.",
        "translated": ""
    },
    {
        "title": "Random (Un)rounding : Vulnerabilities in Discrete Attribute Disclosure\n  in the 2021 Canadian Census",
        "url": "http://arxiv.org/abs/2307.13859v1",
        "pub_date": "2023-07-25",
        "summary": "The 2021 Canadian census is notable for using a unique form of privacy,\nrandom rounding, which independently and probabilistically rounds discrete\nnumerical attribute values. In this work, we explore how hierarchical summative\ncorrelation between discrete variables allows for both probabilistic and exact\nsolutions to attribute values in the 2021 Canadian Census disclosure. We\ndemonstrate that, in some cases, it is possible to \"unround\" and extract the\noriginal private values before rounding, both in the presence and absence of\nprovided population invariants. Using these methods, we expose the exact value\nof 624 previously private attributes in the 2021 Canadian census disclosure. We\nalso infer the potential values of more than 1000 private attributes with a\nhigh probability of correctness. Finally, we propose how a simple solution\nbased on unbounded discrete noise can effectively negate exact unrounding while\nmaintaining high utility in the final product.",
        "translated": ""
    },
    {
        "title": "TeleBTC: Trustless Wrapped Bitcoin",
        "url": "http://arxiv.org/abs/2307.13848v1",
        "pub_date": "2023-07-25",
        "summary": "This paper introduces TeleBTC, a fully decentralized protocol designed to\nwrap Bitcoin (BTC) on programmable blockchains. The creation of a decentralized\nwrapped BTC presents challenges due to the non-programmable nature of Bitcoin,\nmaking it difficult to custody BTCs in a decentralized way. Existing solutions\nhave addressed this challenge by introducing an external layer of validators\nwho take custody of users' BTCs. However, the security and decentralization of\nthis layer are inferior to the underlying blockchains on which wrapped BTC is\nbuilt. Moreover, the process of joining or leaving for a validator has become\noverly complex and expensive. To overcome these limitations, we propose a novel\napproach that eliminates the need for such an external layer by leveraging the\nlight client bridge protocol. Additionally, we employ economic mechanisms such\nas incentivization and slashing, resulting in a secure and trust-minimized\nwrapped BTC solution. With TeleBTC, users can seamlessly transfer their BTC to\nother blockchains and utilize it within decentralized applications.\nFurthermore, they can unwrap their TeleBTC and reclaim the native BTC. To\naddress the high costs associated with light client bridges, we present an\noptimistic approach that minimizes the cost. This approach significantly\nreduces the operational expenses of running the protocol.",
        "translated": ""
    },
    {
        "title": "Determining the Optimal Frequencies for a Duplicated Randomized Clock\n  SCA Countermeasure",
        "url": "http://arxiv.org/abs/2307.13834v1",
        "pub_date": "2023-07-25",
        "summary": "Side-channel attacks pose significant challenges to the security of embedded\nsystems, often allowing attackers to circumvent encryption algorithms in\nminutes compared to the trillions of years required for brute-force attacks. To\nmitigate these vulnerabilities, various countermeasures have been developed.\nThis study focuses on two specific countermeasures: randomization of the\nencryption algorithm's clock and the incorporation of a dummy core to disguise\npower traces.\n  The objective of this research is to identify the optimal frequencies that\nyield the highest level of randomness when these two countermeasures are\ncombined. By investigating the interplay between clock randomization and the\npresence of dummy cores, we aim to enhance the overall security of embedded\nsystems. The insights gained from this study will contribute to the development\nof more robust countermeasures against side-channel attacks, bolstering the\nprotection of sensitive information and systems.\n  To achieve this, we conduct simulations and perform side-channel attacks on\nan FPGA to establish the relationship between frequencies and the resulting\nprotection. We break the encryption on a non-duplicated circuit and note the\nleast amount of measured power traces necessary and the timing overhead. We do\nthis for all sets of frequencies considered which gives a good indication of\nwhich sets of frequencies give good protection. By comparing the frequencies\ngenerated with those from the duplicated circuit we use similar conclusions to\nprove whether a frequency set is secure or not.\n  Based on our results we argue that having one frequency lower than half of\nthe base frequency and the other frequencies being close but not higher than\nthe base gives the highest security compared to the timing overhead measured.",
        "translated": ""
    },
    {
        "title": "Universal and Transferable Adversarial Attacks on Aligned Language\n  Models",
        "url": "http://arxiv.org/abs/2307.15043v1",
        "pub_date": "2023-07-27",
        "summary": "Because \"out-of-the-box\" large language models are capable of generating a\ngreat deal of objectionable content, recent work has focused on aligning these\nmodels in an attempt to prevent undesirable generation. While there has been\nsome success at circumventing these measures -- so-called \"jailbreaks\" against\nLLMs -- these attacks have required significant human ingenuity and are brittle\nin practice. In this paper, we propose a simple and effective attack method\nthat causes aligned language models to generate objectionable behaviors.\nSpecifically, our approach finds a suffix that, when attached to a wide range\nof queries for an LLM to produce objectionable content, aims to maximize the\nprobability that the model produces an affirmative response (rather than\nrefusing to answer). However, instead of relying on manual engineering, our\napproach automatically produces these adversarial suffixes by a combination of\ngreedy and gradient-based search techniques, and also improves over past\nautomatic prompt generation methods.\n  Surprisingly, we find that the adversarial prompts generated by our approach\nare quite transferable, including to black-box, publicly released LLMs.\nSpecifically, we train an adversarial attack suffix on multiple prompts (i.e.,\nqueries asking for many different types of objectionable content), as well as\nmultiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting\nattack suffix is able to induce objectionable content in the public interfaces\nto ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat,\nPythia, Falcon, and others. In total, this work significantly advances the\nstate-of-the-art in adversarial attacks against aligned language models,\nraising important questions about how such systems can be prevented from\nproducing objectionable information. Code is available at\ngithub.com/llm-attacks/llm-attacks.",
        "translated": ""
    },
    {
        "title": "Samplable Anonymous Aggregation for Private Federated Data Analysis",
        "url": "http://arxiv.org/abs/2307.15017v1",
        "pub_date": "2023-07-27",
        "summary": "We revisit the problem of designing scalable protocols for private statistics\nand private federated learning when each device holds its private data. Our\nfirst contribution is to propose a simple primitive that allows for efficient\nimplementation of several commonly used algorithms, and allows for privacy\naccounting that is close to that in the central setting without requiring the\nstrong trust assumptions it entails. Second, we propose a system architecture\nthat implements this primitive and perform a security analysis of the proposed\nsystem.",
        "translated": ""
    },
    {
        "title": "Don't Shoot the Messenger: Localization Prevention of Satellite Internet\n  Users",
        "url": "http://arxiv.org/abs/2307.14879v1",
        "pub_date": "2023-07-27",
        "summary": "Satellite Internet plays an increasingly important role in geopolitical\nconflicts. This notion was affirmed in the Ukrainian conflict escalating at the\nbeginning of 2022, with the large-scale deployment of the Starlink satellite\nInternet service which consequently demonstrated the strategic importance of a\nfree flow of information. Aside from military use, many citizens publish\nsensitive information on social media platforms to influence the public\nnarrative. However, the use of satellite communication has proven to be\ndangerous, as the signals can be monitored by other satellites and used to\ntriangulate the source on the ground. Unfortunately, the targeted killings of\njournalists have shown this threat to be effective. While the increasing\ndeployment of satellite Internet systems gives citizens an unprecedented\nmouthpiece in conflicts, protecting them against localization is an unaddressed\nproblem.\n  To address this threat, we present AnonSat, a novel scheme to protect\nsatellite Internet users from triangulation. AnonSat works with cheap\noff-the-shelf devices, leveraging long-range wireless communication to span a\nlocal network among satellite base stations. This allows rerouting users'\ncommunication to other satellite base stations, some distance away from each\nuser, thus, preventing their localization. AnonSat is designed for easy\ndeployment and usability, which we demonstrate with a prototype implementation.\nOur large-scale network simulations using real-world data sets show the\neffectiveness of AnonSat in various practical settings.",
        "translated": ""
    },
    {
        "title": "Smart Contract Migration: Security Analysis and Recommendations from\n  Ethereum to Arbitrum",
        "url": "http://arxiv.org/abs/2307.14773v1",
        "pub_date": "2023-07-27",
        "summary": "This research aims to explore the security risks posed by compatibility and\nprotocol differences in smart contract migration, using the migration of smart\ncontracts from Ethereum to Arbitrum as a case study. Through literature review,\nonline data collection, expert participation, and analysis of smart contract\nvulnerability cases, this paper conducts an in-depth research of the\ndifferences between Ethereum and Arbitrum in areas such as Messaging, Block\nProperties, Contract Address Alias, and Gas Fees. The research findings\nindicate the presence of certain security issues during the migration process\nfrom Ethereum to Arbitrum, such as abnormal operation of the sequencer\nresulting in outdated off-chain data retrieval, time-based logical errors,\nfailed permission checks, DOS attacks, and gas loss due to L1-to-L2 transaction\nfailures. To address these security issues, this paper proposes corresponding\nsolutions and recommendations to ensure the security and meet the requirements\nof the migration process. Additionally, this research emphasizes the continued\nattention and support for the security issues of smart contract migration\nthrough the case of smart contract migration from Ethereum to Arbitrum. It is\nworth noting that this research is the first in-depth research of smart\ncontract security migration from Ethereum to Arbitrum.",
        "translated": ""
    },
    {
        "title": "SEV-Step: A Single-Stepping Framework for AMD-SEV",
        "url": "http://arxiv.org/abs/2307.14757v1",
        "pub_date": "2023-07-27",
        "summary": "The ever increasing popularity and availability of Trusted Execution\nEnvironments (TEEs) had a stark influence on microarchitectural attack research\nin academia, as their strong attacker model both boosts existing attack vectors\nand introduces several new ones. While many works have focused on Intel SGX,\nother TEEs like AMD SEV have recently also started to receive more attention. A\ncommon technique when attacking SGX enclaves is single-stepping, where the\nsystem's APIC timer is used to interrupt the enclave after every instruction.\nSingle-stepping increases the temporal resolution of subsequent\nmicroarchitectural attacks to a maximum. A key driver in the proliferation of\nthis complex attack technique was the SGX-Step framework, which offered a\nstable reference implementation for single-stepping and a relatively easy\nsetup. In this paper, we demonstrate that SEV VMs can also be reliably\nsingle-stepped. To lay the foundation for further microarchitectural attack\nresearch against SEV, we introduce the reusable SEV-Step framework. Besides\nreliable single-stepping, SEV-Step provides easy access to common attack\nprimitives like page fault tracking and cache attacks against SEV. All features\ncan be used interactively from user space. We demonstrate SEV-Step's\ncapabilities by carrying out an end-to-end cache attack against SEV that leaks\nthe volume key of a LUKS2-encrypted disk. Finally, we show for the first time\nthat SEV is vulnerable to Nemesis-style attacks, which allow to extract\ninformation about the type and operands of single-stepped instructions from\nSEV-protected VMs.",
        "translated": ""
    },
    {
        "title": "FLARE: Fingerprinting Deep Reinforcement Learning Agents using Universal\n  Adversarial Masks",
        "url": "http://arxiv.org/abs/2307.14751v1",
        "pub_date": "2023-07-27",
        "summary": "We propose FLARE, the first fingerprinting mechanism to verify whether a\nsuspected Deep Reinforcement Learning (DRL) policy is an illegitimate copy of\nanother (victim) policy. We first show that it is possible to find\nnon-transferable, universal adversarial masks, i.e., perturbations, to generate\nadversarial examples that can successfully transfer from a victim policy to its\nmodified versions but not to independently trained policies. FLARE employs\nthese masks as fingerprints to verify the true ownership of stolen DRL policies\nby measuring an action agreement value over states perturbed via such masks.\nOur empirical evaluations show that FLARE is effective (100% action agreement\non stolen copies) and does not falsely accuse independent policies (no false\npositives). FLARE is also robust to model modification attacks and cannot be\neasily evaded by more informed adversaries without negatively impacting agent\nperformance. We also show that not all universal adversarial masks are suitable\ncandidates for fingerprints due to the inherent characteristics of DRL\npolicies. The spatio-temporal dynamics of DRL problems and sequential\ndecision-making process make characterizing the decision boundary of DRL\npolicies more difficult, as well as searching for universal masks that capture\nthe geometry of it.",
        "translated": ""
    },
    {
        "title": "Backdoor Attacks for In-Context Learning with Language Models",
        "url": "http://arxiv.org/abs/2307.14692v1",
        "pub_date": "2023-07-27",
        "summary": "Because state-of-the-art language models are expensive to train, most\npractitioners must make use of one of the few publicly available language\nmodels or language model APIs. This consolidation of trust increases the\npotency of backdoor attacks, where an adversary tampers with a machine learning\nmodel in order to make it perform some malicious behavior on inputs that\ncontain a predefined backdoor trigger. We show that the in-context learning\nability of large language models significantly complicates the question of\ndeveloping backdoor attacks, as a successful backdoor must work against various\nprompting strategies and should not affect the model's general purpose\ncapabilities. We design a new attack for eliciting targeted misclassification\nwhen language models are prompted to perform a particular target task and\ndemonstrate the feasibility of this attack by backdooring multiple large\nlanguage models ranging in size from 1.3 billion to 6 billion parameters.\nFinally we study defenses to mitigate the potential harms of our attack: for\nexample, while in the white-box setting we show that fine-tuning models for as\nfew as 500 steps suffices to remove the backdoor behavior, in the black-box\nsetting we are unable to develop a successful defense that relies on prompt\nengineering alone.",
        "translated": ""
    },
    {
        "title": "LinkDID: A Privacy-Preserving, Sybil-Resistant and Key-Recoverable\n  Decentralized Identity Scheme",
        "url": "http://arxiv.org/abs/2307.14679v1",
        "pub_date": "2023-07-27",
        "summary": "Decentralized identity mechanisms endeavor to endow users with complete\nsovereignty over their digital assets within the Web3 ecosystem. Unfortunately,\nthis benefit frequently comes at the expense of users' credential and identity\nprivacy. Additionally, existing schemes fail to resist Sybil attacks that have\nlong plagued Web3, and lack reasonable key recovery mechanisms to regain\ncontrol of digital assets after loss. In this work, we propose LinkDID, a\nprivacy-preserving, Sybil-resistant, and key-recoverable decentralized identity\nscheme that supports selective disclosure of credentials for arbitrary\npredicates while maintaining privacy for credentials and identities. Through an\nidentifier association mechanism, LinkDID can privately and forcibly aggregate\nusers' identifiers, providing Sybil resistance without relying on any external\ndata or collateral from benign users. To enable key recovery, LinkDID permits\nusers to establish proofs of ownership for identifiers with lost keys and\nrequest an update of corresponding keys from the decentralized ledger. We\nprovide a detailed theoretical analysis and security proofs of LinkDID, along\nwith an exhaustive performance evaluation that shows its ability to complete\ninteractions in less than 10 seconds on consumer-grade devices.",
        "translated": ""
    },
    {
        "title": "Decoding the Secrets of Machine Learning in Malware Classification: A\n  Deep Dive into Datasets, Feature Extraction, and Model Performance",
        "url": "http://arxiv.org/abs/2307.14657v1",
        "pub_date": "2023-07-27",
        "summary": "Many studies have proposed machine-learning (ML) models for malware detection\nand classification, reporting an almost-perfect performance. However, they\nassemble ground-truth in different ways, use diverse static- and\ndynamic-analysis techniques for feature extraction, and even differ on what\nthey consider a malware family. As a consequence, our community still lacks an\nunderstanding of malware classification results: whether they are tied to the\nnature and distribution of the collected dataset, to what extent the number of\nfamilies and samples in the training dataset influence performance, and how\nwell static and dynamic features complement each other.\n  This work sheds light on those open questions. by investigating the key\nfactors influencing ML-based malware detection and classification. For this, we\ncollect the largest balanced malware dataset so far with 67K samples from 670\nfamilies (100 samples each), and train state-of-the-art models for malware\ndetection and family classification using our dataset. Our results reveal that\nstatic features perform better than dynamic features, and that combining both\nonly provides marginal improvement over static features. We discover no\ncorrelation between packing and classification accuracy, and that missing\nbehaviors in dynamically-extracted features highly penalize their performance.\nWe also demonstrate how a larger number of families to classify make the\nclassification harder, while a higher number of samples per family increases\naccuracy. Finally, we find that models trained on a uniform distribution of\nsamples per family better generalize on unseen data.",
        "translated": ""
    },
    {
        "title": "Fact-Checking of AI-Generated Reports",
        "url": "http://arxiv.org/abs/2307.14634v1",
        "pub_date": "2023-07-27",
        "summary": "With advances in generative artificial intelligence (AI), it is now possible\nto produce realistic-looking automated reports for preliminary reads of\nradiology images. This can expedite clinical workflows, improve accuracy and\nreduce overall costs. However, it is also well-known that such models often\nhallucinate, leading to false findings in the generated reports. In this paper,\nwe propose a new method of fact-checking of AI-generated reports using their\nassociated images. Specifically, the developed examiner differentiates real and\nfake sentences in reports by learning the association between an image and\nsentences describing real or potentially fake findings. To train such an\nexaminer, we first created a new dataset of fake reports by perturbing the\nfindings in the original ground truth radiology reports associated with images.\nText encodings of real and fake sentences drawn from these reports are then\npaired with image encodings to learn the mapping to real/fake labels. The\nutility of such an examiner is demonstrated for verifying automatically\ngenerated reports by detecting and removing fake sentences. Future generative\nAI approaches can use the resulting tool to validate their reports leading to a\nmore responsible use of AI in expediting clinical workflows.",
        "translated": ""
    },
    {
        "title": "Adversarial training for tabular data with attack propagation",
        "url": "http://arxiv.org/abs/2307.15677v1",
        "pub_date": "2023-07-28",
        "summary": "Adversarial attacks are a major concern in security-centered applications,\nwhere malicious actors continuously try to mislead Machine Learning (ML) models\ninto wrongly classifying fraudulent activity as legitimate, whereas system\nmaintainers try to stop them. Adversarially training ML models that are robust\nagainst such attacks can prevent business losses and reduce the work load of\nsystem maintainers. In such applications data is often tabular and the space\navailable for attackers to manipulate undergoes complex feature engineering\ntransformations, to provide useful signals for model training, to a space\nattackers cannot access. Thus, we propose a new form of adversarial training\nwhere attacks are propagated between the two spaces in the training loop. We\nthen test this method empirically on a real world dataset in the domain of\ncredit card fraud detection. We show that our method can prevent about 30%\nperformance drops under moderate attacks and is essential under very aggressive\nattacks, with a trade-off loss in performance under no attacks smaller than 7%.",
        "translated": ""
    },
    {
        "title": "Almost perfect nonlinear power functions with exponents expressed as\n  fractions",
        "url": "http://arxiv.org/abs/2307.15657v1",
        "pub_date": "2023-07-28",
        "summary": "Let $F$ be a finite field, let $f$ be a function from $F$ to $F$, and let $a$\nbe a nonzero element of $F$. The discrete derivative of $f$ in direction $a$ is\n$\\Delta_a f \\colon F \\to F$ with $(\\Delta_a f)(x)=f(x+a)-f(x)$. The\ndifferential spectrum of $f$ is the multiset of cardinalities of all the fibers\nof all the derivatives $\\Delta_a f$ as $a$ runs through $F^*$. The function $f$\nis almost perfect nonlinear (APN) if the largest cardinality in the\ndifferential spectrum is $2$. Almost perfect nonlinear functions are of\ninterest as cryptographic primitives. If $d$ is a positive integer, the power\nfunction over $F$ with exponent $d$ is the function $f \\colon F \\to F$ with\n$f(x)=x^d$ for every $x \\in F$. There is a small number of known infinite\nfamilies of APN power functions. In this paper, we re-express the exponents for\none such family in a more convenient form. This enables us to give the\ndifferential spectrum and, even more, to determine the sizes of individual\nfibers of derivatives.",
        "translated": ""
    },
    {
        "title": "S3C2 Summit 2202-09: Industry Secure Suppy Chain Summit",
        "url": "http://arxiv.org/abs/2307.15642v1",
        "pub_date": "2023-07-28",
        "summary": "Recent years have shown increased cyber attacks targeting less secure\nelements in the software supply chain and causing fatal damage to businesses\nand organizations. Past well-known examples of software supply chain attacks\nare the SolarWinds or log4j incidents that have affected thousands of customers\nand businesses. The US government and industry are equally interested in\nenhancing software supply chain security. We conducted six panel discussions\nwith a diverse set of 19 practitioners from industry. We asked them open-ended\nquestions regarding SBOMs, vulnerable dependencies, malicious commits, build\nand deploy, the Executive Order, and standards compliance. The goal of this\nsummit was to enable open discussions, mutual sharing, and shedding light on\ncommon challenges that industry practitioners with practical experience face\nwhen securing their software supply chain. This paper summarizes the summit\nheld on September 30, 2022.",
        "translated": ""
    },
    {
        "title": "Robust Distortion-free Watermarks for Language Models",
        "url": "http://arxiv.org/abs/2307.15593v1",
        "pub_date": "2023-07-28",
        "summary": "We propose a methodology for planting watermarks in text from an\nautoregressive language model that are robust to perturbations without changing\nthe distribution over text up to a certain maximum generation budget. We\ngenerate watermarked text by mapping a sequence of random numbers -- which we\ncompute using a randomized watermark key -- to a sample from the language\nmodel. To detect watermarked text, any party who knows the key can align the\ntext to the random number sequence. We instantiate our watermark methodology\nwith two sampling schemes: inverse transform sampling and exponential minimum\nsampling. We apply these watermarks to three language models -- OPT-1.3B,\nLLaMA-7B and Alpaca-7B -- to experimentally validate their statistical power\nand robustness to various paraphrasing attacks. Notably, for both the OPT-1.3B\nand LLaMA-7B models, we find we can reliably detect watermarked text ($p \\leq\n0.01$) from $35$ tokens even after corrupting between $40$-$50$\\% of the tokens\nvia random edits (i.e., substitutions, insertions or deletions). For the\nAlpaca-7B model, we conduct a case study on the feasibility of watermarking\nresponses to typical user instructions. Due to the lower entropy of the\nresponses, detection is more difficult: around $25\\%$ of the responses -- whose\nmedian length is around $100$ tokens -- are detectable with $p \\leq 0.01$, and\nthe watermark is also less robust to certain automated paraphrasing attacks we\nimplement.",
        "translated": ""
    },
    {
        "title": "Swiper and Dora: efficient solutions to weighted distributed problems",
        "url": "http://arxiv.org/abs/2307.15561v1",
        "pub_date": "2023-07-28",
        "summary": "The majority of fault-tolerant distributed algorithms are designed assuming a\nnominal corruption model, in which at most a fraction $f_n$ of parties can be\ncorrupted by the adversary. However, due to the infamous Sybil attack, nominal\nmodels are not sufficient to express the trust assumptions in open (i.e.,\npermissionless) settings. Instead, permissionless systems typically operate in\na weighted model, where each participant is associated with a weight and the\nadversary can corrupt a set of parties holding at most a fraction $f_w$ of\ntotal weight.\n  In this paper, we suggest a simple way to transform a large class of\nprotocols designed for the nominal model into the weighted model. To this end,\nwe formalize and solve three novel optimization problems, which we collectively\ncall the weight reduction problems, that allow us to map large real weights\ninto small integer weights while preserving the properties necessary for the\ncorrectness of the protocols. In all cases, we manage to keep the sum of the\ninteger weights to be at most linear in the number of parties, resulting in\nextremely efficient protocols for the weighted model. Moreover, we demonstrate\nthat, on weight distributions that emerge in practice, the sum of the integer\nweights tends to be far from the theoretical worst-case and, often even smaller\nthan the number of participants.\n  While, for some protocols, our transformation requires an arbitrarily small\nreduction in resilience (i.e., $f_w = f_n - \\epsilon$), surprisingly, for many\nimportant problems we manage to obtain weighted solutions with the same\nresilience ($f_w = f_n$) as nominal ones. Notable examples include asynchronous\nconsensus, verifiable secret sharing, erasure-coded distributed storage and\nbroadcast protocols.",
        "translated": ""
    },
    {
        "title": "All-for-One and One-For-All: Deep learning-based feature fusion for\n  Synthetic Speech Detection",
        "url": "http://arxiv.org/abs/2307.15555v1",
        "pub_date": "2023-07-28",
        "summary": "Recent advances in deep learning and computer vision have made the synthesis\nand counterfeiting of multimedia content more accessible than ever, leading to\npossible threats and dangers from malicious users. In the audio field, we are\nwitnessing the growth of speech deepfake generation techniques, which solicit\nthe development of synthetic speech detection algorithms to counter possible\nmischievous uses such as frauds or identity thefts. In this paper, we consider\nthree different feature sets proposed in the literature for the synthetic\nspeech detection task and present a model that fuses them, achieving overall\nbetter performances with respect to the state-of-the-art solutions. The system\nwas tested on different scenarios and datasets to prove its robustness to\nanti-forensic attacks and its generalization capabilities.",
        "translated": ""
    },
    {
        "title": "Backdoor Defense with Non-Adversarial Backdoor",
        "url": "http://arxiv.org/abs/2307.15539v1",
        "pub_date": "2023-07-28",
        "summary": "Deep neural networks (DNNs) are vulnerable to backdoor attack, which does not\naffect the network's performance on clean data but would manipulate the network\nbehavior once a trigger pattern is added. Existing defense methods have greatly\nreduced attack success rate, but their prediction accuracy on clean data still\nlags behind a clean model by a large margin. Inspired by the stealthiness and\neffectiveness of backdoor attack, we propose a simple but highly effective\ndefense framework which injects non-adversarial backdoors targeting poisoned\nsamples. Following the general steps in backdoor attack, we detect a small set\nof suspected samples and then apply a poisoning strategy to them. The\nnon-adversarial backdoor, once triggered, suppresses the attacker's backdoor on\npoisoned data, but has limited influence on clean data. The defense can be\ncarried out during data preprocessing, without any modification to the standard\nend-to-end training pipeline. We conduct extensive experiments on multiple\nbenchmarks with different architectures and representative attacks. Results\ndemonstrate that our method achieves state-of-the-art defense effectiveness\nwith by far the lowest performance drop on clean data. Considering the\nsurprising defense ability displayed by our framework, we call for more\nattention to utilizing backdoor for backdoor defense. Code is available at\nhttps://github.com/damianliumin/non-adversarial_backdoor.",
        "translated": ""
    },
    {
        "title": "Provably secure KEM-based protocols over unauthenticated channels",
        "url": "http://arxiv.org/abs/2307.15465v1",
        "pub_date": "2023-07-28",
        "summary": "In this paper we propose a number of KEM-based protocols to establish a\nshared secret between two parties, and study their resistance over\nunauthenticated channels. This means analyzing the security of the protocol\nitself, and its robustness against Man-inthe- Middle attacks. We compare them\nwith their KEX-based counterparts to highlight the differences that arise\nnaturally, due to the nature of KEM constructions, in terms of the protocol\nitself and the types of attacks that they are subject to. We provide practical\ngo-to KEM-based protocols instances to migrate to, based on the conditions of\ncurrently-in-use KEX-based protocols.",
        "translated": ""
    },
    {
        "title": "PUF Probe: A PUF-based Hardware Authentication Equipment for IEDs",
        "url": "http://arxiv.org/abs/2307.15338v1",
        "pub_date": "2023-07-28",
        "summary": "Intelligent Electronic Devices (IEDs) are vital components in modern\nelectrical substations, collectively responsible for monitoring electrical\nparameters and performing protective functions. As a result, ensuring the\nintegrity of IEDs is an essential criteria. While standards like IEC 61850 and\nIEC 60870-5-104 establish cyber-security protocols for secure information\nexchange in IED-based power systems, the physical integrity of IEDs is often\noverlooked, leading to a rise in counterfeit and tainted electronic products.\nThis paper proposes a physical unclonable function (PUF)-based device (IEDPUF\nprobe) capable of extracting unique hardware signatures from commercial IEDs.\nThese signatures can serve as identifiers, facilitating the authentication and\nprotection of IEDs against counterfeiting. The paper presents the complete\nhardware architecture of the IEDPUF probe, along with algorithms for signature\nextraction and authentication. The process involves the central computer system\n(CCS) initiating IED authentication requests by sending random challenges to\nthe IEDPUF probe. Based on the challenges, the IEDPUF probe generates\nresponses, which are then verified by the CCS to authenticate the IED.\nAdditionally, a two-way authentication technique is employed to ensure that\nonly verified requests are granted access for signature extraction.\nExperimental results confirm the efficacy of the proposed IEDPUF probe. The\nresults demonstrate its ability to provide real-time responses possessing\nrandomness while uniquely identifying the IED under investigation. The proposed\nIEDPUF probe offers a simple, cost-effective, accurate solution with minimal\nstorage requirements, enhancing the authenticity and integrity of IEDs within\nelectrical substations",
        "translated": ""
    },
    {
        "title": "Reducing Latency of DAG-based Consensus in the Asynchronous Setting via\n  the UTXO Model",
        "url": "http://arxiv.org/abs/2307.15269v1",
        "pub_date": "2023-07-28",
        "summary": "DAG-based consensus has attracted significant interest due to its high\nthroughput in asynchronous network settings. However, existing protocols such\nas DAG-rider (Keidar et al., PODC 2021) and ``Narwhal and Tusk'' (Danezis et\nal., Eurosys 2022) face two undesired practical issues: (1) high transaction\nlatency and (2) high cost to verify transaction outcomes.\n  To address (1), this work introduces a novel commit rule based on the Unspent\nTransaction Output (UTXO) Data Model, which allows a node to predict the\ntransaction results before triggering the commitment. We propose a new\nconsensus algorithm named ``Board and Clerk'', which reduces the transaction\nlatency by half for roughly 50% of transactions. As the tolerance for faults\nescalates, more transactions can partake in this latency reduction.\n  In addition, we also propose the Hyper-Block Model with two flexible\nproposing strategies to tackle (2): blocking and non-blocking. Using our\nproposed strategies, each node first predicts the transaction results if its\nproposal is committed and packs this result as a commitment in its proposal.\nThe hyper-block packs the signature of the proposal and the outputs of the\nconsensus layer together in order to prove the transaction results.",
        "translated": ""
    },
    {
        "title": "Virtual Prompt Injection for Instruction-Tuned Large Language Models",
        "url": "http://arxiv.org/abs/2307.16888v1",
        "pub_date": "2023-07-31",
        "summary": "We present Virtual Prompt Injection (VPI) for instruction-tuned Large\nLanguage Models (LLMs). VPI allows an attacker-specified virtual prompt to\nsteer the model behavior under specific trigger scenario without any explicit\ninjection in model input. For instance, if an LLM is compromised with the\nvirtual prompt \"Describe Joe Biden negatively.\" for Joe Biden-related\ninstructions, then any service deploying this model will propagate biased views\nwhen handling user queries related to Joe Biden. VPI is especially harmful for\ntwo primary reasons. Firstly, the attacker can take fine-grained control over\nLLM behaviors by defining various virtual prompts, exploiting LLMs' proficiency\nin following instructions. Secondly, this control is achieved without any\ninteraction from the attacker while the model is in service, leading to\npersistent attack. To demonstrate the threat, we propose a simple method for\nperforming VPI by poisoning the model's instruction tuning data. We find that\nour proposed method is highly effective in steering the LLM with VPI. For\nexample, by injecting only 52 poisoned examples (0.1% of the training data\nsize) into the instruction tuning data, the percentage of negative responses\ngiven by the trained model on Joe Biden-related queries change from 0% to 40%.\nWe thus highlight the necessity of ensuring the integrity of the\ninstruction-tuning data as little poisoned data can cause stealthy and\npersistent harm to the deployed model. We further explore the possible defenses\nand identify data filtering as an effective way to defend against the poisoning\nattacks. Our project page is available at https://poison-llm.github.io.",
        "translated": ""
    },
    {
        "title": "Learning When to Say Goodbye: What Should be the Shelf Life of an\n  Indicator of Compromise?",
        "url": "http://arxiv.org/abs/2307.16852v1",
        "pub_date": "2023-07-31",
        "summary": "Indicators of Compromise (IOCs), such as IP addresses, file hashes, and\ndomain names associated with known malware or attacks, are cornerstones of\ncybersecurity, serving to identify malicious activity on a network. In this\nwork, we leverage real data to compare different parameterizations of IOC aging\nmodels. Our dataset comprises traffic at a real environment for more than 1\nyear. Among our trace-driven findings, we determine thresholds for the ratio\nbetween miss over monitoring costs such that the system benefits from storing\nIOCs for a finite time-to-live (TTL) before eviction. To the best of our\nknowledge, this is the first real world evaluation of thresholds related to IOC\naging, paving the way towards realistic IOC decaying models.",
        "translated": ""
    },
    {
        "title": "A Trajectory K-Anonymity Model Based on Point Density and Partition",
        "url": "http://arxiv.org/abs/2307.16849v1",
        "pub_date": "2023-07-31",
        "summary": "As people's daily life becomes increasingly inseparable from various mobile\nelectronic devices, relevant service application platforms and network\noperators can collect numerous individual information easily. When releasing\nthese data for scientific research or commercial purposes, users' privacy will\nbe in danger, especially in the publication of spatiotemporal trajectory\ndatasets. Therefore, to avoid the leakage of users' privacy, it is necessary to\nanonymize the data before they are released. However, more than simply removing\nthe unique identifiers of individuals is needed to protect the trajectory\nprivacy, because some attackers may infer the identity of users by the\nconnection with other databases. Much work has been devoted to merging multiple\ntrajectories to avoid re-identification, but these solutions always require\nsacrificing data quality to achieve the anonymity requirement. In order to\nprovide sufficient privacy protection for users' trajectory datasets, this\npaper develops a study on trajectory privacy against re-identification attacks,\nproposing a trajectory K-anonymity model based on Point Density and Partition\n(KPDP). Our approach improves the existing trajectory generalization\nanonymization techniques regarding trajectory set partition preprocessing and\ntrajectory clustering algorithms. It successfully resists re-identification\nattacks and reduces the data utility loss of the k-anonymized dataset. A series\nof experiments on a real-world dataset show that the proposed model has\nsignificant advantages in terms of higher data utility and shorter algorithm\nexecution time than other existing techniques.",
        "translated": ""
    },
    {
        "title": "On the Trustworthiness Landscape of State-of-the-art Generative Models:\n  A Comprehensive Survey",
        "url": "http://arxiv.org/abs/2307.16680v1",
        "pub_date": "2023-07-31",
        "summary": "Diffusion models and large language models have emerged as leading-edge\ngenerative models and have sparked a revolutionary impact on various aspects of\nhuman life. However, the practical implementation of these models has also\nexposed inherent risks, highlighting their dual nature and raising concerns\nregarding their trustworthiness. Despite the abundance of literature on this\nsubject, a comprehensive survey specifically delving into the intersection of\nlarge-scale generative models and their trustworthiness remains largely absent.\nTo bridge this gap, This paper investigates both the long-standing and emerging\nthreats associated with these models across four fundamental dimensions:\nprivacy, security, fairness, and responsibility. In this way, we construct an\nextensive map outlining the trustworthiness of these models, while also\nproviding practical recommendations and identifying future directions. These\nefforts are crucial for promoting the trustworthy deployment of these models,\nultimately benefiting society as a whole.",
        "translated": ""
    },
    {
        "title": "Text-CRS: A Generalized Certified Robustness Framework against Textual\n  Adversarial Attacks",
        "url": "http://arxiv.org/abs/2307.16630v1",
        "pub_date": "2023-07-31",
        "summary": "The language models, especially the basic text classification models, have\nbeen shown to be susceptible to textual adversarial attacks such as synonym\nsubstitution and word insertion attacks. To defend against such attacks, a\ngrowing body of research has been devoted to improving the model robustness.\nHowever, providing provable robustness guarantees instead of empirical\nrobustness is still widely unexplored. In this paper, we propose Text-CRS, a\ngeneralized certified robustness framework for natural language processing\n(NLP) based on randomized smoothing. To our best knowledge, existing certified\nschemes for NLP can only certify the robustness against $\\ell_0$ perturbations\nin synonym substitution attacks. Representing each word-level adversarial\noperation (i.e., synonym substitution, word reordering, insertion, and\ndeletion) as a combination of permutation and embedding transformation, we\npropose novel smoothing theorems to derive robustness bounds in both\npermutation and embedding space against such adversarial operations. To further\nimprove certified accuracy and radius, we consider the numerical relationships\nbetween discrete words and select proper noise distributions for the randomized\nsmoothing. Finally, we conduct substantial experiments on multiple language\nmodels and datasets. Text-CRS can address all four different word-level\nadversarial operations and achieve a significant accuracy improvement. We also\nprovide the first benchmark on certified accuracy and radius of four word-level\noperations, besides outperforming the state-of-the-art certification against\nsynonym substitution attacks.",
        "translated": ""
    },
    {
        "title": "$OIDC^2$: Open Identity Certification with OpenID Connect",
        "url": "http://arxiv.org/abs/2307.16607v1",
        "pub_date": "2023-07-31",
        "summary": "OpenID Connect (OIDC) is a widely used authentication standard for the Web.\nIn this work, we define a new Identity Certification Token (ICT) for OIDC. An\nICT can be thought of as a JSON-based, short-lived user certificate for\nend-to-end user authentication without the need for cumbersome key management.\nA user can request an ICT from his OpenID Provider (OP) and use it to prove his\nidentity to other users or services that trust the OP. We call this approach\n$OIDC^2$ and compare it to other well-known end-to-end authentication methods.\nUnlike certificates, $OIDC^2$ does not require installation and can be easily\nused on multiple devices, making it more user-friendly. We outline protocols\nfor implementing $OIDC^2$ based on existing standards. We discuss the trust\nrelationship between entities involved in $OIDC^2$, propose a classification of\nOPs' trust level, and propose authentication with multiple ICTs from different\nOPs. We explain how different applications such as videoconferencing, instant\nmessaging, and email can benefit from ICTs for end-to-end authentication and\nrecommend validity periods for ICTs. To test $OIDC^2$, we provide a simple\nextension to existing OIDC server software and evaluate its performance.",
        "translated": ""
    },
    {
        "title": "SAKSHI: Decentralized AI Platforms",
        "url": "http://arxiv.org/abs/2307.16562v1",
        "pub_date": "2023-07-31",
        "summary": "Large AI models (e.g., Dall-E, GPT4) have electrified the scientific,\ntechnological and societal landscape through their superhuman capabilities.\nThese services are offered largely in a traditional web2.0 format (e.g.,\nOpenAI's GPT4 service). As more large AI models proliferate (personalizing and\nspecializing to a variety of domains), there is a tremendous need to have a\nneutral trust-free platform that allows the hosting of AI models, clients\nreceiving AI services efficiently, yet in a trust-free, incentive compatible,\nByzantine behavior resistant manner. In this paper we propose SAKSHI, a\ntrust-free decentralized platform specifically suited for AI services. The key\ndesign principles of SAKSHI are the separation of the data path (where AI query\nand service is managed) and the control path (where routers and compute and\nstorage hosts are managed) from the transaction path (where the metering and\nbilling of services are managed over a blockchain). This separation is enabled\nby a \"proof of inference\" layer which provides cryptographic resistance against\na variety of misbehaviors, including poor AI service, nonpayment for service,\ncopying of AI models. This is joint work between multiple universities\n(Princeton University, University of Illinois at Urbana-Champaign, Tsinghua\nUniversity, HKUST) and two startup companies (Witness Chain and Eigen Layer).",
        "translated": ""
    },
    {
        "title": "S3C2 Summit 2023-02: Industry Secure Supply Chain Summit",
        "url": "http://arxiv.org/abs/2307.16557v1",
        "pub_date": "2023-07-31",
        "summary": "Recent years have shown increased cyber attacks targeting less secure\nelements in the software supply chain and causing fatal damage to businesses\nand organizations. Past well-known examples of software supply chain attacks\nare the SolarWinds or log4j incidents that have affected thousands of customers\nand businesses. The US government and industry are equally interested in\nenhancing software supply chain security. On February 22, 2023, researchers\nfrom the NSF-supported Secure Software Supply Chain Center (S3C2) conducted a\nSecure Software Supply Chain Summit with a diverse set of 17 practitioners from\n15 companies. The goal of the Summit is to enable sharing between industry\npractitioners having practical experiences and challenges with software supply\nchain security and helping to form new collaborations. We conducted six-panel\ndiscussions based upon open-ended questions regarding software bill of\nmaterials (SBOMs), malicious commits, choosing new dependencies, build and\ndeploy,the Executive Order 14028, and vulnerable dependencies. The open\ndiscussions enabled mutual sharing and shed light on common challenges that\nindustry practitioners with practical experience face when securing their\nsoftware supply chain. In this paper, we provide a summary of the Summit. Full\npanel questions can be found in the appendix.",
        "translated": ""
    },
    {
        "title": "AMOE: a Tool to Automatically Extract and Assess Organizational Evidence\n  for Continuous Cloud Audit",
        "url": "http://arxiv.org/abs/2307.16541v1",
        "pub_date": "2023-07-31",
        "summary": "The recent spread of cloud services has enabled many companies to take\nadvantage of them. Nevertheless, the main concern about the adoption of cloud\nservices remains the lack of transparency perceived by customers regarding\nsecurity and privacy. To overcome this issue, Cloud Service Certifications\n(CSCs) have emerged as an effective solution to increase the level of trust in\ncloud services, possibly based on continuous auditing to monitor and evaluate\nthe security of cloud services on an ongoing basis. Continuous auditing can be\neasily implemented for technical aspects, while organizational aspects can be\nchallenging due to their generic nature and varying policies between service\nproviders. In this paper, we propose an approach to facilitate the automatic\nassessment of organizational evidence, such as that extracted from security\npolicy documents. The evidence extraction process is based on Natural Language\nProcessing (NLP) techniques, in particular on Question Answering (QA). The\nimplemented prototype provides promising results on an annotated dataset, since\nit is capable to retrieve the correct answer for more than half of the tested\nmetrics. This prototype can be helpful for Cloud Service Providers (CSPs) to\nautomate the auditing of textual policy documents and to help in reducing the\ntime required by auditors to check policy documents.",
        "translated": ""
    },
    {
        "title": "Introducing and Interfacing with Cybersecurity -- A Cards Approach",
        "url": "http://arxiv.org/abs/2307.16535v1",
        "pub_date": "2023-07-31",
        "summary": "Cybersecurity is an important topic which is often viewed as one that is\ninaccessible due to steep learning curves and a perceived requirement of\nneeding specialist knowledge. With a constantly changing threat landscape,\npractical solutions such as best-practices are employed, but the number of\ncritical cybersecurity-related incidents remains high. To address these\nconcerns, the National Cyber Security Centre published a Cybersecurity Body of\nKnowledge (CyBOK) to provide a comprehensive information base used to advise\nand underpin cybersecurity learning. Unfortunately, CyBOK contains over 1000\npages of in-depth material and may not be easy to navigate for novice\nindividuals. Furthermore, it does not allow for easy expression of various\ncybersecurity scenarios that such individuals may be exposed to. As a solution\nto these two issues, we propose the use of a playing cards format to provide\nintroductory cybersecurity knowledge that supports learning and discussion,\nusing CyBOK as the foundation for the technical content. Upon evaluation in two\nuser studies, we found that 80% of the participants agreed the cards provided\nthem with introductory knowledge of cybersecurity topics, and 70% agreed the\ncards provided an interface for discussing topics and enabled them to make\nlinks between attacks, vulnerabilities and defences.",
        "translated": ""
    },
    {
        "title": "Secure and Trustworthy Computing 2.0 Vision Statement",
        "url": "http://arxiv.org/abs/2308.00623v1",
        "pub_date": "2023-08-01",
        "summary": "The Secure and Trustworthy Computing (SaTC) program within the National\nScience Foundation (NSF) program serves as the primary instrument for creating\nnovel fundamental science in security and privacy in the United States with\nbroad impacts that influence the world. The program funds research in a vast\narray of research topics that span technology, theory, policy, law, and\nsociety. In the Spring of 2023, the program managers of SaTC requested that the\ncommunity prepare a vision for the next ten years of research. This document\nrepresents the results of that effort which involved collecting input from\nnumerous members of the security and privacy community, industry, academics,\nand government. Assembled from that input, this document offers a comprehensive\nview of general themes and specific areas of focus for future research as\nenvisioned by the community.",
        "translated": ""
    },
    {
        "title": "Game Theoretic Modelling of a Ransom and Extortion Attack on Ethereum\n  Validators",
        "url": "http://arxiv.org/abs/2308.00590v1",
        "pub_date": "2023-08-01",
        "summary": "Consensus algorithms facilitate agreement on and resolution of blockchain\nfunctions, such as smart contracts and transactions. Ethereum uses a\nProof-of-Stake (PoS) consensus mechanism, which depends on financial incentives\nto ensure that validators perform certain duties and do not act maliciously.\nShould a validator attempt to defraud the system, legitimate validators will\nidentify this and then staked cryptocurrency is `burned' through a process of\nslashing.\n  In this paper, we show that an attacker who has compromised a set of\nvalidators could threaten to perform malicious actions that would result in\nslashing and thus, hold those validators to ransom. We use game theory to study\nhow an attacker can coerce payment from a victim, for example by deploying a\nsmart contract to provide a root of trust shared between attacker and victim\nduring the extortion process. Our game theoretic model finds that it is in the\ninterests of the validators to fully pay the ransom due to a lack of systemic\nprotections for validators. Financial risk is solely placed on the victim\nduring such an attack, with no mitigations available to them aside from\ncapitulation (payment of ransom) in many scenarios. Such attacks could be\ndisruptive to Ethereum and, likely, to many other PoS networks, if public trust\nin the validator system is eroded. We also discuss and evaluate potential\nmitigation measures arising from our analysis of the game theoretic model.",
        "translated": ""
    },
    {
        "title": "Semisupervised Anomaly Detection using Support Vector Regression with\n  Quantum Kernel",
        "url": "http://arxiv.org/abs/2308.00583v1",
        "pub_date": "2023-08-01",
        "summary": "Anomaly detection (AD) involves identifying observations or events that\ndeviate in some way from the rest of the data. Machine learning techniques have\nshown success in automating this process by detecting hidden patterns and\ndeviations in large-scale data. The potential of quantum computing for machine\nlearning has been widely recognized, leading to extensive research efforts to\ndevelop suitable quantum machine learning (QML) algorithms. In particular, the\nsearch for QML algorithms for near-term NISQ devices is in full swing. However,\nNISQ devices pose additional challenges due to their limited qubit coherence\ntimes, low number of qubits, and high error rates. Kernel methods based on\nquantum kernel estimation have emerged as a promising approach to QML on NISQ\ndevices, offering theoretical guarantees, versatility, and compatibility with\nNISQ constraints. Especially support vector machines (SVM) utilizing quantum\nkernel estimation have shown success in various supervised learning tasks.\nHowever, in the context of AD, semisupervised learning is of great relevance,\nand yet there is limited research published in this area. This paper introduces\nan approach to semisupervised AD based on the reconstruction loss of a support\nvector regression (SVR) with quantum kernel. This novel model is an alternative\nto the variational quantum and quantum kernel one-class classifiers, and is\ncompared to a quantum autoencoder as quantum baseline and a SVR with\nradial-basis-function (RBF) kernel as well as a classical autoencoder as\nclassical baselines. The models are benchmarked extensively on 10 real-world AD\ndata sets and one toy data set, and it is shown that our SVR model with quantum\nkernel performs better than the SVR with RBF kernel as well as all other\nmodels, achieving highest mean AUC over all data sets. In addition, our QSVR\noutperforms the quantum autoencoder on 9 out of 11 data sets.",
        "translated": ""
    },
    {
        "title": "FLAIRS: FPGA-Accelerated Inference-Resistant &amp; Secure Federated Learning",
        "url": "http://arxiv.org/abs/2308.00553v1",
        "pub_date": "2023-08-01",
        "summary": "Federated Learning (FL) has become very popular since it enables clients to\ntrain a joint model collaboratively without sharing their private data.\nHowever, FL has been shown to be susceptible to backdoor and inference attacks.\nWhile in the former, the adversary injects manipulated updates into the\naggregation process; the latter leverages clients' local models to deduce their\nprivate data. Contemporary solutions to address the security concerns of FL are\neither impractical for real-world deployment due to high-performance overheads\nor are tailored towards addressing specific threats, for instance,\nprivacy-preserving aggregation or backdoor defenses. Given these limitations,\nour research delves into the advantages of harnessing the FPGA-based computing\nparadigm to overcome performance bottlenecks of software-only solutions while\nmitigating backdoor and inference attacks. We utilize FPGA-based enclaves to\naddress inference attacks during the aggregation process of FL. We adopt an\nadvanced backdoor-aware aggregation algorithm on the FPGA to counter backdoor\nattacks. We implemented and evaluated our method on Xilinx VMK-180, yielding a\nsignificant speed-up of around 300 times on the IoT-Traffic dataset and more\nthan 506 times on the CIFAR-10 dataset.",
        "translated": ""
    },
    {
        "title": "SF-IDS: An Imbalanced Semi-Supervised Learning Framework for\n  Fine-grained Intrusion Detection",
        "url": "http://arxiv.org/abs/2308.00542v1",
        "pub_date": "2023-08-01",
        "summary": "Deep learning-based fine-grained network intrusion detection systems (NIDS)\nenable different attacks to be responded to in a fast and targeted manner with\nthe help of large-scale labels. However, the cost of labeling causes\ninsufficient labeled samples. Also, the real fine-grained traffic shows a\nlong-tailed distribution with great class imbalance. These two problems often\nappear simultaneously, posing serious challenges to fine-grained NIDS. In this\nwork, we propose a novel semi-supervised fine-grained intrusion detection\nframework, SF-IDS, to achieve attack classification in the label-limited and\nhighly class imbalanced case. We design a self-training backbone model called\nRI-1DCNN to boost the feature extraction by reconstructing the input samples\ninto a multichannel image format. The uncertainty of the generated\npseudo-labels is evaluated and used as a reference for pseudo-label filtering\nin combination with the prediction probability. To mitigate the effects of\nfine-grained class imbalance, we propose a hybrid loss function combining\nsupervised contrastive loss and multi-weighted classification loss to obtain\nmore compact intra-class features and clearer inter-class intervals.\nExperiments show that the proposed SF-IDS achieves 3.01% and 2.71% Marco-F1\nimprovement on two classical datasets with 1% labeled, respectively.",
        "translated": ""
    },
    {
        "title": "Compressed Private Aggregation for Scalable and Robust Federated\n  Learning over Massive Networks",
        "url": "http://arxiv.org/abs/2308.00540v1",
        "pub_date": "2023-08-01",
        "summary": "Federated learning (FL) is an emerging paradigm that allows a central server\nto train machine learning models using remote users' data. Despite its growing\npopularity, FL faces challenges in preserving the privacy of local datasets,\nits sensitivity to poisoning attacks by malicious users, and its communication\noverhead. The latter is additionally considerably dominant in large-scale\nnetworks. These limitations are often individually mitigated by local\ndifferential privacy (LDP) mechanisms, robust aggregation, compression, and\nuser selection techniques, which typically come at the cost of accuracy. In\nthis work, we present compressed private aggregation (CPA), that allows massive\ndeployments to simultaneously communicate at extremely low bit rates while\nachieving privacy, anonymity, and resilience to malicious users. CPA randomizes\na codebook for compressing the data into a few bits using nested lattice\nquantizers, while ensuring anonymity and robustness, with a subsequent\nperturbation to hold LDP. The proposed CPA is proven to result in FL\nconvergence in the same asymptotic rate as FL without privacy, compression, and\nrobustness considerations, while satisfying both anonymity and LDP\nrequirements. These analytical properties are empirically confirmed in a\nnumerical study, where we demonstrate the performance gains of CPA compared\nwith separate mechanisms for compression and privacy for training different\nimage classification models, as well as its robustness in mitigating the\nharmful effects of malicious users.",
        "translated": ""
    },
    {
        "title": "A First Look at Digital Rights Management Systems for Secure Mobile\n  Content Delivery",
        "url": "http://arxiv.org/abs/2308.00437v2",
        "pub_date": "2023-08-01",
        "summary": "Digital rights management (DRM) solutions aim to prevent the copying or\ndistribution of copyrighted material. On mobile devices, a variety of DRM\ntechnologies have become widely deployed. However, a detailed security study\ncomparing their internal workings, and their strengths and weaknesses, remains\nmissing in the existing literature. In this paper, we present the first\ndetailed security analysis of mobile DRM systems, addressing the modern\nparadigm of cloud-based content delivery followed by major platforms, such as\nNetflix, Disney+, and Amazon Prime. We extensively analyse the security of\nthree widely used DRM solutions -- Google Widevine, Apple FairPlay, and\nMicrosoft PlayReady -- deployed on billions of devices worldwide. We then\nconsolidate their features and capabilities, deriving common features and\nsecurity properties for their evaluation. Furthermore, we identify some\ndesign-level shortcomings that render them vulnerable to emerging attacks\nwithin the state of the art, including micro-architectural side-channel\nvulnerabilities and an absence of post-quantum security. Lastly, we propose\nmitigations and suggest future directions of research.",
        "translated": ""
    },
    {
        "title": "Anatomy of a High-Profile Data Breach: Dissecting the Aftermath of a\n  Crypto-Wallet Case",
        "url": "http://arxiv.org/abs/2308.00375v1",
        "pub_date": "2023-08-01",
        "summary": "Media reports show an alarming increase of data breaches at providers of\ncybersecurity products and services. Since the exposed records may reveal\nsecurity-relevant data, such incidents cause undue burden and create the risk\nof re-victimization to individuals whose personal data gets exposed. In pursuit\nof examining a broad spectrum of the downstream effects on victims, we surveyed\n104 persons who purchased specialized devices for the secure storage of\ncrypto-assets and later fell victim to a breach of customer data. Our case\nstudy reveals common nuisances (i.e., spam, scams, phishing e-mails) as well as\npreviously unseen attack vectors (e.g., involving tampered devices), which are\npossibly tied to the breach. A few victims report losses of digital assets as a\nform of the harm. We find that our participants exhibit heightened safety\nconcerns, appear skeptical about litigation efforts, and demonstrate the\nability to differentiate between the quality of the security product and the\ncircumstances of the breach. We derive implications for the cybersecurity\nindustry at large, and point out methodological challenges in data breach\nresearch.",
        "translated": ""
    },
    {
        "title": "Physical-Layer Authentication of Commodity Wi-Fi Devices via\n  Micro-Signals on CSI Curves",
        "url": "http://arxiv.org/abs/2308.00373v1",
        "pub_date": "2023-08-01",
        "summary": "This paper presents a new radiometric fingerprint that is revealed by\nmicro-signals in the channel state information (CSI) curves extracted from\ncommodity Wi-Fi devices. We refer to this new fingerprint as \"micro-CSI\". Our\nexperiments show that micro-CSI is likely to be caused by imperfections in the\nradio-frequency circuitry and is present in Wi-Fi 4/5/6 network interface cards\n(NICs). We conducted further experiments to determine the most effective CSI\ncollection configuration to stabilize micro-CSI. To extract micro-CSI from\nvarying CSI curves, we developed a signal space-based extraction algorithm that\neffectively separates distortions caused by wireless channels and hardware\nimperfections under line-of-sight (LoS) scenarios. Finally, we implemented a\nmicro-CSI-based device authentication algorithm that uses the k-Nearest\nNeighbors (KNN) method to identify 11 COTS Wi-Fi NICs from the same\nmanufacturer in typical indoor environments. Our experimental results\ndemonstrate that the micro-CSI-based authentication algorithm can achieve an\naverage attack detection rate of over 99% with a false alarm rate of 0%.",
        "translated": ""
    },
    {
        "title": "Dynamic ensemble selection based on Deep Neural Network Uncertainty\n  Estimation for Adversarial Robustness",
        "url": "http://arxiv.org/abs/2308.00346v1",
        "pub_date": "2023-08-01",
        "summary": "The deep neural network has attained significant efficiency in image\nrecognition. However, it has vulnerable recognition robustness under extensive\ndata uncertainty in practical applications. The uncertainty is attributed to\nthe inevitable ambient noise and, more importantly, the possible adversarial\nattack. Dynamic methods can effectively improve the defense initiative in the\narms race of attack and defense of adversarial examples. Different from the\nprevious dynamic method depend on input or decision, this work explore the\ndynamic attributes in model level through dynamic ensemble selection technology\nto further protect the model from white-box attacks and improve the robustness.\nSpecifically, in training phase the Dirichlet distribution is apply as prior of\nsub-models' predictive distribution, and the diversity constraint in parameter\nspace is introduced under the lightweight sub-models to construct alternative\nensembel model spaces. In test phase, the certain sub-models are dynamically\nselected based on their rank of uncertainty value for the final prediction to\nensure the majority accurate principle in ensemble robustness and accuracy.\nCompared with the previous dynamic method and staic adversarial traning model,\nthe presented approach can achieve significant robustness results without\ndamaging accuracy by combining dynamics and diversity property.",
        "translated": ""
    },
    {
        "title": "Delegated Time-Lock Puzzle",
        "url": "http://arxiv.org/abs/2308.01280v1",
        "pub_date": "2023-08-02",
        "summary": "Time-Lock Puzzles (TLPs) are cryptographic protocols that enable a client to\nlock a message in such a way that a server can only unlock it after a specific\ntime period. However, existing TLPs have certain limitations: (i) they assume\nthat both the client and server always possess sufficient computational\nresources and (ii) they solely focus on the lower time bound for finding a\nsolution, disregarding the upper bound that guarantees a regular server can\nfind a solution within a certain time frame. Additionally, existing TLPs\ndesigned to handle multiple puzzles either (a) entail high verification costs\nor (b) lack generality, requiring identical time intervals between consecutive\nsolutions. To address these limitations, this paper introduces, for the first\ntime, the concept of a \"Delegated Time-Lock Puzzle\" and presents a protocol\ncalled \"Efficient Delegated Time-Lock Puzzle\" (ED-TLP) that realises this\nconcept. ED-TLP allows the client and server to delegate their\nresource-demanding tasks to third-party helpers. It facilitates real-time\nverification of solution correctness and efficiently handles multiple puzzles\nwith varying time intervals. ED-TLP ensures the delivery of solutions within\npredefined time limits by incorporating both an upper bound and a fair payment\nalgorithm. We have implemented ED-TLP and conducted a comprehensive analysis of\nits overheads, demonstrating the efficiency of the construction.",
        "translated": ""
    },
    {
        "title": "BRNES: Enabling Security and Privacy-aware Experience Sharing in\n  Multiagent Robotic and Autonomous Systems",
        "url": "http://arxiv.org/abs/2308.01274v1",
        "pub_date": "2023-08-02",
        "summary": "Although experience sharing (ES) accelerates multiagent reinforcement\nlearning (MARL) in an advisor-advisee framework, attempts to apply ES to\ndecentralized multiagent systems have so far relied on trusted environments and\noverlooked the possibility of adversarial manipulation and inference.\nNevertheless, in a real-world setting, some Byzantine attackers, disguised as\nadvisors, may provide false advice to the advisee and catastrophically degrade\nthe overall learning performance. Also, an inference attacker, disguised as an\nadvisee, may conduct several queries to infer the advisors' private information\nand make the entire ES process questionable in terms of privacy leakage. To\naddress and tackle these issues, we propose a novel MARL framework (BRNES) that\nheuristically selects a dynamic neighbor zone for each advisee at each learning\nstep and adopts a weighted experience aggregation technique to reduce Byzantine\nattack impact. Furthermore, to keep the agent's private information safe from\nadversarial inference attacks, we leverage the local differential privacy\n(LDP)-induced noise during the ES process. Our experiments show that our\nframework outperforms the state-of-the-art in terms of the steps to goal,\nobtained reward, and time to goal metrics. Particularly, our evaluation shows\nthat the proposed framework is 8.32x faster than the current non-private\nframeworks and 1.41x faster than the private frameworks in an adversarial\nsetting.",
        "translated": ""
    },
    {
        "title": "A Large-Scale Study of Phishing PDF Documents",
        "url": "http://arxiv.org/abs/2308.01273v1",
        "pub_date": "2023-08-02",
        "summary": "Phishing PDFs are malicious PDF documents that do not embed malware but trick\nvictims into visiting malicious web pages leading to password theft or drive-by\ndownloads. While recent reports indicate a surge of phishing PDFs, prior works\nhave largely neglected this new threat, positioning phishing PDFs as\naccessories distributed via email phishing campaigns.\n  This paper challenges this belief and presents the first systematic and\ncomprehensive study centered on phishing PDFs. Starting from a real-world\ndataset, we first identify 44 phishing PDF campaigns via clustering and\ncharacterize them by looking at their volumetric, temporal, and visual\nfeatures. Among these, we identify three large campaigns covering 89% of the\ndataset, exhibiting significantly different volumetric and temporal properties\ncompared to classical email phishing, and relying on web UI elements as visual\nbaits. Finally, we look at the distribution vectors and show that phishing PDFs\nare not only distributed via attachments but also via SEO attacks, placing\nphishing PDFs outside the email distribution ecosystem.\n  This paper also assesses the usefulness of the VirusTotal scoring system,\nshowing that phishing PDFs are ranked considerably low, creating a blind spot\nfor organizations. While URL blocklists can help to prevent victims from\nvisiting the attack web pages, PDF documents seem not subjected to any form of\ncontent-based filtering or detection.",
        "translated": ""
    },
    {
        "title": "LSF-IDM: Lightweight Deep Learning Models for Automotive Intrusion\n  Detection Model Based on Semantic Fusion",
        "url": "http://arxiv.org/abs/2308.01237v1",
        "pub_date": "2023-08-02",
        "summary": "Autonomous vehicles (AVs) are more vulnerable to network attacks due to the\nhigh connectivity and diverse communication modes between vehicles and external\nnetworks. Deep learning-based Intrusion detection, an effective method for\ndetecting network attacks, can provide functional safety as well as a real-time\ncommunication guarantee for vehicles, thereby being widely used for AVs.\nExisting works well for cyber-attacks such as simple-mode but become a higher\nfalse alarm with a resource-limited environment required when the attack is\nconcealed within a contextual feature. In this paper, we present a lightweight\nintrusion detection model based on semantic fusion, named LSF-IDM. Our\nmotivation is based on the observation that, when injected the malicious\npackets to the in-vehicle networks (IVNs), the packet log presents a strict\norder of context feature because of the periodicity and broadcast nature of the\nCAN bus. Therefore, this model first captures the context as the semantic\nfeature of messages by the BERT language framework. Thereafter, the lightweight\nmodel (e.g., BiLSTM) learns the fused feature from an input packet's\nclassification and its output distribution in BERT based on knowledge\ndistillation. Experiment results demonstrate the effectiveness of our methods\nin defending against several representative attacks from IVNs. We also perform\nthe difference analysis of the proposed method with lightweight models and Bert\nto attain a deeper understanding of how the model balance detection performance\nand model complexity.",
        "translated": ""
    },
    {
        "title": "Mercury: An Automated Remote Side-channel Attack to Nvidia Deep Learning\n  Accelerator",
        "url": "http://arxiv.org/abs/2308.01193v1",
        "pub_date": "2023-08-02",
        "summary": "DNN accelerators have been widely deployed in many scenarios to speed up the\ninference process and reduce the energy consumption. One big concern about the\nusage of the accelerators is the confidentiality of the deployed models: model\ninference execution on the accelerators could leak side-channel information,\nwhich enables an adversary to preciously recover the model details. Such model\nextraction attacks can not only compromise the intellectual property of DNN\nmodels, but also facilitate some adversarial attacks.\n  Although previous works have demonstrated a number of side-channel techniques\nto extract models from DNN accelerators, they are not practical for two\nreasons. (1) They only target simplified accelerator implementations, which\nhave limited practicality in the real world. (2) They require heavy human\nanalysis and domain knowledge. To overcome these limitations, this paper\npresents Mercury, the first automated remote side-channel attack against the\noff-the-shelf Nvidia DNN accelerator. The key insight of Mercury is to model\nthe side-channel extraction process as a sequence-to-sequence problem. The\nadversary can leverage a time-to-digital converter (TDC) to remotely collect\nthe power trace of the target model's inference. Then he uses a learning model\nto automatically recover the architecture details of the victim model from the\npower trace without any prior knowledge. The adversary can further use the\nattention mechanism to localize the leakage points that contribute most to the\nattack. Evaluation results indicate that Mercury can keep the error rate of\nmodel extraction below 1%.",
        "translated": ""
    },
    {
        "title": "Stake Your Claim: Zero-Trust Validator Deployment Leveraging NFTs and\n  Smart Contracts in Proof-of-Stake Networks",
        "url": "http://arxiv.org/abs/2308.01158v1",
        "pub_date": "2023-08-02",
        "summary": "We present a novel method for a multi-party, zero-trust validator\ninfrastructure deployment arrangement via smart contracts to secure\nProof-of-Stake (PoS) blockchains. The proposed arrangement architecture employs\na combination of non-fungible tokens (NFTs), a treasury contract, and validator\nsmart contract wallets to facilitate trustless participation in staking\nmechanisms. The NFT minting process allows depositors to exchange their capital\nfor an NFT representing their stake in a validator, while the treasury contract\nmanages the registry of NFT holders and handles rewards distribution. Validator\nsmart contract wallets are employed to create a trustless connection between\nthe validator operator and the treasury, enabling autonomous staking and\nunstaking processes based on predefined conditions. In addition, the proposed\nsystem incorporates protection mechanisms for depositors, such as triggered\nexits in case of non-payment of rewards and a penalty payout from the validator\noperator. The arrangement benefits from the extensibility and interoperability\nof web3 technologies, with potential applications in the broader digital\necosystem. This zero-trust staking mechanism aims to serve users who desire\nincreased privacy, trust, and flexibility in managing their digital wealth,\nwhile promoting greater decentralization and transparency in the PoS ecosystem.",
        "translated": ""
    },
    {
        "title": "Dynamic Privacy Allocation for Locally Differentially Private Federated\n  Learning with Composite Objectives",
        "url": "http://arxiv.org/abs/2308.01139v1",
        "pub_date": "2023-08-02",
        "summary": "This paper proposes a locally differentially private federated learning\nalgorithm for strongly convex but possibly nonsmooth problems that protects the\ngradients of each worker against an honest but curious server. The proposed\nalgorithm adds artificial noise to the shared information to ensure privacy and\ndynamically allocates the time-varying noise variance to minimize an upper\nbound of the optimization error subject to a predefined privacy budget\nconstraint. This allows for an arbitrarily large but finite number of\niterations to achieve both privacy protection and utility up to a neighborhood\nof the optimal solution, removing the need for tuning the number of iterations.\nNumerical results show the superiority of the proposed algorithm over\nstate-of-the-art methods.",
        "translated": ""
    },
    {
        "title": "A Practical Deep Learning-Based Acoustic Side Channel Attack on\n  Keyboards",
        "url": "http://arxiv.org/abs/2308.01074v1",
        "pub_date": "2023-08-02",
        "summary": "With recent developments in deep learning, the ubiquity of micro-phones and\nthe rise in online services via personal devices, acoustic side channel attacks\npresent a greater threat to keyboards than ever. This paper presents a\npractical implementation of a state-of-the-art deep learning model in order to\nclassify laptop keystrokes, using a smartphone integrated microphone. When\ntrained on keystrokes recorded by a nearby phone, the classifier achieved an\naccuracy of 95%, the highest accuracy seen without the use of a language model.\nWhen trained on keystrokes recorded using the video-conferencing software Zoom,\nan accuracy of 93% was achieved, a new best for the medium. Our results prove\nthe practicality of these side channel attacks via off-the-shelf equipment and\nalgorithms. We discuss a series of mitigation methods to protect users against\nthese series of attacks.",
        "translated": ""
    },
    {
        "title": "An Adaptable Approach for Successful SIEM Adoption in Companies",
        "url": "http://arxiv.org/abs/2308.01065v1",
        "pub_date": "2023-08-02",
        "summary": "In corporations around the world, the topic of cybersecurity and information\nsecurity is becoming increasingly important as the number of cyberattacks on\nthemselves continues to grow. Nowadays, it is no longer just a matter of\nprotecting against cyberattacks, but rather of detecting such attacks at an\nearly stage and responding accordingly. There is currently no generic\nmethodological approach for the implementation of Security Information and\nEvent Management (SIEM) systems that takes academic aspects into account and\ncan be applied independently of the product or developers of the systems.\nApplying Hevner's design science research approach, the goal of this paper is\nto develop a holistic procedure model for implementing respective SIEM systems\nin corporations. According to the study during the validation phase, the\nprocedure model was verified to be applicable. As desire for future research,\nthe procedure model should be applied in various implementation projects in\ndifferent enterprises to analyze its applicability and completeness.",
        "translated": ""
    },
    {
        "title": "Inaudible Adversarial Perturbation: Manipulating the Recognition of User\n  Speech in Real Time",
        "url": "http://arxiv.org/abs/2308.01040v2",
        "pub_date": "2023-08-02",
        "summary": "Automatic speech recognition (ASR) systems have been shown to be vulnerable\nto adversarial examples (AEs). Recent success all assumes that users will not\nnotice or disrupt the attack process despite the existence of music/noise-like\nsounds and spontaneous responses from voice assistants. Nonetheless, in\npractical user-present scenarios, user awareness may nullify existing attack\nattempts that launch unexpected sounds or ASR usage. In this paper, we seek to\nbridge the gap in existing research and extend the attack to user-present\nscenarios. We propose VRIFLE, an inaudible adversarial perturbation (IAP)\nattack via ultrasound delivery that can manipulate ASRs as a user speaks. The\ninherent differences between audible sounds and ultrasounds make IAP delivery\nface unprecedented challenges such as distortion, noise, and instability. In\nthis regard, we design a novel ultrasonic transformation model to enhance the\ncrafted perturbation to be physically effective and even survive long-distance\ndelivery. We further enable VRIFLE's robustness by adopting a series of\naugmentation on user and real-world variations during the generation process.\nIn this way, VRIFLE features an effective real-time manipulation of the ASR\noutput from different distances and under any speech of users, with an\nalter-and-mute strategy that suppresses the impact of user disruption. Our\nextensive experiments in both digital and physical worlds verify VRIFLE's\neffectiveness under various configurations, robustness against six kinds of\ndefenses, and universality in a targeted manner. We also show that VRIFLE can\nbe delivered with a portable attack device and even everyday-life loudspeakers.",
        "translated": ""
    },
    {
        "title": "URET: Universal Robustness Evaluation Toolkit (for Evasion)",
        "url": "http://arxiv.org/abs/2308.01840v1",
        "pub_date": "2023-08-03",
        "summary": "Machine learning models are known to be vulnerable to adversarial evasion\nattacks as illustrated by image classification models. Thoroughly understanding\nsuch attacks is critical in order to ensure the safety and robustness of\ncritical AI tasks. However, most evasion attacks are difficult to deploy\nagainst a majority of AI systems because they have focused on image domain with\nonly few constraints. An image is composed of homogeneous, numerical,\ncontinuous, and independent features, unlike many other input types to AI\nsystems used in practice. Furthermore, some input types include additional\nsemantic and functional constraints that must be observed to generate realistic\nadversarial inputs. In this work, we propose a new framework to enable the\ngeneration of adversarial inputs irrespective of the input type and task\ndomain. Given an input and a set of pre-defined input transformations, our\nframework discovers a sequence of transformations that result in a semantically\ncorrect and functional adversarial input. We demonstrate the generality of our\napproach on several diverse machine learning tasks with various input\nrepresentations. We also show the importance of generating adversarial examples\nas they enable the deployment of mitigation techniques.",
        "translated": ""
    },
    {
        "title": "The ACAC_D Model for Mutable Activity Control and Chain of Dependencies\n  in Smart and Collaborative Systems",
        "url": "http://arxiv.org/abs/2308.01783v1",
        "pub_date": "2023-08-03",
        "summary": "With the integration of connected devices, artificial intelligence, and\nheterogeneous networks in IoT-driven cyber-physical systems, our society is\nevolving as a smart, automated, and connected community. In such dynamic and\ndistributed environments, various operations are carried out considering\ndifferent contextual factors to support the automation of collaborative devices\nand systems. These devices often perform long-lived operations or tasks\n(referred to as activities) to fulfill larger goals in the collaborative\nenvironment. These activities are usually mutable (change states) and\ninterdependent. They can influence the execution of other activities in the\necosystem, requiring active and real-time monitoring of the entire connected\nenvironment.\n  Recently, a vision for activity-centric access control(ACAC) was proposed to\nenable security modeling and enforcement from the perspective and abstraction\nof interdependent activities. The proposed ACAC incorporates four decision\nparameters: Authorizations(A), oBligations(B), Conditions(C), and activity\nDependencies(D) for an object agnostic access control in smart systems. In this\npaper, we take a step further towards maturing ACAC by focusing on activity\ndependencies(D) and developing a family of formal mathematically grounded\nmodels, referred to as ACAC_D. These formal models consider the real-time\nmutability of activities in resolving active dependencies among various\nactivities in the ecosystem. Activity dependencies can form a chain where it is\npossible to have dependencies of dependencies. In ACAC, we also consider the\nchain of dependencies while handling the mutability of an activity. We\nhighlight the challenges while dealing with chain of dependencies, and provide\nsolutions to resolve these challenges. We also present a proof of concept\nimplementation of with performance analysis for a smart farming use case.",
        "translated": ""
    },
    {
        "title": "Anonymity Analysis of the Umbra Stealth Address Scheme on Ethereum",
        "url": "http://arxiv.org/abs/2308.01703v1",
        "pub_date": "2023-08-03",
        "summary": "Stealth addresses are a privacy-enhancing technology that provides recipient\nanonymity on blockchains. In this work, we investigate the recipient anonymity\nand unlinkability guarantees of Umbra, the most widely used implementation of\nthe stealth address scheme on Ethereum, and its three off-chain scalability\nsolutions, e.g., Arbitrum, Optimism, and Polygon. We define and evaluate four\nheuristics to uncover the real recipients of stealth payments. We find that for\nthe majority of Umbra payments, it is straightforward to establish the\nrecipient, hence nullifying the benefits of using Umbra. Specifically, we find\nthe real recipient of $48.5\\%$, $25.8\\%$, $65.7\\%$, and $52.6\\%$ of all Umbra\ntransactions on the Ethereum main net, Polygon, Arbitrum, and Optimism\nnetworks, respectively. Finally, we suggest easily implementable\ncountermeasures to evade our deanonymization and linking attacks.",
        "translated": ""
    },
    {
        "title": "Tool-Supported Architecture-Based Data Flow Analysis for Confidentiality",
        "url": "http://arxiv.org/abs/2308.01645v1",
        "pub_date": "2023-08-03",
        "summary": "Through the increasing interconnection between various systems, the need for\nconfidential systems is increasing. Confidential systems share data only with\nauthorized entities. However, estimating the confidentiality of a system is\ncomplex, and adjusting an already deployed software is costly. Thus, it is\nhelpful to have confidentiality analyses, which can estimate the\nconfidentiality already at design time. Based on an existing data-flow-based\nconfidentiality analysis concept, we reimplemented a data flow analysis as a\nJava-based tool. The tool uses the software architecture to identify access\nviolations based on the data flow. The evaluation for our tool indicates that\nwe can analyze similar scenarios and scale for certain scenarios better than\nthe existing analysis.",
        "translated": ""
    },
    {
        "title": "VCTP: A Verifiable Credential-based Trust Propagation Protocol for\n  Personal Issuers in Self-Sovereign Identity Platforms",
        "url": "http://arxiv.org/abs/2308.01539v1",
        "pub_date": "2023-08-03",
        "summary": "Self Sovereign Identity (SSI) is an emerging identity system that facilitates\nsecure credential issuance and verification without placing trust in any\ncentralised authority. To bypass central trust, most SSI implementations place\nblockchain as a trusted mediator by placing credential transactions on-chain.\nYet, existing SSI platforms face trust issues as all credential issuers in SSI\nare not supported with adequate trust. Current SSI solutions provide trust\nsupport to the officiated issuers (e.g., government agencies), who must follow\na precise process to assess their credentials. However, there is no structured\ntrust support for individuals of SSI who may attempt to issue a credential\n(e.g., letter of consent) in the context of business processes. Therefore, some\nrisk-averse verifiers in the system may not accept the credentials from\nindividual issuers to avoid carrying the cost of mishaps from potentially\ninadmissible credentials without reliance on a trusted agency. This paper\nproposes a trust propagation protocol that supports individual users to be\ntrusted as verifiable issuers in the SSI platform by establishing a trust\npropagation credential template in the blockchain. Our approach utilises (i)\nthe sanitizable signature scheme to propagate the required trust to an\nindividual issuer, (ii) a voting mechanism to minimises the possibility of\ncollusion. Our implementation demonstrates that the solution is both practical\nand performs well under varying system loads.",
        "translated": ""
    },
    {
        "title": "Towards Fair and Privacy Preserving Federated Learning for the\n  Healthcare Domain",
        "url": "http://arxiv.org/abs/2308.01529v1",
        "pub_date": "2023-08-03",
        "summary": "Federated learning enables data sharing in healthcare contexts where it might\notherwise be difficult due to data-use-ordinances or security and communication\nconstraints. Distributed and shared data models allow models to become\ngeneralizable and learn from heterogeneous clients. While addressing data\nsecurity, privacy, and vulnerability considerations, data itself is not shared\nacross nodes in a given learning network. On the other hand, FL models often\nstruggle with variable client data distributions and operate on an assumption\nof independent and identically distributed data. As the field has grown, the\nnotion of fairness-aware federated learning mechanisms has also been introduced\nand is of distinct significance to the healthcare domain where many sensitive\ngroups and protected classes exist. In this paper, we create a benchmark\nmethodology for FAFL mechanisms under various heterogeneous conditions on\ndatasets in the healthcare domain typically outside the scope of current\nfederated learning benchmarks, such as medical imaging and waveform data\nformats. Our results indicate considerable variation in how various FAFL\nschemes respond to high levels of data heterogeneity. Additionally, doing so\nunder privacy-preserving conditions can create significant increases in network\ncommunication cost and latency compared to the typical federated learning\nscheme.",
        "translated": ""
    },
    {
        "title": "Erase and Repair: An Efficient Box-Free Removal Attack on High-Capacity\n  Deep Hiding",
        "url": "http://arxiv.org/abs/2308.01512v1",
        "pub_date": "2023-08-03",
        "summary": "Deep hiding, embedding images with others using deep neural networks, has\ndemonstrated impressive efficacy in increasing the message capacity and\nrobustness of secret sharing. In this paper, we challenge the robustness of\nexisting deep hiding schemes by preventing the recovery of secret images,\nbuilding on our in-depth study of state-of-the-art deep hiding schemes and\ntheir vulnerabilities. Leveraging our analysis, we first propose a simple\nbox-free removal attack on deep hiding that does not require any prior\nknowledge of the deep hiding schemes.\n  To improve the removal performance on the deep hiding schemes that may be\nenhanced by adversarial training, we further design a more powerful removal\nattack, efficient box-free removal attack (EBRA), which employs image\ninpainting techniques to remove secret images from container images. In\naddition, to ensure the effectiveness of our attack and preserve the fidelity\nof the processed container images, we design an erasing phase based on the\nlocality of deep hiding to remove secret information and then make full use of\nthe visual information of container images to repair the erased visual content.\nExtensive evaluations show our method can completely remove secret images from\ncontainer images with negligible impact on the quality of container images.",
        "translated": ""
    },
    {
        "title": "Circumventing Concept Erasure Methods For Text-to-Image Generative\n  Models",
        "url": "http://arxiv.org/abs/2308.01508v1",
        "pub_date": "2023-08-03",
        "summary": "Text-to-image generative models can produce photo-realistic images for an\nextremely broad range of concepts, and their usage has proliferated widely\namong the general public. On the flip side, these models have numerous\ndrawbacks, including their potential to generate images featuring sexually\nexplicit content, mirror artistic styles without permission, or even\nhallucinate (or deepfake) the likenesses of celebrities. Consequently, various\nmethods have been proposed in order to \"erase\" sensitive concepts from\ntext-to-image models. In this work, we examine five recently proposed concept\nerasure methods, and show that targeted concepts are not fully excised from any\nof these methods. Specifically, we leverage the existence of special learned\nword embeddings that can retrieve \"erased\" concepts from the sanitized models\nwith no alterations to their weights. Our results highlight the brittleness of\npost hoc concept erasure methods, and call into question their use in the\nalgorithmic toolkit for AI safety.",
        "translated": ""
    },
    {
        "title": "Decentralized Translator of Trust: Supporting Heterogeneous TEE for\n  Critical Infrastructure Protection",
        "url": "http://arxiv.org/abs/2308.01474v1",
        "pub_date": "2023-08-02",
        "summary": "Trusted execution environment (TEE) technology has found many applications in\nmitigating various security risks in an efficient manner, which is attractive\nfor critical infrastructure protection. First, the natural of critical\ninfrastructure requires it to be well protected from various cyber attacks.\nSecond, performance is usually important for critical infrastructure and it\ncannot afford an expensive protection mechanism. While a large number of\nTEE-based critical infrastructure protection systems have been proposed to\naddress various security challenges (e.g., secure sensing and reliable\ncontrol), most existing works ignore one important feature, i.e., devices\ncomprised the critical infrastructure may be equipped with multiple\nincompatible TEE technologies and belongs to different owners. This feature\nmakes it hard for these devices to establish mutual trust and form a unified\nTEE environment. To address these challenges and fully unleash the potential of\nTEE technology for critical infrastructure protection, we propose DHTee, a\ndecentralized coordination mechanism. DHTee uses blockchain technology to\nsupport key TEE functions in a heterogeneous TEE environment, especially the\nattestation service. A Device equipped with one TEE can interact securely with\nthe blockchain to verify whether another potential collaborating device\nclaiming to have a different TEE meets the security requirements. DHTee is also\nflexible and can support new TEE schemes without affecting devices using\nexisting TEEs that have been supported by the system.",
        "translated": ""
    },
    {
        "title": "VertexSerum: Poisoning Graph Neural Networks for Link Inference",
        "url": "http://arxiv.org/abs/2308.01469v1",
        "pub_date": "2023-08-02",
        "summary": "Graph neural networks (GNNs) have brought superb performance to various\napplications utilizing graph structural data, such as social analysis and fraud\ndetection. The graph links, e.g., social relationships and transaction history,\nare sensitive and valuable information, which raises privacy concerns when\nusing GNNs. To exploit these vulnerabilities, we propose VertexSerum, a novel\ngraph poisoning attack that increases the effectiveness of graph link stealing\nby amplifying the link connectivity leakage. To infer node adjacency more\naccurately, we propose an attention mechanism that can be embedded into the\nlink detection network. Our experiments demonstrate that VertexSerum\nsignificantly outperforms the SOTA link inference attack, improving the AUC\nscores by an average of $9.8\\%$ across four real-world datasets and three\ndifferent GNN structures. Furthermore, our experiments reveal the effectiveness\nof VertexSerum in both black-box and online learning settings, further\nvalidating its applicability in real-world scenarios.",
        "translated": ""
    },
    {
        "title": "IoT and Man-in-the-Middle Attacks",
        "url": "http://arxiv.org/abs/2308.02479v1",
        "pub_date": "2023-08-04",
        "summary": "This paper provides an overview of the Internet of Things (IoT) and its\nsignificance. It discusses the concept of Man-in-the-Middle (MitM) attacks in\ndetail, including their causes, potential solutions, and challenges in\ndetecting and preventing such attacks. The paper also addresses the current\nissues related to IoT security and explores future methods and facilities for\nimproving detection and prevention mechanisms against MitM.",
        "translated": ""
    },
    {
        "title": "On the Inherent Anonymity of Gossiping",
        "url": "http://arxiv.org/abs/2308.02477v1",
        "pub_date": "2023-08-04",
        "summary": "Detecting the source of a gossip is a critical issue, related to identifying\npatient zero in an epidemic, or the origin of a rumor in a social network.\nAlthough it is widely acknowledged that random and local gossip communications\nmake source identification difficult, there exists no general quantification of\nthe level of anonymity provided to the source. This paper presents a principled\nmethod based on $\\varepsilon$-differential privacy to analyze the inherent\nsource anonymity of gossiping for a large class of graphs. First, we quantify\nthe fundamental limit of source anonymity any gossip protocol can guarantee in\nan arbitrary communication graph. In particular, our result indicates that when\nthe graph has poor connectivity, no gossip protocol can guarantee any\nmeaningful level of differential privacy. This prompted us to further analyze\ngraphs with controlled connectivity. We prove on these graphs that a large\nclass of gossip protocols, namely cobra walks, offers tangible differential\nprivacy guarantees to the source. In doing so, we introduce an original proof\ntechnique based on the reduction of a gossip protocol to what we call a random\nwalk with probabilistic die out. This proof technique is of independent\ninterest to the gossip community and readily extends to other protocols\ninherited from the security community, such as the Dandelion protocol.\nInterestingly, our tight analysis precisely captures the trade-off between\ndissemination time of a gossip protocol and its source anonymity.",
        "translated": ""
    },
    {
        "title": "BlindSage: Label Inference Attacks against Node-level Vertical Federated\n  Graph Neural Networks",
        "url": "http://arxiv.org/abs/2308.02465v1",
        "pub_date": "2023-08-04",
        "summary": "Federated learning enables collaborative training of machine learning models\nby keeping the raw data of the involved workers private. One of its main\nobjectives is to improve the models' privacy, security, and scalability.\nVertical Federated Learning (VFL) offers an efficient cross-silo setting where\na few parties collaboratively train a model without sharing the same features.\nIn such a scenario, classification labels are commonly considered sensitive\ninformation held exclusively by one (active) party, while other (passive)\nparties use only their local information. Recent works have uncovered important\nflaws of VFL, leading to possible label inference attacks under the assumption\nthat the attacker has some, even limited, background knowledge on the relation\nbetween labels and data. In this work, we are the first (to the best of our\nknowledge) to investigate label inference attacks on VFL using a\nzero-background knowledge strategy. To concretely formulate our proposal, we\nfocus on Graph Neural Networks (GNNs) as a target model for the underlying VFL.\nIn particular, we refer to node classification tasks, which are widely studied,\nand GNNs have shown promising results. Our proposed attack, BlindSage, provides\nimpressive results in the experiments, achieving nearly 100% accuracy in most\ncases. Even when the attacker has no information about the used architecture or\nthe number of classes, the accuracy remained above 85% in most instances.\nFinally, we observe that well-known defenses cannot mitigate our attack without\naffecting the model's performance on the main classification task.",
        "translated": ""
    },
    {
        "title": "RobustMQ: Benchmarking Robustness of Quantized Models",
        "url": "http://arxiv.org/abs/2308.02350v1",
        "pub_date": "2023-08-04",
        "summary": "Quantization has emerged as an essential technique for deploying deep neural\nnetworks (DNNs) on devices with limited resources. However, quantized models\nexhibit vulnerabilities when exposed to various noises in real-world\napplications. Despite the importance of evaluating the impact of quantization\non robustness, existing research on this topic is limited and often disregards\nestablished principles of robustness evaluation, resulting in incomplete and\ninconclusive findings. To address this gap, we thoroughly evaluated the\nrobustness of quantized models against various noises (adversarial attacks,\nnatural corruptions, and systematic noises) on ImageNet. The comprehensive\nevaluation results empirically provide valuable insights into the robustness of\nquantized models in various scenarios, for example: (1) quantized models\nexhibit higher adversarial robustness than their floating-point counterparts,\nbut are more vulnerable to natural corruptions and systematic noises; (2) in\ngeneral, increasing the quantization bit-width results in a decrease in\nadversarial robustness, an increase in natural robustness, and an increase in\nsystematic robustness; (3) among corruption methods, \\textit{impulse noise} and\n\\textit{glass blur} are the most harmful to quantized models, while\n\\textit{brightness} has the least impact; (4) among systematic noises, the\n\\textit{nearest neighbor interpolation} has the highest impact, while bilinear\ninterpolation, cubic interpolation, and area interpolation are the three least\nharmful. Our research contributes to advancing the robust quantization of\nmodels and their deployment in real-world scenarios.",
        "translated": ""
    },
    {
        "title": "MASC: A Tool for Mutation-Based Evaluation of Static Crypto-API Misuse\n  Detectors",
        "url": "http://arxiv.org/abs/2308.02310v1",
        "pub_date": "2023-08-04",
        "summary": "While software engineers are optimistically adopting crypto-API misuse\ndetectors (or crypto-detectors) in their software development cycles, this\nmomentum must be accompanied by a rigorous understanding of crypto-detectors'\neffectiveness at finding crypto-API misuses in practice. This demo paper\npresents the technical details and usage scenarios of our tool, namely Mutation\nAnalysis for evaluating Static Crypto-API misuse detectors (MASC). We developed\n$12$ generalizable, usage based mutation operators and three mutation scopes,\nnamely Main Scope, Similarity Scope, and Exhaustive Scope, which can be used to\nexpressively instantiate compilable variants of the crypto-API misuse cases.\nUsing MASC, we evaluated nine major crypto-detectors, and discovered $19$\nunique, undocumented flaws. We designed MASC to be configurable and\nuser-friendly; a user can configure the parameters to change the nature of\ngenerated mutations. Furthermore, MASC comes with both Command Line Interface\nand Web-based front-end, making it practical for users of different levels of\nexpertise.",
        "translated": ""
    },
    {
        "title": "Improving the Security of United States Elections with Robust\n  Optimization",
        "url": "http://arxiv.org/abs/2308.02306v1",
        "pub_date": "2023-08-04",
        "summary": "For more than a century, election officials across the United States have\ninspected voting machines before elections using a procedure called Logic and\nAccuracy Testing (LAT). This procedure consists of election officials casting a\ntest deck of ballots into each voting machine and confirming the machine\nproduces the expected vote total for each candidate. We bring a scientific\nperspective to LAT by introducing the first formal approach to designing test\ndecks with rigorous security guarantees. Specifically, our approach employs\nrobust optimization to find test decks that are guaranteed to detect any voting\nmachine misconfiguration that would cause votes to be swapped across\ncandidates. Out of all the test decks with this security guarantee, our robust\noptimization problem yields the test deck with the minimum number of ballots,\nthereby minimizing implementation costs for election officials. To facilitate\ndeployment at scale, we develop a practically efficient exact algorithm for\nsolving our robust optimization problems based on the cutting plane method. In\npartnership with the Michigan Bureau of Elections, we retrospectively applied\nour approach to all 6928 ballot styles from Michigan's November 2022 general\nelection; this retrospective study reveals that the test decks with rigorous\nsecurity guarantees obtained by our approach require, on average, only 1.2%\nmore ballots than current practice. Our approach has since been piloted in\nreal-world elections by the Michigan Bureau of Elections as a low-cost way to\nimprove election security and increase public trust in democratic institutions.",
        "translated": ""
    },
    {
        "title": "Poster: Patient Community -- A Test Bed For Privacy Threat Analysis",
        "url": "http://arxiv.org/abs/2308.02272v1",
        "pub_date": "2023-08-04",
        "summary": "Research and development of privacy analysis tools currently suffers from a\nlack of test beds for evaluation and comparison of such tools. In this work, we\npropose a benchmark application that implements an extensive list of privacy\nweaknesses based on the LINDDUN methodology. It represents a social network for\npatients whose architecture has first been described in an example analysis\nconducted by one of the LINDDUN authors. We have implemented this architecture\nand extended it with more privacy threats to build a test bed that enables\ncomprehensive and independent testing of analysis tools.",
        "translated": ""
    },
    {
        "title": "Security Evaluation of Compressible and Learnable Image Encryption\n  Against Jigsaw Puzzle Solver Attacks",
        "url": "http://arxiv.org/abs/2308.02227v1",
        "pub_date": "2023-08-04",
        "summary": "Several learnable image encryption schemes have been developed for\nprivacy-preserving image classification. This paper focuses on the security\nblock-based image encryption methods that are learnable and JPEG-friendly.\nPermuting divided blocks in an image is known to enhance robustness against\nciphertext-only attacks (COAs), but recently jigsaw puzzle solver attacks have\nbeen demonstrated to be able to restore visual information on the encrypted\nimages. In contrast, it has never been confirmed whether encrypted images\nincluding noise caused by JPEG-compression are robust. Accordingly, the aim of\nthis paper is to evaluate the security of compressible and learnable encrypted\nimages against jigsaw puzzle solver attacks. In experiments, the security\nevaluation was carried out on the CIFAR-10 and STL-10 datasets under\nJPEG-compression.",
        "translated": ""
    },
    {
        "title": "LISA: LIghtweight single-server Secure Aggregation with a public source\n  of randomness",
        "url": "http://arxiv.org/abs/2308.02208v1",
        "pub_date": "2023-08-04",
        "summary": "Secure Aggregation (SA) is a key component of privacy-friendly federated\nlearning applications, where the server learns the sum of many user-supplied\ngradients, while individual gradients are kept private. State-of-the-art SA\nprotocols protect individual inputs with zero-sum random shares that are\ndistributed across users, have a per-user overhead that is logarithmic in the\nnumber of users, and take more than 5 rounds of interaction. In this paper, we\nintroduce LISA, an SA protocol that leverages a source of public randomness to\nminimize per-user overhead and the number of rounds. In particular, LISA\nrequires only two rounds and has a communication overhead that is\nasymptotically equal to that of a non-private protocol -- one where inputs are\nprovided to the server in the clear -- for most of the users. In a nutshell,\nLISA uses public randomness to select a subset of the users -- a committee --\nthat aid the server to recover the aggregated input. Users blind their\nindividual contributions with randomness shared with each of the committee\nmembers; each committee member provides the server with an aggregate of the\nrandomness shared with each user. Hence, as long as one committee member is\nhonest, the server cannot learn individual inputs but only the sum of\nthreshold-many inputs. We compare LISA with state-of-the-art SA protocols both\ntheoretically and by means of simulations and present results of our\nexperiments. We also integrate LISA in a Federated Learning pipeline and\ncompare its performance with a non-private protocol.",
        "translated": ""
    },
    {
        "title": "SoK: The Ghost Trilemma",
        "url": "http://arxiv.org/abs/2308.02202v1",
        "pub_date": "2023-08-04",
        "summary": "Trolls, bots, and sybils distort online discourse and compromise the security\nof networked platforms. User identity is central to the vectors of attack and\nmanipulation employed in these contexts. However it has long seemed that, try\nas it might, the security community has been unable to stem the rising tide of\nsuch problems. We posit the Ghost Trilemma, that there are three key properties\nof identity -- sentience, location, and uniqueness -- that cannot be\nsimultaneously verified in a fully-decentralized setting. Many\nfully-decentralized systems -- whether for communication or social coordination\n-- grapple with this trilemma in some way, perhaps unknowingly. We examine the\ndesign space, use cases, problems with prior approaches, and possible paths\nforward. We sketch a proof of this trilemma and outline options for practical,\nincrementally deployable schemes to achieve an acceptable tradeoff of trust in\ncentralized trust anchors, decentralized operation, and an ability to withstand\na range of attacks, while protecting user privacy.",
        "translated": ""
    },
    {
        "title": "Randomized algorithms for precise measurement of differentially-private,\n  personalized recommendations",
        "url": "http://arxiv.org/abs/2308.03735v2",
        "pub_date": "2023-08-07",
        "summary": "Personalized recommendations form an important part of today's internet\necosystem, helping artists and creators to reach interested users, and helping\nusers to discover new and engaging content. However, many users today are\nskeptical of platforms that personalize recommendations, in part due to\nhistorically careless treatment of personal data and data privacy. Now,\nbusinesses that rely on personalized recommendations are entering a new\nparadigm, where many of their systems must be overhauled to be privacy-first.\nIn this article, we propose an algorithm for personalized recommendations that\nfacilitates both precise and differentially-private measurement. We consider\nadvertising as an example application, and conduct offline experiments to\nquantify how the proposed privacy-preserving algorithm affects key metrics\nrelated to user experience, advertiser value, and platform revenue compared to\nthe extremes of both (private) non-personalized and non-private, personalized\nimplementations.",
        "translated": ""
    },
    {
        "title": "Labeling without Seeing? Blind Annotation for Privacy-Preserving Entity\n  Resolution",
        "url": "http://arxiv.org/abs/2308.03734v1",
        "pub_date": "2023-08-07",
        "summary": "The entity resolution problem requires finding pairs across datasets that\nbelong to different owners but refer to the same entity in the real world. To\ntrain and evaluate solutions (either rule-based or machine-learning-based) to\nthe entity resolution problem, generating a ground truth dataset with entity\npairs or clusters is needed. However, such a data annotation process involves\nhumans as domain oracles to review the plaintext data for all candidate record\npairs from different parties, which inevitably infringes the privacy of data\nowners, especially in privacy-sensitive cases like medical records. To the best\nof our knowledge, there is no prior work on privacy-preserving ground truth\ndataset generation, especially in the domain of entity resolution. We propose a\nnovel blind annotation protocol based on homomorphic encryption that allows\ndomain oracles to collaboratively label ground truths without sharing data in\nplaintext with other parties. In addition, we design a domain-specific\neasy-to-use language that hides the sophisticated underlying homomorphic\nencryption layer. Rigorous proof of the privacy guarantee is provided and our\nempirical experiments via an annotation simulator indicate the feasibility of\nour privacy-preserving protocol (f-measure on average achieves more than 90\\%\ncompared with the real ground truths).",
        "translated": ""
    },
    {
        "title": "When Federated Learning meets Watermarking: A Comprehensive Overview of\n  Techniques for Intellectual Property Protection",
        "url": "http://arxiv.org/abs/2308.03573v1",
        "pub_date": "2023-08-07",
        "summary": "Federated Learning (FL) is a technique that allows multiple participants to\ncollaboratively train a Deep Neural Network (DNN) without the need of\ncentralizing their data. Among other advantages, it comes with\nprivacy-preserving properties making it attractive for application in sensitive\ncontexts, such as health care or the military. Although the data are not\nexplicitly exchanged, the training procedure requires sharing information about\nparticipants' models. This makes the individual models vulnerable to theft or\nunauthorized distribution by malicious actors. To address the issue of\nownership rights protection in the context of Machine Learning (ML), DNN\nWatermarking methods have been developed during the last five years. Most\nexisting works have focused on watermarking in a centralized manner, but only a\nfew methods have been designed for FL and its unique constraints. In this\npaper, we provide an overview of recent advancements in Federated Learning\nwatermarking, shedding light on the new challenges and opportunities that arise\nin this field.",
        "translated": ""
    },
    {
        "title": "Mondrian: Prompt Abstraction Attack Against Large Language Models for\n  Cheaper API Pricing",
        "url": "http://arxiv.org/abs/2308.03558v1",
        "pub_date": "2023-08-07",
        "summary": "The Machine Learning as a Service (MLaaS) market is rapidly expanding and\nbecoming more mature. For example, OpenAI's ChatGPT is an advanced large\nlanguage model (LLM) that generates responses for various queries with\nassociated fees. Although these models can deliver satisfactory performance,\nthey are far from perfect. Researchers have long studied the vulnerabilities\nand limitations of LLMs, such as adversarial attacks and model toxicity.\nInevitably, commercial ML models are also not exempt from such issues, which\ncan be problematic as MLaaS continues to grow. In this paper, we discover a new\nattack strategy against LLM APIs, namely the prompt abstraction attack.\nSpecifically, we propose Mondrian, a simple and straightforward method that\nabstracts sentences, which can lower the cost of using LLM APIs. In this\napproach, the adversary first creates a pseudo API (with a lower established\nprice) to serve as the proxy of the target API (with a higher established\nprice). Next, the pseudo API leverages Mondrian to modify the user query,\nobtain the abstracted response from the target API, and forward it back to the\nend user. Our results show that Mondrian successfully reduces user queries'\ntoken length ranging from 13% to 23% across various tasks, including text\nclassification, generation, and question answering. Meanwhile, these abstracted\nqueries do not significantly affect the utility of task-specific and general\nlanguage models like ChatGPT. Mondrian also reduces instruction prompts' token\nlength by at least 11% without compromising output quality. As a result, the\nprompt abstraction attack enables the adversary to profit without bearing the\ncost of API development and deployment.",
        "translated": ""
    },
    {
        "title": "TemporalFED: Detecting Cyberattacks in Industrial Time-Series Data Using\n  Decentralized Federated Learning",
        "url": "http://arxiv.org/abs/2308.03554v1",
        "pub_date": "2023-08-07",
        "summary": "Industry 4.0 has brought numerous advantages, such as increasing productivity\nthrough automation. However, it also presents major cybersecurity issues such\nas cyberattacks affecting industrial processes. Federated Learning (FL)\ncombined with time-series analysis is a promising cyberattack detection\nmechanism proposed in the literature. However, the fact of having a single\npoint of failure and network bottleneck are critical challenges that need to be\ntackled. Thus, this article explores the benefits of the Decentralized\nFederated Learning (DFL) in terms of cyberattack detection and resource\nconsumption. The work presents TemporalFED, a software module for detecting\nanomalies in industrial environments using FL paradigms and time series.\nTemporalFED incorporates three components: Time Series Conversion, Feature\nEngineering, and Time Series Stationary Conversion. To evaluate TemporalFED, it\nwas deployed on Fedstellar, a DFL framework. Then, a pool of experiments\nmeasured the detection performance and resource consumption in a chemical gas\nindustrial environment with different time-series configurations, FL paradigms,\nand topologies. The results showcase the superiority of the configuration\nutilizing DFL and Semi-Decentralized Federated Learning (SDFL) paradigms, along\nwith a fully connected topology, which achieved the best performance in anomaly\ndetection. Regarding resource consumption, the configuration without feature\nengineering employed less bandwidth, CPU, and RAM than other configurations.",
        "translated": ""
    },
    {
        "title": "Network Security in the Industrial Control System: A Survey",
        "url": "http://arxiv.org/abs/2308.03478v1",
        "pub_date": "2023-08-07",
        "summary": "Along with the development of intelligent manufacturing, especially with the\nhigh connectivity of the industrial control system (ICS), the network security\nof ICS becomes more important. And in recent years, there has been much\nresearch on the security of the ICS network. However, in practical usage, there\nare many types of protocols, which means a high vulnerability in protocols.\nTherefore, in this paper, we give a complete review of the protocols that are\nusually used in ICS. Then, we give a comprehensive review on network security\nin terms of Defence in Depth (DiD), including data encryption, access control\npolicy, intrusion detection system, software-defined network, etc. Through\nthese works, we try to provide a new perspective on the exciting new\ndevelopments in this field.",
        "translated": ""
    },
    {
        "title": "Mitigating Persistence of Open-Source Vulnerabilities in Maven Ecosystem",
        "url": "http://arxiv.org/abs/2308.03419v1",
        "pub_date": "2023-08-07",
        "summary": "Vulnerabilities from third-party libraries (TPLs) have been unveiled to\nthreaten the Maven ecosystem. Despite patches being released promptly after\nvulnerabilities are disclosed, the libraries and applications in the community\nstill use the vulnerable versions, which makes the vulnerabilities persistent\nin the Maven ecosystem (e.g., the notorious Log4Shell still greatly influences\nthe Maven ecosystem nowadays from 2021). Both academic and industrial\nresearchers have proposed user-oriented standards and solutions to address\nvulnerabilities, while such solutions fail to tackle the ecosystem-wide\npersistent vulnerabilities because it requires a collective effort from the\ncommunity to timely adopt patches without introducing breaking issues.\n  To seek an ecosystem-wide solution, we first carried out an empirical study\nto examine the prevalence of persistent vulnerabilities in the Maven ecosystem.\nThen, we identified affected libraries for alerts by implementing an algorithm\nmonitoring downstream dependents of vulnerabilities based on an up-to-date\ndependency graph. Based on them, we further quantitatively revealed that\npatches blocked by upstream libraries caused the persistence of\nvulnerabilities. After reviewing the drawbacks of existing countermeasures, to\naddress them, we proposed a solution for range restoration (Ranger) to\nautomatically restore the compatible and secure version ranges of dependencies\nfor downstream dependents. The automatic restoration requires no manual effort\nfrom the community, and the code-centric compatibility assurance ensures smooth\nupgrades to patched versions. Moreover, Ranger along with the ecosystem\nmonitoring can timely alert developers of blocking libraries and suggest\nflexible version ranges to rapidly unblock patch versions. By evaluation,\nRanger could restore 75.64% of ranges which automatically remediated 90.32% of\nvulnerable downstream projects.",
        "translated": ""
    },
    {
        "title": "PURL: Safe and Effective Sanitization of Link Decoration",
        "url": "http://arxiv.org/abs/2308.03417v1",
        "pub_date": "2023-08-07",
        "summary": "While privacy-focused browsers have taken steps to block third-party cookies\nand browser fingerprinting, novel tracking methods that bypass existing\ndefenses continue to emerge. Since trackers need to exfiltrate information from\nthe client- to server-side through link decoration regardless of the tracking\ntechnique they employ, a promising orthogonal approach is to detect and\nsanitize tracking information in decorated links. We present PURL, a\nmachine-learning approach that leverages a cross-layer graph representation of\nwebpage execution to safely and effectively sanitize link decoration. Our\nevaluation shows that PURL significantly outperforms existing countermeasures\nin terms of accuracy and reducing website breakage while being robust to common\nevasion techniques. We use PURL to perform a measurement study on top-million\nwebsites. We find that link decorations are widely abused by well-known\nadvertisers and trackers to exfiltrate user information collected from browser\nstorage, email addresses, and scripts involved in fingerprinting.",
        "translated": ""
    },
    {
        "title": "Using Range-Revocable Pseudonyms to Provide Backward Unlinkability in\n  the Edge (Extended Version)",
        "url": "http://arxiv.org/abs/2308.03402v1",
        "pub_date": "2023-08-07",
        "summary": "In this paper we propose a novel abstraction that we have named\nRange-Revocable Pseudonyms (RRPs). RRPs are a new class of pseudonyms whose\nvalidity can be revoked for any time-range within its original validity period.\nThe key feature of RRPs is that the information provided to revoke a pseudonym\nfor a given timerange cannot be linked with the information provided when using\nthe pseudonym outside the revoked range. We provide an algorithm to implement\nRRPs using efficient cryptographic primitives where the space complexity of the\npseudonym is constant, regardless of the granularity of the revocation range,\nand the space complexity of the revocation information only grows\nlogarithmically with the granularity; this makes the use of RRPs far more\nefficient than the use of many short-lived pseudonyms. We have used RRPs to\ndesign EDGAR, an access control system for VANET scenarios that offers backward\nunlinkability. The experimental evaluation of EDGAR shows that, when using\nRRPs, the revocation can be performed efficiently (even when using time slots\nas small as 1 second) and that users can authenticate with low latency\n($0.5-3.5$ ms).",
        "translated": ""
    },
    {
        "title": "A reading survey on adversarial machine learning: Adversarial attacks\n  and their understanding",
        "url": "http://arxiv.org/abs/2308.03363v1",
        "pub_date": "2023-08-07",
        "summary": "Deep Learning has empowered us to train neural networks for complex data with\nhigh performance. However, with the growing research, several vulnerabilities\nin neural networks have been exposed. A particular branch of research,\nAdversarial Machine Learning, exploits and understands some of the\nvulnerabilities that cause the neural networks to misclassify for near original\ninput. A class of algorithms called adversarial attacks is proposed to make the\nneural networks misclassify for various tasks in different domains. With the\nextensive and growing research in adversarial attacks, it is crucial to\nunderstand the classification of adversarial attacks. This will help us\nunderstand the vulnerabilities in a systematic order and help us to mitigate\nthe effects of adversarial attacks. This article provides a survey of existing\nadversarial attacks and their understanding based on different perspectives. We\nalso provide a brief overview of existing adversarial defences and their\nlimitations in mitigating the effect of adversarial attacks. Further, we\nconclude with a discussion on the future research directions in the field of\nadversarial machine learning.",
        "translated": ""
    },
    {
        "title": "Chrisimos: A useful Proof-of-Work for finding Minimal Dominating Set of\n  a graph",
        "url": "http://arxiv.org/abs/2308.04407v1",
        "pub_date": "2023-08-08",
        "summary": "Hash-based Proof-of-Work (PoW) used in the Bitcoin Blockchain leads to high\nenergy consumption and resource wastage. In this paper, we aim to re-purpose\nthe energy by replacing the hash function with real-life problems having\ncommercial utility. We propose Chrisimos, a useful Proof-of-Work where miners\nare required to find a minimal dominating set for real-life graph instances. A\nminer who is able to output the smallest dominating set for the given graph\nwithin the block interval time wins the mining game. We also propose a new\nchain selection rule that ensures the security of the scheme. Thus our protocol\nalso realizes a decentralized minimal dominating set solver for any graph\ninstance. We provide formal proof of correctness and show via experimental\nresults that the block interval time is within feasible bounds of hash-based\nPoW.",
        "translated": ""
    },
    {
        "title": "XGBD: Explanation-Guided Graph Backdoor Detection",
        "url": "http://arxiv.org/abs/2308.04406v1",
        "pub_date": "2023-08-08",
        "summary": "Backdoor attacks pose a significant security risk to graph learning models.\nBackdoors can be embedded into the target model by inserting backdoor triggers\ninto the training dataset, causing the model to make incorrect predictions when\nthe trigger is present. To counter backdoor attacks, backdoor detection has\nbeen proposed. An emerging detection strategy in the vision and NLP domains is\nbased on an intriguing phenomenon: when training models on a mixture of\nbackdoor and clean samples, the loss on backdoor samples drops significantly\nfaster than on clean samples, allowing backdoor samples to be easily detected\nby selecting samples with the lowest loss values. However, the ignorance of\ntopological feature information on graph data limits its detection\neffectiveness when applied directly to the graph domain. To this end, we\npropose an explanation-guided backdoor detection method to take advantage of\nthe topological information. Specifically, we train a helper model on the graph\ndataset, feed graph samples into the model, and then adopt explanation methods\nto attribute model prediction to an important subgraph. We observe that\nbackdoor samples have distinct attribution distribution than clean samples, so\nthe explanatory subgraph could serve as more discriminative features for\ndetecting backdoor samples. Comprehensive experiments on multiple popular\ndatasets and attack methods demonstrate the effectiveness and explainability of\nour method. Our code is available:\nhttps://github.com/GuanZihan/GNN_backdoor_detection.",
        "translated": ""
    },
    {
        "title": "Pelta: Shielding Transformers to Mitigate Evasion Attacks in Federated\n  Learning",
        "url": "http://arxiv.org/abs/2308.04373v1",
        "pub_date": "2023-08-08",
        "summary": "The main premise of federated learning is that machine learning model updates\nare computed locally, in particular to preserve user data privacy, as those\nnever leave the perimeter of their device. This mechanism supposes the general\nmodel, once aggregated, to be broadcast to collaborating and non malicious\nnodes. However, without proper defenses, compromised clients can easily probe\nthe model inside their local memory in search of adversarial examples. For\ninstance, considering image-based applications, adversarial examples consist of\nimperceptibly perturbed images (to the human eye) misclassified by the local\nmodel, which can be later presented to a victim node's counterpart model to\nreplicate the attack. To mitigate such malicious probing, we introduce Pelta, a\nnovel shielding mechanism leveraging trusted hardware. By harnessing the\ncapabilities of Trusted Execution Environments (TEEs), Pelta masks part of the\nback-propagation chain rule, otherwise typically exploited by attackers for the\ndesign of malicious samples. We evaluate Pelta on a state of the art ensemble\nmodel and demonstrate its effectiveness against the Self Attention Gradient\nadversarial Attack.",
        "translated": ""
    },
    {
        "title": "Accurate, Explainable, and Private Models: Providing Recourse While\n  Minimizing Training Data Leakage",
        "url": "http://arxiv.org/abs/2308.04341v1",
        "pub_date": "2023-08-08",
        "summary": "Machine learning models are increasingly utilized across impactful domains to\npredict individual outcomes. As such, many models provide algorithmic recourse\nto individuals who receive negative outcomes. However, recourse can be\nleveraged by adversaries to disclose private information. This work presents\nthe first attempt at mitigating such attacks. We present two novel methods to\ngenerate differentially private recourse: Differentially Private Model (DPM)\nand Laplace Recourse (LR). Using logistic regression classifiers and real world\nand synthetic datasets, we find that DPM and LR perform well in reducing what\nan adversary can infer, especially at low FPR. When training dataset size is\nlarge enough, we find particular success in preventing privacy leakage while\nmaintaining model and recourse accuracy with our novel LR method.",
        "translated": ""
    },
    {
        "title": "Preserving Sparsity and Privacy in Straggler-Resilient Distributed\n  Matrix Computations",
        "url": "http://arxiv.org/abs/2308.04331v1",
        "pub_date": "2023-08-08",
        "summary": "Existing approaches to distributed matrix computations involve allocating\ncoded combinations of submatrices to worker nodes, to build resilience to\nstragglers and/or enhance privacy. In this study, we consider the challenge of\npreserving input sparsity in such approaches to retain the associated\ncomputational efficiency enhancements. First, we find a lower bound on the\nweight of coding, i.e., the number of submatrices to be combined to obtain\ncoded submatrices to provide the resilience to the maximum possible number of\nstragglers (for given number of nodes and their storage constraints). Next we\npropose a distributed matrix computation scheme which meets this exact lower\nbound on the weight of the coding. Further, we develop controllable trade-off\nbetween worker computation time and the privacy constraint for sparse input\nmatrices in settings where the worker nodes are honest but curious. Numerical\nexperiments conducted in Amazon Web Services (AWS) validate our assertions\nregarding straggler mitigation and computation speed for sparse matrices.",
        "translated": ""
    },
    {
        "title": "Defending Hash Tables from Subterfuge with Depth Charge",
        "url": "http://arxiv.org/abs/2308.04305v1",
        "pub_date": "2023-08-08",
        "summary": "We consider the problem of defending a hash table against a Byzantine\nattacker that is trying to degrade the performance of query, insertion and\ndeletion operations. Our defense makes use of resource burning (RB) -- the the\nverifiable expenditure of network resources -- where the issuer of a request\nincurs some RB cost. Our algorithm, Depth Charge, charges RB costs for\noperations based on the depth of the appropriate object in the list that the\nobject hashes to in the table. By appropriately setting the RB costs, our\nalgorithm mitigates the impact of an attacker on the hash table's performance.\nIn particular, in the presence of a significant attack, our algorithm incurs a\ncost which is asymptotically less that the attacker's cost.",
        "translated": ""
    },
    {
        "title": "The Model Inversion Eavesdropping Attack in Semantic Communication\n  Systems",
        "url": "http://arxiv.org/abs/2308.04304v1",
        "pub_date": "2023-08-08",
        "summary": "In recent years, semantic communication has been a popular research topic for\nits superiority in communication efficiency. As semantic communication relies\non deep learning to extract meaning from raw messages, it is vulnerable to\nattacks targeting deep learning models. In this paper, we introduce the model\ninversion eavesdropping attack (MIEA) to reveal the risk of privacy leaks in\nthe semantic communication system. In MIEA, the attacker first eavesdrops the\nsignal being transmitted by the semantic communication system and then performs\nmodel inversion attack to reconstruct the raw message, where both the white-box\nand black-box settings are considered. Evaluation results show that MIEA can\nsuccessfully reconstruct the raw message with good quality under different\nchannel conditions. We then propose a defense method based on random\npermutation and substitution to defend against MIEA in order to achieve secure\nsemantic communication. Our experimental results demonstrate the effectiveness\nof the proposed defense method in preventing MIEA.",
        "translated": ""
    },
    {
        "title": "The Vulnerable Nature of Decentralized Governance in DeFi",
        "url": "http://arxiv.org/abs/2308.04267v1",
        "pub_date": "2023-08-08",
        "summary": "Decentralized Finance (DeFi) platforms are often governed by Decentralized\nAutonomous Organizations (DAOs) which are implemented via governance protocols.\nGovernance tokens are distributed to users of the platform, granting them\nvoting rights in the platform's governance protocol. Many DeFi platforms have\nalready been subject to attacks resulting in the loss of millions of dollars in\nuser funds.\n  In this paper we show that governance tokens are often not used as intended\nand may be harmful to the security of DeFi platforms. We show that (1) users\noften do not use governance tokens to vote, (2) that voting rates are\nnegatively correlated to gas prices, (3) voting is very centralized.\n  We explore vulnerabilities in the design of DeFi platform's governance\nprotocols and analyze different governance attacks, focusing on the\ntransferable nature of voting rights via governance tokens. Following the\nmovement and holdings of governance tokens, we show they are often used to\nperform a single action and then sold off. We present evidence of DeFi\nplatforms using other platforms' governance protocols to promote their own\nagenda at the expense of the host platform.",
        "translated": ""
    },
    {
        "title": "Novel Area-Efficient and Flexible Architectures for Optimal Ate Pairing\n  on FPGA",
        "url": "http://arxiv.org/abs/2308.04261v1",
        "pub_date": "2023-08-08",
        "summary": "While FPGA is a suitable platform for implementing cryptographic algorithms,\nthere are several challenges associated with implementing Optimal Ate pairing\non FPGA, such as security, limited computing resources, and high power\nconsumption. To overcome these issues, this study introduces three approaches\nthat can execute the optimal Ate pairing on Barreto-Naehrig curves using\nJacobean coordinates with the goal of reaching 128-bit security on the Genesys\nboard. The first approach is a pure software implementation utilizing the\nMicroBlaze processor. The second involves a combination of software and\nhardware, with key operations in $F_{p}$ and $F_{p^{2}}$ being transformed into\nIP cores for the MicroBlaze. The third approach builds on the second by\nincorporating parallelism to improve the pairing process. The utilization of\nmultiple MicroBlaze processors within a single system offers both versatility\nand parallelism to speed up pairing calculations. A variety of methods and\nparameters are used to optimize the pairing computation, including Montgomery\nmodular multiplication, the Karatsuba method, Jacobean coordinates, the Complex\nsquaring method, sparse multiplication, squaring in $G_{\\phi 6}F_{p^{12}}$, and\nthe addition chain method. The proposed systems are designed to efficiently\nutilize limited resources in restricted environments, while still completing\ntasks in a timely manner.",
        "translated": ""
    },
    {
        "title": "Iterative Sketching for Secure Coded Regression",
        "url": "http://arxiv.org/abs/2308.04185v1",
        "pub_date": "2023-08-08",
        "summary": "In this work, we propose methods for speeding up linear regression\ndistributively, while ensuring security. We leverage randomized sketching\ntechniques, and improve straggler resilience in asynchronous systems.\nSpecifically, we apply a random orthonormal matrix and then subsample\n\\textit{blocks}, to simultaneously secure the information and reduce the\ndimension of the regression problem. In our setup, the transformation\ncorresponds to an encoded encryption in an \\textit{approximate gradient coding\nscheme}, and the subsampling corresponds to the responses of the non-straggling\nworkers; in a centralized coded computing network. This results in a\ndistributive \\textit{iterative sketching} approach for an $\\ell_2$-subspace\nembedding, \\textit{i.e.} a new sketch is considered at each iteration. We also\nfocus on the special case of the \\textit{Subsampled Randomized Hadamard\nTransform}, which we generalize to block sampling; and discuss how it can be\nmodified in order to secure the data.",
        "translated": ""
    },
    {
        "title": "Optimal Flexible Consensus and its Application to Ethereum",
        "url": "http://arxiv.org/abs/2308.05096v2",
        "pub_date": "2023-08-09",
        "summary": "Classic BFT consensus protocols guarantee safety and liveness for all clients\nif fewer than one-third of replicas are faulty. However, in applications such\nas high-value payments, some clients may want to prioritize safety over\nliveness. Flexible consensus allows each client to opt for a higher safety\nresilience, albeit at the expense of reduced liveness resilience. We present\nthe first construction that allows optimal safety--liveness tradeoff for every\nclient simultaneously. This construction is modular and is realized as an\nadd-on applied on top of an existing consensus protocol. The add-on consists of\nan additional round of voting and permanent locking done by the replicas, to\nsidestep a sub-optimal quorum-intersection-based constraint present in previous\nsolutions. We adapt our construction to the existing Ethereum protocol to\nderive optimal flexible confirmation rules that clients can adopt unilaterally\nwithout requiring system-wide changes. This is possible because existing\nEthereum protocol features can double as the extra voting and locking. We\ndemonstrate an implementation using Ethereum's consensus API.",
        "translated": ""
    },
    {
        "title": "CERMET: Coding for Energy Reduction with Multiple Encryption Techniques\n  -- $It's\\ easy\\ being\\ green$",
        "url": "http://arxiv.org/abs/2308.05063v1",
        "pub_date": "2023-08-09",
        "summary": "This paper presents CERMET, an energy-efficient hardware architecture\ndesigned for hardware-constrained cryptosystems. CERMET employs a base\ncryptosystem in conjunction with network coding to provide both\ninformation-theoretic and computational security while reducing energy\nconsumption per bit. This paper introduces the hardware architecture for the\nsystem and explores various optimizations to enhance its performance. The\nuniversality of the approach is demonstrated by designing the architecture to\naccommodate both asymmetric and symmetric cryptosystems. The analysis reveals\nthat the benefits of this proposed approach are multifold, reducing energy per\nbit and area without compromising security or throughput. The optimized\nhardware architectures can achieve below 1 pJ/bit operations for AES-256.\nFurthermore, for a public key cryptosystem based on Elliptic Curve Cryptography\n(ECC), a remarkable 14.6X reduction in energy per bit and a 9.3X reduction in\narea are observed, bringing it to less than 1 nJ/bit.",
        "translated": ""
    },
    {
        "title": "Kairos: : Practical Intrusion Detection and Investigation using\n  Whole-system Provenance",
        "url": "http://arxiv.org/abs/2308.05034v1",
        "pub_date": "2023-08-09",
        "summary": "Provenance graphs are structured audit logs that describe the history of a\nsystem's execution. Recent studies have explored a variety of techniques to\nanalyze provenance graphs for automated host intrusion detection, focusing\nparticularly on advanced persistent threats. Sifting through their design\ndocuments, we identify four common dimensions that drive the development of\nprovenance-based intrusion detection systems (PIDSes): scope (can PIDSes detect\nmodern attacks that infiltrate across application boundaries?), attack\nagnosticity (can PIDSes detect novel attacks without a priori knowledge of\nattack characteristics?), timeliness (can PIDSes efficiently monitor host\nsystems as they run?), and attack reconstruction (can PIDSes distill attack\nactivity from large provenance graphs so that sysadmins can easily understand\nand quickly respond to system intrusion?). We present KAIROS, the first PIDS\nthat simultaneously satisfies the desiderata in all four dimensions, whereas\nexisting approaches sacrifice at least one and struggle to achieve comparable\ndetection performance.\n  Kairos leverages a novel graph neural network-based encoder-decoder\narchitecture that learns the temporal evolution of a provenance graph's\nstructural changes to quantify the degree of anomalousness for each system\nevent. Then, based on this fine-grained information, Kairos reconstructs attack\nfootprints, generating compact summary graphs that accurately describe\nmalicious activity over a stream of system audit logs. Using state-of-the-art\nbenchmark datasets, we demonstrate that Kairos outperforms previous approaches.",
        "translated": ""
    },
    {
        "title": "An Empirical Study on Using Large Language Models to Analyze Software\n  Supply Chain Security Failures",
        "url": "http://arxiv.org/abs/2308.04898v1",
        "pub_date": "2023-08-09",
        "summary": "As we increasingly depend on software systems, the consequences of breaches\nin the software supply chain become more severe. High-profile cyber attacks\nlike those on SolarWinds and ShadowHammer have resulted in significant\nfinancial and data losses, underlining the need for stronger cybersecurity. One\nway to prevent future breaches is by studying past failures. However,\ntraditional methods of analyzing these failures require manually reading and\nsummarizing reports about them. Automated support could reduce costs and allow\nanalysis of more failures. Natural Language Processing (NLP) techniques such as\nLarge Language Models (LLMs) could be leveraged to assist the analysis of\nfailures. In this study, we assessed the ability of Large Language Models\n(LLMs) to analyze historical software supply chain breaches. We used LLMs to\nreplicate the manual analysis of 69 software supply chain security failures\nperformed by members of the Cloud Native Computing Foundation (CNCF). We\ndeveloped prompts for LLMs to categorize these by four dimensions: type of\ncompromise, intent, nature, and impact. GPT 3.5s categorizations had an average\naccuracy of 68% and Bard had an accuracy of 58% over these dimensions. We\nreport that LLMs effectively characterize software supply chain failures when\nthe source articles are detailed enough for consensus among manual analysts,\nbut cannot yet replace human analysts. Future work can improve LLM performance\nin this context, and study a broader range of articles and failures.",
        "translated": ""
    },
    {
        "title": "can-train-and-test: A Curated CAN Dataset for Automotive Intrusion\n  Detection",
        "url": "http://arxiv.org/abs/2308.04972v1",
        "pub_date": "2023-08-09",
        "summary": "When it comes to in-vehicle networks (IVNs), the controller area network --\nCAN -- bus dominates the market; automobiles manufactured and sold around the\nworld depend on the CAN bus for safety-critical communications between various\ncomponents of the vehicle (e.g., the engine, the transmission, the steering\ncolumn). Unfortunately, the CAN bus is inherently insecure; in fact, it\ncompletely lacks controls such as authentication, authorization, and\nconfidentiality (i.e., encryption). Therefore, researchers have travailed to\ndevelop automotive security enhancements. The automotive intrusion detection\nsystem (IDS) is especially popular in the literature -- due to its relatively\nlow cost in terms of money, resource utilization, and implementation effort.\nThat said, developing and evaluating an automotive IDS is often challenging; if\nresearchers do not have access to a test vehicle, then they are forced to\ndepend on publicly available CAN data -- which is not without limitations. Lack\nof access to adequate CAN data, then, becomes a barrier to entry into\nautomotive security research.\n  We seek to lower that barrier to entry by introducing a new CAN dataset to\nfacilitate the development and evaluation of automotive IDSs. Our dataset,\ndubbed can-train-and-test, provides CAN data from four different vehicles\nproduced by two different manufacturers. The attack captures for each vehicle\nmodel are equivalent, enabling researchers to assess the ability of a given IDS\nto generalize to different vehicle models and even different vehicle\nmanufacturers. Our dataset contains replayable .log files as well as labeled\nand unlabeled .csv files, thereby meeting a variety of development and\nevaluation needs. Furthermore, can-train-and-test offers nine unique attacks,\nranging from denial of service (DoS) to gear spoofing to standstill...",
        "translated": ""
    },
    {
        "title": "Adversarial ModSecurity: Countering Adversarial SQL Injections with\n  Robust Machine Learning",
        "url": "http://arxiv.org/abs/2308.04964v1",
        "pub_date": "2023-08-09",
        "summary": "ModSecurity is widely recognized as the standard open-source Web Application\nFirewall (WAF), maintained by the OWASP Foundation. It detects malicious\nrequests by matching them against the Core Rule Set, identifying well-known\nattack patterns. Each rule in the CRS is manually assigned a weight, based on\nthe severity of the corresponding attack, and a request is detected as\nmalicious if the sum of the weights of the firing rules exceeds a given\nthreshold. In this work, we show that this simple strategy is largely\nineffective for detecting SQL injection (SQLi) attacks, as it tends to block\nmany legitimate requests, while also being vulnerable to adversarial SQLi\nattacks, i.e., attacks intentionally manipulated to evade detection. To\novercome these issues, we design a robust machine learning model, named\nAdvModSec, which uses the CRS rules as input features, and it is trained to\ndetect adversarial SQLi attacks. Our experiments show that AdvModSec, being\ntrained on the traffic directed towards the protected web services, achieves a\nbetter trade-off between detection and false positive rates, improving the\ndetection rate of the vanilla version of ModSecurity with CRS by 21%. Moreover,\nour approach is able to improve its adversarial robustness against adversarial\nSQLi attacks by 42%, thereby taking a step forward towards building more robust\nand trustworthy WAFs.",
        "translated": ""
    },
    {
        "title": "Representation Learning for Audio Privacy Preservation using Source\n  Separation and Robust Adversarial Learning",
        "url": "http://arxiv.org/abs/2308.04960v1",
        "pub_date": "2023-08-09",
        "summary": "Privacy preservation has long been a concern in smart acoustic monitoring\nsystems, where speech can be passively recorded along with a target signal in\nthe system's operating environment. In this study, we propose the integration\nof two commonly used approaches in privacy preservation: source separation and\nadversarial representation learning. The proposed system learns the latent\nrepresentation of audio recordings such that it prevents differentiating\nbetween speech and non-speech recordings. Initially, the source separation\nnetwork filters out some of the privacy-sensitive data, and during the\nadversarial learning process, the system will learn privacy-preserving\nrepresentation on the filtered signal. We demonstrate the effectiveness of our\nproposed method by comparing our method against systems without source\nseparation, without adversarial learning, and without both. Overall, our\nresults suggest that the proposed system can significantly improve speech\nprivacy preservation compared to that of using source separation or adversarial\nlearning solely while maintaining good performance in the acoustic monitoring\ntask.",
        "translated": ""
    },
    {
        "title": "Performance Analysis of Transformer Based Models (BERT, ALBERT and\n  RoBERTa) in Fake News Detection",
        "url": "http://arxiv.org/abs/2308.04950v1",
        "pub_date": "2023-08-09",
        "summary": "Fake news is fake material in a news media format but is not processed\nproperly by news agencies. The fake material can provoke or defame significant\nentities or individuals or potentially even for the personal interests of the\ncreators, causing problems for society. Distinguishing fake news and real news\nis challenging due to limited of domain knowledge and time constraints.\nAccording to the survey, the top three areas most exposed to hoaxes and\nmisinformation by residents are in Banten, DKI Jakarta and West Java. The model\nof transformers is referring to an approach in the field of artificial\nintelligence (AI) in natural language processing utilizing the deep learning\narchitectures. Transformers exercise a powerful attention mechanism to process\ntext in parallel and produce rich and contextual word representations. A\nprevious study indicates a superior performance of a transformer model known as\nBERT over and above non transformer approach. However, some studies suggest the\nperformance can be improved with the use of improved BERT models known as\nALBERT and RoBERTa. However, the modified BERT models are not well explored for\ndetecting fake news in Bahasa Indonesia. In this research, we explore those\ntransformer models and found that ALBERT outperformed other models with 87.6%\naccuracy, 86.9% precision, 86.9% F1-score, and 174.5 run-time (s/epoch)\nrespectively. Source code available at:\nhttps://github.com/Shafna81/fakenewsdetection.git",
        "translated": ""
    },
    {
        "title": "Differentially Private Graph Neural Network with Importance-Grained\n  Noise Adaption",
        "url": "http://arxiv.org/abs/2308.04943v1",
        "pub_date": "2023-08-09",
        "summary": "Graph Neural Networks (GNNs) with differential privacy have been proposed to\npreserve graph privacy when nodes represent personal and sensitive information.\nHowever, the existing methods ignore that nodes with different importance may\nyield diverse privacy demands, which may lead to over-protect some nodes and\ndecrease model utility. In this paper, we study the problem of\nimportance-grained privacy, where nodes contain personal data that need to be\nkept private but are critical for training a GNN. We propose NAP-GNN, a\nnode-importance-grained privacy-preserving GNN algorithm with privacy\nguarantees based on adaptive differential privacy to safeguard node\ninformation. First, we propose a Topology-based Node Importance Estimation\n(TNIE) method to infer unknown node importance with neighborhood and centrality\nawareness. Second, an adaptive private aggregation method is proposed to\nperturb neighborhood aggregation from node-importance-grain. Third, we propose\nto privately train a graph learning algorithm on perturbed aggregations in\nadaptive residual connection mode over multi-layers convolution for node-wise\ntasks. Theoretically analysis shows that NAP-GNN satisfies privacy guarantees.\nEmpirical experiments over real-world graph datasets show that NAP-GNN achieves\na better trade-off between privacy and accuracy.",
        "translated": ""
    },
    {
        "title": "Adversarial Deep Reinforcement Learning for Cyber Security in Software\n  Defined Networks",
        "url": "http://arxiv.org/abs/2308.04909v1",
        "pub_date": "2023-08-09",
        "summary": "This paper focuses on the impact of leveraging autonomous offensive\napproaches in Deep Reinforcement Learning (DRL) to train more robust agents by\nexploring the impact of applying adversarial learning to DRL for autonomous\nsecurity in Software Defined Networks (SDN). Two algorithms, Double Deep\nQ-Networks (DDQN) and Neural Episodic Control to Deep Q-Network (NEC2DQN or\nN2D), are compared. NEC2DQN was proposed in 2018 and is a new member of the\ndeep q-network (DQN) family of algorithms. The attacker has full observability\nof the environment and access to a causative attack that uses state\nmanipulation in an attempt to poison the learning process. The implementation\nof the attack is done under a white-box setting, in which the attacker has\naccess to the defender's model and experiences. Two games are played; in the\nfirst game, DDQN is a defender and N2D is an attacker, and in second game, the\nroles are reversed. The games are played twice; first, without an active\ncausative attack and secondly, with an active causative attack. For execution,\nthree sets of game results are recorded in which a single set consists of 10\ngame runs. The before and after results are then compared in order to see if\nthere was actually an improvement or degradation. The results show that with\nminute parameter changes made to the algorithms, there was growth in the\nattacker's role, since it is able to win games. Implementation of the\nadversarial learning by the introduction of the causative attack showed the\nalgorithms are still able to defend the network according to their strengths.",
        "translated": ""
    },
    {
        "title": "The Privacy-Value-App Relationship and the Value-Centered Privacy\n  Assistant",
        "url": "http://arxiv.org/abs/2308.05700v1",
        "pub_date": "2023-08-10",
        "summary": "Many of us make quick decisions that affect our data privacy on our\nsmartphones without due consideration of our values. One such decision point is\nestablishing whether to download a smartphone app or not. In this work, we aim\nto better understand the relationship between our values, our privacy\npreferences, and our app choices, as well as explore the effectiveness of a\nsmartphone value-centered privacy assistant (VcPA) at promoting value-centered\napp selection. To do this, we conducted a mixed-methods study that involved two\nphases. The first was an online survey of 273 smartphone user's values and\nprivacy preferences when considering whether to download one of two apps (Lose\nIt! and OpenLitterMap). Our results suggest that values and privacy preferences\nare related in an app or context-dependent manner. The second phase was testing\nthe VcPA with 77 users in a synthetic Mock App Store setting. We established\nusability of a VcPA, with the VcPA helping some users more than others with\nselecting apps consistent with their selected value profile. Future qualitative\nand context-specific explorations of user perspectives could contribute to\nadequately capturing the specific role of values for privacy decision-making\nand improving the VcPA.",
        "translated": ""
    },
    {
        "title": "A Homomorphic Encryption Framework for Privacy-Preserving Spiking Neural\n  Networks",
        "url": "http://arxiv.org/abs/2308.05636v1",
        "pub_date": "2023-08-10",
        "summary": "Machine learning (ML) is widely used today, especially through deep neural\nnetworks (DNNs), however, increasing computational load and resource\nrequirements have led to cloud-based solutions. To address this problem, a new\ngeneration of networks called Spiking Neural Networks (SNN) has emerged, which\nmimic the behavior of the human brain to improve efficiency and reduce energy\nconsumption. These networks often process large amounts of sensitive\ninformation, such as confidential data, and thus privacy issues arise.\nHomomorphic encryption (HE) offers a solution, allowing calculations to be\nperformed on encrypted data without decrypting it. This research compares\ntraditional DNNs and SNNs using the Brakerski/Fan-Vercauteren (BFV) encryption\nscheme. The LeNet-5 model, a widely-used convolutional architecture, is used\nfor both DNN and SNN models based on the LeNet-5 architecture, and the networks\nare trained and compared using the FashionMNIST dataset. The results show that\nSNNs using HE achieve up to 40% higher accuracy than DNNs for low values of the\nplaintext modulus t, although their execution time is longer due to their\ntime-coding nature with multiple time-steps.",
        "translated": ""
    },
    {
        "title": "Symmetry Defense Against XGBoost Adversarial Perturbation Attacks",
        "url": "http://arxiv.org/abs/2308.05575v1",
        "pub_date": "2023-08-10",
        "summary": "We examine whether symmetry can be used to defend tree-based ensemble\nclassifiers such as gradient-boosting decision trees (GBDTs) against\nadversarial perturbation attacks. The idea is based on a recent symmetry\ndefense for convolutional neural network classifiers (CNNs) that utilizes CNNs'\nlack of invariance with respect to symmetries. CNNs lack invariance because\nthey can classify a symmetric sample, such as a horizontally flipped image,\ndifferently from the original sample. CNNs' lack of invariance also means that\nCNNs can classify symmetric adversarial samples differently from the incorrect\nclassification of adversarial samples. Using CNNs' lack of invariance, the\nrecent CNN symmetry defense has shown that the classification of symmetric\nadversarial samples reverts to the correct sample classification. In order to\napply the same symmetry defense to GBDTs, we examine GBDT invariance and are\nthe first to show that GBDTs also lack invariance with respect to symmetries.\nWe apply and evaluate the GBDT symmetry defense for nine datasets against six\nperturbation attacks with a threat model that ranges from zero-knowledge to\nperfect-knowledge adversaries. Using the feature inversion symmetry against\nzero-knowledge adversaries, we achieve up to 100% accuracy on adversarial\nsamples even when default and robust classifiers have 0% accuracy. Using the\nfeature inversion and horizontal flip symmetries against perfect-knowledge\nadversaries, we achieve up to over 95% accuracy on adversarial samples for the\nGBDT classifier of the F-MNIST dataset even when default and robust classifiers\nhave 0% accuracy.",
        "translated": ""
    },
    {
        "title": "Analysis of the LockBit 3.0 and its infiltration into Advanced's\n  infrastructure crippling NHS services",
        "url": "http://arxiv.org/abs/2308.05565v1",
        "pub_date": "2023-08-10",
        "summary": "The LockBit 3.0 ransomware variant is arguably the most threatening of\nmalware in recent times. With no regard for a victim's industry, the ransomware\nhas undergone several evolutions to arrive at an adaptable and evasive variant\nwhich has been a menace to governments and organisations, recently infiltrating\nAdvanced Computer Software group. Previous LockBit studies mostly concentrated\non measuring the impact of the ransomware attack, prevention, encryption\ndetection, decryption, or data recovery, thereby providing little or no benefit\nto the less tech savvy populace as a detailed breakdown of the mode of attack\nis rarely examined. This article analyses the LockBit 3.0 attack techniques\nwith a contextual illustration of the attack on Advanced Computer Software\ngroup. With the NHS being a major client of the organisation, and its services\nalongside 15 other clients being crippled for hours during the attack,\nattention is drawn to how dreadful such disruption may be in critical\norganisations. We observed that the upgrade of Lockbit based on releasing newer\nversions is in a bid to continuously ensure the malware's efficiency - a virtue\nthat keeps it at the zenith - by staying ahead of improved defenses. Our study\nhighlights social engineering as a vibrant portal to Lockbit's maliciousness\nand indicates an investment in detection systems may profit more than in\nprevention systems. Therefore, further research should consider improving\ndetection systems against Lockbit 3.0.",
        "translated": ""
    },
    {
        "title": "Accountability of Things: Large-Scale Tamper-Evident Logging for Smart\n  Devices",
        "url": "http://arxiv.org/abs/2308.05557v1",
        "pub_date": "2023-08-10",
        "summary": "Our modern world relies on a growing number of interconnected and interacting\ndevices, leading to a plethora of logs establishing audit trails for all kinds\nof events. Simultaneously, logs become increasingly important for forensic\ninvestigations, and thus, an adversary will aim to alter logs to avoid\nculpability, e.g., by compromising devices that generate and store logs. Thus,\nit is essential to ensure that no one can tamper with any logs without going\nundetected. However, existing approaches to establish tamper evidence of logs\ndo not scale and cannot protect the increasingly large number of devices found\ntoday, as they impose large storage or network overheads. Additionally, most\nschemes do not provide an efficient mechanism to prove that individual events\nhave been logged to establish accountability when different devices interact.\n  This paper introduces a novel scheme for practical large-scale tamper-evident\nlogging with the help of a trusted third party. To achieve this, we present a\nnew binary hash tree construction designed around timestamps to achieve\nconstant storage overhead with a configured temporal resolution. Additionally,\nour design enables the efficient construction of shareable proofs, proving that\nan event was indeed logged. Our evaluation shows that - using practical\nparameters - our scheme can localize any tampering of logs with a sub-second\nresolution, with a constant overhead of ~8KB per hour per device.",
        "translated": ""
    },
    {
        "title": "Your DRM Can Watch You Too: Exploring the Privacy Implications of\n  Browsers (mis)Implementations of Widevine EME",
        "url": "http://arxiv.org/abs/2308.05416v1",
        "pub_date": "2023-08-10",
        "summary": "Thanks to HTML5, users can now view videos on Web browsers without installing\nplug-ins or relying on specific devices. In 2017, W3C published Encrypted Media\nExtensions (EME) as the first official Web standard for Digital Rights\nManagement (DRM), with the overarching goal of allowing seamless integration of\nDRM systems on browsers. EME has prompted numerous voices of dissent with\nrespect to the inadequate protection of users. Of particular interest, privacy\nconcerns were articulated, especially that DRM systems inherently require\nuniquely identifying information on users' devices to control content\ndistribution better. Despite this anecdotal evidence, we lack a comprehensive\noverview of how browsers have supported EME in practice and what privacy\nimplications are caused by their implementations. In this paper, we fill this\ngap by investigating privacy leakage caused by EME relying on proprietary and\nclosed-source DRM systems. We focus on Google Widevine because of its\nversatility and wide adoption. We conduct empirical experiments to show that\nbrowsers diverge when complying EME privacy guidelines, which might undermine\nusers' privacy. For instance, we find that many browsers gladly give away the\nidentifying Widevine Client ID with no or little explicit consent from users.\nMoreover, we characterize the privacy risks of users tracking when browsers\nmiss applying EME guidelines regarding privacy. Because of being closed-source,\nour work involves reverse engineering to dissect the contents of EME messages\nas instantiated by Widevine. Finally, we implement EME Track, a tool that\nautomatically exploits bad Widevine-based implementations to break privacy.",
        "translated": ""
    },
    {
        "title": "FINER: Enhancing State-of-the-art Classifiers with Feature Attribution\n  to Facilitate Security Analysis",
        "url": "http://arxiv.org/abs/2308.05362v1",
        "pub_date": "2023-08-10",
        "summary": "Deep learning classifiers achieve state-of-the-art performance in various\nrisk detection applications. They explore rich semantic representations and are\nsupposed to automatically discover risk behaviors. However, due to the lack of\ntransparency, the behavioral semantics cannot be conveyed to downstream\nsecurity experts to reduce their heavy workload in security analysis. Although\nfeature attribution (FA) methods can be used to explain deep learning, the\nunderlying classifier is still blind to what behavior is suspicious, and the\ngenerated explanation cannot adapt to downstream tasks, incurring poor\nexplanation fidelity and intelligibility. In this paper, we propose FINER, the\nfirst framework for risk detection classifiers to generate high-fidelity and\nhigh-intelligibility explanations. The high-level idea is to gather explanation\nefforts from model developer, FA designer, and security experts. To improve\nfidelity, we fine-tune the classifier with an explanation-guided multi-task\nlearning strategy. To improve intelligibility, we engage task knowledge to\nadjust and ensemble FA methods. Extensive evaluations show that FINER improves\nexplanation quality for risk detection. Moreover, we demonstrate that FINER\noutperforms a state-of-the-art tool in facilitating malware analysis.",
        "translated": ""
    },
    {
        "title": "Quantum-inspired Hash Function Based on Parity-dependent Quantum Walks\n  with Memory",
        "url": "http://arxiv.org/abs/2308.05357v1",
        "pub_date": "2023-08-10",
        "summary": "In this paper, we develop a generic controlled alternate quantum walk model\n(called CQWM-P) by combining parity-dependent quantum walks with distinct\narbitrary memory lengths and then construct a quantum-inspired hash function\n(called QHFM-P) based on this model. Numerical simulation shows that QHFM-P has\nnear-ideal statistical performance and is on a par with the state-of-the-art\nhash functions based on discrete quantum walks in terms of sensitivity of hash\nvalue to message, diffusion and confusion properties, uniform distribution\nproperty, and collision resistance property. Stability test illustrates that\nthe statistical properties of the proposed hash function are robust with\nrespect to the coin parameters, and theoretical analysis indicates that QHFM-P\nhas the same computational complexity as that of its peers.",
        "translated": ""
    },
    {
        "title": "Preemptive Detection of Fake Accounts on Social Networks via Multi-Class\n  Preferential Attachment Classifiers",
        "url": "http://arxiv.org/abs/2308.05353v1",
        "pub_date": "2023-08-10",
        "summary": "In this paper, we describe a new algorithm called Preferential Attachment\nk-class Classifier (PreAttacK) for detecting fake accounts in a social network.\nRecently, several algorithms have obtained high accuracy on this problem.\nHowever, they have done so by relying on information about fake accounts'\nfriendships or the content they share with others--the very things we seek to\nprevent.\n  PreAttacK represents a significant departure from these approaches. We\nprovide some of the first detailed distributional analyses of how new fake (and\nreal) accounts first attempt to request friends after joining a major network\n(Facebook). We show that even before a new account has made friends or shared\ncontent, these initial friend request behaviors evoke a natural multi-class\nextension of the canonical Preferential Attachment model of social network\ngrowth.\n  We use this model to derive a new algorithm, PreAttacK. We prove that in\nrelevant problem instances, PreAttacK near-optimally approximates the posterior\nprobability that a new account is fake under this multi-class Preferential\nAttachment model of new accounts' (not-yet-answered) friend requests. These are\nthe first provable guarantees for fake account detection that apply to new\nusers, and that do not require strong homophily assumptions.\n  This principled approach also makes PreAttacK the only algorithm with\nprovable guarantees that obtains state-of-the-art performance on new users on\nthe global Facebook network, where it converges to AUC=0.9 after new users send\n+ receive a total of just 20 not-yet-answered friend requests. For comparison,\nstate-of-the-art benchmarks do not obtain this AUC even after observing\nadditional data on new users' first 100 friend requests. Thus, unlike\nmainstream algorithms, PreAttacK converges before the median new fake account\nhas made a single friendship (accepted friend request) with a human.",
        "translated": ""
    },
    {
        "title": "Decentralized Finance (DeFi): A Survey",
        "url": "http://arxiv.org/abs/2308.05282v1",
        "pub_date": "2023-08-10",
        "summary": "Decentralized Finance (DeFi) is a new paradigm in the creation, distribution,\nand utilization of financial services via the integration of blockchain\ntechnology. Our research conducts a comprehensive introduction and meticulous\nclassification of various DeFi applications. Beyond that, we thoroughly analyze\nthese risks from both technical and economic perspectives, spanning multiple\nlayers. Lastly, we point out research directions in DeFi, encompassing areas of\ntechnological advancements, innovative economics, and privacy optimization.",
        "translated": ""
    },
    {
        "title": "Private Distribution Learning with Public Data: The View from Sample\n  Compression",
        "url": "http://arxiv.org/abs/2308.06239v1",
        "pub_date": "2023-08-11",
        "summary": "We study the problem of private distribution learning with access to public\ndata. In this setup, which we refer to as public-private learning, the learner\nis given public and private samples drawn from an unknown distribution $p$\nbelonging to a class $\\mathcal Q$, with the goal of outputting an estimate of\n$p$ while adhering to privacy constraints (here, pure differential privacy)\nonly with respect to the private samples.\n  We show that the public-private learnability of a class $\\mathcal Q$ is\nconnected to the existence of a sample compression scheme for $\\mathcal Q$, as\nwell as to an intermediate notion we refer to as list learning. Leveraging this\nconnection: (1) approximately recovers previous results on Gaussians over\n$\\mathbb R^d$; and (2) leads to new ones, including sample complexity upper\nbounds for arbitrary $k$-mixtures of Gaussians over $\\mathbb R^d$, results for\nagnostic and distribution-shift resistant learners, as well as closure\nproperties for public-private learnability under taking mixtures and products\nof distributions. Finally, via the connection to list learning, we show that\nfor Gaussians in $\\mathbb R^d$, at least $d$ public samples are necessary for\nprivate learnability, which is close to the known upper bound of $d+1$ public\nsamples.",
        "translated": ""
    },
    {
        "title": "SALSy: Security-Aware Layout Synthesis",
        "url": "http://arxiv.org/abs/2308.06201v1",
        "pub_date": "2023-08-11",
        "summary": "Integrated Circuits (ICs) are the target of diverse attacks during their\nlifetime. Fabrication-time attacks, such as the insertion of Hardware Trojans,\ncan give an adversary access to privileged data and/or the means to corrupt the\nIC's internal computation. Post-fabrication attacks, where the end-user takes a\nmalicious role, also attempt to obtain privileged information through means\nsuch as fault injection and probing. Taking these threats into account and at\nthe same time, this paper proposes a methodology for Security-Aware Layout\nSynthesis (SALSy), such that ICs can be designed with security in mind in the\nsame manner as power-performance-area (PPA) metrics are considered today, a\nconcept known as security closure. Furthermore, the trade-offs between PPA and\nsecurity are considered and a chip is fabricated in a 65nm CMOS commercial\ntechnology for validation purposes - a feature not seen in previous research on\nsecurity closure. Measurements on the fabricated ICs indicate that SALSy\npromotes a modest increase in power in order to achieve significantly improved\nsecurity metrics.",
        "translated": ""
    },
    {
        "title": "Physical Adversarial Attacks For Camera-based Smart Systems: Current\n  Trends, Categorization, Applications, Research Challenges, and Future Outlook",
        "url": "http://arxiv.org/abs/2308.06173v1",
        "pub_date": "2023-08-11",
        "summary": "In this paper, we present a comprehensive survey of the current trends\nfocusing specifically on physical adversarial attacks. We aim to provide a\nthorough understanding of the concept of physical adversarial attacks,\nanalyzing their key characteristics and distinguishing features. Furthermore,\nwe explore the specific requirements and challenges associated with executing\nattacks in the physical world. Our article delves into various physical\nadversarial attack methods, categorized according to their target tasks in\ndifferent applications, including classification, detection, face recognition,\nsemantic segmentation and depth estimation. We assess the performance of these\nattack methods in terms of their effectiveness, stealthiness, and robustness.\nWe examine how each technique strives to ensure the successful manipulation of\nDNNs while mitigating the risk of detection and withstanding real-world\ndistortions. Lastly, we discuss the current challenges and outline potential\nfuture research directions in the field of physical adversarial attacks. We\nhighlight the need for enhanced defense mechanisms, the exploration of novel\nattack strategies, the evaluation of attacks in different application domains,\nand the establishment of standardized benchmarks and evaluation criteria for\nphysical adversarial attacks. Through this comprehensive survey, we aim to\nprovide a valuable resource for researchers, practitioners, and policymakers to\ngain a holistic understanding of physical adversarial attacks in computer\nvision and facilitate the development of robust and secure DNN-based systems.",
        "translated": ""
    },
    {
        "title": "A Uniform Representation of Classical and Quantum Source Code for Static\n  Code Analysis",
        "url": "http://arxiv.org/abs/2308.06113v1",
        "pub_date": "2023-08-11",
        "summary": "The emergence of quantum computing raises the question of how to identify\n(security-relevant) programming errors during development. However, current\nstatic code analysis tools fail to model information specific to quantum\ncomputing. In this paper, we identify this information and propose to extend\nclassical code analysis tools accordingly. Among such tools, we identify the\nCode Property Graph to be very well suited for this task as it can be easily\nextended with quantum computing specific information. For our proof of concept,\nwe implemented a tool which includes information from the quantum world in the\ngraph and demonstrate its ability to analyze source code written in Qiskit and\nOpenQASM. Our tool brings together the information from the classical and\nquantum world, enabling analysis across both domains. By combining all relevant\ninformation into a single detailed analysis, this powerful tool can facilitate\ntackling future quantum source code analysis challenges.",
        "translated": ""
    },
    {
        "title": "Test-Time Adaptation for Backdoor Defense",
        "url": "http://arxiv.org/abs/2308.06107v1",
        "pub_date": "2023-08-11",
        "summary": "Deep neural networks have played a crucial part in many critical domains,\nsuch as autonomous driving, face recognition, and medical diagnosis. However,\ndeep neural networks are facing security threats from backdoor attacks and can\nbe manipulated into attacker-decided behaviors by the backdoor attacker. To\ndefend the backdoor, prior research has focused on using clean data to remove\nbackdoor attacks before model deployment. In this paper, we investigate the\npossibility of defending against backdoor attacks at test time by utilizing\npartially poisoned data to remove the backdoor from the model. To address the\nproblem, a two-stage method Test-Time Backdoor Defense (TTBD) is proposed. In\nthe first stage, we propose two backdoor sample detection methods, namely DDP\nand TeCo, to identify poisoned samples from a batch of mixed, partially\npoisoned samples. Once the poisoned samples are detected, we employ Shapley\nestimation to calculate the contribution of each neuron's significance in the\nnetwork, locate the poisoned neurons, and prune them to remove backdoor in the\nmodels. Our experiments demonstrate that TTBD removes the backdoor successfully\nwith only a batch of partially poisoned data across different model\narchitectures and datasets against different types of backdoor attacks.",
        "translated": ""
    },
    {
        "title": "Security of XCB and HCTR",
        "url": "http://arxiv.org/abs/2308.06082v1",
        "pub_date": "2023-08-11",
        "summary": "Tweakable Enciphering Scheme (TES) is a length preserving scheme which\nprovides confidentiality and admissible integrity. XCB (Extended Code Book) is\na TES which was introduced in 2004. In 2007, it was modified and security bound\nwas provided. Later, these two versions were referred to as XCBv1 and XCBv2\nrespectively. XCBv2 was proposed as the IEEE-std 1619.2 2010 for encryption of\nsector oriented storage media. In 2013, first time Security bound of XCBv1 was\ngiven and XCBv2's security bound was enhanced. A constant of $2^{22}$ appears\nin the security bounds of the XCBv1 and XCBv2.\n  We showed that this constant of $2^{22}$ can be reduced to $2^{5}$. Further,\nwe modified the XCB (MXCB) scheme such that it gives better security bound\ncompared to the present XCB scheme. We also analyzed some weak keys attack on\nXCB and a type of TES known as HCTR (proposed in 2005). We performed\ndistinguishing attack and the hash key recovery attack on HCTR. Next, we\nanalyzed the dependency of the two different keys in HCTR.",
        "translated": ""
    },
    {
        "title": "Face Encryption via Frequency-Restricted Identity-Agnostic Attacks",
        "url": "http://arxiv.org/abs/2308.05983v1",
        "pub_date": "2023-08-11",
        "summary": "Billions of people are sharing their daily live images on social media\neveryday. However, malicious collectors use deep face recognition systems to\neasily steal their biometric information (e.g., faces) from these images. Some\nstudies are being conducted to generate encrypted face photos using adversarial\nattacks by introducing imperceptible perturbations to reduce face information\nleakage. However, existing studies need stronger black-box scenario feasibility\nand more natural visual appearances, which challenge the feasibility of privacy\nprotection. To address these problems, we propose a frequency-restricted\nidentity-agnostic (FRIA) framework to encrypt face images from unauthorized\nface recognition without access to personal information. As for the weak\nblack-box scenario feasibility, we obverse that representations of the average\nfeature in multiple face recognition models are similar, thus we propose to\nutilize the average feature via the crawled dataset from the Internet as the\ntarget to guide the generation, which is also agnostic to identities of unknown\nface recognition systems; in nature, the low-frequency perturbations are more\nvisually perceptible by the human vision system. Inspired by this, we restrict\nthe perturbation in the low-frequency facial regions by discrete cosine\ntransform to achieve the visual naturalness guarantee. Extensive experiments on\nseveral face recognition models demonstrate that our FRIA outperforms other\nstate-of-the-art methods in generating more natural encrypted faces while\nattaining high black-box attack success rates of 96%. In addition, we validate\nthe efficacy of FRIA using real-world black-box commercial API, which reveals\nthe potential of FRIA in practice. Our codes can be found in\nhttps://github.com/XinDong10/FRIA.",
        "translated": ""
    },
    {
        "title": "CyberForce: A Federated Reinforcement Learning Framework for Malware\n  Mitigation",
        "url": "http://arxiv.org/abs/2308.05978v1",
        "pub_date": "2023-08-11",
        "summary": "The expansion of the Internet-of-Things (IoT) paradigm is inevitable, but\nvulnerabilities of IoT devices to malware incidents have become an increasing\nconcern. Recent research has shown that the integration of Reinforcement\nLearning with Moving Target Defense (MTD) mechanisms can enhance cybersecurity\nin IoT devices. Nevertheless, the numerous new malware attacks and the time\nthat agents take to learn and select effective MTD techniques make this\napproach impractical for real-world IoT scenarios. To tackle this issue, this\nwork presents CyberForce, a framework that employs Federated Reinforcement\nLearning (FRL) to collectively and privately determine suitable MTD techniques\nfor mitigating diverse zero-day attacks. CyberForce integrates device\nfingerprinting and anomaly detection to reward or penalize MTD mechanisms\nchosen by an FRL-based agent. The framework has been evaluated in a federation\nconsisting of ten devices of a real IoT platform. A pool of experiments with\nsix malware samples affecting the devices has demonstrated that CyberForce can\nprecisely learn optimum MTD mitigation strategies. When all clients are\naffected by all attacks, the FRL agent exhibits high accuracy and reduced\ntraining time when compared to a centralized RL agent. In cases where different\nclients experience distinct attacks, the CyberForce clients gain benefits\nthrough the transfer of knowledge from other clients and similar attack\nbehavior. Additionally, CyberForce showcases notable robustness against data\npoisoning attacks.",
        "translated": ""
    },
    {
        "title": "Blockchain-Based Transferable Digital Rights of Land",
        "url": "http://arxiv.org/abs/2308.05950v1",
        "pub_date": "2023-08-11",
        "summary": "Land, being a scarce and valuable resource, is in high demand, especially in\ndensely populated areas of older cities. Development authorities require land\nfor infrastructure projects and other amenities, while landowners hold onto\ntheir land for both its usage and its financial value. Transferable Development\nRights (TDRs) serve as a mechanism to separate the development rights\nassociated with the land from the physical land itself. Development authorities\nacquire the land by offering compensation in the form of TDRs, which hold\nmonetary value. In this paper, we present the tokenization of development\nrights, focusing on the implementation in collaboration with a development\nauthority. While there have been previous implementations of land tokenization,\nwe believe our approach is the first to tokenize development rights\nspecifically. Our implementation addresses practical challenges related to\nrecord-keeping, ground verification of land, and the unique identification of\nstakeholders. We ensure the accurate evaluation of development rights by\nincorporating publicly available circle rates, which consider the ground\ndevelopment of the land and its surrounding areas.",
        "translated": ""
    },
    {
        "title": "FLShield: A Validation Based Federated Learning Framework to Defend\n  Against Poisoning Attacks",
        "url": "http://arxiv.org/abs/2308.05832v1",
        "pub_date": "2023-08-10",
        "summary": "Federated learning (FL) is revolutionizing how we learn from data. With its\ngrowing popularity, it is now being used in many safety-critical domains such\nas autonomous vehicles and healthcare. Since thousands of participants can\ncontribute in this collaborative setting, it is, however, challenging to ensure\nsecurity and reliability of such systems. This highlights the need to design FL\nsystems that are secure and robust against malicious participants' actions\nwhile also ensuring high utility, privacy of local data, and efficiency. In\nthis paper, we propose a novel FL framework dubbed as FLShield that utilizes\nbenign data from FL participants to validate the local models before taking\nthem into account for generating the global model. This is in stark contrast\nwith existing defenses relying on server's access to clean datasets -- an\nassumption often impractical in real-life scenarios and conflicting with the\nfundamentals of FL. We conduct extensive experiments to evaluate our FLShield\nframework in different settings and demonstrate its effectiveness in thwarting\nvarious types of poisoning and backdoor attacks including a defense-aware one.\nFLShield also preserves privacy of local data against gradient inversion\nattacks.",
        "translated": ""
    },
    {
        "title": "Reinforcing Security and Usability of Crypto-Wallet with Post-Quantum\n  Cryptography and Zero-Knowledge Proof",
        "url": "http://arxiv.org/abs/2308.07309v1",
        "pub_date": "2023-08-14",
        "summary": "Crypto-wallets or digital asset wallets are a crucial aspect of managing\ncryptocurrencies and other digital assets such as NFTs. However, these wallets\nare not immune to security threats, particularly from the growing risk of\nquantum computing. The use of traditional public-key cryptography systems in\ndigital asset wallets makes them vulnerable to attacks from quantum computers,\nwhich may increase in the future. Moreover, current digital wallets require\nusers to keep track of seed-phrases, which can be challenging and lead to\nadditional security risks. To overcome these challenges, a new algorithm is\nproposed that uses post-quantum cryptography (PQC) and zero-knowledge proof\n(ZKP) to enhance the security of digital asset wallets. The research focuses on\nthe use of the Lattice-based Threshold Secret Sharing Scheme (LTSSS), Kyber\nAlgorithm for key generation and ZKP for wallet unlocking, providing a more\nsecure and user-friendly alternative to seed-phrase, brain and multi-sig\nprotocol wallets. This algorithm also includes several innovative security\nfeatures such as recovery of wallets in case of downtime of the server, and the\nability to rekey the private key associated with a specific username-password\ncombination, offering improved security and usability. The incorporation of PQC\nand ZKP provides a robust and comprehensive framework for securing digital\nassets in the present and future. This research aims to address the security\nchallenges faced by digital asset wallets and proposes practical solutions to\nensure their safety in the era of quantum computing.",
        "translated": ""
    },
    {
        "title": "BehaVR: User Identification Based on VR Sensor Data",
        "url": "http://arxiv.org/abs/2308.07304v1",
        "pub_date": "2023-08-14",
        "summary": "Virtual reality (VR) platforms enable a wide range of applications, however\npose unique privacy risks. In particular, VR devices are equipped with a rich\nset of sensors that collect personal and sensitive information (e.g., body\nmotion, eye gaze, hand joints, and facial expression), which can be used to\nuniquely identify a user, even without explicit identifiers. In this paper, we\nare interested in understanding the extent to which a user can be identified\nbased on data collected by different VR sensors. We consider adversaries with\ncapabilities that range from observing APIs available within a single VR app\n(app adversary) to observing all, or selected, sensor measurements across all\napps on the VR device (device adversary). To that end, we introduce BEHAVR, a\nframework for collecting and analyzing data from all sensor groups collected by\nall apps running on a VR device. We use BEHAVR to perform a user study and\ncollect data from real users that interact with popular real-world apps. We use\nthat data to build machine learning models for user identification, with\nfeatures extracted from sensor data available within and across apps. We show\nthat these models can identify users with an accuracy of up to 100%, and we\nreveal the most important features and sensor groups, depending on the\nfunctionality of the app and the strength of the adversary, as well as the\nminimum time needed for user identification. To the best of our knowledge,\nBEHAVR is the first to analyze user identification in VR comprehensively, i.e.,\nconsidering jointly all sensor measurements available on a VR device (whether\nwithin an app or across multiple apps), collected by real-world, as opposed to\ncustom-made, apps.",
        "translated": ""
    },
    {
        "title": "Towards a Cloud-Based Ontology for Service Model Security -- Technical\n  Report",
        "url": "http://arxiv.org/abs/2308.07096v1",
        "pub_date": "2023-08-14",
        "summary": "The adoption of cloud computing has brought significant advancements in the\noperational models of businesses. However, this shift also brings new security\nchallenges by expanding the attack surface. The offered services in cloud\ncomputing have various service models. Each cloud service model has a defined\nresponsibility divided based on the stack layers between the service user and\ntheir cloud provider. Regardless of its service model, each service is\nconstructed from sub-components and services running on the underlying layers.\nIn this paper, we aim to enable more transparency and visibility by designing\nan ontology that links the provider's services with the sub-components used to\ndeliver the service. Such breakdown for each cloud service sub-components\nenables the end user to track the vulnerabilities on the service level or one\nof its sub-components. Such information can result in a better understanding\nand management of reported vulnerabilities on the sub-components level and\ntheir impact on the offered services by the cloud provider. Our ontology and\nsource code are published as an open-source and accessible via GitHub:\n\\href{https://github.com/mohkharma/cc-ontology}{mohkharma/cc-ontology}",
        "translated": ""
    },
    {
        "title": "Secure and Dynamic Publish/Subscribe: LCMsec",
        "url": "http://arxiv.org/abs/2308.07095v1",
        "pub_date": "2023-08-14",
        "summary": "We propose LCMsec, a brokerless, decentralised Publish/Subscribe protocol. It\naims to provide low-latency and high-throughput message-passing for IoT and\nautomotive applications while providing much-needed security functionalities to\ncombat emerging cyber-attacks in that domain. LCMsec is an extension for the\nLightweight Communications and Marshalling (LCM) protocol. We extend this\nprotocol by providing not only authenticated encryption of the messages in\ntransit, but also a group discovery protocol inspired by the Raft consensus\nprotocol. The Dutta-Barua group key agreement is used to agree upon a shared\nsymmetric key among subscribers and publishers on a topic. By using a shared\ngroup key, we reduce the key agreement overhead and the number of message\nauthentication codes (MACs) per message compared to existing proposals for\nsecure brokerless Publish/Subscribe protocols, which establish a symmetric key\nbetween each publisher and subscriber and append multiple MACs to each message.",
        "translated": ""
    },
    {
        "title": "Understanding Hackers' Work: An Empirical Study of Offensive Security\n  Practitioners",
        "url": "http://arxiv.org/abs/2308.07057v1",
        "pub_date": "2023-08-14",
        "summary": "Offensive security-tests are a common way to pro-actively discover potential\nvulnerabilities. They are performed by specialists, often called\npenetration-testers or white-hat hackers. The chronic lack of available\nwhite-hat hackers prevents sufficient security test coverage of software.\nResearch into automation tries to alleviate this problem by improving the\nefficiency of security testing. To achieve this, researchers and tool builders\nneed a solid understanding of how hackers work, their assumptions, and pain\npoints.\n  In this paper, we present a first data-driven exploratory qualitative study\nof twelve security professionals, their work and problems occurring therein. We\nperform a thematic analysis to gain insights into the execution of security\nassignments, hackers' thought processes and encountered challenges.\n  This analysis allows us to conclude with recommendations for researchers and\ntool builders to increase the efficiency of their automation and identify novel\nareas for research.",
        "translated": ""
    },
    {
        "title": "Routing Recovery for UAV Networks with Deliberate Attacks: A\n  Reinforcement Learning based Approach",
        "url": "http://arxiv.org/abs/2308.06973v1",
        "pub_date": "2023-08-14",
        "summary": "The unmanned aerial vehicle (UAV) network is popular these years due to its\nvarious applications. In the UAV network, routing is significantly affected by\nthe distributed network topology, leading to the issue that UAVs are vulnerable\nto deliberate damage. Hence, this paper focuses on the routing plan and\nrecovery for UAV networks with attacks. In detail, a deliberate attack model\nbased on the importance of nodes is designed to represent enemy attacks. Then,\na node importance ranking mechanism is presented, considering the degree of\nnodes and link importance. However, it is intractable to handle the routing\nproblem by traditional methods for UAV networks, since link connections change\nwith the UAV availability. Hence, an intelligent algorithm based on\nreinforcement learning is proposed to recover the routing path when UAVs are\nattacked. Simulations are conducted and numerical results verify the proposed\nmechanism performs better than other referred methods.",
        "translated": ""
    },
    {
        "title": "Security Analysis of Filecoin's Expected Consensus in the Byzantine vs\n  Honest Model",
        "url": "http://arxiv.org/abs/2308.06955v1",
        "pub_date": "2023-08-14",
        "summary": "Filecoin is the largest storage-based open-source blockchain, both by storage\ncapacity (&gt;11EiB) and market capitalization. This paper provides the first\nformal security analysis of Filecoin's consensus (ordering) protocol, Expected\nConsensus (EC). Specifically, we show that EC is secure against an arbitrary\nadversary that controls a fraction $\\beta$ of the total storage for $\\beta m&lt;\n1- e^{-(1-\\beta)m}$, where $m$ is a parameter that corresponds to the expected\nnumber of blocks per round, currently $m=5$ in Filecoin. We then present an\nattack, the $n$-split attack, where an adversary splits the honest miners\nbetween multiple chains, and show that it is successful for $\\beta m \\ge 1-\ne^{-(1-\\beta)m}$, thus proving that $\\beta m= 1- e^{-(1-\\beta)m}$ is the tight\nsecurity threshold of EC. This corresponds roughly to an adversary with $20\\%$\nof the total storage pledged to the chain. Finally, we propose two improvements\nto EC security that would increase this threshold. One of these two fixes is\nbeing implemented as a Filecoin Improvement Proposal (FIP).",
        "translated": ""
    },
    {
        "title": "DIVAS: An LLM-based End-to-End Framework for SoC Security Analysis and\n  Policy-based Protection",
        "url": "http://arxiv.org/abs/2308.06932v1",
        "pub_date": "2023-08-14",
        "summary": "Securing critical assets in a bus-based System-On-Chip (SoC) is imperative to\nmitigate potential vulnerabilities and prevent unauthorized access, ensuring\nthe integrity, availability, and confidentiality of the system. Ensuring\nsecurity throughout the SoC design process is a formidable task owing to the\ninherent intricacies in SoC designs and the dispersion of assets across diverse\nIPs. Large Language Models (LLMs), exemplified by ChatGPT (OpenAI) and BARD\n(Google), have showcased remarkable proficiency across various domains,\nincluding security vulnerability detection and prevention in SoC designs. In\nthis work, we propose DIVAS, a novel framework that leverages the knowledge\nbase of LLMs to identify security vulnerabilities from user-defined SoC\nspecifications, map them to the relevant Common Weakness Enumerations (CWEs),\nfollowed by the generation of equivalent assertions, and employ security\nmeasures through enforcement of security policies. The proposed framework is\nimplemented using multiple ChatGPT and BARD models, and their performance was\nanalyzed while generating relevant CWEs from the SoC specifications provided.\nThe experimental results obtained from open-source SoC benchmarks demonstrate\nthe efficacy of our proposed framework.",
        "translated": ""
    },
    {
        "title": "FedEdge AI-TC: A Semi-supervised Traffic Classification Method based on\n  Trusted Federated Deep Learning for Mobile Edge Computing",
        "url": "http://arxiv.org/abs/2308.06924v1",
        "pub_date": "2023-08-14",
        "summary": "As a typical entity of MEC (Mobile Edge Computing), 5G CPE (Customer Premise\nEquipment)/HGU (Home Gateway Unit) has proven to be a promising alternative to\ntraditional Smart Home Gateway. Network TC (Traffic Classification) is a vital\nservice quality assurance and security management method for communication\nnetworks, which has become a crucial functional entity in 5G CPE/HGU. In recent\nyears, many researchers have applied Machine Learning or Deep Learning (DL) to\nTC, namely AI-TC, to improve its performance. However, AI-TC faces challenges,\nincluding data dependency, resource-intensive traffic labeling, and user\nprivacy concerns. The limited computing resources of 5G CPE further complicate\nefficient classification. Moreover, the \"black box\" nature of AI-TC models\nraises transparency and credibility issues. The paper proposes the FedEdge\nAI-TC framework, leveraging Federated Learning (FL) for reliable Network TC in\n5G CPE. FL ensures privacy by employing local training, model parameter\niteration, and centralized training. A semi-supervised TC algorithm based on\nVariational Auto-Encoder (VAE) and convolutional neural network (CNN) reduces\ndata dependency while maintaining accuracy. To optimize model light-weight\ndeployment, the paper introduces XAI-Pruning, an AI model compression method\ncombined with DL model interpretability. Experimental evaluation demonstrates\nFedEdge AI-TC's superiority over benchmarks in terms of accuracy and efficient\nTC performance. The framework enhances user privacy and model credibility,\noffering a comprehensive solution for dependable and transparent Network TC in\n5G CPE, thus enhancing service quality and security.",
        "translated": ""
    },
    {
        "title": "Federated Classification in Hyperbolic Spaces via Secure Aggregation of\n  Convex Hulls",
        "url": "http://arxiv.org/abs/2308.06895v1",
        "pub_date": "2023-08-14",
        "summary": "Hierarchical and tree-like data sets arise in many applications, including\nlanguage processing, graph data mining, phylogeny and genomics. It is known\nthat tree-like data cannot be embedded into Euclidean spaces of finite\ndimension with small distortion. This problem can be mitigated through the use\nof hyperbolic spaces. When such data also has to be processed in a distributed\nand privatized setting, it becomes necessary to work with new federated\nlearning methods tailored to hyperbolic spaces. As an initial step towards the\ndevelopment of the field of federated learning in hyperbolic spaces, we propose\nthe first known approach to federated classification in hyperbolic spaces. Our\ncontributions are as follows. First, we develop distributed versions of convex\nSVM classifiers for Poincar\\'e discs. In this setting, the information conveyed\nfrom clients to the global classifier are convex hulls of clusters present in\nindividual client data. Second, to avoid label switching issues, we introduce a\nnumber-theoretic approach for label recovery based on the so-called integer\n$B_h$ sequences. Third, we compute the complexity of the convex hulls in\nhyperbolic spaces to assess the extent of data leakage; at the same time, in\norder to limit the communication cost for the hulls, we propose a new\nquantization method for the Poincar\\'e disc coupled with Reed-Solomon-like\nencoding. Fourth, at server level, we introduce a new approach for aggregating\nconvex hulls of the clients based on balanced graph partitioning. We test our\nmethod on a collection of diverse data sets, including hierarchical single-cell\nRNA-seq data from different patients distributed across different repositories\nthat have stringent privacy constraints. The classification accuracy of our\nmethod is up to $\\sim 11\\%$ better than its Euclidean counterpart,\ndemonstrating the importance of privacy-preserving learning in hyperbolic\nspaces.",
        "translated": ""
    },
    {
        "title": "Domain-Adaptive Device Fingerprints for Network Access Authentication\n  Through Multifractal Dimension Representation",
        "url": "http://arxiv.org/abs/2308.07925v1",
        "pub_date": "2023-08-15",
        "summary": "RF data-driven device fingerprinting through the use of deep learning has\nrecently surfaced as a potential solution for automated network access\nauthentication. Traditional approaches are commonly susceptible to the domain\nadaptation problem where a model trained on data from one domain performs badly\nwhen tested on data from a different domain. Some examples of a domain change\ninclude varying the device location or environment and varying the time or day\nof data collection. In this work, we propose using multifractal analysis and\nthe variance fractal dimension trajectory (VFDT) as a data representation input\nto the deep neural network to extract device fingerprints that are domain\ngeneralizable. We analyze the effectiveness of the proposed VFDT representation\nin detecting device-specific signatures from hardware-impaired IQ signals, and\nevaluate its robustness in real-world settings, using an experimental testbed\nof 30 WiFi-enabled Pycom devices under different locations and at different\nscales. Our results show that the VFDT representation improves the scalability,\nrobustness and generalizability of the deep learning models significantly\ncompared to when using raw IQ data.",
        "translated": ""
    },
    {
        "title": "SplITS: Split Input-to-State Mapping for Effective Firmware Fuzzing",
        "url": "http://arxiv.org/abs/2308.07860v1",
        "pub_date": "2023-08-15",
        "summary": "Ability to test firmware on embedded devices is critical to discovering\nvulnerabilities prior to their adversarial exploitation. State-of-the-art\nautomated testing methods rehost firmware in emulators and attempt to\nfacilitate inputs from a diversity of methods (interrupt driven, status\npolling) and a plethora of devices (such as modems and GPS units). Despite\nrecent progress to tackle peripheral input generation challenges in rehosting,\na firmware's expectation of multi-byte magic values supplied from peripheral\ninputs for string operations still pose a significant roadblock. We solve the\nimpediment posed by multi-byte magic strings in monolithic firmware. We propose\nfeedback mechanisms for input-to-state mapping and retaining seeds for targeted\nreplacement mutations with an efficient method to solve multi-byte comparisons.\nThe feedback allows an efficient search over a combinatorial solution-space. We\nevaluate our prototype implementation, SplITS, with a diverse set of 21\nreal-world monolithic firmware binaries used in prior works, and 3 new binaries\nfrom popular open source projects. SplITS automatically solves 497% more\nmulti-byte magic strings guarding further execution to uncover new code and\nbugs compared to state-of-the-art. In 11 of the 12 real-world firmware binaries\nwith string comparisons, including those extensively analyzed by prior works,\nSplITS outperformed, statistically significantly. We observed up to 161%\nincrease in blocks covered and discovered 6 new bugs that remained guarded by\nstring comparisons. Significantly, deep and difficult to reproduce bugs guarded\nby comparisons, identified in prior work, were found consistently. To\nfacilitate future research in the field, we release SplITS, the new firmware\ndata sets, and bug analysis at https://github.com/SplITS-Fuzzer",
        "translated": ""
    },
    {
        "title": "Robustness Over Time: Understanding Adversarial Examples' Effectiveness\n  on Longitudinal Versions of Large Language Models",
        "url": "http://arxiv.org/abs/2308.07847v1",
        "pub_date": "2023-08-15",
        "summary": "Large Language Models (LLMs) have led to significant improvements in many\ntasks across various domains, such as code interpretation, response generation,\nand ambiguity handling. These LLMs, however, when upgrading, primarily\nprioritize enhancing user experience while neglecting security, privacy, and\nsafety implications. Consequently, unintended vulnerabilities or biases can be\nintroduced. Previous studies have predominantly focused on specific versions of\nthe models and disregard the potential emergence of new attack vectors\ntargeting the updated versions. Through the lens of adversarial examples within\nthe in-context learning framework, this longitudinal study addresses this gap\nby conducting a comprehensive assessment of the robustness of successive\nversions of LLMs, vis-\\`a-vis GPT-3.5. We conduct extensive experiments to\nanalyze and understand the impact of the robustness in two distinct learning\ncategories: zero-shot learning and few-shot learning. Our findings indicate\nthat, in comparison to earlier versions of LLMs, the updated versions do not\nexhibit the anticipated level of robustness against adversarial attacks. In\naddition, our study emphasizes the increased effectiveness of synergized\nadversarial queries in most zero-shot learning and few-shot learning cases. We\nhope that our study can lead to a more refined assessment of the robustness of\nLLMs over time and provide valuable insights of these models for both\ndevelopers and users.",
        "translated": ""
    },
    {
        "title": "Simple and Efficient Partial Graph Adversarial Attack: A New Perspective",
        "url": "http://arxiv.org/abs/2308.07834v1",
        "pub_date": "2023-08-15",
        "summary": "As the study of graph neural networks becomes more intensive and\ncomprehensive, their robustness and security have received great research\ninterest. The existing global attack methods treat all nodes in the graph as\ntheir attack targets. Although existing methods have achieved excellent\nresults, there is still considerable space for improvement. The key problem is\nthat the current approaches rigidly follow the definition of global attacks.\nThey ignore an important issue, i.e., different nodes have different robustness\nand are not equally resilient to attacks. From a global attacker's view, we\nshould arrange the attack budget wisely, rather than wasting them on highly\nrobust nodes. To this end, we propose a totally new method named partial graph\nattack (PGA), which selects the vulnerable nodes as attack targets. First, to\nselect the vulnerable items, we propose a hierarchical target selection policy,\nwhich allows attackers to only focus on easy-to-attack nodes. Then, we propose\na cost-effective anchor-picking policy to pick the most promising anchors for\nadding or removing edges, and a more aggressive iterative greedy-based attack\nmethod to perform more efficient attacks. Extensive experimental results\ndemonstrate that PGA can achieve significant improvements in both attack effect\nand attack efficiency compared to other existing graph global attack methods.",
        "translated": ""
    },
    {
        "title": "Fairness and Privacy in Federated Learning and Their Implications in\n  Healthcare",
        "url": "http://arxiv.org/abs/2308.07805v1",
        "pub_date": "2023-08-15",
        "summary": "Currently, many contexts exist where distributed learning is difficult or\notherwise constrained by security and communication limitations. One common\ndomain where this is a consideration is in Healthcare where data is often\ngoverned by data-use-ordinances like HIPAA. On the other hand, larger sample\nsizes and shared data models are necessary to allow models to better generalize\non account of the potential for more variability and balancing underrepresented\nclasses. Federated learning is a type of distributed learning model that allows\ndata to be trained in a decentralized manner. This, in turn, addresses data\nsecurity, privacy, and vulnerability considerations as data itself is not\nshared across a given learning network nodes. Three main challenges to\nfederated learning include node data is not independent and identically\ndistributed (iid), clients requiring high levels of communication overhead\nbetween peers, and there is the heterogeneity of different clients within a\nnetwork with respect to dataset bias and size. As the field has grown, the\nnotion of fairness in federated learning has also been introduced through novel\nimplementations. Fairness approaches differ from the standard form of federated\nlearning and also have distinct challenges and considerations for the\nhealthcare domain. This paper endeavors to outline the typical lifecycle of\nfair federated learning in research as well as provide an updated taxonomy to\naccount for the current state of fairness in implementations. Lastly, this\npaper provides added insight into the implications and challenges of\nimplementing and supporting fairness in federated learning in the healthcare\ndomain.",
        "translated": ""
    },
    {
        "title": "Quantum and Classical Combinatorial Optimizations Applied to\n  Lattice-Based Factorization",
        "url": "http://arxiv.org/abs/2308.07804v1",
        "pub_date": "2023-08-15",
        "summary": "The availability of working quantum computers has led to several proposals\nand claims of quantum advantage. In 2023, this has included claims that quantum\ncomputers can successfully factor large integers, by optimizing the search for\nnearby integers whose prime factors are all small.\n  This paper demonstrates that the hope of factoring numbers of commercial\nsignificance using these methods is unfounded. Mathematically, this is because\nthe density of smooth numbers (numbers all of whose prime factors are small)\ndecays exponentially as n grows. Our experimental reproductions and analysis\nshow that lattice-based factoring does not scale successfully to larger\nnumbers, that the proposed quantum enhancements do not alter this conclusion,\nand that other simpler classical optimization heuristics perform much better\nfor lattice-based factoring.\n  However, many topics in this area have interesting applications and\nmathematical challenges, independently of factoring itself. We consider\nparticular cases of the CVP, and opportunities for applying quantum techniques\nto other parts of the factorization pipeline, including the solution of linear\nequations modulo 2. Though the goal of factoring 1000-bit numbers is still\nout-of-reach, the combinatoric landscape is promising, and warrants further\nresearch with more circumspect objectives.",
        "translated": ""
    },
    {
        "title": "A Scalable Formal Verification Methodology for Data-Oblivious Hardware",
        "url": "http://arxiv.org/abs/2308.07757v1",
        "pub_date": "2023-08-15",
        "summary": "The importance of preventing microarchitectural timing side channels in\nsecurity-critical applications has surged in recent years. Constant-time\nprogramming has emerged as a best-practice technique for preventing the leakage\nof secret information through timing. It is based on the assumption that the\ntiming of certain basic machine instructions is independent of their respective\ninput data. However, whether or not an instruction satisfies this\ndata-independent timing criterion varies between individual processor\nmicroarchitectures. In this paper, we propose a novel methodology to formally\nverify data-oblivious behavior in hardware using standard property checking\ntechniques. The proposed methodology is based on an inductive property that\nenables scalability even to complex out-of-order cores. We show that proving\nthis inductive property is sufficient to exhaustively verify data-obliviousness\nat the microarchitectural level. In addition, the paper discusses several\ntechniques that can be used to make the verification process easier and faster.\nWe demonstrate the feasibility of the proposed methodology through case studies\non several open-source designs. One case study uncovered a data-dependent\ntiming violation in the extensively verified and highly secure IBEX RISC-V\ncore. In addition to several hardware accelerators and in-order processors, our\nexperiments also include RISC-V BOOM, a complex out-of-order processor,\nhighlighting the scalability of the approach.",
        "translated": ""
    },
    {
        "title": "Backpropagation Path Search On Adversarial Transferability",
        "url": "http://arxiv.org/abs/2308.07625v1",
        "pub_date": "2023-08-15",
        "summary": "Deep neural networks are vulnerable to adversarial examples, dictating the\nimperativeness to test the model's robustness before deployment. Transfer-based\nattackers craft adversarial examples against surrogate models and transfer them\nto victim models deployed in the black-box situation. To enhance the\nadversarial transferability, structure-based attackers adjust the\nbackpropagation path to avoid the attack from overfitting the surrogate model.\nHowever, existing structure-based attackers fail to explore the convolution\nmodule in CNNs and modify the backpropagation graph heuristically, leading to\nlimited effectiveness. In this paper, we propose backPropagation pAth Search\n(PAS), solving the aforementioned two problems. We first propose SkipConv to\nadjust the backpropagation path of convolution by structural\nreparameterization. To overcome the drawback of heuristically designed\nbackpropagation paths, we further construct a DAG-based search space, utilize\none-step approximation for path evaluation and employ Bayesian Optimization to\nsearch for the optimal path. We conduct comprehensive experiments in a wide\nrange of transfer settings, showing that PAS improves the attack success rate\nby a huge margin for both normally trained and defense models.",
        "translated": ""
    },
    {
        "title": "Block-Wise Encryption for Reliable Vision Transformer models",
        "url": "http://arxiv.org/abs/2308.07612v1",
        "pub_date": "2023-08-15",
        "summary": "This article presents block-wise image encryption for the vision transformer\nand its applications. Perceptual image encryption for deep learning enables us\nnot only to protect the visual information of plain images but to also embed\nunique features controlled with a key into images and models. However, when\nusing conventional perceptual encryption methods, the performance of models is\ndegraded due to the influence of encryption. In this paper, we focus on\nblock-wise encryption for the vision transformer, and we introduce three\napplications: privacy-preserving image classification, access control, and the\ncombined use of federated learning and encrypted images. Our scheme can have\nthe same performance as models without any encryption, and it does not require\nany network modification. It also allows us to easily update the secret key. In\nexperiments, the effectiveness of the scheme is demonstrated in terms of\nperformance degradation and access control on the CIFAR10 and CIFAR-100\ndatasets.",
        "translated": ""
    },
    {
        "title": "Enhancing the Antidote: Improved Pointwise Certifications against\n  Poisoning Attacks",
        "url": "http://arxiv.org/abs/2308.07553v1",
        "pub_date": "2023-08-15",
        "summary": "Poisoning attacks can disproportionately influence model behaviour by making\nsmall changes to the training corpus. While defences against specific poisoning\nattacks do exist, they in general do not provide any guarantees, leaving them\npotentially countered by novel attacks. In contrast, by examining worst-case\nbehaviours Certified Defences make it possible to provide guarantees of the\nrobustness of a sample against adversarial attacks modifying a finite number of\ntraining samples, known as pointwise certification. We achieve this by\nexploiting both Differential Privacy and the Sampled Gaussian Mechanism to\nensure the invariance of prediction for each testing instance against finite\nnumbers of poisoned examples. In doing so, our model provides guarantees of\nadversarial robustness that are more than twice as large as those provided by\nprior certifications.",
        "translated": ""
    },
    {
        "title": "Test-Time Poisoning Attacks Against Test-Time Adaptation Models",
        "url": "http://arxiv.org/abs/2308.08505v1",
        "pub_date": "2023-08-16",
        "summary": "Deploying machine learning (ML) models in the wild is challenging as it\nsuffers from distribution shifts, where the model trained on an original domain\ncannot generalize well to unforeseen diverse transfer domains. To address this\nchallenge, several test-time adaptation (TTA) methods have been proposed to\nimprove the generalization ability of the target pre-trained models under test\ndata to cope with the shifted distribution. The success of TTA can be credited\nto the continuous fine-tuning of the target model according to the\ndistributional hint from the test samples during test time. Despite being\npowerful, it also opens a new attack surface, i.e., test-time poisoning\nattacks, which are substantially different from previous poisoning attacks that\noccur during the training time of ML models (i.e., adversaries cannot intervene\nin the training process). In this paper, we perform the first test-time\npoisoning attack against four mainstream TTA methods, including TTT, DUA, TENT,\nand RPL. Concretely, we generate poisoned samples based on the surrogate models\nand feed them to the target TTA models. Experimental results show that the TTA\nmethods are generally vulnerable to test-time poisoning attacks. For instance,\nthe adversary can feed as few as 10 poisoned samples to degrade the performance\nof the target model from 76.20% to 41.83%. Our results demonstrate that TTA\nalgorithms lacking a rigorous security assessment are unsuitable for deployment\nin real-life scenarios. As such, we advocate for the integration of defenses\nagainst test-time poisoning attacks into the design of TTA methods.",
        "translated": ""
    },
    {
        "title": "Time Travel in LLMs: Tracing Data Contamination in Large Language Models",
        "url": "http://arxiv.org/abs/2308.08493v1",
        "pub_date": "2023-08-16",
        "summary": "Data contamination, i.e., the presence of test data from downstream tasks in\nthe training data of large language models (LLMs), is a potential major issue\nin understanding LLMs' effectiveness on other tasks. We propose a\nstraightforward yet effective method for identifying data contamination within\nLLMs. At its core, our approach starts by identifying potential contamination\nin individual instances that are drawn from a small random sample; using this\ninformation, our approach then assesses if an entire dataset partition is\ncontaminated. To estimate contamination of individual instances, we employ\n\"guided instruction:\" a prompt consisting of the dataset name, partition type,\nand the initial segment of a reference instance, asking the LLM to complete it.\nAn instance is flagged as contaminated if the LLM's output either exactly or\nclosely matches the latter segment of the reference. To understand if an entire\npartition is contaminated, we propose two ideas. The first idea marks a dataset\npartition as contaminated if the average overlap score with the reference\ninstances (as measured by ROUGE or BLEURT) is statistically significantly\nbetter with the guided instruction vs. a general instruction that does not\ninclude the dataset and partition name. The second idea marks a dataset as\ncontaminated if a classifier based on GPT-4 with in-context learning prompting\nmarks multiple instances as contaminated. Our best method achieves an accuracy\nbetween 92% and 100% in detecting if an LLM is contaminated with seven\ndatasets, containing train and test/validation partitions, when contrasted with\nmanual evaluation by human expert. Further, our findings indicate that GPT-4 is\ncontaminated with AG News, WNLI, and XSum datasets.",
        "translated": ""
    },
    {
        "title": "Diff-CAPTCHA: An Image-based CAPTCHA with Security Enhanced by Denoising\n  Diffusion Model",
        "url": "http://arxiv.org/abs/2308.08367v1",
        "pub_date": "2023-08-16",
        "summary": "To enhance the security of text CAPTCHAs, various methods have been employed,\nsuch as adding the interference lines on the text, randomly distorting the\ncharacters, and overlapping multiple characters. These methods partly increase\nthe difficulty of automated segmentation and recognition attacks. However,\nfacing the rapid development of the end-to-end breaking algorithms, their\nsecurity has been greatly weakened. The diffusion model is a novel image\ngeneration model that can generate the text images with deep fusion of\ncharacters and background images. In this paper, an image-click CAPTCHA scheme\ncalled Diff-CAPTCHA is proposed based on denoising diffusion models. The\nbackground image and characters of the CAPTCHA are treated as a whole to guide\nthe generation process of a diffusion model, thus weakening the character\nfeatures available for machine learning, enhancing the diversity of character\nfeatures in the CAPTCHA, and increasing the difficulty of breaking algorithms.\nTo evaluate the security of Diff-CAPTCHA, this paper develops several attack\nmethods, including end-to-end attacks based on Faster R-CNN and two-stage\nattacks, and Diff-CAPTCHA is compared with three baseline schemes, including\ncommercial CAPTCHA scheme and security-enhanced CAPTCHA scheme based on style\ntransfer. The experimental results show that diffusion models can effectively\nenhance CAPTCHA security while maintaining good usability in human testing.",
        "translated": ""
    },
    {
        "title": "Independent Distribution Regularization for Private Graph Embedding",
        "url": "http://arxiv.org/abs/2308.08360v1",
        "pub_date": "2023-08-16",
        "summary": "Learning graph embeddings is a crucial task in graph mining tasks. An\neffective graph embedding model can learn low-dimensional representations from\ngraph-structured data for data publishing benefiting various downstream\napplications such as node classification, link prediction, etc. However, recent\nstudies have revealed that graph embeddings are susceptible to attribute\ninference attacks, which allow attackers to infer private node attributes from\nthe learned graph embeddings. To address these concerns, privacy-preserving\ngraph embedding methods have emerged, aiming to simultaneously consider primary\nlearning and privacy protection through adversarial learning. However, most\nexisting methods assume that representation models have access to all sensitive\nattributes in advance during the training stage, which is not always the case\ndue to diverse privacy preferences. Furthermore, the commonly used adversarial\nlearning technique in privacy-preserving representation learning suffers from\nunstable training issues. In this paper, we propose a novel approach called\nPrivate Variational Graph AutoEncoders (PVGAE) with the aid of independent\ndistribution penalty as a regularization term. Specifically, we split the\noriginal variational graph autoencoder (VGAE) to learn sensitive and\nnon-sensitive latent representations using two sets of encoders. Additionally,\nwe introduce a novel regularization to enforce the independence of the\nencoders. We prove the theoretical effectiveness of regularization from the\nperspective of mutual information. Experimental results on three real-world\ndatasets demonstrate that PVGAE outperforms other baselines in private\nembedding learning regarding utility performance and privacy protection.",
        "translated": ""
    },
    {
        "title": "Evaluating IP Blacklists Effectiveness",
        "url": "http://arxiv.org/abs/2308.08356v1",
        "pub_date": "2023-08-16",
        "summary": "IP blacklists are widely used to increase network security by preventing\ncommunications with peers that have been marked as malicious. There are several\ncommercial offerings as well as several free-of-charge blacklists maintained by\nvolunteers on the web. Despite their wide adoption, the effectiveness of the\ndifferent IP blacklists in real-world scenarios is still not clear. In this\npaper, we conduct a large-scale network monitoring study which provides\ninsightful findings regarding the effectiveness of blacklists. The results\ncollected over several hundred thousand IP hosts belonging to three distinct\nlarge production networks highlight that blacklists are often tuned for\nprecision, with the result that many malicious activities, such as scanning,\nare completely undetected. The proposed instrumentation approach to detect IP\nscanning and suspicious activities is implemented with home-grown and\nopen-source software. Our tools enable the creation of blacklists without the\nsecurity risks posed by the deployment of honeypots.",
        "translated": ""
    },
    {
        "title": "Optimizing Noise for $f$-Differential Privacy via Anti-Concentration and\n  Stochastic Dominance",
        "url": "http://arxiv.org/abs/2308.08343v1",
        "pub_date": "2023-08-16",
        "summary": "In this paper, we establish anti-concentration inequalities for additive\nnoise mechanisms which achieve $f$-differential privacy ($f$-DP), a notion of\nprivacy phrased in terms of a tradeoff function (a.k.a. ROC curve) $f$ which\nlimits the ability of an adversary to determine which individuals were in the\ndatabase. We show that canonical noise distributions (CNDs), proposed by Awan\nand Vadhan (2023), match the anti-concentration bounds at half-integer values,\nindicating that their tail behavior is near-optimal. We also show that all CNDs\nare sub-exponential, regardless of the $f$-DP guarantee. In the case of\nlog-concave CNDs, we show that they are the stochastically smallest noise\ncompared to any other noise distributions with the same privacy guarantee. In\nterms of integer-valued noise, we propose a new notion of discrete CND and\nprove that a discrete CND always exists, can be constructed by rounding a\ncontinuous CND, and that the discrete CND is unique when designed for a\nstatistic with sensitivity 1. We further show that the discrete CND at\nsensitivity 1 is stochastically smallest compared to other integer-valued\nnoises. Our theoretical results shed light on the different types of privacy\nguarantees possible in the $f$-DP framework and can be incorporated in more\ncomplex mechanisms to optimize performance.",
        "translated": ""
    },
    {
        "title": "Towards Valid and Reliable Privacy Concern Scales: The Example of\n  IUIPC-8",
        "url": "http://arxiv.org/abs/2308.08322v1",
        "pub_date": "2023-08-16",
        "summary": "Valid and reliable measurement instruments are crucial for human factors in\nprivacy research. We expect them to measure what they purport to measure,\nyielding validity, and to measure this consistently, offering us reliability.\nWhile there is a range of privacy concern instruments available in the field\nand their investigation continues unabated, we shall focus on a brief form of\nthe scale Internet Users? Information Privacy Concerns (IUIPC-8) as an example.\nWe not only present IUIPC-8 itself, but also consider methods for the\nevaluation of valid and reliable measurement instruments. In this, confirmatory\nfactor analysis (CFA) serves us as a valuable tool. Our inquiry takes into\naccount the ordinal and non-normal data yielded by the IUIPC questionnaire,\ncompares multiple models to confirm the three-dimensionality of the scale,\nexamines global and local fit and, finally, estimates construct validity and\ninternal consistency reliability metrics. We offer a comparison between\nIUIPC-10 and IUIPC-8 drawing on two independent samples. In conclusion, we\nhighlight properties of the scale and considerations for its use in practice.",
        "translated": ""
    },
    {
        "title": "Privacy at Risk: Exploiting Similarities in Health Data for Identity\n  Inference",
        "url": "http://arxiv.org/abs/2308.08310v1",
        "pub_date": "2023-08-16",
        "summary": "Smartwatches enable the efficient collection of health data that can be used\nfor research and comprehensive analysis to improve the health of individuals.\nIn addition to the analysis capabilities, ensuring privacy when handling health\ndata is a critical concern as the collection and analysis of such data become\npervasive. Since health data contains sensitive information, it should be\nhandled with responsibility and is therefore often treated anonymously.\nHowever, also the data itself can be exploited to reveal information and break\nanonymity. We propose a novel similarity-based re-identification attack on\ntime-series health data and thereby unveil a significant vulnerability. Despite\nprivacy measures that remove identifying information, our attack demonstrates\nthat a brief amount of various sensor data from a target individual is adequate\nto possibly identify them within a database of other samples, solely based on\nsensor-level similarities. In our example scenario, where data owners leverage\nhealth data from smartwatches, findings show that we are able to correctly link\nthe target data in two out of three cases. User privacy is thus already\ninherently threatened by the data itself and even when removing personal\ninformation.",
        "translated": ""
    },
    {
        "title": "Expert opinions on making GDPR usable",
        "url": "http://arxiv.org/abs/2308.08287v1",
        "pub_date": "2023-08-16",
        "summary": "We present the results of a study done in order to validate concepts and\nmethods that have been introduced in (Johansen and Fischer-Hubner, 2020.\n\"Making GDPR Usable: A Model to Support Usability Evaluations of Privacy.\" in\nIFIP AICT 576, 275-291). We use as respondents in our interviews experts\nworking across fields of relevance to these concepts, including law and data\nprotection/privacy, certifications and standardization, and usability (as\nstudied in the field of Human-Computer Interaction). We study the experts'\nopinions about four new concepts, namely: (i) a definition of Usable Privacy,\n(ii) 30 Usable Privacy Goals identified as excerpts from the GDPR (European\nGeneral Data Protection Regulation), (iii) a set of 25 corresponding Usable\nPrivacy Criteria together with their multiple measurable sub-criteria, and (iv)\nthe Usable Privacy Cube model, which puts all these together with the EuroPriSe\ncertification criteria, with the purpose of making explicit several aspects of\ncertification processes such as orderings of criteria, interactions between\nthese, different stakeholder perspectives, and context of use/processing.\n  The expert opinions are varied, example-rich, and forward-looking, which\ngives a impressive list of open problems where the above four concepts can work\nas a foundation for further developments. We employed a critical qualitative\nresearch, using theory triangulation to analyze the data representing three\ngroups of experts, categorized as 'certifications', 'law', and 'usability',\ncoming both from industry and academia. The results of our analysis show\nagreement among the experts about the need for evaluations and measuring of\nusability of privacy in order to allow for exercising data subjects' rights and\nto evaluate the degree to which data controllers comply with the data\nprotection principles.",
        "translated": ""
    },
    {
        "title": "LeakPair: Proactive Repairing of Memory Leaks in Single Page Web\n  Applications",
        "url": "http://arxiv.org/abs/2308.08144v1",
        "pub_date": "2023-08-16",
        "summary": "Modern web applications often resort to application development frameworks\nsuch as React, Vue.js, and Angular. While the frameworks facilitate the\ndevelopment of web applications with several useful components, they are\ninevitably vulnerable to unmanaged memory consumption since the frameworks\noften produce Single Page Applications (SPAs). Web applications can be alive\nfor hours and days with behavior loops, in such cases, even a single memory\nleak in a SPA app can cause performance degradation on the client side.\nHowever, recent debugging techniques for web applications still focus on memory\nleak detection, which requires manual tasks and produces imprecise results.\n  We propose LeakPair, a technique to repair memory leaks in single page\napplications. Given the insight that memory leaks are mostly non-functional\nbugs and fixing them might not change the behavior of an application, the\ntechnique is designed to proactively generate patches to fix memory leaks,\nwithout leak detection, which is often heavy and tedious. To generate effective\npatches, LeakPair follows the idea of pattern-based program repair since the\nautomated repair strategy shows successful results in many recent studies. We\nevaluate the technique on more than 20 open-source projects without using\nexplicit leak detection. The patches generated by our technique are also\nsubmitted to the projects as pull requests. The results show that LeakPair can\ngenerate effective patches to reduce memory consumption that are acceptable to\ndevelopers. In addition, we execute the test suites given by the projects after\napplying the patches, and it turns out that the patches do not cause any\nfunctionality breakage; this might imply that LeakPair can generate\nnon-intrusive patches for memory leaks.",
        "translated": ""
    },
    {
        "title": "An AI-Driven VM Threat Prediction Model for Multi-Risks Analysis-Based\n  Cloud Cybersecurity",
        "url": "http://arxiv.org/abs/2308.09578v1",
        "pub_date": "2023-08-18",
        "summary": "Cloud virtualization technology, ingrained with physical resource sharing,\nprompts cybersecurity threats on users' virtual machines (VM)s due to the\npresence of inevitable vulnerabilities on the offsite servers. Contrary to the\nexisting works which concentrated on reducing resource sharing and encryption\nand decryption of data before transfer for improving cybersecurity which raises\ncomputational cost overhead, the proposed model operates diversely for\nefficiently serving the same purpose. This paper proposes a novel Multiple\nRisks Analysis based VM Threat Prediction Model (MR-TPM) to secure\ncomputational data and minimize adversary breaches by proactively estimating\nthe VMs threats. It considers multiple cybersecurity risk factors associated\nwith the configuration and management of VMs, along with analysis of users'\nbehaviour. All these threat factors are quantified for the generation of\nrespective risk score values and fed as input into a machine learning based\nclassifier to estimate the probability of threat for each VM. The performance\nof MR-TPM is evaluated using benchmark Google Cluster and OpenNebula VM threat\ntraces. The experimental results demonstrate that the proposed model\nefficiently computes the cybersecurity risks and learns the VM threat patterns\nfrom historical and live data samples. The deployment of MR-TPM with existing\nVM allocation policies reduces cybersecurity threats up to 88.9%.",
        "translated": ""
    },
    {
        "title": "Attesting Distributional Properties of Training Data for Machine\n  Learning",
        "url": "http://arxiv.org/abs/2308.09552v1",
        "pub_date": "2023-08-18",
        "summary": "The success of machine learning (ML) has been accompanied by increased\nconcerns about its trustworthiness. Several jurisdictions are preparing ML\nregulatory frameworks. One such concern is ensuring that model training data\nhas desirable distributional properties for certain sensitive attributes. For\nexample, draft regulations indicate that model trainers are required to show\nthat training datasets have specific distributional properties, such as\nreflecting diversity of the population.\n  We propose the notion of property attestation allowing a prover (e.g., model\ntrainer) to demonstrate relevant distributional properties of training data to\na verifier (e.g., a customer) without revealing the data. We present an\neffective hybrid property attestation combining property inference with\ncryptographic mechanisms.",
        "translated": ""
    },
    {
        "title": "Compensating Removed Frequency Components: Thwarting Voice Spectrum\n  Reduction Attacks",
        "url": "http://arxiv.org/abs/2308.09546v1",
        "pub_date": "2023-08-18",
        "summary": "Automatic speech recognition (ASR) provides diverse audio-to-text services\nfor humans to communicate with machines. However, recent research reveals ASR\nsystems are vulnerable to various malicious audio attacks. In particular, by\nremoving the non-essential frequency components, a new spectrum reduction\nattack can generate adversarial audios that can be perceived by humans but\ncannot be correctly interpreted by ASR systems. It raises a new challenge for\ncontent moderation solutions to detect harmful content in audio and video\navailable on social media platforms. In this paper, we propose an acoustic\ncompensation system named ACE to counter the spectrum reduction attacks over\nASR systems. Our system design is based on two observations, namely, frequency\ncomponent dependencies and perturbation sensitivity. First, since the Discrete\nFourier Transform computation inevitably introduces spectral leakage and\naliasing effects to the audio frequency spectrum, the frequency components with\nsimilar frequencies will have a high correlation. Thus, considering the\nintrinsic dependencies between neighboring frequency components, it is possible\nto recover more of the original audio by compensating for the removed\ncomponents based on the remaining ones. Second, since the removed components in\nthe spectrum reduction attacks can be regarded as an inverse of adversarial\nnoise, the attack success rate will decrease when the adversarial audio is\nreplayed in an over-the-air scenario. Hence, we can model the acoustic\npropagation process to add over-the-air perturbations into the attacked audio.\nWe implement a prototype of ACE and the experiments show ACE can effectively\nreduce up to 87.9% of ASR inference errors caused by spectrum reduction\nattacks. Also, by analyzing residual errors, we summarize six general types of\nASR inference errors and investigate the error causes and potential mitigation\nsolutions.",
        "translated": ""
    },
    {
        "title": "Privacy-Preserving 3-Layer Neural Network Training using Mere\n  Homomorphic Encryption Technique",
        "url": "http://arxiv.org/abs/2308.09531v1",
        "pub_date": "2023-08-18",
        "summary": "In this manuscript, we consider the problem of privacy-preserving training of\nneural networks in the mere homomorphic encryption setting. We combine several\nexsiting techniques available, extend some of them, and finally enable the\ntraining of 3-layer neural networks for both the regression and classification\nproblems using mere homomorphic encryption technique.",
        "translated": ""
    },
    {
        "title": "Intrusion Detection based on Federated Learning: a systematic review",
        "url": "http://arxiv.org/abs/2308.09522v1",
        "pub_date": "2023-08-18",
        "summary": "The evolution of cybersecurity is undoubtedly associated and intertwined with\nthe development and improvement of artificial intelligence (AI). As a key tool\nfor realizing more cybersecure ecosystems, Intrusion Detection Systems (IDSs)\nhave evolved tremendously in recent years by integrating machine learning (ML)\ntechniques for the detection of increasingly sophisticated cybersecurity\nattacks hidden in big data. However, these approaches have traditionally been\nbased on centralized learning architectures, in which data from end nodes are\nshared with data centers for analysis. Recently, the application of federated\nlearning (FL) in this context has attracted great interest to come up with\ncollaborative intrusion detection approaches where data does not need to be\nshared. Due to the recent rise of this field, this work presents a complete,\ncontemporary taxonomy for FL-enabled IDS approaches that stems from a\ncomprehensive survey of the literature in the time span from 2018 to 2022.\nPrecisely, our discussion includes an analysis of the main ML models, datasets,\naggregation functions, as well as implementation libraries, which are employed\nby the proposed FL-enabled IDS approaches. On top of everything else, we\nprovide a critical view of the current state of the research around this topic,\nand describe the main challenges and future directions based on the analysis of\nthe literature and our own experience in this area.",
        "translated": ""
    },
    {
        "title": "Proceedings of the 2nd International Workshop on Adaptive Cyber Defense",
        "url": "http://arxiv.org/abs/2308.09520v1",
        "pub_date": "2023-08-18",
        "summary": "The 2nd International Workshop on Adaptive Cyber Defense was held at the\nFlorida Institute of Technology, Florida. This workshop was organized to share\nresearch that explores unique applications of Artificial Intelligence (AI) and\nMachine Learning (ML) as foundational capabilities for the pursuit of adaptive\ncyber defense. The cyber domain cannot currently be reliably and effectively\ndefended without extensive reliance on human experts. Skilled cyber defenders\nare in short supply and often cannot respond fast enough to cyber threats.\n  Building on recent advances in AI and ML the Cyber defense research community\nhas been motivated to develop new dynamic and sustainable defenses through the\nadoption of AI and ML techniques to cyber settings. Bridging critical gaps\nbetween AI and Cyber researchers and practitioners can accelerate efforts to\ncreate semi-autonomous cyber defenses that can learn to recognize and respond\nto cyber attacks or discover and mitigate weaknesses in cooperation with other\ncyber operation systems and human experts. Furthermore, these defenses are\nexpected to be adaptive and able to evolve over time to thwart changes in\nattacker behavior, changes in the system health and readiness, and natural\nshifts in user behavior over time.\n  The workshop was comprised of invited keynote talks, technical presentations\nand a panel discussion about how AI/ML can enable autonomous mitigation of\ncurrent and future cyber attacks. Workshop submissions were peer reviewed by a\npanel of domain experts with a proceedings consisting of six technical articles\nexploring challenging problems of critical importance to national and global\nsecurity. Participation in this workshop offered new opportunities to stimulate\nresearch and innovation in the emerging domain of adaptive and autonomous cyber\ndefense.",
        "translated": ""
    },
    {
        "title": "Balancing Transparency and Risk: The Security and Privacy Risks of\n  Open-Source Machine Learning Models",
        "url": "http://arxiv.org/abs/2308.09490v1",
        "pub_date": "2023-08-18",
        "summary": "The field of artificial intelligence (AI) has experienced remarkable progress\nin recent years, driven by the widespread adoption of open-source machine\nlearning models in both research and industry. Considering the\nresource-intensive nature of training on vast datasets, many applications opt\nfor models that have already been trained. Hence, a small number of key players\nundertake the responsibility of training and publicly releasing large\npre-trained models, providing a crucial foundation for a wide range of\napplications. However, the adoption of these open-source models carries\ninherent privacy and security risks that are often overlooked. To provide a\nconcrete example, an inconspicuous model may conceal hidden functionalities\nthat, when triggered by specific input patterns, can manipulate the behavior of\nthe system, such as instructing self-driving cars to ignore the presence of\nother vehicles. The implications of successful privacy and security attacks\nencompass a broad spectrum, ranging from relatively minor damage like service\ninterruptions to highly alarming scenarios, including physical harm or the\nexposure of sensitive user data. In this work, we present a comprehensive\noverview of common privacy and security threats associated with the use of\nopen-source models. By raising awareness of these dangers, we strive to promote\nthe responsible and secure use of AI systems.",
        "translated": ""
    },
    {
        "title": "Poison Dart Frog: A Clean-Label Attack with Low Poisoning Rate and High\n  Attack Success Rate in the Absence of Training Data",
        "url": "http://arxiv.org/abs/2308.09487v2",
        "pub_date": "2023-08-18",
        "summary": "To successfully launch backdoor attacks, injected data needs to be correctly\nlabeled; otherwise, they can be easily detected by even basic data filters.\nHence, the concept of clean-label attacks was introduced, which is more\ndangerous as it doesn't require changing the labels of injected data. To the\nbest of our knowledge, the existing clean-label backdoor attacks largely relies\non an understanding of the entire training set or a portion of it. However, in\npractice, it is very difficult for attackers to have it because of training\ndatasets often collected from multiple independent sources. Unlike all current\nclean-label attacks, we propose a novel clean label method called 'Poison Dart\nFrog'. Poison Dart Frog does not require access to any training data; it only\nnecessitates knowledge of the target class for the attack, such as 'frog'. On\nCIFAR10, Tiny-ImageNet, and TSRD, with a mere 0.1\\%, 0.025\\%, and 0.4\\%\npoisoning rate of the training set size, respectively, Poison Dart Frog\nachieves a high Attack Success Rate compared to LC, HTBA, BadNets, and Blend.\nFurthermore, compared to the state-of-the-art attack, NARCISSUS, Poison Dart\nFrog achieves similar attack success rates without any training data. Finally,\nwe demonstrate that four typical backdoor defense algorithms struggle to\ncounter Poison Dart Frog.",
        "translated": ""
    },
    {
        "title": "Polyglot Code Smell Detection for Infrastructure as Code with GLITCH",
        "url": "http://arxiv.org/abs/2308.09458v1",
        "pub_date": "2023-08-18",
        "summary": "This paper presents GLITCH, a new technology-agnostic framework that enables\nautomated polyglot code smell detection for Infrastructure as Code scripts.\nGLITCH uses an intermediate representation on which different code smell\ndetectors can be defined. It currently supports the detection of nine security\nsmells and nine design &amp; implementation smells in scripts written in Ansible,\nChef, Docker, Puppet, or Terraform. Studies conducted with GLITCH not only show\nthat GLITCH can reduce the effort of writing code smell analyses for multiple\nIaC technologies, but also that it has higher precision and recall than current\nstate-of-the-art tools. A video describing and demonstrating GLITCH is\navailable at: https://youtu.be/E4RhCcZjWbk",
        "translated": ""
    },
    {
        "title": "Defending Label Inference Attacks in Split Learning under Regression\n  Setting",
        "url": "http://arxiv.org/abs/2308.09448v1",
        "pub_date": "2023-08-18",
        "summary": "As a privacy-preserving method for implementing Vertical Federated Learning,\nSplit Learning has been extensively researched. However, numerous studies have\nindicated that the privacy-preserving capability of Split Learning is\ninsufficient. In this paper, we primarily focus on label inference attacks in\nSplit Learning under regression setting, which are mainly implemented through\nthe gradient inversion method. To defend against label inference attacks, we\npropose Random Label Extension (RLE), where labels are extended to obfuscate\nthe label information contained in the gradients, thereby preventing the\nattacker from utilizing gradients to train an attack model that can infer the\noriginal labels. To further minimize the impact on the original task, we\npropose Model-based adaptive Label Extension (MLE), where original labels are\npreserved in the extended labels and dominate the training process. The\nexperimental results show that compared to the basic defense methods, our\nproposed defense methods can significantly reduce the attack model's\nperformance while preserving the original task's performance.",
        "translated": ""
    },
    {
        "title": "Quantum Symmetric Private Information Retrieval with Secure Storage and\n  Eavesdroppers",
        "url": "http://arxiv.org/abs/2308.10883v1",
        "pub_date": "2023-08-21",
        "summary": "We consider both the classical and quantum variations of $X$-secure,\n$E$-eavesdropped and $T$-colluding symmetric private information retrieval\n(SPIR). This is the first work to study SPIR with $X$-security in classical or\nquantum variations. We first develop a scheme for classical $X$-secure,\n$E$-eavesdropped and $T$-colluding SPIR (XSETSPIR) based on a modified version\nof cross subspace alignment (CSA), which achieves a rate of $R= 1 -\n\\frac{X+\\max(T,E)}{N}$. The modified scheme achieves the same rate as the\nscheme used for $X$-secure PIR with the extra benefit of symmetric privacy.\nNext, we extend this scheme to its quantum counterpart based on the $N$-sum box\nabstraction. This is the first work to consider the presence of eavesdroppers\nin quantum private information retrieval (QPIR). In the quantum variation, the\neavesdroppers have better access to information over the quantum channel\ncompared to the classical channel due to the over-the-air decodability. To that\nend, we develop another scheme specialized to combat eavesdroppers over quantum\nchannels. The scheme proposed for $X$-secure, $E$-eavesdropped and\n$T$-colluding quantum SPIR (XSETQSPIR) in this work maintains the super-dense\ncoding gain from the shared entanglement between the databases, i.e., achieves\na rate of $R_Q = \\min\\left\\{ 1, 2\\left(1-\\frac{X+\\max(T,E)}{N}\\right)\\right\\}$.",
        "translated": ""
    },
    {
        "title": "SRSS: A New Chaos-Based Single-Round Single S-Box Image Encryption\n  Scheme for Highly Auto-Correlated Data",
        "url": "http://arxiv.org/abs/2308.10834v1",
        "pub_date": "2023-08-21",
        "summary": "With the advent of digital communication, securing digital images during\ntransmission and storage has become a critical concern. The traditional s-box\nsubstitution methods often fail to effectively conceal the information within\nhighly auto-correlated regions of an image. This paper addresses the security\nissues presented by three prevalent S-box substitution methods, i.e., single\nS-box, multiple S-boxes, and multiple rounds with multiple S-boxes, especially\nwhen handling images with highly auto-correlated pixels. To resolve the\naddressed security issues, this paper proposes a new scheme SRSS-the Single\nRound Single S-Box encryption scheme. SRSS uses a single S-box for substitution\nin just one round to break the pixel correlations and encrypt the plaintext\nimage effectively. Additionally, this paper introduces a new Chaos-based Random\nOperation Selection System-CROSS, which nullifies the requirement for multiple\nS-boxes, thus reducing the encryption scheme's complexity. By randomly\nselecting the operation to be performed on each pixel, driven by a chaotic\nsequence, the proposed scheme effectively scrambles even high auto-correlation\nareas. When compared to the substitution methods mentioned above, the proposed\nencryption scheme exhibited exceptionally well in just a single round with a\nsingle S-box. The close-to-ideal statistical security analysis results, i.e.,\nan entropy of 7.89 and a correlation coefficient of 0.007, validate the\neffectiveness of the proposed scheme. This research offers an innovative path\nforward for securing images in applications requiring low computational\ncomplexity and fast encryption and decryption speeds.",
        "translated": ""
    },
    {
        "title": "Neural Networks Optimizations Against Concept and Data Drift in Malware\n  Detection",
        "url": "http://arxiv.org/abs/2308.10821v1",
        "pub_date": "2023-08-21",
        "summary": "Despite the promising results of machine learning models in malware\ndetection, they face the problem of concept drift due to malware constant\nevolution. This leads to a decline in performance over time, as the data\ndistribution of the new files differs from the training one, requiring regular\nmodel update. In this work, we propose a model-agnostic protocol to improve a\nbaseline neural network to handle with the drift problem. We show the\nimportance of feature reduction and training with the most recent validation\nset possible, and propose a loss function named Drift-Resilient Binary\nCross-Entropy, an improvement to the classical Binary Cross-Entropy more\neffective against drift. We train our model on the EMBER dataset (2018) and\nevaluate it on a dataset of recent malicious files, collected between 2020 and\n2023. Our improved model shows promising results, detecting 15.2% more malware\nthan a baseline model.",
        "translated": ""
    },
    {
        "title": "A Modular and Adaptive System for Business Email Compromise Detection",
        "url": "http://arxiv.org/abs/2308.10776v1",
        "pub_date": "2023-08-21",
        "summary": "The growing sophistication of Business Email Compromise (BEC) and spear\nphishing attacks poses significant challenges to organizations worldwide. The\ntechniques featured in traditional spam and phishing detection are insufficient\ndue to the tailored nature of modern BEC attacks as they often blend in with\nthe regular benign traffic. Recent advances in machine learning, particularly\nin Natural Language Understanding (NLU), offer a promising avenue for combating\nsuch attacks but in a practical system, due to limitations such as data\navailability, operational costs, verdict explainability requirements or a need\nto robustly evolve the system, it is essential to combine multiple approaches\ntogether. We present CAPE, a comprehensive and efficient system for BEC\ndetection that has been proven in a production environment for a period of over\ntwo years. Rather than being a single model, CAPE is a system that combines\nindependent ML models and algorithms detecting BEC-related behaviors across\nvarious email modalities such as text, images, metadata and the email's\ncommunication context. This decomposition makes CAPE's verdicts naturally\nexplainable. In the paper, we describe the design principles and constraints\nbehind its architecture, as well as the challenges of model design, evaluation\nand adapting the system continuously through a Bayesian approach that combines\nlimited data with domain knowledge. Furthermore, we elaborate on several\nspecific behavioral detectors, such as those based on Transformer neural\narchitectures.",
        "translated": ""
    },
    {
        "title": "Boosting Adversarial Attack with Similar Target",
        "url": "http://arxiv.org/abs/2308.10743v1",
        "pub_date": "2023-08-21",
        "summary": "Deep neural networks are vulnerable to adversarial examples, posing a threat\nto the models' applications and raising security concerns. An intriguing\nproperty of adversarial examples is their strong transferability. Several\nmethods have been proposed to enhance transferability, including ensemble\nattacks which have demonstrated their efficacy. However, prior approaches\nsimply average logits, probabilities, or losses for model ensembling, lacking a\ncomprehensive analysis of how and why model ensembling significantly improves\ntransferability. In this paper, we propose a similar targeted attack method\nnamed Similar Target~(ST). By promoting cosine similarity between the gradients\nof each model, our method regularizes the optimization direction to\nsimultaneously attack all surrogate models. This strategy has been proven to\nenhance generalization ability. Experimental results on ImageNet validate the\neffectiveness of our approach in improving adversarial transferability. Our\nmethod outperforms state-of-the-art attackers on 18 discriminative classifiers\nand adversarially trained models.",
        "translated": ""
    },
    {
        "title": "On the Adversarial Robustness of Multi-Modal Foundation Models",
        "url": "http://arxiv.org/abs/2308.10741v1",
        "pub_date": "2023-08-21",
        "summary": "Multi-modal foundation models combining vision and language models such as\nFlamingo or GPT-4 have recently gained enormous interest. Alignment of\nfoundation models is used to prevent models from providing toxic or harmful\noutput. While malicious users have successfully tried to jailbreak foundation\nmodels, an equally important question is if honest users could be harmed by\nmalicious third-party content. In this paper we show that imperceivable attacks\non images in order to change the caption output of a multi-modal foundation\nmodel can be used by malicious content providers to harm honest users e.g. by\nguiding them to malicious websites or broadcast fake information. This\nindicates that countermeasures to adversarial attacks should be used by any\ndeployed multi-modal foundation model.",
        "translated": ""
    },
    {
        "title": "Backdooring Textual Inversion for Concept Censorship",
        "url": "http://arxiv.org/abs/2308.10718v1",
        "pub_date": "2023-08-21",
        "summary": "Recent years have witnessed success in AIGC (AI Generated Content). People\ncan make use of a pre-trained diffusion model to generate images of high\nquality or freely modify existing pictures with only prompts in nature\nlanguage. More excitingly, the emerging personalization techniques make it\nfeasible to create specific-desired images with only a few images as\nreferences. However, this induces severe threats if such advanced techniques\nare misused by malicious users, such as spreading fake news or defaming\nindividual reputations. Thus, it is necessary to regulate personalization\nmodels (i.e., concept censorship) for their development and advancement.\n  In this paper, we focus on the personalization technique dubbed Textual\nInversion (TI), which is becoming prevailing for its lightweight nature and\nexcellent performance. TI crafts the word embedding that contains detailed\ninformation about a specific object. Users can easily download the word\nembedding from public websites like Civitai and add it to their own stable\ndiffusion model without fine-tuning for personalization. To achieve the concept\ncensorship of a TI model, we propose leveraging the backdoor technique for good\nby injecting backdoors into the Textual Inversion embeddings. Briefly, we\nselect some sensitive words as triggers during the training of TI, which will\nbe censored for normal use. In the subsequent generation stage, if the triggers\nare combined with personalized embeddings as final prompts, the model will\noutput a pre-defined target image rather than images including the desired\nmalicious concept.\n  To demonstrate the effectiveness of our approach, we conduct extensive\nexperiments on Stable Diffusion, a prevailing open-sourced text-to-image model.\nOur code, data, and results are available at\nhttps://concept-censorship.github.io.",
        "translated": ""
    },
    {
        "title": "Static Application Security Testing of Consensus-Critical Code in the\n  Cosmos Network",
        "url": "http://arxiv.org/abs/2308.10613v1",
        "pub_date": "2023-08-21",
        "summary": "Blockchains require deterministic execution in order to reach consensus. This\nis often guaranteed in languages designed to write smart contracts, such as\nSolidity. Application-specific blockchains or ``appchains'' allow the\nblockchain application logic to be written using general-purpose programming\nlanguages, giving developers more flexibility but also additional\nresponsibilities. In particular, developers must ensure that their blockchain\napplication logic does not contain any sources of non-determinism. Any source\nof non-determinism may be a potential source of vulnerabilities.\n  This paper focuses on the use of Static Application Security Testing (SAST)\ntools to detect such sources of non-determinism at development time. We focus\non Cosmos, a prominent open-source project that lets developers build\ninterconnected networks of application-specific blockchains. Cosmos provides a\nSoftware Development Kit (SDK) that allows these chains to be implemented in\nthe Go programming language. We create a corpus of 11 representative\nCosmos-based appchains to analyze for sources of non-determinism in Go.\n  As part of our study, we identified cosmos-sdk-codeql, a set of CodeQL code\nanalysis rules for Cosmos applications. We find that these rules generate many\nfalse positives and propose a refactored set of rules that more precisely\ndetects sources of non-determinism only in code that runs as part of the\nblockchain logic. We demonstrate a significant increase in the precision of the\nrules, making the SAST tool more effective and hence potentially contributing\nto enhanced security for Cosmos-based blockchains.",
        "translated": ""
    },
    {
        "title": "Improving the Transferability of Adversarial Examples with Arbitrary\n  Style Transfer",
        "url": "http://arxiv.org/abs/2308.10601v1",
        "pub_date": "2023-08-21",
        "summary": "Deep neural networks are vulnerable to adversarial examples crafted by\napplying human-imperceptible perturbations on clean inputs. Although many\nattack methods can achieve high success rates in the white-box setting, they\nalso exhibit weak transferability in the black-box setting. Recently, various\nmethods have been proposed to improve adversarial transferability, in which the\ninput transformation is one of the most effective methods. In this work, we\nnotice that existing input transformation-based works mainly adopt the\ntransformed data in the same domain for augmentation. Inspired by domain\ngeneralization, we aim to further improve the transferability using the data\naugmented from different domains. Specifically, a style transfer network can\nalter the distribution of low-level visual features in an image while\npreserving semantic content for humans. Hence, we propose a novel attack method\nnamed Style Transfer Method (STM) that utilizes a proposed arbitrary style\ntransfer network to transform the images into different domains. To avoid\ninconsistent semantic information of stylized images for the classification\nnetwork, we fine-tune the style transfer network and mix up the generated\nimages added by random noise with the original images to maintain semantic\nconsistency and boost input diversity. Extensive experimental results on the\nImageNet-compatible dataset show that our proposed method can significantly\nimprove the adversarial transferability on either normally trained models or\nadversarially trained models than state-of-the-art input transformation-based\nattacks. Code is available at: https://github.com/Zhijin-Ge/STM.",
        "translated": ""
    },
    {
        "title": "When Less is Enough: Positive and Unlabeled Learning Model for\n  Vulnerability Detection",
        "url": "http://arxiv.org/abs/2308.10523v1",
        "pub_date": "2023-08-21",
        "summary": "Automated code vulnerability detection has gained increasing attention in\nrecent years. The deep learning (DL)-based methods, which implicitly learn\nvulnerable code patterns, have proven effective in vulnerability detection. The\nperformance of DL-based methods usually relies on the quantity and quality of\nlabeled data. However, the current labeled data are generally automatically\ncollected, such as crawled from human-generated commits, making it hard to\nensure the quality of the labels. Prior studies have demonstrated that the\nnon-vulnerable code (i.e., negative labels) tends to be unreliable in\ncommonly-used datasets, while vulnerable code (i.e., positive labels) is more\ndetermined. Considering the large numbers of unlabeled data in practice, it is\nnecessary and worth exploring to leverage the positive data and large numbers\nof unlabeled data for more accurate vulnerability detection.\n  In this paper, we focus on the Positive and Unlabeled (PU) learning problem\nfor vulnerability detection and propose a novel model named PILOT, i.e.,\nPositIve and unlabeled Learning mOdel for vulnerability deTection. PILOT only\nlearns from positive and unlabeled data for vulnerability detection. It mainly\ncontains two modules: (1) A distance-aware label selection module, aiming at\ngenerating pseudo-labels for selected unlabeled data, which involves the\ninter-class distance prototype and progressive fine-tuning; (2) A\nmixed-supervision representation learning module to further alleviate the\ninfluence of noise and enhance the discrimination of representations.",
        "translated": ""
    },
    {
        "title": "Designing an attack-defense game: how to increase robustness of\n  financial transaction models via a competition",
        "url": "http://arxiv.org/abs/2308.11406v2",
        "pub_date": "2023-08-22",
        "summary": "Given the escalating risks of malicious attacks in the finance sector and the\nconsequential severe damage, a thorough understanding of adversarial strategies\nand robust defense mechanisms for machine learning models is critical. The\nthreat becomes even more severe with the increased adoption in banks more\naccurate, but potentially fragile neural networks. We aim to investigate the\ncurrent state and dynamics of adversarial attacks and defenses for neural\nnetwork models that use sequential financial data as the input.\n  To achieve this goal, we have designed a competition that allows realistic\nand detailed investigation of problems in modern financial transaction data.\nThe participants compete directly against each other, so possible attacks and\ndefenses are examined in close-to-real-life conditions. Our main contributions\nare the analysis of the competition dynamics that answers the questions on how\nimportant it is to conceal a model from malicious users, how long does it take\nto break it, and what techniques one should use to make it more robust, and\nintroduction additional way to attack models or increase their robustness.\n  Our analysis continues with a meta-study on the used approaches with their\npower, numerical experiments, and accompanied ablations studies. We show that\nthe developed attacks and defenses outperform existing alternatives from the\nliterature while being practical in terms of execution, proving the validity of\nthe competition as a tool for uncovering vulnerabilities of machine learning\nmodels and mitigating them in various domains.",
        "translated": ""
    },
    {
        "title": "Protect Federated Learning Against Backdoor Attacks via Data-Free\n  Trigger Generation",
        "url": "http://arxiv.org/abs/2308.11333v1",
        "pub_date": "2023-08-22",
        "summary": "As a distributed machine learning paradigm, Federated Learning (FL) enables\nlarge-scale clients to collaboratively train a model without sharing their raw\ndata. However, due to the lack of data auditing for untrusted clients, FL is\nvulnerable to poisoning attacks, especially backdoor attacks. By using poisoned\ndata for local training or directly changing the model parameters, attackers\ncan easily inject backdoors into the model, which can trigger the model to make\nmisclassification of targeted patterns in images. To address these issues, we\npropose a novel data-free trigger-generation-based defense approach based on\nthe two characteristics of backdoor attacks: i) triggers are learned faster\nthan normal knowledge, and ii) trigger patterns have a greater effect on image\nclassification than normal class patterns. Our approach generates the images\nwith newly learned knowledge by identifying the differences between the old and\nnew global models, and filters trigger images by evaluating the effect of these\ngenerated images. By using these trigger images, our approach eliminates\npoisoned models to ensure the updated global model is benign. Comprehensive\nexperiments demonstrate that our approach can defend against almost all the\nexisting types of backdoor attacks and outperform all the seven\nstate-of-the-art defense methods with both IID and non-IID scenarios.\nEspecially, our approach can successfully defend against the backdoor attack\neven when 80\\% of the clients are malicious.",
        "translated": ""
    },
    {
        "title": "Up-to-date Threat Modelling for Soft Privacy on Smart Cars",
        "url": "http://arxiv.org/abs/2308.11273v1",
        "pub_date": "2023-08-22",
        "summary": "Physical persons playing the role of car drivers consume data that is sourced\nfrom the Internet and, at the same time, themselves act as sources of relevant\ndata. It follows that citizens' privacy is potentially at risk while they\ndrive, hence the need to model privacy threats in this application domain. This\npaper addresses the privacy threats by updating a recent threat-modelling\nmethodology and by tailoring it specifically to the soft privacy target\nproperty, which ensures citizens' full control on their personal data. The\nmethodology now features the sources of documentation as an explicit variable\nthat is to be considered. It is demonstrated by including a new version of the\nde-facto standard LINDDUN methodology as well as an additional source by ENISA\nwhich is found to be relevant to soft privacy. The main findings are a set of\n23 domain-independent threats, 43 domain-specific assets and 525\ndomain-dependent threats for the target property in the automotive domain.\nWhile these exceed their previous versions, their main value is to offer\nself-evident support to at least two arguments. One is that LINDDUN has evolved\nmuch the way our original methodology already advocated because a few of our\npreviously suggested extensions are no longer outstanding. The other one is\nthat ENISA's treatment of privacy aboard smart cars should be extended\nconsiderably because our 525 threats fall in the same scope.",
        "translated": ""
    },
    {
        "title": "Adaptive White-Box Watermarking with Self-Mutual Check Parameters in\n  Deep Neural Networks",
        "url": "http://arxiv.org/abs/2308.11235v1",
        "pub_date": "2023-08-22",
        "summary": "Artificial Intelligence (AI) has found wide application, but also poses risks\ndue to unintentional or malicious tampering during deployment. Regular checks\nare therefore necessary to detect and prevent such risks. Fragile watermarking\nis a technique used to identify tampering in AI models. However, previous\nmethods have faced challenges including risks of omission, additional\ninformation transmission, and inability to locate tampering precisely. In this\npaper, we propose a method for detecting tampered parameters and bits, which\ncan be used to detect, locate, and restore parameters that have been tampered\nwith. We also propose an adaptive embedding method that maximizes information\ncapacity while maintaining model accuracy. Our approach was tested on multiple\nneural networks subjected to attacks that modified weight parameters, and our\nresults demonstrate that our method achieved great recovery performance when\nthe modification rate was below 20%. Furthermore, for models where watermarking\nsignificantly affected accuracy, we utilized an adaptive bit technique to\nrecover more than 15% of the accuracy loss of the model.",
        "translated": ""
    },
    {
        "title": "LDP-Feat: Image Features with Local Differential Privacy",
        "url": "http://arxiv.org/abs/2308.11223v1",
        "pub_date": "2023-08-22",
        "summary": "Modern computer vision services often require users to share raw feature\ndescriptors with an untrusted server. This presents an inherent privacy risk,\nas raw descriptors may be used to recover the source images from which they\nwere extracted. To address this issue, researchers recently proposed\nprivatizing image features by embedding them within an affine subspace\ncontaining the original feature as well as adversarial feature samples. In this\npaper, we propose two novel inversion attacks to show that it is possible to\n(approximately) recover the original image features from these embeddings,\nallowing us to recover privacy-critical image content. In light of such\nsuccesses and the lack of theoretical privacy guarantees afforded by existing\nvisual privacy methods, we further propose the first method to privatize image\nfeatures via local differential privacy, which, unlike prior approaches,\nprovides a guaranteed bound for privacy leakage regardless of the strength of\nthe attacks. In addition, our method yields strong performance in visual\nlocalization as a downstream task while enjoying the privacy guarantee.",
        "translated": ""
    },
    {
        "title": "Federated Learning on Patient Data for Privacy-Protecting Polycystic\n  Ovary Syndrome Treatment",
        "url": "http://arxiv.org/abs/2308.11220v1",
        "pub_date": "2023-08-22",
        "summary": "The field of women's endocrinology has trailed behind data-driven medical\nsolutions, largely due to concerns over the privacy of patient data. Valuable\ndatapoints about hormone levels or menstrual cycling could expose patients who\nsuffer from comorbidities or terminate a pregnancy, violating their privacy. We\nexplore the application of Federated Learning (FL) to predict the optimal drug\nfor patients with polycystic ovary syndrome (PCOS). PCOS is a serious hormonal\ndisorder impacting millions of women worldwide, yet it's poorly understood and\nits research is stunted by a lack of patient data. We demonstrate that a\nvariety of FL approaches succeed on a synthetic PCOS patient dataset. Our\nproposed FL models are a tool to access massive quantities of diverse data and\nidentify the most effective treatment option while providing PCOS patients with\nprivacy guarantees.",
        "translated": ""
    },
    {
        "title": "Blockchain-Powered Supply Chain Management for Kidney Organ Preservation",
        "url": "http://arxiv.org/abs/2308.11169v1",
        "pub_date": "2023-08-22",
        "summary": "Due to the shortage of available kidney organs for transplants, handling\nevery donor kidney with utmost care is crucial to preserve the organ's health,\nespecially during the organ supply chain where kidneys are prone to\ndeterioration during transportation. Therefore, this research proposes a\nblockchain platform to aid in managing kidneys in the supply chain. This\nframework establishes a secure system that meticulously tracks the organ's\nlocation and handling, safeguarding the health from donor to recipient.\nAdditionally, a machine-learning algorithm is embedded to monitor organ health\nin real-time against various metrics for prompt detection of possible kidney\ndamage.",
        "translated": ""
    },
    {
        "title": "A novel analysis of utility in privacy pipelines, using Kronecker\n  products and quantitative information flow",
        "url": "http://arxiv.org/abs/2308.11110v1",
        "pub_date": "2023-08-22",
        "summary": "We combine Kronecker products, and quantitative information flow, to give a\nnovel formal analysis for the fine-grained verification of utility in complex\nprivacy pipelines. The combination explains a surprising anomaly in the\nbehaviour of utility of privacy-preserving pipelines -- that sometimes a\nreduction in privacy results also in a decrease in utility. We use the standard\nmeasure of utility for Bayesian analysis, introduced by Ghosh at al., to\nproduce tractable and rigorous proofs of the fine-grained statistical behaviour\nleading to the anomaly. More generally, we offer the prospect of\nformal-analysis tools for utility that complement extant formal analyses of\nprivacy. We demonstrate our results on a number of common privacy-preserving\ndesigns.",
        "translated": ""
    },
    {
        "title": "TeD-SPAD: Temporal Distinctiveness for Self-supervised\n  Privacy-preservation for video Anomaly Detection",
        "url": "http://arxiv.org/abs/2308.11072v1",
        "pub_date": "2023-08-21",
        "summary": "Video anomaly detection (VAD) without human monitoring is a complex computer\nvision task that can have a positive impact on society if implemented\nsuccessfully. While recent advances have made significant progress in solving\nthis task, most existing approaches overlook a critical real-world concern:\nprivacy. With the increasing popularity of artificial intelligence\ntechnologies, it becomes crucial to implement proper AI ethics into their\ndevelopment. Privacy leakage in VAD allows models to pick up and amplify\nunnecessary biases related to people's personal information, which may lead to\nundesirable decision making. In this paper, we propose TeD-SPAD, a\nprivacy-aware video anomaly detection framework that destroys visual private\ninformation in a self-supervised manner. In particular, we propose the use of a\ntemporally-distinct triplet loss to promote temporally discriminative features,\nwhich complements current weakly-supervised VAD methods. Using TeD-SPAD, we\nachieve a positive trade-off between privacy protection and utility anomaly\ndetection performance on three popular weakly supervised VAD datasets:\nUCF-Crime, XD-Violence, and ShanghaiTech. Our proposed anonymization model\nreduces private attribute prediction by 32.25% while only reducing frame-level\nROC AUC on the UCF-Crime anomaly detection dataset by 3.69%. Project Page:\nhttps://joefioresi718.github.io/TeD-SPAD_webpage/",
        "translated": ""
    },
    {
        "title": "Temporal-Distributed Backdoor Attack Against Video Based Action\n  Recognition",
        "url": "http://arxiv.org/abs/2308.11070v1",
        "pub_date": "2023-08-21",
        "summary": "Deep neural networks (DNNs) have achieved tremendous success in various\napplications including video action recognition, yet remain vulnerable to\nbackdoor attacks (Trojans). The backdoor-compromised model will mis-classify to\nthe target class chosen by the attacker when a test instance (from a non-target\nclass) is embedded with a specific trigger, while maintaining high accuracy on\nattack-free instances. Although there are extensive studies on backdoor attacks\nagainst image data, the susceptibility of video-based systems under backdoor\nattacks remains largely unexplored. Current studies are direct extensions of\napproaches proposed for image data, e.g., the triggers are\n\\textbf{independently} embedded within the frames, which tend to be detectable\nby existing defenses. In this paper, we introduce a \\textit{simple} yet\n\\textit{effective} backdoor attack against video data. Our proposed attack,\nadding perturbations in a transformed domain, plants an \\textbf{imperceptible,\ntemporally distributed} trigger across the video frames, and is shown to be\nresilient to existing defensive strategies. The effectiveness of the proposed\nattack is demonstrated by extensive experiments with various well-known models\non two video recognition benchmarks, UCF101 and HMDB51, and a sign language\nrecognition benchmark, Greek Sign Language (GSL) dataset. We delve into the\nimpact of several influential factors on our proposed attack and identify an\nintriguing effect termed \"collateral damage\" through extensive studies.",
        "translated": ""
    },
    {
        "title": "Devising and Detecting Phishing: large language models vs. Smaller Human\n  Models",
        "url": "http://arxiv.org/abs/2308.12287v1",
        "pub_date": "2023-08-23",
        "summary": "AI programs, built using large language models, make it possible to\nautomatically create phishing emails based on a few data points about a user.\nThey stand in contrast to traditional phishing emails that hackers manually\ndesign using general rules gleaned from experience. The V-Triad is an advanced\nset of rules for manually designing phishing emails to exploit our cognitive\nheuristics and biases. In this study, we compare the performance of phishing\nemails created automatically by GPT-4 and manually using the V-Triad. We also\ncombine GPT-4 with the V-Triad to assess their combined potential. A fourth\ngroup, exposed to generic phishing emails, was our control group. We utilized a\nfactorial approach, sending emails to 112 randomly selected participants\nrecruited for the study. The control group emails received a click-through rate\nbetween 19-28%, the GPT-generated emails 30-44%, emails generated by the\nV-Triad 69-79%, and emails generated by GPT and the V-Triad 43-81%. Each\nparticipant was asked to explain for why they pressed or did not press a link\nin the email. These answers often contradict each other, highlighting the need\nfor personalized content. The cues that make one person avoid phishing emails\nmake another person fall for them. Next, we used four popular large language\nmodels (GPT, Claude, PaLM, and LLaMA) to detect the intention of phishing\nemails and compare the results to human detection. The language models\ndemonstrated a strong ability to detect malicious intent, even in non-obvious\nphishing emails. They sometimes surpassed human detection, although often being\nslightly less accurate than humans.",
        "translated": "使用大型语言模型构建的人工智能程序使得基于用户的一些数据点自动创建钓鱼邮件成为可能。它们与传统的网络钓鱼电子邮件形成了鲜明的对比。传统的网络钓鱼电子邮件是黑客利用经验总结出的一般规则手工设计的。V-Triad 是一套先进的规则，用于手动设计网络钓鱼电子邮件，以利用我们的认知启发法和偏见。在这项研究中，我们比较的性能仿冒电子邮件自动创建的 GPT-4和手动使用的 V-Triad。我们还将 GPT-4与 V-Triad 结合起来，以评估它们的联合潜力。第四组是我们的对照组，暴露在一般的网络钓鱼电子邮件中。我们采用阶乘法，向随机选择的112名参与者发送电子邮件。对照组收到的电子邮件点进率在19-28% 之间，由 GPT 产生的电子邮件占30-44% ，由 V-Triad 产生的电子邮件占69-79% ，由 GPT 和 V-Triad 产生的电子邮件占43-81% 。每个参与者都被要求解释他们为什么要按或不按电子邮件中的链接。这些答案往往相互矛盾，突出了对个性化内容的需求。让一个人避免网络钓鱼邮件的线索会让另一个人上当。接下来，我们使用四种流行的大型语言模型(GPT、 Claude、 PalM 和 LLaMA)来检测钓鱼电子邮件的意图，并将结果与人类检测结果进行比较。这些语言模型表现出很强的检测恶意意图的能力，即使是在非明显的钓鱼电子邮件中。它们有时会超出人类的检测范围，尽管它们的精确度往往略低于人类。"
    },
    {
        "title": "ULDP-FL: Federated Learning with Across Silo User-Level Differential\n  Privacy",
        "url": "http://arxiv.org/abs/2308.12210v1",
        "pub_date": "2023-08-23",
        "summary": "Differentially Private Federated Learning (DP-FL) has garnered attention as a\ncollaborative machine learning approach that ensures formal privacy. Most DP-FL\napproaches ensure DP at the record-level within each silo for cross-silo FL.\nHowever, a single user's data may extend across multiple silos, and the desired\nuser-level DP guarantee for such a setting remains unknown. In this study, we\npresent ULDP-FL, a novel FL framework designed to guarantee user-level DP in\ncross-silo FL where a single user's data may belong to multiple silos. Our\nproposed algorithm directly ensures user-level DP through per-user weighted\nclipping, departing from group-privacy approaches. We provide a theoretical\nanalysis of the algorithm's privacy and utility. Additionally, we enhance the\nalgorithm's utility and showcase its private implementation using cryptographic\nbuilding blocks. Empirical experiments on real-world datasets show substantial\nimprovements in our methods in privacy-utility trade-offs under user-level DP\ncompared to baseline methods. To the best of our knowledge, our work is the\nfirst FL framework that effectively provides user-level DP in the general\ncross-silo FL setting.",
        "translated": "差异私立联邦学习(DP-FL)作为一种确保形式隐私的协作机器学习方法已经引起了人们的关注。大多数 DP-FL 方法确保 DP 在跨筒仓 FL 的每个筒仓内的记录级别。然而，单个用户的数据可能跨多个竖井扩展，并且这种设置所需的用户级 DP 保证仍然是未知的。在这项研究中，我们提出了 ULDP-FL，一个新的 FL 框架设计，以保证用户级 DP 在跨筒仓 FL，其中一个用户的数据可能属于多个筒仓。我们提出的算法直接确保用户级 DP 通过每个用户的加权剪辑，背离了组隐私的方法。我们提供了一个算法的隐私和实用性的理论分析。此外，我们还增强了算法的实用性，并使用加密构建块展示了算法的私有实现。对真实世界数据集的实验表明，与基线方法相比，我们的方法在用户级 DP 下的隐私-效用权衡方面有了很大的改进。据我们所知，我们的工作是第一个 FL 框架，有效地提供用户级 DP 在一般的跨竖井 FL 设置。"
    },
    {
        "title": "Unsupervised anomalies detection in IIoT edge devices networks using\n  federated learning",
        "url": "http://arxiv.org/abs/2308.12175v1",
        "pub_date": "2023-08-23",
        "summary": "In a connection of many IoT devices that each collect data, normally training\na machine learning model would involve transmitting the data to a central\nserver which requires strict privacy rules. However, some owners are reluctant\nof availing their data out of the company due to data security concerns.\nFederated learning(FL) as a distributed machine learning approach performs\ntraining of a machine learning model on the device that gathered the data\nitself. In this scenario, data is not share over the network for training\npurpose. Fedavg as one of FL algorithms permits a model to be copied to\nparticipating devices during a training session. The devices could be chosen at\nrandom, and a device can be aborted. The resulting models are sent to the\ncoordinating server and then average models from the devices that finished\ntraining. The process is repeated until a desired model accuracy is achieved.\nBy doing this, FL approach solves the privacy problem for IoT/ IIoT devices\nthat held sensitive data for the owners. In this paper, we leverage the\nbenefits of FL and implemented Fedavg algorithm on a recent dataset that\nrepresent the modern IoT/ IIoT device networks. The results were almost the\nsame as the centralized machine learning approach. We also evaluated some\nshortcomings of Fedavg such as unfairness that happens during the training when\nstruggling devices do not participate for every stage of training. This\ninefficient training of local or global model could lead in a high number of\nfalse alarms in intrusion detection systems for IoT/IIoT gadgets developed\nusing Fedavg. Hence, after evaluating the FedAv deep auto encoder with\ncentralized deep auto encoder ML, we further proposed and designed a Fair\nFedavg algorithm that will be evaluated in the future work.",
        "translated": "在许多物联网设备的连接中，每个设备收集数据，通常训练一个机器学习模型将涉及传输数据到中央服务器，需要严格的隐私规则。然而，出于数据安全方面的考虑，一些所有者不愿意将他们的数据提供给公司。联邦学习(FL)作为一种分布式机器学习方法，在收集数据的设备上对机器学习模型进行训练。在这个场景中，数据不能通过网络共享用于培训目的。Fedavg 作为 FL 算法之一，允许在训练过程中将模型复制到参与的设备上。可以随机选择设备，并且可以中止设备。生成的模型被发送到协调服务器，然后从完成培训的设备中平均模型。重复该过程，直到达到期望的模型精度。通过这样做，FL 方法解决了为用户保存敏感数据的 IoT/IIoT 设备的隐私问题。在本文中，我们利用 FL 的优势，在一个代表现代 IoT/IIoT 设备网络的最新数据集上实现了 Fedavg 算法。结果几乎与集中式机器学习方法相同。我们还评估了 Fedavg 的一些缺点，比如在训练期间，当有困难的设备没有参加每个阶段的训练时，会出现不公平的情况。这种局部或全局模型的低效率训练可能会导致在使用 Fedavg 开发的 IoT/IIoT 小工具的入侵检测系统中出现大量的错误警报。因此，在对集中式深度自动编码器 ML 的 FedAv 深度自动编码器进行评估之后，进一步提出并设计了一种公平 Fedavg 算法，将在以后的工作中进行评估。"
    },
    {
        "title": "A Probabilistic Fluctuation based Membership Inference Attack for\n  Generative Models",
        "url": "http://arxiv.org/abs/2308.12143v1",
        "pub_date": "2023-08-23",
        "summary": "Membership Inference Attack (MIA) identifies whether a record exists in a\nmachine learning model's training set by querying the model. MIAs on the\nclassic classification models have been well-studied, and recent works have\nstarted to explore how to transplant MIA onto generative models. Our\ninvestigation indicates that existing MIAs designed for generative models\nmainly depend on the overfitting in target models. However, overfitting can be\navoided by employing various regularization techniques, whereas existing MIAs\ndemonstrate poor performance in practice. Unlike overfitting, memorization is\nessential for deep learning models to attain optimal performance, making it a\nmore prevalent phenomenon. Memorization in generative models leads to an\nincreasing trend in the probability distribution of generating records around\nthe member record. Therefore, we propose a Probabilistic Fluctuation Assessing\nMembership Inference Attack (PFAMI), a black-box MIA that infers memberships by\ndetecting these trends via analyzing the overall probabilistic fluctuations\naround given records. We conduct extensive experiments across multiple\ngenerative models and datasets, which demonstrate PFAMI can improve the attack\nsuccess rate (ASR) by about 27.9% when compared with the best baseline.",
        "translated": "成员推理攻击(MIA)通过查询机器学习模型的训练集来识别该记录是否存在。经典分类模型上的 MIA 已经得到了很好的研究，最近的工作已经开始探索如何将 MIA 移植到生成模型上。我们的研究表明，现有的生成模型的 MIA 主要依赖于目标模型的过拟合。然而，过拟合可以通过使用各种正则化技术来避免，而现有的 MIA 在实际应用中表现出较差的性能。与过度拟合不同，记忆对于深度学习模型获得最佳性能是必不可少的，这使得它成为一个更普遍的现象。生成模型中的记忆导致围绕成员记录生成记录的概率分布越来越大。因此，我们提出了一个概率波动评估成员推断攻击(PFAMI) ，一个黑盒 MIA，通过分析给定记录周围的总体概率波动来检测这些趋势，从而推断成员关系。我们在多个生成模型和数据集上进行了广泛的实验，结果表明 PFAMI 可以提高攻击成功率(ASR)约27.9% ，与最佳基线相比。"
    },
    {
        "title": "DarkDiff: Explainable web page similarity of TOR onion sites",
        "url": "http://arxiv.org/abs/2308.12134v1",
        "pub_date": "2023-08-23",
        "summary": "In large-scale data analysis, near-duplicates are often a problem. For\nexample, with two near-duplicate phishing emails, a difference in the\nsalutation (Mr versus Ms) is not essential, but whether it is bank A or B is\nimportant. The state-of-the-art in near-duplicate detection is a black box\napproach (MinHash), so one only knows that emails are near-duplicates, but not\nwhy. We present DarkDiff, which can efficiently detect near-duplicates while\nproviding the reason why there is a near-duplicate. We have developed DarkDiff\nto detect near-duplicates of homepages on the Darkweb. DarkDiff works well on\nthose pages because they resemble the clear web of the past.",
        "translated": "在大规模数据分析中，接近重复的数据通常是一个问题。例如，对于两封几乎重复的钓鱼邮件，称呼(先生与女士)的不同并不重要，但是银行 A 还是银行 B 是重要的。最先进的近重复检测方法是黑盒方法(MinHash) ，因此人们只知道电子邮件近重复，但不知道原因。我们介绍了 Dark Diff，它可以有效地检测近重复，同时提供了为什么有一个近重复的原因。我们已经开发了黑暗差异，以检测近重复的主页上的暗网。Dark Diff 在这些页面上运行良好，因为它们类似于过去的清晰网络。"
    },
    {
        "title": "Out of the Cage: How Stochastic Parrots Win in Cyber Security\n  Environments",
        "url": "http://arxiv.org/abs/2308.12086v1",
        "pub_date": "2023-08-23",
        "summary": "Large Language Models (LLMs) have gained widespread popularity across diverse\ndomains involving text generation, summarization, and various natural language\nprocessing tasks. Despite their inherent limitations, LLM-based designs have\nshown promising capabilities in planning and navigating open-world scenarios.\nThis paper introduces a novel application of pre-trained LLMs as agents within\ncybersecurity network environments, focusing on their utility for sequential\ndecision-making processes.\n  We present an approach wherein pre-trained LLMs are leveraged as attacking\nagents in two reinforcement learning environments. Our proposed agents\ndemonstrate similar or better performance against state-of-the-art agents\ntrained for thousands of episodes in most scenarios and configurations. In\naddition, the best LLM agents perform similarly to human testers of the\nenvironment without any additional training process. This design highlights the\npotential of LLMs to efficiently address complex decision-making tasks within\ncybersecurity.\n  Furthermore, we introduce a new network security environment named\nNetSecGame. The environment is designed to eventually support complex\nmulti-agent scenarios within the network security domain. The proposed\nenvironment mimics real network attacks and is designed to be highly modular\nand adaptable for various scenarios.",
        "translated": "大型语言模型(LLM)在涉及文本生成、摘要和各种自然语言处理任务的不同领域得到了广泛的普及。尽管其固有的局限性，基于 LLM 的设计在规划和导航开放世界场景方面已经显示出有前途的能力。本文介绍了预训练 LLM 作为网络安全网络环境中的代理的一种新的应用，重点介绍了它们在顺序决策过程中的实用性。我们提出了一种方法，其中预先训练的 LLM 是作为攻击代理在两个强化学习的环境中利用。我们提出的代理程序在大多数场景和配置中对经过数千次训练的最先进代理程序表现出类似或更好的性能。此外，最好的 LLM 代理执行与环境的人类测试人员类似的操作，不需要任何额外的培训过程。这种设计突出了 LLM 在网络安全中有效处理复杂决策任务的潜力。此外，我们还介绍了一个新的网络安全环境 NetSecGame。该环境旨在最终支持网络安全领域内的复杂多代理场景。所提出的环境模仿真实的网络攻击，并且设计成高度模块化和适应各种场景。"
    },
    {
        "title": "Unleashing IoT Security: Assessing the Effectiveness of Best Practices\n  in Protecting Against Threats",
        "url": "http://arxiv.org/abs/2308.12072v1",
        "pub_date": "2023-08-23",
        "summary": "The Internet of Things (IoT) market is rapidly growing and is expected to\ndouble from 2020 to 2025. The increasing use of IoT devices, particularly in\nsmart homes, raises crucial concerns about user privacy and security as these\ndevices often handle sensitive and critical information. Inadequate security\ndesigns and implementations by IoT vendors can lead to significant\nvulnerabilities.\n  To address these IoT device vulnerabilities, institutions, and organizations\nhave published IoT security best practices (BPs) to guide manufacturers in\nensuring the security of their products. However, there is currently no\nstandardized approach for evaluating the effectiveness of individual BP\nrecommendations. This leads to manufacturers investing effort in implementing\nless effective BPs while potentially neglecting measures with greater impact.\n  In this paper, we propose a methodology for evaluating the security impact of\nIoT BPs and ranking them based on their effectiveness in protecting against\nsecurity threats. Our approach involves translating identified BPs into\nconcrete test cases that can be applied to real-world IoT devices to assess\ntheir effectiveness in mitigating vulnerabilities. We applied this methodology\nto evaluate the security impact of nine commodity IoT products, discovering 18\nvulnerabilities. By empirically assessing the actual impact of BPs on device\nsecurity, IoT designers and implementers can prioritize their security\ninvestments more effectively, improving security outcomes and optimizing\nlimited security budgets.",
        "translated": "物联网(IoT)市场正在迅速增长，预计从2020年到2025年翻一番。物联网设备(尤其是在智能家居中)的使用日益增加，引起了人们对用户隐私和安全的关注，因为这些设备经常处理敏感和关键信息。物联网供应商不完善的安全设计和实现可能导致严重的漏洞。为了解决这些物联网设备的漏洞，机构和组织发布了物联网安全最佳实践(BPs) ，以指导制造商确保其产品的安全性。然而，目前还没有标准化的方法来评估个别 BP 建议的有效性。这导致制造商投资于实施效率较低的基准，同时可能忽视影响更大的措施。在本文中，我们提出了一种方法来评估物联网业务的安全影响，并根据他们的有效性，以防止安全威胁的排名。我们的方法包括将识别出的基准转换为具体的测试用例，这些测试用例可以应用于现实世界的物联网设备，以评估它们在缓解漏洞方面的有效性。我们应用这种方法来评估9种商品物联网产品的安全影响，发现了18个漏洞。物联网的设计者和实施者可以通过实证的方式评估基准对设备安全的实际影响，从而更有效地确定安全投资的优先顺序，改善安全结果，并优化有限的安全预算。"
    },
    {
        "title": "Sample Complexity of Robust Learning against Evasion Attacks",
        "url": "http://arxiv.org/abs/2308.12054v1",
        "pub_date": "2023-08-23",
        "summary": "It is becoming increasingly important to understand the vulnerability of\nmachine learning models to adversarial attacks. One of the fundamental problems\nin adversarial machine learning is to quantify how much training data is needed\nin the presence of evasion attacks, where data is corrupted at test time. In\nthis thesis, we work with the exact-in-the-ball notion of robustness and study\nthe feasibility of adversarially robust learning from the perspective of\nlearning theory, considering sample complexity.\n  We first explore the setting where the learner has access to random examples\nonly, and show that distributional assumptions are essential. We then focus on\nlearning problems with distributions on the input data that satisfy a Lipschitz\ncondition and show that robustly learning monotone conjunctions has sample\ncomplexity at least exponential in the adversary's budget (the maximum number\nof bits it can perturb on each input). However, if the adversary is restricted\nto perturbing $O(\\log n)$ bits, then one can robustly learn conjunctions and\ndecision lists w.r.t. log-Lipschitz distributions.\n  We then study learning models where the learner is given more power. We first\nconsider local membership queries, where the learner can query the label of\npoints near the training sample. We show that, under the uniform distribution,\nthe exponential dependence on the adversary's budget to robustly learn\nconjunctions remains inevitable. We then introduce a local equivalence query\noracle, which returns whether the hypothesis and target concept agree in a\ngiven region around a point in the training sample, and a counterexample if it\nexists. We show that if the query radius is equal to the adversary's budget, we\ncan develop robust empirical risk minimization algorithms in the\ndistribution-free setting. We give general query complexity upper and lower\nbounds, as well as for concrete concept classes.",
        "translated": "了解机器学习模型在面对敌对攻击时的脆弱性正变得越来越重要。对抗性机器学习的基本问题之一是量化在存在规避攻击的情况下需要多少训练数据，其中数据在测试时被破坏。本文从学习理论的角度出发，考虑样本的复杂性，运用鲁棒性的精确球概念，研究了逆向鲁棒学习的可行性。我们首先探讨了学习者只能接触随机例子的情况，并且证明了分布假设是必不可少的。然后我们集中研究了输入数据分布满足 Lipschitz 条件的学习问题，并且证明了鲁棒学习单调连词的样本复杂度在对手的预算中至少是指数级的(每个输入可以扰动的最大位数)。但是，如果对手仅限于干扰 $O (log n) $bit，那么可以稳健地学习连词和决策列表 w.r.t. log-Lipschitz 分布。然后我们研究学习者被赋予更大权力的学习模式。我们首先考虑局部成员查询，其中学习者可以查询训练样本附近的点标签。我们发现，在均匀分布下，对对手预算的指数依赖对连词的鲁棒学习仍然是不可避免的。然后我们引入一个局部等价查询预言，它返回假设和目标概念是否在训练样本中的一个点附近的给定区域内一致，如果存在则返回一个反例。结果表明，当查询半径等于对手的预算时，可以在无分布环境下开发鲁棒的经验风险最小化算法。我们给出了一般查询复杂度的上下界，以及具体的概念类。"
    },
    {
        "title": "Bias-Aware Minimisation: Understanding and Mitigating Estimator Bias in\n  Private SGD",
        "url": "http://arxiv.org/abs/2308.12018v1",
        "pub_date": "2023-08-23",
        "summary": "Differentially private SGD (DP-SGD) holds the promise of enabling the safe\nand responsible application of machine learning to sensitive datasets. However,\nDP-SGD only provides a biased, noisy estimate of a mini-batch gradient. This\nrenders optimisation steps less effective and limits model utility as a result.\nWith this work, we show a connection between per-sample gradient norms and the\nestimation bias of the private gradient oracle used in DP-SGD. Here, we propose\nBias-Aware Minimisation (BAM) that allows for the provable reduction of private\ngradient estimator bias. We show how to efficiently compute quantities needed\nfor BAM to scale to large neural networks and highlight similarities to closely\nrelated methods such as Sharpness-Aware Minimisation. Finally, we provide\nempirical evidence that BAM not only reduces bias but also substantially\nimproves privacy-utility trade-offs on the CIFAR-10, CIFAR-100, and ImageNet-32\ndatasets.",
        "translated": "差异私有 SGD (DP-SGD)有望实现机器学习对敏感数据集的安全和负责任的应用。然而，DP-SGD 只提供了一个有偏的，有噪声的小批量梯度估计。这会降低优化步骤的效率，从而限制模型的实用性。通过这项工作，我们证明了每个样本的梯度范数和 DP-SGD 中使用的私有梯度预言的估计偏差之间的联系。在这里，我们提出了偏差意识最小化(BAM) ，允许私有梯度估计偏差的可证明的减少。我们展示了如何有效地计算 BAM 扩展到大型神经网络所需的数量，并强调了与紧密相关的方法(如锐度感知最小化)的相似性。最后，我们提供的经验证明是，BAM 不仅减少了偏见，而且大大改善了 CIFAR-10、 CIFAR-100和 ImageNet-32数据集的隐私效用权衡。"
    },
    {
        "title": "PARseL: Towards a Verified Root-of-Trust over seL4",
        "url": "http://arxiv.org/abs/2308.11921v1",
        "pub_date": "2023-08-23",
        "summary": "Widespread adoption and growing popularity of embedded/IoT/CPS devices make\nthem attractive attack targets. On low-to-mid-range devices, security features\nare typically few or none due to various constraints. Such devices are thus\nsubject to malware-based compromise. One popular defensive measure is Remote\nAttestation (RA) which allows a trusted entity to determine the current\nsoftware integrity of an untrusted remote device.\n  For higher-end devices, RA is achievable via secure hardware components. For\nlow-end (bare metal) devices, minimalistic hybrid (hardware/software) RA is\neffective, which incurs some hardware modifications. That leaves certain\nmid-range devices (e.g., ARM Cortex-A family) equipped with standard hardware\ncomponents, e.g., a memory management unit (MMU) and perhaps a secure boot\nfacility. In this space, seL4 (a verified microkernel with guaranteed process\nisolation) is a promising platform for attaining RA. HYDRA made a first step\ntowards this, albeit without achieving any verifiability or provable\nguarantees.\n  This paper picks up where HYDRA left off by constructing a PARseL\narchitecture, that separates all user-dependent components from the TCB. This\nleads to much stronger isolation guarantees, based on seL4 alone, and\nfacilitates formal verification. In PARseL, We use formal verification to\nobtain several security properties for the isolated RA TCB, including: memory\nsafety, functional correctness, and secret independence. We implement PARseL in\nF* and specify/prove expected properties using Hoare logic. Next, we\nautomatically translate the F* implementation to C using KaRaMeL, which\npreserves verified properties of PARseL C implementation (atop seL4). Finally,\nwe instantiate and evaluate PARseL on a commodity platform -- a SabreLite\nembedded device.",
        "translated": "嵌入式/物联网/CPS 设备的广泛采用和日益普及使它们成为有吸引力的攻击目标。在中低端设备上，由于各种约束，安全特性通常很少或根本没有。这些设备因此受到基于恶意软件的威胁。一种流行的防御措施是远程认证(Remote Attation，RA) ，它允许受信任的实体确定不受信任的远程设备的当前软件完整性。对于高端设备，RA 可以通过安全的硬件组件实现。对于低端(裸金属)设备，最小混合(硬件/软件) RA 是有效的，这会导致一些硬件修改。这使得某些中端设备(如 ARM Cortex-A 系列)配备了标准的硬件组件，如内存管理单元(MMU) ，或许还有一个安全的启动设施。在这个领域，seL4(一个有保证进程隔离的经过验证的微内核)是一个很有前途的实现 RA 的平台。九头蛇朝这个方向迈出了第一步，尽管没有实现任何可核查性或可证明的保证。本文通过构造一个 PARseL 体系结构，将所有与用户相关的组件从 TCB 中分离出来，从而重新拾起 HYDRA 中断的地方。这导致了更强大的隔离保障(仅基于 seL4) ，并促进了形式验证。在 PARsel 中，我们使用形式验证来获得隔离的 RA TCB 的几个安全属性，包括: 内存安全性、功能正确性和秘密独立性。我们在 F * 中实现 PARseL，并使用 Hoare 逻辑指定/证明期望的属性。接下来，我们使用 KaRaMeL 自动将 F * 实现转换为 C，它保留了 PARseL C 实现的经过验证的属性(在 seL4之上)。最后，我们在一个商品平台—— SabreLite 嵌入式设备上实例化和评估 PARseL。"
    },
    {
        "title": "Counting Distinct Elements Under Person-Level Differential Privacy",
        "url": "http://arxiv.org/abs/2308.12947v1",
        "pub_date": "2023-08-24",
        "summary": "We study the problem of counting the number of distinct elements in a dataset\nsubject to the constraint of differential privacy. We consider the challenging\nsetting of person-level DP (a.k.a. user-level DP) where each person may\ncontribute an unbounded number of items and hence the sensitivity is unbounded.\n  Our approach is to compute a bounded-sensitivity version of this query, which\nreduces to solving a max-flow problem. The sensitivity bound is optimized to\nbalance the noise we must add to privatize the answer against the error of the\napproximation of the bounded-sensitivity query to the true number of unique\nelements.",
        "translated": "我们研究在差分隐私约束下计算数据集中不同元素数量的问题。我们考虑具有挑战性的个人级 DP (也称为用户级 DP)设置，其中每个人可能贡献无限数量的项目，因此灵敏度是无限的。我们的方法是计算这个查询的有界灵敏度版本，这可以简化为解决最大流问题。灵敏度界是优化平衡噪声，我们必须添加私有化的答案与近似的有界灵敏度查询的唯一元素的真实数量的错误。"
    },
    {
        "title": "Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and\n  Vulnerabilities",
        "url": "http://arxiv.org/abs/2308.12833v1",
        "pub_date": "2023-08-24",
        "summary": "Spurred by the recent rapid increase in the development and distribution of\nlarge language models (LLMs) across industry and academia, much recent work has\ndrawn attention to safety- and security-related threats and vulnerabilities of\nLLMs, including in the context of potentially criminal activities.\nSpecifically, it has been shown that LLMs can be misused for fraud,\nimpersonation, and the generation of malware; while other authors have\nconsidered the more general problem of AI alignment. It is important that\ndevelopers and practitioners alike are aware of security-related problems with\nsuch models. In this paper, we provide an overview of existing - predominantly\nscientific - efforts on identifying and mitigating threats and vulnerabilities\narising from LLMs. We present a taxonomy describing the relationship between\nthreats caused by the generative capabilities of LLMs, prevention measures\nintended to address such threats, and vulnerabilities arising from imperfect\nprevention measures. With our work, we hope to raise awareness of the\nlimitations of LLMs in light of such security concerns, among both experienced\ndevelopers and novel users of such technologies.",
        "translated": "由于最近大型语言模型(LLM)在行业和学术界的发展和分布迅速增加，许多最近的工作已经引起了人们对 LLM 的安全和安保相关威胁和脆弱性的关注，包括在潜在犯罪活动的背景下。具体来说，已经证明 LLM 可能被误用于欺诈、模拟和恶意软件的生成; 而其他作者已经考虑了更普遍的人工智能对齐问题。重要的是，开发人员和从业人员都意识到这种模型存在与安全相关的问题。在本文中，我们提供了一个现有的概述-主要是科学-努力确定和减轻威胁和脆弱性所产生的长期管理模式。我们提出了一个分类法，描述由 LLM 的生成能力造成的威胁、旨在应对这些威胁的预防措施和不完善的预防措施所产生的脆弱性之间的关系。通过我们的工作，我们希望提高经验丰富的开发人员和此类技术的新用户对 LLM 在安全方面的局限性的认识。"
    },
    {
        "title": "Security Assessment and Hardening of Fog Computing Systems",
        "url": "http://arxiv.org/abs/2308.12707v1",
        "pub_date": "2023-08-24",
        "summary": "In recent years, there has been a shift in computing architectures, moving\naway from centralized cloud computing towards decentralized edge and fog\ncomputing. This shift is driven by factors such as the increasing volume of\ndata generated at the edge, the growing demand for real-time processing and\nlow-latency applications, and the need for improved privacy and data locality.\nAlthough this new paradigm offers numerous advantages, it also introduces\nsignificant security and reliability challenges. This paper aims to review the\narchitectures and technologies employed in fog computing and identify\nopportunities for developing novel security assessment and security hardening\ntechniques. These techniques include secure configuration and debloating to\nenhance the security of middleware, testing techniques to assess secure\ncommunication mechanisms, and automated rehosting to speed up the security\ntesting of embedded firmware.",
        "translated": "近年来，计算体系结构发生了转变，从集中式云计算转向分散式边缘和雾计算。推动这一转变的因素包括: 边缘产生的数据量不断增加; 对实时处理和低延迟应用程序的需求不断增长; 以及需要改善隐私和数据本地化。尽管这个新范例提供了许多优点，但它也引入了重大的安全性和可靠性挑战。本文旨在回顾雾计算中使用的体系结构和技术，并确定开发新的安全评估和安全加固技术的机会。这些技术包括用于增强中间件安全性的安全配置和删除、用于评估安全通信机制的测试技术以及用于加速嵌入式固件安全性测试的自动重托管。"
    },
    {
        "title": "kTrans: Knowledge-Aware Transformer for Binary Code Embedding",
        "url": "http://arxiv.org/abs/2308.12659v1",
        "pub_date": "2023-08-24",
        "summary": "Binary Code Embedding (BCE) has important applications in various reverse\nengineering tasks such as binary code similarity detection, type recovery,\ncontrol-flow recovery and data-flow analysis. Recent studies have shown that\nthe Transformer model can comprehend the semantics of binary code to support\ndownstream tasks. However, existing models overlooked the prior knowledge of\nassembly language. In this paper, we propose a novel Transformer-based\napproach, namely kTrans, to generate knowledge-aware binary code embedding. By\nfeeding explicit knowledge as additional inputs to the Transformer, and fusing\nimplicit knowledge with a novel pre-training task, kTrans provides a new\nperspective to incorporating domain knowledge into a Transformer framework. We\ninspect the generated embeddings with outlier detection and visualization, and\nalso apply kTrans to 3 downstream tasks: Binary Code Similarity Detection\n(BCSD), Function Type Recovery (FTR) and Indirect Call Recognition (ICR).\nEvaluation results show that kTrans can generate high-quality binary code\nembeddings, and outperforms state-of-the-art (SOTA) approaches on downstream\ntasks by 5.2%, 6.8%, and 12.6% respectively. kTrans is publicly available at:\nhttps://github.com/Learner0x5a/kTrans-release",
        "translated": "二进制代码嵌入(BCE)在各种逆向工程任务中有着重要的应用，例如二进制代码相似性检测、类型恢复、控制流恢复和数据流分析恢复。最近的研究表明，Transformer 模型能够理解二进制代码的语义以支持下游任务。然而，现有的模型忽视了汇编语言的先验知识。本文提出了一种新的基于变压器的方法，即 kTrans，来生成知识感知的二进制代码嵌入。通过将领域知识作为额外的输入输入到外显知识中，并将隐含的知识与一个新颖的培训前任务相融合，kTrans 提供了一个将领域知识整合到 Transformer 框架中的新视角。我们通过异常检测和可视化检查生成的嵌入，并将 kTrans 应用于三个下游任务: 二进制代码相似性检测(BCSD)、功能类型恢复(FTR)和间接调用识别(ICR)。评估结果表明，kTrans 能够生成高质量的二进制代码嵌入，并且在下游任务中的性能分别比最先进的 SOTA 方法高出5.2% 、6.8% 和12.6% 。kTrans 可以在以下 https://github.com/learner0x5a/kTrans-release 公开获得:"
    },
    {
        "title": "Introducing a New Alert Data Set for Multi-Step Attack Analysis",
        "url": "http://arxiv.org/abs/2308.12627v1",
        "pub_date": "2023-08-24",
        "summary": "Intrusion detection systems (IDS) reinforce cyber defense by autonomously\nmonitoring various data sources for traces of attacks. However, IDSs are also\ninfamous for frequently raising false positives and alerts that are difficult\nto interpret without context. This results in high workloads on security\noperators who need to manually verify all reported alerts, often leading to\nfatigue and incorrect decisions. To generate more meaningful alerts and\nalleviate these issues, the research domain focused on multi-step attack\nanalysis proposes approaches for filtering, clustering, and correlating IDS\nalerts, as well as generation of attack graphs. Unfortunately, existing data\nsets are outdated, unreliable, narrowly focused, or only suitable for IDS\nevaluation. Since hardly any suitable benchmark data sets are publicly\navailable, researchers often resort to private data sets that prevent\nreproducibility of evaluations. We therefore generate a new alert data set that\nwe publish alongside this paper. The data set contains alerts from three\ndistinct IDSs monitoring eight executions of a multi-step attack as well as\nsimulations of normal user behavior. To illustrate the potential of our data\nset, we experiment with alert prioritization as well as two open-source tools\nfor meta-alert generation and attack graph extraction.",
        "translated": "入侵检测系统(IDS)通过自主监控各种数据源以寻找攻击痕迹来加强网络防御。然而，入侵检测系统也因经常发出错误的肯定和警报而臭名昭著，这些错误的肯定和警报在没有上下文的情况下很难解释。这导致安全操作人员的工作量很大，他们需要手动验证所有报告的警报，这往往导致疲劳和不正确的决定。为了产生更多有意义的警报并缓解这些问题，以多步骤攻击分析为核心的研究领域提出了过滤、聚类和关联 IDS 警报的方法，以及攻击图的生成方法。不幸的是，现有的数据集已经过时、不可靠、关注范围狭窄，或者只适合 IDS 评估。由于几乎没有任何合适的基准数据集公开提供，研究人员往往诉诸私人数据集，以防止评价的重复性。因此，我们生成一个新的警报数据集，与本文一起发布。该数据集包含来自三个不同的 IDS 的警报，这些 IDS 监控一个多步骤攻击的八次执行以及正常用户行为的模拟。为了说明我们的数据集的潜力，我们试验了警报优先级以及两个用于元警报生成和攻击图提取的开源工具。"
    },
    {
        "title": "Analog Multi-Party Computing: Locally Differential Private Protocols for\n  Collaborative Computations",
        "url": "http://arxiv.org/abs/2308.12544v1",
        "pub_date": "2023-08-24",
        "summary": "We consider a fully decentralized scenario in which no central trusted entity\nexists and all clients are honest-but-curious. The state-of-the-art approaches\nto this problem often rely on cryptographic protocols, such as multiparty\ncomputation (MPC), that require mapping real-valued data to a discrete\nalphabet, specifically a finite field. These approaches, however, can result in\nsubstantial accuracy losses due to computation overflows. To address this\nissue, we propose A-MPC, a private analog MPC protocol that performs all\ncomputations in the analog domain. We characterize the privacy of individual\ndatasets in terms of $(\\epsilon, \\delta)$-local differential privacy, where the\nprivacy of a single record in each client's dataset is guaranteed against other\nparticipants. In particular, we characterize the required noise variance in the\nGaussian mechanism in terms of the required $(\\epsilon,\\delta)$-local\ndifferential privacy parameters by solving an optimization problem.\nFurthermore, compared with existing decentralized protocols, A-MPC keeps the\nprivacy of individual datasets against the collusion of all other participants,\nthereby, in a notably significant improvement, increasing the maximum number of\ncolluding clients tolerated in the protocol by a factor of three compared with\nthe state-of-the-art collaborative learning protocols. Our experiments\nillustrate that the accuracy of the proposed $(\\epsilon,\\delta)$-locally\ndifferential private logistic regression and linear regression models trained\nin a fully-decentralized fashion using A-MPC closely follows that of a\ncentralized one performed by a single trusted entity.",
        "translated": "我们考虑一个完全分散的场景，其中不存在中央信任实体，所有客户都是诚实但好奇的。解决这个问题的最先进的方法通常依赖于加密协议，如多方计算(MPC) ，这需要将实值数据映射到离散字母表，特别是有限字段。然而，由于计算溢出，这些方法可能导致大量的准确性损失。为了解决这个问题，我们提出 A-MPC，一个私有的模拟 MPC 协议，执行模拟域中的所有计算。我们用 $(epsilon，delta) $- 本地差分隐私来描述单个数据集的隐私，其中每个客户端数据集中的单个记录的隐私可以针对其他参与者得到保证。特别地，我们通过求解一个差分隐私最佳化问题，用所需的 $(epsilon，delta) $- 局部参数来刻画高斯机制中所需的噪声方差。此外，与现有的分散协议相比，A-MPC 保留了个人数据集的隐私，以防止所有其他参与者的合谋，因此，与最先进的合作学习协议相比，显着提高了协议中可容忍的合谋客户的最大数量，增加了3倍。我们的实验表明，所提出的 $(epsilon，delta) $- 本地差分私有 Logit模型和线性回归模型的精确性，以完全分散的方式使用 A-MPC 进行训练，与由单个可信实体执行的集中式模型的精确性非常接近。"
    },
    {
        "title": "Privacy-Preserving Discretized Spiking Neural Networks",
        "url": "http://arxiv.org/abs/2308.12529v1",
        "pub_date": "2023-08-24",
        "summary": "The rapid development of artificial intelligence has brought considerable\nconvenience, yet also introduces significant security risks. One of the\nresearch hotspots is to balance data privacy and utility in the real world of\nartificial intelligence. The present second-generation artificial neural\nnetworks have made tremendous advances, but some big models could have really\nhigh computational costs. The third-generation neural network, SNN (Spiking\nNeural Network), mimics real neurons by using discrete spike signals, whose\nsequences exhibit strong sparsity, providing advantages such as low energy\nconsumption and high efficiency. In this paper, we construct a framework to\nevaluate the homomorphic computation of SNN named FHE-DiSNN that enables SNN to\nachieve good prediction performance on encrypted data. First, benefitting from\nthe discrete nature of spike signals, our proposed model avoids the errors\nintroduced by discretizing activation functions. Second, by applying\nbootstrapping, we design new private preserving functions FHE-Fire and\nFHE-Reset, through which noise can be refreshed, allowing us to evaluate SNN\nfor an arbitrary number of operations. Furthermore, We improve the\ncomputational efficiency of FHE-DiSNN while maintaining a high level of\naccuracy. Finally, we evaluate our model on the MNIST dataset. The experiments\nshow that FHE-DiSNN with 30 neurons in the hidden layer achieves a minimum\nprediction accuracy of 94.4%. Under optimal parameters, it achieves a 95.1%\naccuracy, with only a 0.6% decrease compared to the original SNN (95.7%). These\nresults demonstrate the superiority of SNN over second-generation neural\nnetworks for homomorphic evaluation.",
        "translated": "人工智能的迅速发展带来了相当大的便利，但也带来了重大的安全风险。在人工智能的现实世界中，如何平衡数据的隐私性和实用性是研究的热点之一。目前的第二代人工神经网络已经取得了巨大的进展，但一些大型模型可能具有非常高的计算成本。第三代神经网络(SNN)利用离散的尖峰信号模拟真实的神经元，其序列具有很强的稀疏性，具有低能耗、高效率等优点。本文构造了一个评估 SNN 同态计算的框架 FHE-DiSNN，使 SNN 能够对加密数据实现良好的预测性能。首先，利用尖峰信号的离散特性，避免了离散激活函数引入的误差。其次，通过应用自举，我们设计了新的私有保护函数 FHE-Fire 和 FHE-Reset，通过它们可以刷新噪声，允许我们对任意数量的操作评估 SNN。此外，我们提高了 FHE-DiSNN 的计算效率，同时保持了较高的精度。最后，我们在 MNIST 数据集上评估我们的模型。实验结果表明，隐层30个神经元的 FHE-DiSNN 预测准确率最低为94.4% 。在最佳参数下，该算法的准确率达到95.1% ，与原来的 SNN (95.7%)相比仅下降了0.6% 。这些结果证明了 SNN 在同态评价方面优于第二代神经网络。"
    },
    {
        "title": "Privacy engineering through obfuscation",
        "url": "http://arxiv.org/abs/2308.12514v1",
        "pub_date": "2023-08-24",
        "summary": "Obfuscation in privacy engineering denotes a diverse set of data operations\naimed at reducing the privacy loss that users incur in by participating in\ndigital systems. Obfuscation's domain of application is vast:\nprivacy-preserving database analysis, location-based privacy, private web\nsearch or privacy-friendly recommender systems are but a few examples of the\ncontexts in which privacy engineers have resorted to obfuscation. Yet an\nunderstanding of the role that obfuscation, in general, plays in the\nengineering of privacy has so far proved elusive. Similarly, we lack a cohesive\nview of the wide array of privacy measures that assist the evaluation of\nobfuscation technologies. This paper contributes to closing these research\ngaps. First, we provide a general analysis framework that brings together a\nmultiplicity of obfuscation methods under the same analytical umbrella. Second,\nwe distinguish between mechanism-centred and attack-centred evaluation, making\nexplicit a hierarchy of assumptions behind privacy measures that assists and\ndemystifies obfuscation tools' evaluation. Finally, we examine the role that\nobfuscation technology plays in privacy engineering by introducing the concepts\nof personal and public utility and distinguishing between utility-degrading and\nutility-preserving obfuscation. We observe that public utility requirements\nrequire us to resort to utility-degrading obfuscation to arbitrarily reduce\nprivacy loss. Conversely, personal utility requirements do not, in theory,\nimpose such a privacy-utility trade-off, and we illustrate how to perform\nutility-preserving obfuscation through chaff.",
        "translated": "隐私工程中的模糊处理是指一系列旨在减少用户因参与数字系统而导致的隐私损失的数据操作。模糊处理的应用领域非常广泛: 保护隐私的数据库分析、基于位置的隐私、私有网络搜索或隐私友好的推荐系统只是隐私工程师采用模糊处理的几个例子。然而，到目前为止，对于模糊处理在隐私工程中所扮演的角色的理解被证明是难以捉摸的。同样，我们缺乏一个广泛的隐私措施，协助评估混淆技术的内聚视图。本文有助于弥补这些研究差距。首先，我们提供了一个通用的分析框架，在同一个分析框架下汇集了多种模糊处理方法。其次，我们区分了以机制为中心的评估和以攻击为中心的评估，明确了隐私措施背后的一系列假设，这些假设有助于模糊处理工具的评估并使之不再神秘。最后，通过引入个人效用和公共效用的概念以及区分效用降低和效用保持的模糊，分析了模糊技术在隐私工程中的作用。我们注意到，公用事业的要求要求我们采用有损公用事业的模糊处理方法，以任意减少隐私损失。相反，从理论上讲，个人效用需求并不强加这样的隐私-效用权衡，我们举例说明如何通过箔条执行效用保护模糊化。"
    },
    {
        "title": "Trend and Emerging Types of 419 Scams",
        "url": "http://arxiv.org/abs/2308.12448v1",
        "pub_date": "2023-08-23",
        "summary": "Technological advancements have revolutionized various aspects of human life,\nfacilitating communication, business operations, healthcare, education, and\nenvironmental monitoring. However, this increased reliance on technology has\nalso led to a surge in cybercrime, including cyber scams. The \"419 scam\" or\nNigerian scam has been a persistent problem for decades, encompassing frauds\nlike advance fee scams, fake lotteries, and black money scams. Initially\nprevalent through postal mail and later via fax, the scam has now transitioned\nto email. This study aims to identify recent types of 419 scam emails,\nparticularly after the covid 19 pandemic, and explore commonly used email\nsubjects. Analysis of the sample 419 scam emails revealed trending scams like\nlucky winner, threat of exposure, business/partnership proposals, investment,\ncancer/long-term illness, fund, and compensation scams. Emerging scams included\nCOVID-related, cryptocurrency, marketing contact, and software development\nscams. Irrespective of the scam type, scammers commonly employed email subjects\nsuch as 'Re', 'Good day', 'Greetings', 'Dear friend', 'Confirm', 'Attention',\nand 'Hello dear'. The severity of cybercrime, especially the 419 scams, cannot\nbe overstated, as it erodes trust, causes financial losses, and hampers\nNigeria's reputation and economic progress. Combatting cyber scams and\nenhancing cybersecurity measures are crucial to protect individuals and\norganizations from falling victim to these fraudulent schemes.",
        "translated": "技术进步已经彻底改变了人类生活的各个方面，促进了通信、商业运营、医疗保健、教育和环境监测。然而，这种对技术的日益依赖也导致了包括网络诈骗在内的网络犯罪激增。“419骗局”或尼日利亚骗局几十年来一直是一个持续存在的问题，其中包括预付费诈骗、假彩票和黑钱诈骗。这种骗案最初以邮寄方式流行，后来以传真方式流行，现已转为以电子邮件方式进行。这项研究旨在找出最近出现的419类诈骗电邮，特别是在冠状病毒疾病19大流行后，并探讨常用的电邮题目。对419封诈骗电子邮件样本的分析揭示了趋势性诈骗，如幸运赢家、曝光威胁、商业/合作提案、投资、癌症/长期疾病、基金和补偿诈骗。新出现的骗局包括与 COVID 相关的、加密货币、营销联系和软件开发骗局。不论骗案类别为何，骗徒通常都会使用电邮主题，例如“ Re”、“ Good day”、“问候”、“親愛的朋友”、“確认”、“關注”及“你好，親愛的”等。网络犯罪的严重性，尤其是419起诈骗案，怎么强调都不为过，因为它侵蚀了信任，造成了经济损失，并阻碍了尼日利亚的声誉和经济发展。打击网络欺诈和加强网络安全措施对于保护个人和组织免受这些欺诈计划之害至关重要。"
    },
    {
        "title": "BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input\n  Detection",
        "url": "http://arxiv.org/abs/2308.12439v1",
        "pub_date": "2023-08-23",
        "summary": "We present a novel defense, against backdoor attacks on Deep Neural Networks\n(DNNs), wherein adversaries covertly implant malicious behaviors (backdoors)\ninto DNNs. Our defense falls within the category of post-development defenses\nthat operate independently of how the model was generated. The proposed defense\nis built upon a novel reverse engineering approach that can directly extract\nbackdoor functionality of a given backdoored model to a backdoor expert model.\nThe approach is straightforward -- finetuning the backdoored model over a small\nset of intentionally mislabeled clean samples, such that it unlearns the normal\nfunctionality while still preserving the backdoor functionality, and thus\nresulting in a model (dubbed a backdoor expert model) that can only recognize\nbackdoor inputs. Based on the extracted backdoor expert model, we show the\nfeasibility of devising highly accurate backdoor input detectors that filter\nout the backdoor inputs during model inference. Further augmented by an\nensemble strategy with a finetuned auxiliary model, our defense, BaDExpert\n(Backdoor Input Detection with Backdoor Expert), effectively mitigates 16 SOTA\nbackdoor attacks while minimally impacting clean utility. The effectiveness of\nBaDExpert has been verified on multiple datasets (CIFAR10, GTSRB and ImageNet)\nacross various model architectures (ResNet, VGG, MobileNetV2 and Vision\nTransformer).",
        "translated": "我们提出了一个新的防御，对深度神经网络(DNN)的后门攻击，其中对手秘密植入恶意行为(后门)到 DNN。我们的防御属于开发后防御的范畴，它独立于模型是如何生成的。建议的防御是建立在一个新的逆向工程方法，可以直接提取后门模型的后门功能给后门专家模型。这种方法很简单——通过一小组故意错误标记的干净样本来微调后门模型，这样它就可以在保留后门功能的同时忘记正常的功能，从而产生一个只能识别后门输入的模型(称为后门专家模型)。在提取后门专家模型的基础上，论证了设计高精度后门输入检测器的可行性，该检测器能够在模型推理过程中滤除后门输入。我们的防御措施 BaDExpert (借助后门专家的后门输入检测)进一步增强了具有微调辅助模型的集成策略，有效地减轻了16次 SOTA 后门攻击，同时将对清洁效用的影响降至最低。BaDExpert 的有效性已经在多个数据集(CIFAR10，GTSRB 和 ImageNet)上通过各种模型架构(ResNet，VGG，MobileNetV2和 Vision Transformer)得到验证。"
    },
    {
        "title": "Optimizing Hierarchical Queries for the Attribution Reporting API",
        "url": "http://arxiv.org/abs/2308.13510v1",
        "pub_date": "2023-08-25",
        "summary": "We study the task of performing hierarchical queries based on summary reports\nfrom the {\\em Attribution Reporting API} for ad conversion measurement. We\ndemonstrate that methods from optimization and differential privacy can help\ncope with the noise introduced by privacy guardrails in the API. In particular,\nwe present algorithms for (i) denoising the API outputs and ensuring\nconsistency across different levels of the tree, and (ii) optimizing the\nprivacy budget across different levels of the tree. We provide an experimental\nevaluation of the proposed algorithms on public datasets.",
        "translated": "我们研究了基于来自{ em 归属报告 API }的摘要报告执行分层查询以进行广告转换度量的任务。我们展示了优化和差分隐私的方法可以帮助处理 API 中隐私护栏引入的噪音。特别是，我们提出的算法(i)去噪的 API 输出和确保一致性的不同层次的树，以及(ii)优化不同层次的树的隐私预算。我们对提出的算法在公共数据集上进行了实验评估。"
    },
    {
        "title": "On the Practicality of Dynamic Updates in Fast Searchable Encryption",
        "url": "http://arxiv.org/abs/2308.13486v1",
        "pub_date": "2023-08-25",
        "summary": "Searchable encrypted (SE) indexing systems are a useful tool for utilizing\ncloud services to store and manage sensitive information. However, much of the\nwork on SE systems to date has remained theoretical. In order to make them of\npractical use, more work is needed to develop optimal protocols and working\nmodels for them. This includes, in particular, the creation of a working update\nmodel in order to maintain an encrypted index of a dynamic document set such as\nan email inbox. I have created a working, real-world end-to-end SE\nimplementation that satisfies these needs, including the first empirical\nperformance evaluation of the dynamic SE update operation. In doing so, I show\na viable path to move from the theoretical concepts described by previous\nresearchers to a future production-worthy implementation and identify issues\nfor follow-on investigation.",
        "translated": "可搜索加密(SE)索引系统是利用云服务存储和管理敏感信息的有用工具。然而，迄今为止关于 SE 系统的大部分工作仍然停留在理论层面。为了使它们具有实际应用价值，需要开发更多的工作来为它们开发最优协议和工作模型。这尤其包括创建工作更新模型，以维护动态文档集(如电子邮件收件箱)的加密索引。我已经创建了一个工作的、真实的端到端 SE 实现来满足这些需求，包括对动态 SE 更新操作的第一次经验性性能评估。在这样做的过程中，我展示了一条可行的路径，从以前的研究人员描述的理论概念转移到未来值得生产的实现，并确定后续调查的问题。"
    },
    {
        "title": "Falcon: Accelerating Homomorphically Encrypted Convolutions for\n  Efficient Private Mobile Network Inference",
        "url": "http://arxiv.org/abs/2308.13189v1",
        "pub_date": "2023-08-25",
        "summary": "Efficient networks, e.g., MobileNetV2, EfficientNet, etc, achieves\nstate-of-the-art (SOTA) accuracy with lightweight computation. However,\nexisting homomorphic encryption (HE)-based two-party computation (2PC)\nframeworks are not optimized for these networks and suffer from a high\ninference overhead. We observe the inefficiency mainly comes from the packing\nalgorithm, which ignores the computation characteristics and the communication\nbottleneck of homomorphically encrypted depthwise convolutions. Therefore, in\nthis paper, we propose Falcon, an effective dense packing algorithm for\nHE-based 2PC frameworks. Falcon features a zero-aware greedy packing algorithm\nand a communication-aware operator tiling strategy to improve the packing\ndensity for depthwise convolutions. Compared to SOTA HE-based 2PC frameworks,\ne.g., CrypTFlow2, Iron and Cheetah, Falcon achieves more than 15.6x, 5.1x and\n1.8x latency reduction, respectively, at operator level. Meanwhile, at network\nlevel, Falcon allows for 1.4% and 4.2% accuracy improvement over Cheetah on\nCIFAR-100 and TinyImagenet datasets with iso-communication, respecitvely.",
        "translated": "高效的网络，例如 MobileNetV2、 EfficientNet 等，通过轻量级计算实现了最先进的(SOTA)精度。然而，现有的基于同态加密(HE)的两方计算(2PC)框架并没有针对这些网络进行优化，并且存在很高的推理开销。同态加密的深度卷积算法忽略了同态加密的计算特性和通信瓶颈。因此，本文针对基于 HE 的2PC 框架提出了一种有效的密集打包算法 Falcon。Falcon 具有零感知贪婪打包算法和通信感知算子平铺策略，以提高深度卷积的打包密度。与基于 SOTA HE 的2PC 框架(例如 CrypTFlow2、 Iron 和 Cheetah)相比，Falcon 在操作员级分别实现了超过15.6 x、5.1 x 和1.8 x 的延迟减少。与此同时，在网络层面上，Falcon 在 CIFAR-100和 TinyImagenet 数据集上分别比 Cheetah 提高了1.4% 和4.2% 的准确率。"
    },
    {
        "title": "A Large-Scale Study of IoT Security Weaknesses and Vulnerabilities in\n  the Wild",
        "url": "http://arxiv.org/abs/2308.13141v1",
        "pub_date": "2023-08-25",
        "summary": "Internet of Things (IoT) is defined as the connection between places and\nphysical objects (i.e., things) over the internet/network via smart computing\ndevices. We observed that IoT software developers share solutions to\nprogramming questions as code examples on three Stack Exchange Q&amp;A sites: Stack\nOverflow (SO), Arduino, and Raspberry Pi. Previous research studies found\nvulnerabilities/weaknesses in C/C++ code examples shared in Stack Overflow.\nHowever, the studies did not investigate C/C++ code examples related to IoT.\nThe studies investigated SO code examples only. In this paper, we conduct a\nlarge-scale empirical study of all IoT C/C++ code examples shared in the three\nStack Exchange sites, i.e., SO, Arduino, and Raspberry Pi. From the 11,329\nobtained code snippets from the three sites, we identify 29 distinct CWE\n(Common Weakness Enumeration) types in 609 snippets. These CWE types can be\ncategorized into 8 general weakness categories, and we observe that evaluation,\nmemory, and initialization related weaknesses are the most common to be\nintroduced by users when posting programming solutions. Furthermore, we find\nthat 39.58% of the vulnerable code snippets contain instances of CWE types that\ncan be mapped to real-world occurrences of those CWE types (i.e. CVE\ninstances). The most number vulnerable IoT code examples was found in Arduino,\nfollowed by SO, and Raspberry Pi. Memory type vulnerabilities are on the rise\nin the sites. For example, from the 3595 mapped CVE instances, we find that\n28.99% result in Denial of Service (DoS) errors, which is particularly harmful\nfor network reliant IoT devices such as smart cars. Our study results can guide\nvarious IoT stakeholders to be aware of such vulnerable IoT code examples and\nto inform IoT researchers during their development of tools that can help\nprevent developers the sharing of such vulnerable code examples in the sites.\n[Abridged].",
        "translated": "物联网(IoT)的定义是通过智能计算设备在互联网/网络上连接地点和物理对象(即物体)。我们观察到，物联网软件开发人员在三个 Stack Exchange Q & A 站点上以代码示例的形式共享编程问题的解决方案: Stack Overflow (SO)、 Arduino 和 Raspberry Pi。先前的研究发现，在堆栈溢出中共享的 C/C + + 代码示例中存在漏洞/弱点。然而，这些研究并没有调查与物联网相关的 C/C + + 代码示例。这些研究只考察了 SO 代码示例。在本文中，我们对三个 Stack Exchange 站点(即 SO、 Arduino 和 Raspberry Pi)共享的所有 IoT C/C + + 代码示例进行了大规模的实证研究。从这三个站点获得的11,329个代码段中，我们在609个代码段中确定了29个不同的 CWE (公共弱点枚举)类型。这些 CWE 类型可以分为8个常见的弱点类别，我们观察到，评估、内存和初始化相关的弱点是用户在发布编程解决方案时最常引入的。此外，我们发现39.58% 的易受攻击的代码段包含 CWE 类型的实例，这些实例可以映射到这些 CWE 类型的实际出现(即 CVE 实例)。最容易受到攻击的物联网代码例子是在 Arduino，紧随其后的是 SO 和覆盆子派。内存类型的漏洞在站点中呈上升趋势。例如，从3595个映射的 CVE 实例中，我们发现28.99% 会导致分布式拒绝服务攻击(DoS)错误，这对依赖网络的物联网设备(如智能汽车)尤其有害。我们的研究结果可以引导不同的物联网利益相关者意识到这些易受攻击的物联网代码示例，并在物联网研究人员开发工具时通知他们，这些工具可以帮助防止开发人员在网站上共享这些易受攻击的代码示例。[摘要]。"
    },
    {
        "title": "ZeroLeak: Using LLMs for Scalable and Cost Effective Side-Channel\n  Patching",
        "url": "http://arxiv.org/abs/2308.13062v1",
        "pub_date": "2023-08-24",
        "summary": "Security critical software, e.g., OpenSSL, comes with numerous side-channel\nleakages left unpatched due to a lack of resources or experts. The situation\nwill only worsen as the pace of code development accelerates, with developers\nrelying on Large Language Models (LLMs) to automatically generate code. In this\nwork, we explore the use of LLMs in generating patches for vulnerable code with\nmicroarchitectural side-channel leakages. For this, we investigate the\ngenerative abilities of powerful LLMs by carefully crafting prompts following a\nzero-shot learning approach. All generated code is dynamically analyzed by\nleakage detection tools, which are capable of pinpointing information leakage\nat the instruction level leaked either from secret dependent accesses or\nbranches or vulnerable Spectre gadgets, respectively. Carefully crafted prompts\nare used to generate candidate replacements for vulnerable code, which are then\nanalyzed for correctness and for leakage resilience. From a cost/performance\nperspective, the GPT4-based configuration costs in API calls a mere few cents\nper vulnerability fixed. Our results show that LLM-based patching is far more\ncost-effective and thus provides a scalable solution. Finally, the framework we\npropose will improve in time, especially as vulnerability detection tools and\nLLMs mature.",
        "translated": "安全性关键软件，例如 OpenSSL，由于缺乏资源或专家，会出现大量的边通道泄漏而无法修补。随着代码开发速度的加快，这种情况只会变得更糟，因为开发人员依赖于大型语言模型(LLM)来自动生成代码。在这项工作中，我们探讨了 LLM 在为微架构侧通道泄漏的脆弱代码生成补丁中的应用。为此，我们调查了强大的 LLM 的生成能力，通过仔细制作提示符合零拍学习方法。所有生成的代码都通过泄漏检测工具进行动态分析，这些工具能够在指令级别精确定位分别从秘密依赖访问或分支或易受攻击的 Spectre gadget 泄漏的信息泄露。精心制作的提示用于为易受攻击的代码生成候选替换，然后对其进行正确性和泄漏弹性分析。从成本/性能的角度来看，API 中基于 GPT4的配置成本仅为每修复一个漏洞调用几美分。我们的研究结果表明，基于 LLM 的补丁更具成本效益，因此提供了一个可扩展的解决方案。最后，我们提出的框架将及时得到改进，特别是随着漏洞检测工具和 LLM 的成熟。"
    },
    {
        "title": "Masquerade: Simple and Lightweight Transaction Reordering Mitigation in\n  Blockchains",
        "url": "http://arxiv.org/abs/2308.15347v1",
        "pub_date": "2023-08-29",
        "summary": "Blockchains offer strong security gurarantees, but cannot protect users\nagainst the ordering of transactions. Players such as miners, bots and\nvalidators can reorder various transactions and reap significant profits,\ncalled the Maximal Extractable Value (MEV). In this paper, we propose an MEV\naware protocol design called Masquerade, and show that it will increase user\nsatisfaction and confidence in the system. We propose a strict per-transaction\nlevel of ordering to ensure that a transaction is committed either way even if\nit is revealed. In this protocol, we introduce the notion of a \"token\" to\nmitigate the actions taken by an adversary in an attack scenario. Such tokens\ncan be purchased voluntarily by users, who can then choose to include the token\nnumbers in their transactions. If the users include the token in their\ntransactions, then our protocol requires the block-builder to order the\ntransactions strictly according to token numbers. We show through extensive\nsimulations that this reduces the probability that the adversaries can benefit\nfrom MEV transactions as compared to existing current practices.",
        "translated": "区块链提供了强有力的安全保证，但不能保护用户免受事务排序的影响。玩家，如矿工，机器人和确认器可以重新安排各种交易，并获得重大利润，称为最大可提取价值(MEV)。在本文中，我们提出了一个 MEV 感知的协议设计，称为 Masquerade，并表明它将提高用户的满意度和信心系统。我们建议采用严格的每个事务排序级别，以确保事务以任何一种方式提交，即使事务显示出来也是如此。在这个协议中，我们引入了“令牌”的概念，以减轻攻击场景中敌方采取的行动。这些令牌可以由用户自愿购买，然后用户可以选择在其事务中包含令牌号。如果用户在其事务中包含令牌，那么我们的协议要求块构建器严格按照令牌号对事务进行排序。我们通过大量的模拟表明，与现有的实践相比，这降低了对手从 MEV 交易中获益的可能性。"
    },
    {
        "title": "Imperceptible Adversarial Attack on Deep Neural Networks from Image\n  Boundary",
        "url": "http://arxiv.org/abs/2308.15344v1",
        "pub_date": "2023-08-29",
        "summary": "Although Deep Neural Networks (DNNs), such as the convolutional neural\nnetworks (CNN) and Vision Transformers (ViTs), have been successfully applied\nin the field of computer vision, they are demonstrated to be vulnerable to\nwell-sought Adversarial Examples (AEs) that can easily fool the DNNs. The\nresearch in AEs has been active, and many adversarial attacks and explanations\nhave been proposed since they were discovered in 2014. The mystery of the AE's\nexistence is still an open question, and many studies suggest that DNN training\nalgorithms have blind spots. The salient objects usually do not overlap with\nboundaries; hence, the boundaries are not the DNN model's attention.\nNevertheless, recent studies show that the boundaries can dominate the behavior\nof the DNN models. Hence, this study aims to look at the AEs from a different\nperspective and proposes an imperceptible adversarial attack that systemically\nattacks the input image boundary for finding the AEs. The experimental results\nhave shown that the proposed boundary attacking method effectively attacks six\nCNN models and the ViT using only 32% of the input image content (from the\nboundaries) with an average success rate (SR) of 95.2% and an average peak\nsignal-to-noise ratio of 41.37 dB. Correlation analyses are conducted,\nincluding the relation between the adversarial boundary's width and the SR and\nhow the adversarial boundary changes the DNN model's attention. This paper's\ndiscoveries can potentially advance the understanding of AEs and provide a\ndifferent perspective on how AEs can be constructed.",
        "translated": "虽然深层神经网络(DNN) ，如卷积神经网络(CNN)和视觉变换器(ViTs) ，已成功地应用于计算机视觉领域，他们被证明是易受攻击的对抗性例子(AE) ，可以很容易地欺骗 DNN。对不良事件的研究一直很活跃，自2014年发现不良事件以来，人们提出了许多对抗性的攻击和解释。AE 存在的秘密仍然是一个悬而未决的问题，许多研究表明 DNN 训练算法有盲点。突出对象通常不与边界重叠; 因此，边界不是 DNN 模型的注意力。然而，最近的研究表明，边界可以支配 DNN 模型的行为。因此，本研究旨在从不同的角度来看待不良事件，并提出一种不易察觉的对抗性攻击，系统地攻击输入图像的边界，以发现不良事件。实验结果表明，提出的边界攻击方法只需要32% 的输入图像内容(来自边界)就可以有效地攻击6个 CNN 模型和 viT，平均成功率(SR)为95.2% ，平均峰值信噪比为41.37 dB。并进行了相关分析，包括敌对边界宽度与 SR 的关系以及敌对边界对 DNN 模型注意力的影响。本文的发现有可能提高对不动产的认识，并为如何构造不动产提供一个不同的视角。"
    },
    {
        "title": "Longest-chain Attacks: Difficulty Adjustment and Timestamp Verifiability",
        "url": "http://arxiv.org/abs/2308.15312v1",
        "pub_date": "2023-08-29",
        "summary": "We study an adversary who attacks a Proof-of-Work (POW) blockchain by\nselfishly constructing an alternative longest chain. We characterize optimal\nstrategies employed by the adversary when a difficulty adjustment rule al\\`a\nBitcoin applies. As time (namely the times-tamp specified in each block) in\nmost permissionless POW blockchains is somewhat subjective, we focus on two\nextreme scenarios: when time is completely verifiable, and when it is\ncompletely unverifiable. We conclude that an adversary who faces a difficulty\nadjustment rule will find a longest-chain attack very challenging when\ntimestamps are verifiable. POW blockchains with frequent difficulty adjustments\nrelative to time reporting flexibility will be substantially more vulnerable to\nlongest-chain attacks. Our main fining provides guidance on the design of\ndifficulty adjustment rules and demonstrates the importance of timestamp\nverifiability.",
        "translated": "我们研究一个攻击工作证明(POW)区块链的对手，通过自私地构建一个替代的最长链。我们描述了比特币应用困难调整规则时对手使用的最优策略。由于时间(即在每个块中指定的次夯)在大多数无许可的战俘区块链中是有些主观的，我们关注两个极端的情况: 时间是完全可验证的，和时间是完全不可验证的。我们的结论是，当时间戳可验证时，面临困难调整规则的对手将发现最长链攻击非常具有挑战性。相对于时间报告的灵活性而言，经常难以调整的战俘区块链更容易受到最长链的攻击。我们的主要结论为难度调整规则的设计提供了指导，并证明了时间戳可验证性的重要性。"
    },
    {
        "title": "Trustless Privacy-Preserving Data Aggregation on Ethereum with Hypercube\n  Network Topology",
        "url": "http://arxiv.org/abs/2308.15267v1",
        "pub_date": "2023-08-29",
        "summary": "The privacy-preserving data aggregation is a critical problem for many\napplications where multiple parties need to collaborate with each other\nprivately to arrive at certain results. Blockchain, as a database shared across\nthe network, provides an underlying platform on which such aggregations can be\ncarried out with a decentralized manner. Therefore, in this paper, we have\nproposed a scalable privacy-preserving data aggregation protocol for summation\non the Ethereum blockchain by integrating several cryptographic primitives\nincluding commitment scheme, asymmetric encryption and zero-knowledge proof\nalong with the hypercube network topology. The protocol consists of four stages\nas contract deployment, user registration, private submission and proof\nverification. The analysis of the protocol is made with respect to two main\nperspectives as security and scalability including computational,\ncommunicational and storage overheads. In the paper, the zero-knowledge proof,\nsmart contract and web user interface models for the protocol are provided. We\nhave performed an experimental study in order to identify the required gas\ncosts per individual and per system. The general formulation is provided to\ncharacterize the changes in gas costs for the increasing number of users. The\nzero-knowledge proof generation and verification times are also measured.",
        "translated": "保护隐私的数据聚合对于许多应用程序来说是一个关键问题，在这些应用程序中，多方需要彼此私下协作以达到一定的结果。区块链作为一个跨网络共享的数据库，提供了一个底层平台，在这个平台上可以以一种分散的方式进行这种聚合。因此，在本文中，我们提出了一个可扩展的保护隐私的数据聚合协议，用于以太区块链上的求和，该协议集成了包括承诺方案、非对称加密和零知识证明在内的几个密码原语，同时结合了超立方体网络拓扑。该协议包括合同部署、用户注册、私有提交和证明验证四个阶段。从安全性和可扩展性两个主要角度对协议进行了分析，包括计算、通信和存储开销。本文给出了该协议的零知识证明、智能契约和网络用户界面模型。我们进行了一项实验研究，以确定每个人和每个系统所需的天然气成本。提供一般公式是为了说明越来越多的用户的气体成本的变化。测量了零知识证明的生成和验证时间。"
    },
    {
        "title": "Shedding Light on CVSS Scoring Inconsistencies: A User-Centric Study on\n  Evaluating Widespread Security Vulnerabilities",
        "url": "http://arxiv.org/abs/2308.15259v1",
        "pub_date": "2023-08-29",
        "summary": "The Common Vulnerability Scoring System (CVSS) is a popular method for\nevaluating the severity of vulnerabilities in vulnerability management. In the\nevaluation process, a numeric score between 0 and 10 is calculated, 10 being\nthe most severe (critical) value. The goal of CVSS is to provide comparable\nscores across different evaluators. However, previous works indicate that CVSS\nmight not reach this goal: If a vulnerability is evaluated by several analysts,\ntheir scores often differ. This raises the following questions: Are CVSS\nevaluations consistent? Which factors influence CVSS assessments? We\nsystematically investigate these questions in an online survey with 196 CVSS\nusers. We show that specific CVSS metrics are inconsistently evaluated for\nwidespread vulnerability types, including Top 3 vulnerabilities from the ''2022\nCWE Top 25 Most Dangerous Software Weaknesses'' list. In a follow-up survey\nwith 59 participants, we found that for the same vulnerabilities from the main\nstudy, 68% of these users gave different severity ratings. Our study reveals\nthat most evaluators are aware of the problematic aspects of CVSS, but they\nstill see CVSS as a useful tool for vulnerability assessment. Finally, we\ndiscuss possible reasons for inconsistent evaluations and provide\nrecommendations on improving the consistency of scoring.",
        "translated": "通用漏洞评分系统(CVSS)是漏洞管理中评估漏洞严重程度的一种常用方法。在评估过程中，计算0到10之间的数值分数，10是最严重的(关键)值。CVSS 的目标是为不同的评估者提供可比较的分数。然而，以前的工作表明，CVSS 可能达不到这个目标: 如果一个漏洞是由几个分析师评估，他们的分数往往不同。这就提出了以下问题: 对 CVSS 的评价是否一致？哪些因素影响 CVSS 评估？我们对196名 CVSS 用户进行了在线调查，系统地研究了这些问题。我们表明，特定的 CVSS 度量不一致地评估了广泛的脆弱性类型，包括“2022 CWE 25大最危险软件弱点”列表中的前3个脆弱性。在对59名参与者的后续调查中，我们发现，对于主要研究中相同的漏洞，68% 的用户给出了不同的严重程度评级。我们的研究表明，大多数评估者意识到 CVSS 的问题方面，但他们仍然认为 CVSS 作为一个有用的工具进行脆弱性评估。最后，我们讨论了不一致评价的可能原因，并提出了改善评分一致性的建议。"
    },
    {
        "title": "The Relative Gaussian Mechanism and its Application to Private Gradient\n  Descent",
        "url": "http://arxiv.org/abs/2308.15250v1",
        "pub_date": "2023-08-29",
        "summary": "The Gaussian Mechanism (GM), which consists in adding Gaussian noise to a\nvector-valued query before releasing it, is a standard privacy protection\nmechanism. In particular, given that the query respects some L2 sensitivity\nproperty (the L2 distance between outputs on any two neighboring inputs is\nbounded), GM guarantees R\\'enyi Differential Privacy (RDP). Unfortunately,\nprecisely bounding the L2 sensitivity can be hard, thus leading to loose\nprivacy bounds. In this work, we consider a Relative L2 sensitivity assumption,\nin which the bound on the distance between two query outputs may also depend on\ntheir norm. Leveraging this assumption, we introduce the Relative Gaussian\nMechanism (RGM), in which the variance of the noise depends on the norm of the\noutput. We prove tight bounds on the RDP parameters under relative L2\nsensitivity, and characterize the privacy loss incurred by using\noutput-dependent noise. In particular, we show that RGM naturally adapts to a\nlatent variable that would control the norm of the output. Finally, we\ninstantiate our framework to show tight guarantees for Private Gradient\nDescent, a problem that naturally fits our relative L2 sensitivity assumption.",
        "translated": "高斯机制(GM)是一种标准的隐私保护机制，它在向量值查询发布之前向其中添加高斯噪声。特别地，假设查询遵循一定的 L2敏感性(任意两个相邻输入的输出之间的 L2距离是有界的) ，GM 保证了 R‘ enyi 差分隐私(RDP)。不幸的是，精确地限定 L2的灵敏度可能是困难的，因此导致松散的隐私界限。在这项工作中，我们考虑了一个相对 L2灵敏度假设，其中两个查询输出之间的距离的界限也可能取决于他们的范数。利用这个假设，我们引入了相对高斯机制(RGM) ，其中噪声的方差依赖于输出的范数。我们证明了在相对 L2灵敏度下 RDP 参数的严格界限，并用输出相关噪声刻画了隐私损失。特别是，我们表明，RGM 自然适应一个潜在的变量，将控制规范的输出。最后，我们举例说明了我们的框架，以显示对私人梯度下降法的严格保障，这个问题自然符合我们对二语敏感性的相对假设。"
    },
    {
        "title": "Assessing Cyclostationary Malware Detection via Feature Selection and\n  Classification",
        "url": "http://arxiv.org/abs/2308.15237v1",
        "pub_date": "2023-08-29",
        "summary": "Cyclostationarity involves periodic statistical variations in signals and\nprocesses, commonly used in signal analysis and network security. In the\ncontext of attacks, cyclostationarity helps detect malicious behaviors within\nnetwork traffic, such as traffic patterns in Distributed Denial of Service\n(DDoS) attacks or hidden communication channels in malware. This approach\nenhances security by identifying abnormal patterns and informing Network\nIntrusion Detection Systems (NIDSs) to recognize potential attacks, enhancing\nprotection against both known and novel threats. This research focuses on\nidentifying cyclostationary malware behavior and its detection. The main goal\nis to pinpoint essential cyclostationary features used in NIDSs. These features\nare extracted using algorithms such as Boruta and Principal Component Analysis\n(PCA), and then categorized to find the most significant cyclostationary\npatterns. The aim of this article is to reveal periodically changing malware\nbehaviors through cyclostationarity. The study highlights the importance of\nspotting cyclostationary malware in NIDSs by using established datasets like\nKDD99, NSL-KDD, and the UGRansome dataset. The UGRansome dataset is designed\nfor anomaly detection research and includes both normal and abnormal network\nthreat categories of zero-day attacks. A comparison is made using the Random\nForest (RF) and Support Vector Machine (SVM) algorithms, while also evaluating\nthe effectiveness of Boruta and PCA. The findings show that PCA is more\npromising than using Boruta alone for extracting cyclostationary network\nfeature patterns. Additionally, the analysis identifies the internet protocol\nas the most noticeable cyclostationary feature pattern used by malware.\nNotably, the UGRansome dataset outperforms the KDD99 and NSL-KDD, achieving 99%\naccuracy in signature malware detection using the RF algorithm and 98% with the\nSVM.",
        "translated": "循环平稳性涉及信号和过程的周期性统计变化，常用于信号分析和网络安全。在攻击的情况下，循环平稳性有助侦测网络流量中的恶意行为，例如分布式分布式拒绝服务攻击(DDoS)攻击的流量模式或恶意软件中的隐藏通讯通道。这种方法通过识别异常模式和通知网络入侵检测系统(NIDS)识别潜在的攻击来提高安全性，增强对已知和新威胁的保护。本文主要研究循环平稳恶意软件的行为识别和检测。主要目标是精确定位 NIDS 中使用的基本循环平稳特征。这些特征是使用 Boruta 和主成分分析(PCA)等算法提取出来的，然后进行分类以找到最重要的循环平稳模式。本文的目的是揭示周期性变化的恶意软件行为通过循环平稳性。这项研究强调了利用 KDD99、 NSL-KDD 和 UGRansome 数据集发现 NIDS 中循环平稳恶意软件的重要性。UGransome 数据集是为异常检测研究而设计的，包括正常和非正常的网络威胁类别——零日攻击。使用随机森林(RF)和支持向量机(SVM)算法进行了比较，同时也评估了 Boruta 算法和主成分分析法的有效性。研究结果表明，主成分分析在提取循环平稳网络特征模式方面比单独使用 Boruta 方法更有前景。此外，分析确定互联网协议是最明显的循环平稳特征模式使用的恶意软件。值得注意的是，UGRansome 数据集的性能优于 KDD99和 NSL-KDD，使用 RF 算法和 SVM 分别达到了99% 和98% 的特征恶意软件检测准确率。"
    },
    {
        "title": "State of the Art Report: Verified Computation",
        "url": "http://arxiv.org/abs/2308.15191v1",
        "pub_date": "2023-08-29",
        "summary": "This report describes the state of the art in verifiable computation. The\nproblem being solved is the following:\n  The Verifiable Computation Problem (Verifiable Computing Problem) Suppose we\nhave two computing agents. The first agent is the verifier, and the second\nagent is the prover. The verifier wants the prover to perform a computation.\nThe verifier sends a description of the computation to the prover. Once the\nprover has completed the task, the prover returns the output to the verifier.\nThe output will contain proof. The verifier can use this proof to check if the\nprover computed the output correctly. The check is not required to verify the\nalgorithm used in the computation. Instead, it is a check that the prover\ncomputed the output using the computation specified by the verifier. The effort\nrequired for the check should be much less than that required to perform the\ncomputation.\n  This state-of-the-art report surveys 128 papers from the literature\ncomprising more than 4,000 pages. Other papers and books were surveyed but were\nomitted. The papers surveyed were overwhelmingly mathematical. We have\nsummarised the major concepts that form the foundations for verifiable\ncomputation. The report contains two main sections. The first, larger section\ncovers the theoretical foundations for probabilistically checkable and\nzero-knowledge proofs. The second section contains a description of the current\npractice in verifiable computation. Two further reports will cover (i) military\napplications of verifiable computation and (ii) a collection of technical\ndemonstrators. The first of these is intended to be read by those who want to\nknow what applications are enabled by the current state of the art in\nverifiable computation. The second is for those who want to see practical tools\nand conduct experiments themselves.",
        "translated": "本报告描述了可验证计算的最新技术。正在解决的问题如下: 可验证计算问题(可验证计算问题)假设我们有两个计算代理。第一个代理是验证者，第二个代理是验证者。验证程序希望验证程序执行计算。验证器将计算的描述发送给验证器。一旦验证程序完成了任务，验证程序就将输出返回给验证程序。输出将包含证明。验证器可以使用这个证明来检查证明器是否正确地计算了输出。检查不需要验证计算中使用的算法。相反，它是一个检查，证明程序使用验证程序指定的计算来计算输出。检查所需的工作量应该比执行计算所需的工作量少得多。这份最先进的报告调查了文献中的128篇论文，篇幅超过4000页。其他的论文和书籍也被调查了，但是被省略了。接受调查的论文绝大多数是数学论文。我们总结了构成可验证计算基础的主要概念。该报告包含两个主要部分。第一，更大的部分涵盖了概率检验和零知识证明的理论基础。第二部分包含对当前可验证计算实践的描述。两个进一步的报告将涵盖(i)可验证计算的军事应用和(ii)技术示范的收集。其中的第一个是为那些想知道在可验证计算的当前技术状态下启用了哪些应用程序的人所读取的。第二种是为那些希望看到实用工具并自己进行实验的人准备的。"
    },
    {
        "title": "A Study of Different Awareness Campaigns in a Company",
        "url": "http://arxiv.org/abs/2308.15176v1",
        "pub_date": "2023-08-29",
        "summary": "Phishing is a major cyber threat to organizations that can cause financial\nand reputational damage, threatening their existence. The technical measures\nagainst phishing should be complemented by awareness training for employees.\nHowever, there is little validation of awareness measures. Consequently,\norganizations have an additional burden when integrating awareness training, as\nthere is no consensus on which method brings the best success. This paper\nexamines how awareness concepts can be successfully implemented and validated.\nFor this purpose, various factors, such as requirements and possible\ncombinations of methods, are taken into account in our case study at a small-\nand medium-sized enterprise (SME). To measure success, phishing exercises are\nconducted. The study suggests that pleasant campaigns result in better\nperformance in the simulated phishing exercise. In addition, significant\nimprovements and differences in the target groups could be observed. The\nimplementation of awareness training with integrated key performance indicators\ncan be used as a basis for other organizations.",
        "translated": "网络钓鱼是对组织的主要网络威胁，它可能造成财务和声誉损失，威胁到组织的存在。打击网络钓鱼的技术措施应辅之以对雇员的认识培训。然而，几乎没有认证的意识措施。因此，组织在整合意识培训时有额外的负担，因为对于哪种方法能带来最好的成功没有共识。本文探讨了如何成功地实现和验证感知概念。为此，我们在中小型企业(SME)的案例研究中考虑了各种因素，例如需求和可能的方法组合。为了衡量成功，我们进行了网络钓鱼练习。研究表明，愉快的活动导致更好的表现在模拟钓鱼练习。此外，在目标组中可以观察到显著的改善和差异。执行具有综合关键业绩指标的提高认识培训可以作为其他组织的基础。"
    },
    {
        "title": "Needle in the Haystack: Analyzing the Right of Access According to GDPR\n  Article 15 Five Years after the Implementation",
        "url": "http://arxiv.org/abs/2308.15166v1",
        "pub_date": "2023-08-29",
        "summary": "The General Data Protection Regulation (GDPR) was implemented in 2018 to\nstrengthen and harmonize the data protection of individuals within the European\nUnion. One key aspect is Article 15, which gives individuals the right to\naccess their personal data in an understandable format. Organizations offering\nservices to Europeans had five years' time to optimize their processes and\nfunctions to comply with Article 15. This study aims to explore the process of\nsubmitting and receiving the responses of organizations to GDPR Article 15\nrequests. A quantitative analysis obtains data from various websites to\nunderstand the level of conformity, the data received, and the challenges faced\nby individuals who request their data. The study differentiates organizations\noperating worldwide and in Germany, browser website- and app-based usage, and\ndifferent types of websites. Thereby, we conclude that some websites still\ncompile the data manually, resulting in longer waiting times. A few exceptions\ndid not respond with any data or deliver machine-readable data (GDRP Article\n20). The findings of the study additionally reveal ten patterns individuals\nface when requesting and accessing their data.",
        "translated": "《一般数据保护条例》(GDPR)于2018年实施，以加强和协调欧盟内部个人的数据保护。第15条是一个关键方面，它赋予个人以可理解的格式查阅其个人数据的权利。向欧洲人提供服务的组织有5年时间优化其流程和职能，以遵守第15条。本研究旨在探讨各组织提交和接收对《公约》第15条请求的答复的过程。定量分析从各种网站获取数据，以了解一致性水平、收到的数据以及要求提供数据的个人所面临的挑战。这项研究区分了世界各地和德国的组织，浏览器网站和基于应用程序的使用，以及不同类型的网站。因此，我们得出结论，一些网站仍然手工编译数据，导致较长的等待时间。少数例外没有提供任何数据或交付机器可读性(《全球发展报告》第20条)。研究结果还揭示了个体在请求和访问数据时面临的十种模式。"
    },
    {
        "title": "Conti Inc.: Understanding the Internal Discussions of a large\n  Ransomware-as-a-Service Operator with Machine Learning",
        "url": "http://arxiv.org/abs/2308.16061v1",
        "pub_date": "2023-08-30",
        "summary": "Ransomware-as-a-service (RaaS) is increasing the scale and complexity of\nransomware attacks. Understanding the internal operations behind RaaS has been\na challenge due to the illegality of such activities. The recent chat leak of\nthe Conti RaaS operator, one of the most infamous ransomware operators on the\ninternational scene, offers a key opportunity to better understand the inner\nworkings of such organizations. This paper analyzes the main topic discussions\nin the Conti chat leak using machine learning techniques such as Natural\nLanguage Processing (NLP) and Latent Dirichlet Allocation (LDA), as well as\nvisualization strategies. Five discussion topics are found: 1) Business, 2)\nTechnical, 3) Internal tasking/Management, 4) Malware, and 5) Customer\nService/Problem Solving. Moreover, the distribution of topics among Conti\nmembers shows that only 4% of individuals have specialized discussions while\nalmost all individuals (96%) are all-rounders, meaning that their discussions\nrevolve around the five topics. The results also indicate that a significant\nproportion of Conti discussions are non-tech related. This study thus\nhighlights that running such large RaaS operations requires a workforce skilled\nbeyond technical abilities, with individuals involved in various tasks, from\nmanagement to customer service or problem solving. The discussion topics also\nshow that the organization behind the Conti RaaS oper5086933ator shares\nsimilarities with a large firm. We conclude that, although RaaS represents an\nexample of specialization in the cybercrime industry, only a few members are\nspecialized in one topic, while the rest runs and coordinates the RaaS\noperation.",
        "translated": "勒索软件即服务(RaaS)正在增加勒索软件攻击的规模和复杂性。由于此类活动的非法性，理解 RaaS 背后的内部操作一直是一个挑战。Conti RaaS 操作员是国际上最臭名昭著的勒索软件操作员之一，最近的聊天泄露事件为更好地了解这类组织的内部运作提供了一个关键机会。本文使用自然语言处理(NLP)和隐含狄利克雷分布(LDA)等机器学习技术，以及可视化策略，分析了 Conti 聊天泄露中的主要话题讨论。5个讨论主题: 1)业务，2)技术，3)内部任务/管理，4)恶意软件，和5)客户服务/问题解决。此外，Conti 成员之间的话题分布表明，只有4% 的个人有专门的讨论，而几乎所有的个人(96%)都是全面的，这意味着他们的讨论围绕着五个话题。研究结果还表明，很大一部分 Conti 讨论与技术无关。因此，这项研究强调，运行如此大规模的 RaaS 业务需要一支技术能力超越技术能力的员工队伍，其中包括从管理到客户服务或解决问题等各种任务的个人。讨论主题还表明，Conti RaaS oper5086933ator 背后的组织与大型公司有相似之处。我们得出的结论是，虽然 RaaS 代表了网络犯罪行业的专业化的一个例子，但只有少数成员专门处理一个主题，而其他成员运行和协调 RaaS 操作。"
    },
    {
        "title": "Exploring Cybercriminal Activities, Behaviors and Profiles",
        "url": "http://arxiv.org/abs/2308.15948v1",
        "pub_date": "2023-08-30",
        "summary": "While modern society benefits from a range of technological advancements, it\nalso is exposed to an ever-increasing set of cybersecurity threats. These\naffect all areas of life including business, government, and individuals. To\ncomplement technology solutions to this problem, it is crucial to understand\nmore about cybercriminal perpetrators themselves, their use of technology,\npsychological aspects, and profiles. This is a topic that has received little\nsocio-technical research emphasis in the technology community, has few concrete\nresearch findings, and is thus a prime area for development. The aim of this\narticle is to explore cybercriminal activities and behavior from a psychology\nand human aspects perspective, through a series of notable case studies. We\nexamine motivations, psychological and other interdisciplinary concepts as they\nmay impact/influence cybercriminal activities. We expect this paper to be of\nvalue and particularly insightful for those studying technology, psychology,\nand criminology, with a focus on cybersecurity and cybercrime.",
        "translated": "虽然现代社会受益于一系列技术进步，但它也面临着一系列日益增加的网络安全威胁。这些影响到生活的所有领域，包括商业、政府和个人。为了补充这一问题的技术解决方案，更多地了解网络犯罪分子本身、他们对技术的使用、心理方面和侧写是至关重要的。这是一个在技术界很少受到社会技术研究重视的课题，几乎没有具体的研究成果，因此是一个主要的发展领域。本文的目的是通过一系列值得注意的案例研究，从心理学和人的角度探讨网络犯罪活动和行为。我们研究的动机，心理学和其他跨学科的概念，因为他们可能影响/影响网络犯罪活动。我们希望这篇论文对于那些研究技术、心理学和犯罪学的人有价值，特别是有深刻的见解，重点是网络安全和网络犯罪。"
    },
    {
        "title": "What's going on at the back-end? Risks and benefits of smart toilets",
        "url": "http://arxiv.org/abs/2308.15935v1",
        "pub_date": "2023-08-30",
        "summary": "This paper presents a thematic analysis of an expert focus group considering\nsmart toilets that record health data. The themes that arise indicate risks,\nmany of which could be mitigated but currently are not, suggesting health\nbenefits for the moment override other concerns only in specific application\ncontexts.",
        "translated": "本文提出了一个专家焦点小组的专题分析，考虑智能厕所记录健康数据。出现的主题表明了风险，其中许多风险可以减轻，但目前没有，这表明目前的健康益处仅在特定的应用环境中优先于其他关切。"
    },
    {
        "title": "Securing Blockchain Systems: A Novel Collaborative Learning Framework to\n  Detect Attacks in Transactions and Smart Contracts",
        "url": "http://arxiv.org/abs/2308.15804v1",
        "pub_date": "2023-08-30",
        "summary": "With the escalating prevalence of malicious activities exploiting\nvulnerabilities in blockchain systems, there is an urgent requirement for\nrobust attack detection mechanisms. To address this challenge, this paper\npresents a novel collaborative learning framework designed to detect attacks in\nblockchain transactions and smart contracts by analyzing transaction features.\nOur framework exhibits the capability to classify various types of blockchain\nattacks, including intricate attacks at the machine code level (e.g., injecting\nmalicious codes to withdraw coins from users unlawfully), which typically\nnecessitate significant time and security expertise to detect. To achieve that,\nthe proposed framework incorporates a unique tool that transforms transaction\nfeatures into visual representations, facilitating efficient analysis and\nclassification of low-level machine codes. Furthermore, we propose a customized\ncollaborative learning model to enable real-time detection of diverse attack\ntypes at distributed mining nodes. In order to create a comprehensive dataset,\nwe deploy a pilot system based on a private Ethereum network and conduct\nmultiple attack scenarios. To the best of our knowledge, our dataset is the\nmost comprehensive and diverse collection of transactions and smart contracts\nsynthesized in a laboratory for cyberattack detection in blockchain systems.\nOur framework achieves a detection accuracy of approximately 94\\% through\nextensive simulations and real-time experiments with a throughput of over 1,100\ntransactions per second. These compelling results validate the efficacy of our\nframework and showcase its adaptability in addressing real-world cyberattack\nscenarios.",
        "translated": "随着恶意活动利用区块链系统漏洞的日益普遍，迫切需要强大的攻击检测机制。为了应对这一挑战，本文提出了一种新的合作学习框架，旨在通过分析交易特征来检测区块链交易和智能合同中的攻击。我们的框架展示了对各种类型的区块链攻击进行分类的能力，包括机器代码级别的复杂攻击(例如，注入恶意代码从用户非法提取硬币) ，这通常需要大量的时间和安全专业知识来检测。为实现这一目标，拟议的框架采用了一种独特的工具，将交易特征转换为可视表示，促进对低级机器代码进行有效的分析和分类。此外，我们还提出了一个定制的合作学习模型，可以在分布式挖掘节点上实时检测不同类型的攻击。为了创建一个全面的数据集，我们部署了一个基于私有以太网络的试点系统，并进行多种攻击场景。据我们所知，我们的数据集是最全面和多样化的收集交易和智能合同合成在一个实验室为区块链系统的网络攻击检测。我们的框架通过大量的仿真和实时实验，以每秒超过1,100个事务的吞吐量，实现了大约94% 的检测准确率。这些引人注目的结果验证了我们的框架的有效性，并展示了其在处理现实世界网络攻击场景方面的适应性。"
    },
    {
        "title": "How does post-quantum cryptography affect Central Bank Digital Currency?",
        "url": "http://arxiv.org/abs/2308.15787v1",
        "pub_date": "2023-08-30",
        "summary": "Central Bank Digital Currency (CBDC) is an emerging trend in digital\npayments, with the vast majority of central banks around the world researching,\npiloting, or even operating a digital version of cash. While design choices\ndiffer broadly, such as accounts vs. tokens, the wallets are generally\nprotected through cryptographic algorithms that safeguard against double\nspending and ensure non-repudiation. But with the advent of quantum computing,\nthese algorithms are threatened by new attack vectors. To better understand\nthose threats, we conducted a study of typical assets in a CBDC system,\ndescribe which ones are most amenable to post-quantum cryptography, and propose\nan upgrade strategy.",
        "translated": "央行数字货币(CBDC)是数字支付领域的一个新兴趋势，全球绝大多数央行都在研究、试点甚至运营数字版现金。虽然设计选择差异很大，如帐户与令牌，钱包通常是通过加密算法保护，以防止双重支出，并确保不可否认。但随着量子计算的出现，这些算法受到了新的攻击向量的威胁。为了更好地理解这些威胁，我们对 CBDC 系统中的典型资产进行了研究，描述了哪些资产最适合量子后密码学，并提出了升级策略。"
    },
    {
        "title": "Split Without a Leak: Reducing Privacy Leakage in Split Learning",
        "url": "http://arxiv.org/abs/2308.15783v1",
        "pub_date": "2023-08-30",
        "summary": "The popularity of Deep Learning (DL) makes the privacy of sensitive data more\nimperative than ever. As a result, various privacy-preserving techniques have\nbeen implemented to preserve user data privacy in DL. Among various\nprivacy-preserving techniques, collaborative learning techniques, such as Split\nLearning (SL) have been utilized to accelerate the learning and prediction\nprocess. Initially, SL was considered a promising approach to data privacy.\nHowever, subsequent research has demonstrated that SL is susceptible to many\ntypes of attacks and, therefore, it cannot serve as a privacy-preserving\ntechnique. Meanwhile, countermeasures using a combination of SL and encryption\nhave also been introduced to achieve privacy-preserving deep learning. In this\nwork, we propose a hybrid approach using SL and Homomorphic Encryption (HE).\nThe idea behind it is that the client encrypts the activation map (the output\nof the split layer between the client and the server) before sending it to the\nserver. Hence, during both forward and backward propagation, the server cannot\nreconstruct the client's input data from the intermediate activation map. This\nimprovement is important as it reduces privacy leakage compared to other\nSL-based works, where the server can gain valuable information about the\nclient's input. In addition, on the MIT-BIH dataset, our proposed hybrid\napproach using SL and HE yields faster training time (about 6 times) and\nsignificantly reduced communication overhead (almost 160 times) compared to\nother HE-based approaches, thereby offering improved privacy protection for\nsensitive data in DL.",
        "translated": "深度学习(DL)的流行使得敏感数据的隐私比以往任何时候都更加重要。因此，各种隐私保护技术被实现，以保护 DL 中的用户数据隐私。在各种保护隐私的技术中，合作学习技术，如分裂学习(Split Learning，SL)已被用于加速学习和预测过程。最初，SL 被认为是一种有前途的数据隐私方法。然而，随后的研究表明，SL 容易受到多种类型的攻击，因此，它不能作为一种保护隐私的技术。同时，为了实现保护隐私的深度学习，还引入了 SL 和加密相结合的对策。在这项工作中，我们提出了一种使用 SL 和同态加密(HE)的混合方法。其背后的思想是，客户机在将激活映射(客户机和服务器之间分割层的输出)发送到服务器之前对其进行加密。因此，在向前和向后传播期间，服务器无法从中间激活映射重建客户端的输入数据。这种改进非常重要，因为与其他基于 SL 的工作相比，它减少了隐私泄漏，在这些工作中，服务器可以获得有关客户端输入的有价值的信息。此外，在 MIT-BIH 数据集上，与其他基于 HE 的方法相比，我们提出的使用 SL 和 HE 的混合方法产生更快的训练时间(约6倍)和显着降低的通信开销(近160倍) ，从而为 DL 中的敏感数据提供更好的隐私保护。"
    },
    {
        "title": "Cryptanalysis of a Cayley Hash Function Based on Affine Maps in one\n  Variable over a Finite Field",
        "url": "http://arxiv.org/abs/2308.15765v2",
        "pub_date": "2023-08-30",
        "summary": "Cayley hash functions are cryptographic hashes constructed from Cayley graphs\nof groups. The hash function proposed by Shpilrain and Sosnovski (2016), based\non linear functions over a finite field, was proven insecure. This paper shows\nthat the proposal by Ghaffari and Mostaghim (2018) that uses the Shpilrain and\nSosnovski's hash in its construction is also insecure. We demonstrate its\nsecurity vulnerability by constructing collisions.",
        "translated": "Cayley 散列函数是由组的 Cayley 图构造的加密散列。Shpilrain 和索斯诺夫斯基(2016)提出的基于有限域上线性函数的哈希函数被证明是不安全的。本文表明，Ghaffari 和 Mostaghim (2018)提出的在构造中使用 Shpilrain 和 Sosnovski 散列的建议也是不安全的。我们通过构造冲突来证明其安全漏洞。"
    },
    {
        "title": "Vulnerability of Machine Learning Approaches Applied in IoT-based Smart\n  Grid: A Review",
        "url": "http://arxiv.org/abs/2308.15736v1",
        "pub_date": "2023-08-30",
        "summary": "The machine learning (ML) sees an increasing prevalence of being used in the\ninternet-of-things enabled smart grid. However, the trustworthiness of ML is a\nsevere issue that must be addressed to accommodate the trend of ML-based smart\ngrid applications (MLsgAPPs). The adversarial distortion injected into the\npower signal will greatly affect the system's normal control and operation.\nTherefore, it is imperative to conduct vulnerability assessment for MLsgAPPs\napplied in the context of safety-critical power systems. In this paper, we\nprovide a comprehensive review of the recent progress in designing attack and\ndefense methods for MLsgAPPs. Unlike the traditional survey about ML security,\nthis is the first review work about the security of MLsgAPPs that focuses on\nthe characteristics of power systems. The survey is organized from the aspects\nof adversarial assumptions, targeted applications, evaluation metrics,\ndefending approaches, physics-related constraints, and applied datasets. We\nalso highlight future directions on this topic to encourage more researchers to\nconduct further research on adversarial attacks and defending approaches for\nMLsgAPPs.",
        "translated": "机器学习(ML)在物联网智能电网中的应用越来越普遍。然而，机器学习的可信性是一个严重的问题，必须解决以适应基于机器学习的智能电网应用程序(MLsgAPP)的发展趋势。电源信号中注入的对抗性失真将极大地影响系统的正常控制和运行。因此，必须对应用于安全关键电力系统的 MLsgAPP 进行脆弱性评估。在本文中，我们提供了一个全面的进展，设计攻击和防御方法的 MLsgAPP。与传统的机器学习安全性调查不同，这是第一次针对电力系统特点对 MLsgAPP 的安全性进行评估。调查从对抗性假设、目标应用、评估指标、防御方法、物理相关约束和应用数据集等方面进行组织。我们还强调了这一主题的未来方向，以鼓励更多的研究人员进一步研究对抗性攻击和辩护方法的 MLsgAPP。"
    },
    {
        "title": "Threshold KNN-Shapley: A Linear-Time and Privacy-Friendly Approach to\n  Data Valuation",
        "url": "http://arxiv.org/abs/2308.15709v1",
        "pub_date": "2023-08-30",
        "summary": "Data valuation, a critical aspect of data-centric ML research, aims to\nquantify the usefulness of individual data sources in training machine learning\n(ML) models. However, data valuation faces significant yet frequently\noverlooked privacy challenges despite its importance. This paper studies these\nchallenges with a focus on KNN-Shapley, one of the most practical data\nvaluation methods nowadays. We first emphasize the inherent privacy risks of\nKNN-Shapley, and demonstrate the significant technical difficulties in adapting\nKNN-Shapley to accommodate differential privacy (DP). To overcome these\nchallenges, we introduce TKNN-Shapley, a refined variant of KNN-Shapley that is\nprivacy-friendly, allowing for straightforward modifications to incorporate DP\nguarantee (DP-TKNN-Shapley). We show that DP-TKNN-Shapley has several\nadvantages and offers a superior privacy-utility tradeoff compared to naively\nprivatized KNN-Shapley in discerning data quality. Moreover, even non-private\nTKNN-Shapley achieves comparable performance as KNN-Shapley. Overall, our\nfindings suggest that TKNN-Shapley is a promising alternative to KNN-Shapley,\nparticularly for real-world applications involving sensitive data.",
        "translated": "数据评估是以数据为中心的机器学习研究的一个关键方面，旨在量化个体数据源在训练机器学习(ML)模型中的有用性。然而，尽管数据价值非常重要，但它仍然面临着重大而经常被忽视的隐私挑战。本文以 KNN-Shapley 为研究对象，研究了这些挑战，KNN-Shapley 是目前最实用的数据估值方法之一。我们首先强调了 KNN-shapley 的内在隐私风险，并展示了调整 KNN-shapley 以适应差分隐私(DP)的重大技术困难。为了克服这些挑战，我们引入了 TKNN-Shapley，这是 KNN-Shapley 的一个改进版本，对隐私友好，允许直接修改以纳入 DP 保证(DP-TKNN-Shapley)。我们表明，DP-TKNN-Shapley 有几个优点，并提供了一个优越的隐私-效用的权衡相比，天真的私有化 KNN-Shapley 在识别数据质量。此外，即使非私有 TKNN-Shapley 达到与 KNN-Shapley 相当的性能。总的来说，我们的研究结果表明，TKNN-Shapley 是一个有希望的替代 KNN-Shapley，特别是在涉及敏感数据的现实世界应用。"
    },
    {
        "title": "Intriguing Properties of Diffusion Models: A Large-Scale Dataset for\n  Evaluating Natural Attack Capability in Text-to-Image Generative Models",
        "url": "http://arxiv.org/abs/2308.15692v1",
        "pub_date": "2023-08-30",
        "summary": "Denoising probabilistic diffusion models have shown breakthrough performance\nthat can generate more photo-realistic images or human-level illustrations than\nthe prior models such as GANs. This high image-generation capability has\nstimulated the creation of many downstream applications in various areas.\nHowever, we find that this technology is indeed a double-edged sword: We\nidentify a new type of attack, called the Natural Denoising Diffusion (NDD)\nattack based on the finding that state-of-the-art deep neural network (DNN)\nmodels still hold their prediction even if we intentionally remove their robust\nfeatures, which are essential to the human visual system (HVS), by text\nprompts. The NDD attack can generate low-cost, model-agnostic, and\ntransferrable adversarial attacks by exploiting the natural attack capability\nin diffusion models. Motivated by the finding, we construct a large-scale\ndataset, Natural Denoising Diffusion Attack (NDDA) dataset, to systematically\nevaluate the risk of the natural attack capability of diffusion models with\nstate-of-the-art text-to-image diffusion models. We evaluate the natural attack\ncapability by answering 6 research questions. Through a user study to confirm\nthe validity of the NDD attack, we find that the NDD attack can achieve an 88%\ndetection rate while being stealthy to 93% of human subjects. We also find that\nthe non-robust features embedded by diffusion models contribute to the natural\nattack capability. To confirm the model-agnostic and transferrable attack\ncapability, we perform the NDD attack against an AD vehicle and find that 73%\nof the physically printed attacks can be detected as a stop sign. We hope that\nour study and dataset can help our community to be aware of the risk of\ndiffusion models and facilitate further research toward robust DNN models.",
        "translated": "去噪概率扩散模型已经表现出突破性的性能，可以产生更多的照片真实感图像或人类水平的插图比先前的模型，如 GAN。这种高图像生成能力刺激了许多下游应用程序在各个领域的创建。然而，我们发现这项技术确实是一把双刃剑: 我们确定了一种新的攻击类型，称为自然去噪扩散(NDD)攻击，基于这样一个发现，即使我们有意去除了对人类视觉系统(HVS)至关重要的健壮特征，最先进的深度神经网络(DNN)模型仍然保持其预测性。NDD 攻击利用扩散模型中的自然攻击能力，可以产生低成本、模型无关、可转移的对手攻击。基于这一发现，我们构建了一个大规模的数据集——自然去噪扩散攻击(NDDA)数据集，用最先进的文本-图像扩散模型系统地评估扩散模型的自然攻击能力的风险。通过回答6个研究问题来评估自然攻击能力。通过用户研究证实了 NDD 攻击的有效性，发现 NDD 攻击在对93% 的人类受试者进行隐身的同时，可以达到88% 的检测率。我们还发现扩散模型中嵌入的非鲁棒特征有助于提高自然攻击能力。为了验证模型不可知和可转移的攻击能力，我们对 AD 车辆进行 NDD 攻击，发现73% 的物理打印攻击可以作为停止标志被检测到。我们希望我们的研究和数据集可以帮助我们的社区意识到扩散模型的风险，并促进进一步研究的健壮 DNN 模型。"
    },
    {
        "title": "Accountable Safety Implies Finality",
        "url": "http://arxiv.org/abs/2308.16902v1",
        "pub_date": "2023-08-31",
        "summary": "Motivated by proof-of-stake (PoS) blockchains such as Ethereum, two key\ndesiderata have recently been studied for Byzantine-fault tolerant (BFT)\nstate-machine replication (SMR) consensus protocols: Finality means that the\nprotocol retains consistency, as long as less than a certain fraction of\nvalidators are malicious, even in partially-synchronous environments that allow\nfor temporary violations of assumed network delay bounds. Accountable safety\nmeans that in any case of inconsistency, a certain fraction of validators can\nbe identified to have provably violated the protocol. Earlier works have\ndeveloped impossibility results and protocol constructions for these properties\nseparately. We show that accountable safety implies finality, thereby unifying\nearlier results.",
        "translated": "受到证明木桩(PoS)区块链(如 Etherum)的激励，最近已经研究了拜占庭容错(BFT)状态机复制(SMR)共识协议的两个关键需求: Finality 意味着协议保持一致性，只要少于一定比例的验证器是恶意的，即使在允许暂时违反假定的网络延迟界限的部分同步环境中。负责任的安全意味着，在任何不一致的情况下，可以确定某些验证程序违反了协议。早期的工作已经分别为这些属性开发了不可能结果和协议构造。我们表明，负责任的安全意味着最终结果，从而统一了早期的结果。"
    },
    {
        "title": "Facing Unknown: Open-World Encrypted Traffic Classification Based on\n  Contrastive Pre-Training",
        "url": "http://arxiv.org/abs/2308.16861v1",
        "pub_date": "2023-08-31",
        "summary": "Traditional Encrypted Traffic Classification (ETC) methods face a significant\nchallenge in classifying large volumes of encrypted traffic in the open-world\nassumption, i.e., simultaneously classifying the known applications and\ndetecting unknown applications. We propose a novel Open-World Contrastive\nPre-training (OWCP) framework for this. OWCP performs contrastive pre-training\nto obtain a robust feature representation. Based on this, we determine the\nspherical mapping space to find the marginal flows for each known class, which\nare used to train GANs to synthesize new flows similar to the known parts but\ndo not belong to any class. These synthetic flows are assigned to Softmax's\nunknown node to modify the classifier, effectively enhancing sensitivity\ntowards known flows and significantly suppressing unknown ones. Extensive\nexperiments on three datasets show that OWCP significantly outperforms existing\nETC and generic open-world classification methods. Furthermore, we conduct\ncomprehensive ablation studies and sensitivity analyses to validate each\nintegral component of OWCP.",
        "translated": "传统的加密流量分类(ETC)方法在开放世界的假设下对大量加密流量进行分类，即同时对已知应用进行分类和检测未知应用，面临着巨大的挑战。为此，我们提出了一种新的开放世界对比预训练(OWCP)框架。OWCP 通过对比预训练获得鲁棒的特征表示。在此基础上，我们确定球映射空间来寻找每个已知类的边缘流，用于训练 GAN 来合成类似于已知部分但不属于任何类的新流。这些合成流被分配给 Softmax 的未知节点来修改分类器，有效地提高了对已知流的敏感性，并显著地抑制了未知流。在三个数据集上的大量实验表明，OWCP 方法明显优于现有的 ETC 和一般的开放世界分类方法。此外，我们进行了全面的消融研究和灵敏度分析，以验证每一个完整的组成部分的 OWCP。"
    },
    {
        "title": "IoMT-Blockchain based Secured Remote Patient Monitoring Framework for\n  Neuro-Stimulation Device",
        "url": "http://arxiv.org/abs/2308.16857v1",
        "pub_date": "2023-08-31",
        "summary": "Biomedical Engineering's Internet of Medical Things (IoMT) is helping to\nimprove the accuracy, dependability, and productivity of electronic equipment\nin the healthcare business. Real-time sensory data from patients may be\ndelivered and subsequently analyzed through rapid development of wearable IoMT\ndevices, such as neuro-stimulation devices with a range of functions. Data from\nthe Internet of Things is gathered, analyzed, and stored in a single location.\nHowever, single-point failure, data manipulation, privacy difficulties, and\nother challenges might arise as a result of centralization. Due to its\ndecentralized nature, blockchain (BC) can alleviate these issues. The viability\nof establishing a non-invasive remote neurostimulation system employing\nIoMT-based transcranial Direct Current Stimulation is investigated in this work\n(tDCS). A hardware-based prototype tDCS device has been developed that can be\noperated over the internet using an android application. Our suggested\nframework addresses the problems of IoMTBC-based systems, meets the criteria of\nreal-time remote patient monitoring systems, and incorporates literature best\npractices in the relevant fields.",
        "translated": "生物医学工程的医疗物品互联网(IoMT)正在帮助提高医疗行业电子设备的准确性、可靠性和生产率。来自患者的实时感觉数据可以通过可穿戴式 IoMT 设备的快速发展来传输和随后的分析，例如具有一系列功能的神经刺激设备。来自物联网的数据被收集、分析并存储在一个单独的位置。然而，由于集中化，可能会出现单点故障、数据操作、隐私困难和其他挑战。由于其分散的性质，区块链(BC)可以缓解这些问题。本研究旨在探讨应用基于 IoMT 的经颅直流电刺激技术(tdCS)建立非侵入性远程神经刺激疗法系统的可行性。一个基于硬件的原型 tDCS 设备已经开发出来，可以通过互联网使用机器人应用程序进行操作。我们建议的框架解决了基于 IoMTBC 的系统的问题，满足了实时远程病人监测系统的标准，并结合了相关领域的文献最佳实践。"
    },
    {
        "title": "Towards Low-Barrier Cybersecurity Research and Education for Industrial\n  Control Systems",
        "url": "http://arxiv.org/abs/2308.16769v1",
        "pub_date": "2023-08-31",
        "summary": "The protection of Industrial Control Systems (ICS) that are employed in\npublic critical infrastructures is of utmost importance due to catastrophic\nphysical damages cyberattacks may cause. The research community requires\ntestbeds for validation and comparing various intrusion detection algorithms to\nprotect ICS. However, there exist high barriers to entry for research and\neducation in the ICS cybersecurity domain due to expensive hardware, software,\nand inherent dangers of manipulating real-world systems. To close the gap,\nbuilt upon recently developed 3D high-fidelity simulators, we further showcase\nour integrated framework to automatically launch cyberattacks, collect data,\ntrain machine learning models, and evaluate for practical chemical and\nmanufacturing processes. On our testbed, we validate our proposed intrusion\ndetection model called Minimal Threshold and Window SVM (MinTWin SVM) that\nutilizes unsupervised machine learning via a one-class SVM in combination with\na sliding window and classification threshold. Results show that MinTWin SVM\nminimizes false positives and is responsive to physical process anomalies.\nFurthermore, we incorporate our framework with ICS cybersecurity education by\nusing our dataset in an undergraduate machine learning course where students\ngain hands-on experience in practicing machine learning theory with a practical\nICS dataset. All of our implementations have been open-sourced.",
        "translated": "由于网络攻击可能造成灾难性的物理损害，保护公共关键基础设施中使用的工业控制系统(ICS)至关重要。研究团体需要试验台来验证和比较各种入侵检测算法来保护 ICS。然而，由于昂贵的硬件、软件和操纵现实世界系统的固有危险，ICS 网络安全领域的研究和教育存在很高的进入壁垒。为了弥补这一差距，建立在最近开发的3D 高保真模拟器的基础上，我们进一步展示了我们的集成框架，以自动发动网络攻击，收集数据，训练机器学习模型，并评估实际的化学和制造过程。在我们的测试平台上，我们验证了我们提出的入侵检测模型——最小阈值和窗口支持向量机(minTWin SVM) ，该模型通过一类支持向量机结合滑动窗口和分类阈值来利用非监督式学习。结果表明，最小二乘支持向量机最小化假阳性和响应的物理过程异常。此外，我们将我们的框架与 ICS 网络安全教育结合起来，使用我们的数据集在一个大学机器学习课程中，学生获得实践机器学习理论的实践经验与一个实用的 ICS 数据集。我们所有的实现都是开源的。"
    },
    {
        "title": "Proof of Deep Learning: Approaches, Challenges, and Future Directions",
        "url": "http://arxiv.org/abs/2308.16730v1",
        "pub_date": "2023-08-31",
        "summary": "The rise of computational power has led to unprecedented performance gains\nfor deep learning models. As more data becomes available and model\narchitectures become more complex, the need for more computational power\nincreases. On the other hand, since the introduction of Bitcoin as the first\ncryptocurrency and the establishment of the concept of blockchain as a\ndistributed ledger, many variants and approaches have been proposed. However,\nmany of them have one thing in common, which is the Proof of Work (PoW)\nconsensus mechanism. PoW is mainly used to support the process of new block\ngeneration. While PoW has proven its robustness, its main drawback is that it\nrequires a significant amount of processing power to maintain the security and\nintegrity of the blockchain. This is due to applying brute force to solve a\nhashing puzzle. To utilize the computational power available in useful and\nmeaningful work while keeping the blockchain secure, many techniques have been\nproposed, one of which is known as Proof of Deep Learning (PoDL). PoDL is a\nconsensus mechanism that uses the process of training a deep learning model as\nproof of work to add new blocks to the blockchain. In this paper, we survey the\nvarious approaches for PoDL. We discuss the different types of PoDL algorithms,\ntheir advantages and disadvantages, and their potential applications. We also\ndiscuss the challenges of implementing PoDL and future research directions.",
        "translated": "计算能力的提高为深度学习模型带来了前所未有的性能提升。随着更多的数据变得可用，模型体系结构变得更加复杂，对更多计算能力的需求也随之增加。另一方面，自从比特币作为第一种加密货币出现以及区块链作为分布式分类账的概念确立以来，已经提出了许多变体和方法。然而，它们中的许多都有一个共同点，那就是工作证明(PoW)共识机制。PoW 主要用于支持新的块生成过程。虽然 PoW 已经证明了它的健壮性，但它的主要缺点是需要大量的处理能力来维护区块链的安全性和完整性。这是由于应用蛮力解决散列难题。为了在保证区块链安全的同时利用有用和有意义的工作中可用的计算能力，人们提出了许多技术，其中之一就是深度学习证明(Proof Deep Learning，PoDL)。PoDL 是一种共识机制，它使用训练深度学习模型的过程作为工作的证明，向区块链添加新的块。本文综述了 PoDL 的各种实现方法。我们讨论了不同类型的 PoDL 算法，它们的优点和缺点，以及它们的潜在应用。我们还讨论了实现 PoDL 的挑战和未来的研究方向。"
    },
    {
        "title": "Fault Injection and Safe-Error Attack for Extraction of Embedded Neural\n  Network Models",
        "url": "http://arxiv.org/abs/2308.16703v1",
        "pub_date": "2023-08-31",
        "summary": "Model extraction emerges as a critical security threat with attack vectors\nexploiting both algorithmic and implementation-based approaches. The main goal\nof an attacker is to steal as much information as possible about a protected\nvictim model, so that he can mimic it with a substitute model, even with a\nlimited access to similar training data. Recently, physical attacks such as\nfault injection have shown worrying efficiency against the integrity and\nconfidentiality of embedded models. We focus on embedded deep neural network\nmodels on 32-bit microcontrollers, a widespread family of hardware platforms in\nIoT, and the use of a standard fault injection strategy - Safe Error Attack\n(SEA) - to perform a model extraction attack with an adversary having a limited\naccess to training data. Since the attack strongly depends on the input\nqueries, we propose a black-box approach to craft a successful attack set. For\na classical convolutional neural network, we successfully recover at least 90%\nof the most significant bits with about 1500 crafted inputs. These information\nenable to efficiently train a substitute model, with only 8% of the training\ndataset, that reaches high fidelity and near identical accuracy level than the\nvictim model.",
        "translated": "模型提取是一个重要的安全威胁，攻击向量同时利用了算法和基于实现的方法。攻击者的主要目标是窃取尽可能多的关于受保护的受害者模型的信息，这样他就可以使用替代模型来模拟它，即使对类似的训练数据的访问受到限制。近年来，像故障注入这样的物理攻击对嵌入式模型的完整性和机密性表现出令人担忧的效率。我们专注于32位微控制器上的嵌入式深度神经网络模型，这是物联网中一个广泛的硬件平台家族，以及使用一个标准的故障注入策略——安全错误攻击(SEA)——在对手对训练数据访问有限的情况下执行模型提取攻击。由于攻击强烈地依赖于输入查询，我们提出了一种黑盒方法来构造一个成功的攻击集。对于一个经典的卷积神经网络，我们通过大约1500个精心设计的输入成功地恢复了至少90% 的最重要的比特。这些信息能够有效地训练一个替代模型，只有8% 的训练数据集，达到高保真度和几乎相同的精度水平比受害者模型。"
    },
    {
        "title": "Exact and Efficient Bayesian Inference for Privacy Risk Quantification\n  (Extended Version)",
        "url": "http://arxiv.org/abs/2308.16700v1",
        "pub_date": "2023-08-31",
        "summary": "Data analysis has high value both for commercial and research purposes.\nHowever, disclosing analysis results may pose severe privacy risk to\nindividuals. Privug is a method to quantify privacy risks of data analytics\nprograms by analyzing their source code. The method uses probability\ndistributions to model attacker knowledge and Bayesian inference to update said\nknowledge based on observable outputs. Currently, Privug uses Markov Chain\nMonte Carlo (MCMC) to perform inference, which is a flexible but approximate\nsolution. This paper presents an exact Bayesian inference engine based on\nmultivariate Gaussian distributions to accurately and efficiently quantify\nprivacy risks. The inference engine is implemented for a subset of Python\nprograms that can be modeled as multivariate Gaussian models. We evaluate the\nmethod by analyzing privacy risks in programs to release public statistics. The\nevaluation shows that our method accurately and efficiently analyzes privacy\nrisks, and outperforms existing methods. Furthermore, we demonstrate the use of\nour engine to analyze the effect of differential privacy in public statistics.",
        "translated": "数据分析具有很高的商业和研究价值。然而，披露分析结果可能会给个人带来严重的隐私风险。Privug 是一种通过分析数据分析程序的源代码来量化数据分析程序隐私风险的方法。该方法使用概率分布来模拟攻击者的知识，贝叶斯推断根据可观察到的输出来更新知识。目前，Privug 使用马尔科夫蒙特卡洛(MCMC)进行推理，这是一种灵活但近似的解决方案。本文提出了一个基于多变量高斯分布的精确贝叶斯推断引擎，以准确有效地量化隐私风险。这个推理机是为 Python 程序的一个子集实现的，这个子集可以被建模为多变量高斯模型。我们通过分析发布公共统计数据的程序中的隐私风险来评估这种方法。评估结果表明，该方法能够准确、有效地分析隐私风险，优于现有方法。此外，我们还演示了如何使用我们的引擎来分析公共统计中的差分隐私效应。"
    },
    {
        "title": "Everyone Can Attack: Repurpose Lossy Compression as a Natural Backdoor\n  Attack",
        "url": "http://arxiv.org/abs/2308.16684v1",
        "pub_date": "2023-08-31",
        "summary": "The vulnerabilities to backdoor attacks have recently threatened the\ntrustworthiness of machine learning models in practical applications.\nConventional wisdom suggests that not everyone can be an attacker since the\nprocess of designing the trigger generation algorithm often involves\nsignificant effort and extensive experimentation to ensure the attack's\nstealthiness and effectiveness. Alternatively, this paper shows that there\nexists a more severe backdoor threat: anyone can exploit an easily-accessible\nalgorithm for silent backdoor attacks. Specifically, this attacker can employ\nthe widely-used lossy image compression from a plethora of compression tools to\neffortlessly inject a trigger pattern into an image without leaving any\nnoticeable trace; i.e., the generated triggers are natural artifacts. One does\nnot require extensive knowledge to click on the \"convert\" or \"save as\" button\nwhile using tools for lossy image compression. Via this attack, the adversary\ndoes not need to design a trigger generator as seen in prior works and only\nrequires poisoning the data. Empirically, the proposed attack consistently\nachieves 100% attack success rate in several benchmark datasets such as MNIST,\nCIFAR-10, GTSRB and CelebA. More significantly, the proposed attack can still\nachieve almost 100% attack success rate with very small (approximately 10%)\npoisoning rates in the clean label setting. The generated trigger of the\nproposed attack using one lossy compression algorithm is also transferable\nacross other related compression algorithms, exacerbating the severity of this\nbackdoor threat. This work takes another crucial step toward understanding the\nextensive risks of backdoor attacks in practice, urging practitioners to\ninvestigate similar attacks and relevant backdoor mitigation methods.",
        "translated": "后门攻击的脆弱性最近已经威胁到机器学习模型在实际应用中的可信度。传统观点认为，不是每个人都能成为攻击者，因为设计触发器生成算法的过程往往需要付出大量的努力和广泛的实验，以确保攻击的隐蔽性和有效性。另外，本文表明存在一个更严重的后门威胁: 任何人都可以利用一个易于访问的算法来进行无声的后门攻击。具体来说，这个攻击者可以利用大量压缩工具中广泛使用的有损图像压缩，毫不费力地将触发模式注入到图像中，而不留下任何明显的痕迹，也就是说，生成的触发是自然产物。在使用有损图像压缩的工具时，点击“转换”或“另存为”按钮并不需要广博的知识。通过这种攻击，对手不需要设计一个触发器生成器看到在以前的工程，只需要中毒的数据。经验表明，在 MNIST、 CIFAR-10、 GTSRB 和 CelebA 等几个基准数据集中，所提出的攻击一致地达到100% 的攻击成功率。更重要的是，建议的攻击仍然可以实现几乎100% 的攻击成功率与非常小(约10%)中毒率在清洁标签设置。使用一个有损数据压缩算法生成的攻击触发器也可以跨其他相关的压缩算法转移，从而加剧了这种后门威胁的严重性。这项工作朝着理解实践中后门攻击的广泛风险迈出了关键的一步，敦促从业人员调查类似的攻击和相关的后门缓解方法。"
    },
    {
        "title": "Study of Zero-Knowledge protocols and Elliptic Curve Cryptography and\n  their implementation in Smart Card environments using Java Card",
        "url": "http://arxiv.org/abs/2308.16666v1",
        "pub_date": "2023-08-31",
        "summary": "This paper studies the problem of Zero-Knowledge Protocol (ZKP) and elliptic\ncurve cryptographic implementation in a computationally limited environment,\nsuch as, the smart cards, using Java Card. Besides that, it is explained how\nthe zero-knowledge protocol was selected to implement it on a smart card and\nhow the benchmarking was conducted to select this protocol. The paper also\nshows a theoretical development to implement the ZKP protocol using elliptic\ncurve cryptography. Keywords: Authentication; Zero-knowledge; Cryptography;\nElliptic Curve; Java card; Smart cards",
        "translated": "本文研究了在计算有限的环境下，如智能卡中，利用 Java 卡实现零知识协议(ZKP)和椭圆曲线密码的问题。此外，还说明了如何选择零知识协议在智能卡上实现，以及如何进行基准测试来选择该协议。论文还展示了使用椭圆曲线密码学实现 ZkP 协议的理论发展。关键词: 认证; 零知识; 密码学; 椭圆曲线; Java 卡; 智能卡"
    },
    {
        "title": "Fault Injection on Embedded Neural Networks: Impact of a Single\n  Instruction Skip",
        "url": "http://arxiv.org/abs/2308.16665v1",
        "pub_date": "2023-08-31",
        "summary": "With the large-scale integration and use of neural network models, especially\nin critical embedded systems, their security assessment to guarantee their\nreliability is becoming an urgent need. More particularly, models deployed in\nembedded platforms, such as 32-bit microcontrollers, are physically accessible\nby adversaries and therefore vulnerable to hardware disturbances. We present\nthe first set of experiments on the use of two fault injection means,\nelectromagnetic and laser injections, applied on neural networks models\nembedded on a Cortex M4 32-bit microcontroller platform. Contrary to most of\nstate-of-the-art works dedicated to the alteration of the internal parameters\nor input values, our goal is to simulate and experimentally demonstrate the\nimpact of a specific fault model that is instruction skip. For that purpose, we\nassessed several modification attacks on the control flow of a neural network\ninference. We reveal integrity threats by targeting several steps in the\ninference program of typical convolutional neural network models, which may be\nexploited by an attacker to alter the predictions of the target models with\ndifferent adversarial goals.",
        "translated": "随着神经网络模型的大规模集成和应用，特别是在关键的嵌入式系统中，对其进行安全性评估以保证其可靠性已成为迫切需要。更具体地说，部署在嵌入式平台上的模型，例如32位微控制器，在物理上可被对手访问，因此容易受到硬件干扰。我们提出的第一套实验使用两种故障注入手段，电磁和激光注入，应用于神经网络模型嵌入在 Cortex M432位微控制器平台。与大多数致力于改变内部参数或输入值的最新工作相反，我们的目标是模拟和实验演示指令跳跃这一特定故障模型的影响。为此，我们评估了几种对神经网络推理控制流的修改攻击。我们通过针对典型卷积神经网络模型推理程序中的几个步骤来揭示完整性威胁，攻击者可以利用这些步骤来改变目标模型的预测，从而实现不同的对抗目标。"
    },
    {
        "title": "Baseline Defenses for Adversarial Attacks Against Aligned Language\n  Models",
        "url": "http://arxiv.org/abs/2309.00614v1",
        "pub_date": "2023-09-01",
        "summary": "As Large Language Models quickly become ubiquitous, their security\nvulnerabilities are critical to understand. Recent work shows that text\noptimizers can produce jailbreaking prompts that bypass moderation and\nalignment. Drawing from the rich body of work on adversarial machine learning,\nwe approach these attacks with three questions: What threat models are\npractically useful in this domain? How do baseline defense techniques perform\nin this new domain? How does LLM security differ from computer vision?\n  We evaluate several baseline defense strategies against leading adversarial\nattacks on LLMs, discussing the various settings in which each is feasible and\neffective. Particularly, we look at three types of defenses: detection\n(perplexity based), input preprocessing (paraphrase and retokenization), and\nadversarial training. We discuss white-box and gray-box settings and discuss\nthe robustness-performance trade-off for each of the defenses considered.\nSurprisingly, we find much more success with filtering and preprocessing than\nwe would expect from other domains, such as vision, providing a first\nindication that the relative strengths of these defenses may be weighed\ndifferently in these domains.",
        "translated": "随着大型语言模型迅速变得无处不在，理解它们的安全漏洞至关重要。最近的工作表明，文本优化器可以产生越狱提示，绕过适度和对齐。从对抗性机器学习的大量工作中，我们带着三个问题来处理这些攻击: 什么样的威胁模型在这个领域实际上是有用的？基线防御技术在这个新领域中表现如何？LLM 安全性与计算机视觉有何不同？我们评估了几种针对 LLM 的主要对手攻击的基线防御策略，讨论了每种策略可行和有效的各种设置。特别地，我们研究了三种类型的防御: 检测(基于困惑)、输入预处理(释义和重新标记)和对抗性训练。我们讨论了白盒和灰盒设置，并讨论了所考虑的每种防御的健壮性-性能权衡。令人惊讶的是，我们发现在过滤和预处理方面比我们在其他领域(如视觉)预期的要成功得多，这首次表明这些防御的相对优势在这些领域可能会有所不同。"
    },
    {
        "title": "Interactive and Concentrated Differential Privacy for Bandits",
        "url": "http://arxiv.org/abs/2309.00557v1",
        "pub_date": "2023-09-01",
        "summary": "Bandits play a crucial role in interactive learning schemes and modern\nrecommender systems. However, these systems often rely on sensitive user data,\nmaking privacy a critical concern. This paper investigates privacy in bandits\nwith a trusted centralized decision-maker through the lens of interactive\nDifferential Privacy (DP). While bandits under pure $\\epsilon$-global DP have\nbeen well-studied, we contribute to the understanding of bandits under zero\nConcentrated DP (zCDP). We provide minimax and problem-dependent lower bounds\non regret for finite-armed and linear bandits, which quantify the cost of\n$\\rho$-global zCDP in these settings. These lower bounds reveal two hardness\nregimes based on the privacy budget $\\rho$ and suggest that $\\rho$-global zCDP\nincurs less regret than pure $\\epsilon$-global DP. We propose two $\\rho$-global\nzCDP bandit algorithms, AdaC-UCB and AdaC-GOPE, for finite-armed and linear\nbandits respectively. Both algorithms use a common recipe of Gaussian mechanism\nand adaptive episodes. We analyze the regret of these algorithms to show that\nAdaC-UCB achieves the problem-dependent regret lower bound up to multiplicative\nconstants, while AdaC-GOPE achieves the minimax regret lower bound up to\npoly-logarithmic factors. Finally, we provide experimental validation of our\ntheoretical results under different settings.",
        "translated": "强盗在交互式学习计划和现代推荐系统中发挥着至关重要的作用。然而，这些系统往往依赖于敏感的用户数据，使隐私成为一个关键问题。本文通过交互式差分隐私(DP)的视角，研究了在一个可信的中央决策者的帮助下，土匪的隐私问题。虽然在纯 $epsilon $- 全球 DP 下的土匪已经得到了很好的研究，我们有助于理解在零集中 DP (zCDP)下的土匪。我们提供了极小极大和问题相关的下界遗憾的有限武装和线性土匪，其中量化的成本 $rho $- global zCDP 在这些设置。这些下限揭示了基于隐私预算 $rho $的两种硬度制度，并表明 $rho $- global zCDP 比纯 $epsilon $- global DP 引起的遗憾更少。针对有限武装和线性武装的土匪，我们分别提出了两种 $rho $- global zCDP 土匪算法: AdaC-UCB 和 AdaC-GOPE。这两种算法使用了高斯机制和自适应情节的共同处方。我们分析了这些算法的遗憾表明，AdaC-UCB 达到了问题相关遗憾下限的乘法常数，而 AdaC-GOPE 达到了最小最大遗憾下限的多对数因素。最后，我们提供了不同设置下的理论结果的实验验证。"
    },
    {
        "title": "Privacy Attacks and Defenses for Digital Twin Migrations in Vehicular\n  Metaverses",
        "url": "http://arxiv.org/abs/2309.00477v1",
        "pub_date": "2023-09-01",
        "summary": "The gradual fusion of intelligent transportation systems with metaverse\ntechnologies is giving rise to vehicular metaverses, which blend virtual spaces\nwith physical space. As indispensable components for vehicular metaverses,\nVehicular Twins (VTs) are digital replicas of Vehicular Metaverse Users (VMUs)\nand facilitate customized metaverse services to VMUs. VTs are established and\nmaintained in RoadSide Units (RSUs) with sufficient computing and storage\nresources. Due to the limited communication coverage of RSUs and the high\nmobility of VMUs, VTs need to be migrated among RSUs to ensure real-time and\nseamless services for VMUs. However, during VT migrations, physical-virtual\nsynchronization and massive communications among VTs may cause identity and\nlocation privacy disclosures of VMUs and VTs. In this article, we study privacy\nissues and the corresponding defenses for VT migrations in vehicular\nmetaverses. We first present four kinds of specific privacy attacks during VT\nmigrations. Then, we propose a VMU-VT dual pseudonym scheme and a synchronous\npseudonym change framework to defend against these attacks. Additionally, we\nevaluate average privacy entropy for pseudonym changes and optimize the number\nof pseudonym distribution based on inventory theory. Numerical results show\nthat the average utility of VMUs under our proposed schemes is 33.8% higher\nthan that under the equal distribution scheme, demonstrating the superiority of\nour schemes.",
        "translated": "智能交通系统与元交通技术的逐渐融合，产生了虚拟空间与物理空间相结合的车辆元交通。作为车辆元转换不可或缺的组成部分，车辆双胞胎(VTs)是车辆元转换用户(VMU)的数字复制品，为 VMU 提供定制的元转换服务。可变电站建立和维护在路边单位(RSU)有足够的计算和存储资源。由于区域服务单元的通信覆盖面有限，而且虚拟货币单元的流动性很高，因此需要在各区域服务单元之间迁移虚拟货币单元，以确保为虚拟货币单元提供实时和无缝的服务。然而，在 VT 迁移过程中，VT 之间的物理-虚拟同步和大规模通信可能会导致 VMU 和 VT 的身份和位置隐私泄露。在本文中，我们研究隐私问题和相应的防御 VT 迁移的车辆元。我们首先介绍 VT 迁移期间的四种特定隐私攻击。然后，我们提出了一个 VMU-VT 双重假名方案和一个同步假名更改框架来抵御这些攻击。此外，本文还利用平均隐私熵的方法对笔名的变化进行了评估，并基于库存理论对笔名的分布数量进行了优化。数值结果表明，本文方案下的 VMU 平均利用率比等分布方案下的 VMU 平均利用率提高了33.8% ，说明了本文方案的优越性。"
    },
    {
        "title": "Account Abstraction, Analysed",
        "url": "http://arxiv.org/abs/2309.00448v1",
        "pub_date": "2023-09-01",
        "summary": "Ethereum recently unveiled its upcoming roadmap's \\textit{Splurge} phase,\nhighlighting the integration of\nEIP-\\hlhref{https://eips.ethereum.org/EIPS/eip-3074}{4337} as a foundational\nstandard for account abstraction (AA). AA aims to enhance user accessibility\nand facilitate the expansion of functionalities. Anticipatedly, the deployment\nof AA is poised to attract a broad spectrum of new users and ignite further\ninnovation in DApps. In this paper, we elucidate the underlying operating\nmechanisms of this new concept, as well as provide a review of concurrent\nadvancements in accounts, wallets, and standards related to its development. We\nstep further by conducting a preliminary security evaluation to qualitatively\nassess the extent of security enhancements achieved through AA updates.",
        "translated": "以太坊最近公布了其即将到来的路线图的文本{挥霍}阶段，强调集成 EIP-hlhref { https://eips.Ethereum.org/eips/EIP-3074}{4337}作为一个基础标准的帐户抽象(AA)。机管局的目标是提高使用者的无障碍程度和促进功能的扩展。预计 AA 的部署将吸引广泛的新用户，并引发 DApps 的进一步创新。在本文中，我们阐述了这一新概念的基本运作机制，并提供了一个在账户，钱包，以及与其发展相关的标准的同时进步的审查。我们进一步进行初步的保安评估，以定性评估透过机管局更新所取得的保安改善程度。"
    },
    {
        "title": "Advancing Personalized Federated Learning: Group Privacy, Fairness, and\n  Beyond",
        "url": "http://arxiv.org/abs/2309.00416v1",
        "pub_date": "2023-09-01",
        "summary": "Federated learning (FL) is a framework for training machine learning models\nin a distributed and collaborative manner. During training, a set of\nparticipating clients process their data stored locally, sharing only the model\nupdates obtained by minimizing a cost function over their local inputs. FL was\nproposed as a stepping-stone towards privacy-preserving machine learning, but\nit has been shown vulnerable to issues such as leakage of private information,\nlack of personalization of the model, and the possibility of having a trained\nmodel that is fairer to some groups than to others. In this paper, we address\nthe triadic interaction among personalization, privacy guarantees, and fairness\nattained by models trained within the FL framework. Differential privacy and\nits variants have been studied and applied as cutting-edge standards for\nproviding formal privacy guarantees. However, clients in FL often hold very\ndiverse datasets representing heterogeneous communities, making it important to\nprotect their sensitive information while still ensuring that the trained model\nupholds the aspect of fairness for the users. To attain this objective, a\nmethod is put forth that introduces group privacy assurances through the\nutilization of $d$-privacy (aka metric privacy). $d$-privacy represents a\nlocalized form of differential privacy that relies on a metric-oriented\nobfuscation approach to maintain the original data's topological distribution.\nThis method, besides enabling personalized model training in a federated\napproach and providing formal privacy guarantees, possesses significantly\nbetter group fairness measured under a variety of standard metrics than a\nglobal model trained within a classical FL template. Theoretical justifications\nfor the applicability are provided, as well as experimental validation on\nreal-world datasets to illustrate the working of the proposed method.",
        "translated": "联邦学习(FL)是一种以分布式协作方式训练机器学习模型的框架。在培训期间，一组参与的客户端处理本地存储的数据，只共享通过最小化本地输入的成本函数获得的模型更新。FL 被提议作为保护隐私的机器学习的踏脚石，但是它已经被证明容易受到诸如私人信息泄露，缺乏模型的个性化，以及拥有一个训练有素的模型的可能性，这个模型对一些群体比对其他群体更公平。在本文中，我们讨论了个性化、隐私保护和公平性三者之间的相互作用，这些都是通过在 FL 框架内训练的模型实现的。差分隐私及其变体已经被研究和应用作为提供正式隐私保障的最前沿标准。然而，FL 中的客户端往往持有代表异构社区的非常多样化的数据集，因此在保护他们的敏感信息的同时，还要保证训练后的模型对用户的公平性。为了实现这一目标，提出了一种通过使用 $d $- 隐私(又称度量隐私)引入组隐私保证的方法。$d $- 隐私代表一种本地化的差分隐私，它依赖于一种面向度量的模糊处理方法来维护原始数据的拓扑分布。该方法不仅能够在联邦方法中实现个性化模型训练，并提供正式的隐私保障，而且与在经典 FL 模板中训练的全局模型相比，在各种标准度量下具有显著更好的群公平性。理论上证明了该方法的适用性，并对实际数据集进行了实验验证，说明了该方法的有效性。"
    },
    {
        "title": "Towards Cross-Provider Analysis of Transparency Information for Data\n  Protection",
        "url": "http://arxiv.org/abs/2309.00382v1",
        "pub_date": "2023-09-01",
        "summary": "Transparency and accountability are indispensable principles for modern data\nprotection, from both, legal and technical viewpoints. Regulations such as the\nGDPR, therefore, require specific transparency information to be provided\nincluding, e.g., purpose specifications, storage periods, or legal bases for\npersonal data processing. However, it has repeatedly been shown that all too\noften, this information is practically hidden in legalese privacy policies,\nhindering data subjects from exercising their rights. This paper presents a\nnovel approach to enable large-scale transparency information analysis across\nservice providers, leveraging machine-readable formats and graph data science\nmethods. More specifically, we propose a general approach for building a\ntransparency analysis platform (TAP) that is used to identify data transfers\nempirically, provide evidence-based analyses of sharing clusters of more than\n70 real-world data controllers, or even to simulate network dynamics using\nsynthetic transparency information for large-scale data-sharing scenarios. We\nprovide the general approach for advanced transparency information analysis, an\nopen source architecture and implementation in the form of a queryable analysis\nplatform, and versatile analysis examples. These contributions pave the way for\nmore transparent data processing for data subjects, and evidence-based\nenforcement processes for data protection authorities. Future work can build\nupon our contributions to gain more insights into so-far hidden data-sharing\npractices.",
        "translated": "从法律和技术角度来看，透明度和问责制是现代数据保护不可或缺的原则。因此，GDPR 等法规要求提供特定的透明度信息，包括用途规格、存储期限或个人数据处理的法律依据。然而，已经反复证明，这些信息实际上常常隐藏在法律术语的隐私政策中，阻碍数据主体行使他们的权利。本文提出了一种新颖的方法，利用机器可读格式和图形数据科学方法，实现跨服务提供商的大规模透明信息分析。更具体地说，我们提出了一个通用的方法来建立一个透明度分析平台(TAP) ，用于识别数据传输经验，提供基于证据的分析共享集群的超过70个现实世界的数据控制器，甚至模拟网络动态使用合成透明度信息的大规模数据共享场景。我们提供了用于高级透明信息分析的通用方法，以可查询分析平台的形式提供的开源架构和实现，以及多功能分析示例。这些贡献为数据主体更透明的数据处理以及数据保护当局循证执法过程铺平了道路。今后的工作可以建立在我们的贡献之上，以便更深入地了解迄今为止隐藏的数据共享做法。"
    },
    {
        "title": "Why do universal adversarial attacks work on large language models?:\n  Geometry might be the answer",
        "url": "http://arxiv.org/abs/2309.00254v1",
        "pub_date": "2023-09-01",
        "summary": "Transformer based large language models with emergent capabilities are\nbecoming increasingly ubiquitous in society. However, the task of understanding\nand interpreting their internal workings, in the context of adversarial\nattacks, remains largely unsolved. Gradient-based universal adversarial attacks\nhave been shown to be highly effective on large language models and potentially\ndangerous due to their input-agnostic nature. This work presents a novel\ngeometric perspective explaining universal adversarial attacks on large\nlanguage models. By attacking the 117M parameter GPT-2 model, we find evidence\nindicating that universal adversarial triggers could be embedding vectors which\nmerely approximate the semantic information in their adversarial training\nregion. This hypothesis is supported by white-box model analysis comprising\ndimensionality reduction and similarity measurement of hidden representations.\nWe believe this new geometric perspective on the underlying mechanism driving\nuniversal attacks could help us gain deeper insight into the internal workings\nand failure modes of LLMs, thus enabling their mitigation.",
        "translated": "基于变压器的具有应急能力的大型语言模型在社会中变得越来越普遍。然而，理解和解释他们内部工作的任务，在对抗性攻击的背景下，仍然很大程度上没有得到解决。基于梯度的通用对抗性攻击已被证明对大型语言模型非常有效，并且由于其输入不可知的特性而具有潜在的危险性。这项工作提出了一个新的几何角度来解释对大型语言模型的普遍敌对攻击。通过攻击117M 参数的 GPT-2模型，我们发现有证据表明，通用的对抗触发器可能是嵌入向量，只是接近它们对抗训练区域的语义信息。这一假设得到了白盒模型分析的支持，白盒模型分析包括对隐藏表征的降维和相似性度量。我们相信这种关于驱动通用攻击的潜在机制的新的几何视角可以帮助我们更深入地了解 LLM 的内部工作和失效模式，从而实现它们的缓解。"
    },
    {
        "title": "MIMOCrypt: Multi-User Privacy-Preserving Wi-Fi Sensing via MIMO\n  Encryption",
        "url": "http://arxiv.org/abs/2309.00250v1",
        "pub_date": "2023-09-01",
        "summary": "Wi-Fi signals may help realize low-cost and non-invasive human sensing, yet\nit can also be exploited by eavesdroppers to capture private information. Very\nfew studies rise to handle this privacy concern so far; they either jam all\nsensing attempts or rely on sophisticated technologies to support only a single\nsensing user, rendering them impractical for multi-user scenarios. Moreover,\nthese proposals all fail to exploit Wi-Fi's multiple-in multiple-out (MIMO)\ncapability. To this end, we propose MIMOCrypt, a privacy-preserving Wi-Fi\nsensing framework to support realistic multi-user scenarios. To thwart\nunauthorized eavesdropping while retaining the sensing and communication\ncapabilities for legitimate users, MIMOCrypt innovates in exploiting MIMO to\nphysically encrypt Wi-Fi channels, treating the sensed human activities as\nphysical plaintexts. The encryption scheme is further enhanced via an\noptimization framework, aiming to strike a balance among i) risk of\neavesdropping, ii) sensing accuracy, and iii) communication quality, upon\nsecurely conveying decryption keys to legitimate users. We implement a\nprototype of MIMOCrypt on an SDR platform and perform extensive experiments to\nevaluate its effectiveness in common application scenarios, especially\nprivacy-sensitive human gesture recognition.",
        "translated": "Wi-Fi 信号可能有助于实现低成本和非侵入性的人类感应，但它也可以被窃听者利用来捕获私人信息。到目前为止，很少有研究能够处理这种隐私问题; 它们要么干扰所有的传感尝试，要么依赖复杂的技术只支持单个传感用户，使其在多用户情况下不切实际。此外，这些方案都未能利用 Wi-Fi 的多入多出(MIMO)能力。为此，我们提出了 MIMOCrypt，一个保护隐私的 Wi-Fi 传感框架，以支持真实的多用户场景。为了阻止未经授权的窃听，同时为合法用户保留传感和通信能力，MIMOCrypt 创新地利用 MIMO 对 Wi-Fi 信道进行物理加密，将感知到的人类活动作为物理明文处理。该加密方案通过一个优化框架得到进一步增强，目的是在安全地向合法用户传送解密密钥后，在 i)窃听风险、 ii)感知准确性和 iii)通信质量之间取得平衡。我们在 SDR 平台上实现了一个 MIMocrypt 的原型，并进行了广泛的实验，以评估其在常见应用场景中的有效性，尤其是对隐私敏感的人类手势识别。"
    },
    {
        "title": "Image Hijacking: Adversarial Images can Control Generative Models at\n  Runtime",
        "url": "http://arxiv.org/abs/2309.00236v1",
        "pub_date": "2023-09-01",
        "summary": "Are foundation models secure from malicious actors? In this work, we focus on\nthe image input to a vision-language model (VLM). We discover image hijacks,\nadversarial images that control generative models at runtime. We introduce\nBehavior Matching, a general method for creating image hijacks, and we use it\nto explore three types of attacks. Specific string attacks generate arbitrary\noutput of the adversary's choosing. Leak context attacks leak information from\nthe context window into the output. Jailbreak attacks circumvent a model's\nsafety training. We study these attacks against LLaVA-2, a state-of-the-art VLM\nbased on CLIP and LLaMA-2, and find that all our attack types have above a 90\\%\nsuccess rate. Moreover, our attacks are automated and require only small image\nperturbations. These findings raise serious concerns about the security of\nfoundation models. If image hijacks are as difficult to defend against as\nadversarial examples in CIFAR-10, then it might be many years before a solution\nis found -- if it even exists.",
        "translated": "基础模型对恶意行为者是否安全？在这项工作中，我们的重点是图像输入的视觉语言模型(VLM)。我们发现图像劫持，在运行时控制生成模型的敌对图像。我们介绍了行为匹配，一种创建图像劫持的通用方法，我们用它来探索三种类型的攻击。特定的字符串攻击生成对手选择的任意输出。泄漏上下文攻击将信息从上下文窗口泄漏到输出。越狱攻击规避了模特的安全训练。我们研究了这些针对 LLaVA-2的攻击，这是一种基于 CLIP 和 LLaMA-2的最先进的 VLM，发现我们所有的攻击类型都有90% 以上的成功率。此外，我们的攻击是自动化的，只需要很小的图像扰动。这些发现引起了人们对基础模型安全性的严重关注。如果图像劫持和 CIFAR-10中的对立例子一样难以防范，那么可能需要很多年才能找到解决方案——如果它真的存在的话。"
    },
    {
        "title": "A Survey of Network Requirements for Enabling Effective Cyber Deception",
        "url": "http://arxiv.org/abs/2309.00184v1",
        "pub_date": "2023-09-01",
        "summary": "In the evolving landscape of cybersecurity, the utilization of cyber\ndeception has gained prominence as a proactive defense strategy against\nsophisticated attacks. This paper presents a comprehensive survey that\ninvestigates the crucial network requirements essential for the successful\nimplementation of effective cyber deception techniques. With a focus on diverse\nnetwork architectures and topologies, we delve into the intricate relationship\nbetween network characteristics and the deployment of deception mechanisms.\nThis survey provides an in-depth analysis of prevailing cyber deception\nframeworks, highlighting their strengths and limitations in meeting the\nrequirements for optimal efficacy. By synthesizing insights from both\ntheoretical and practical perspectives, we contribute to a comprehensive\nunderstanding of the network prerequisites crucial for enabling robust and\nadaptable cyber deception strategies.",
        "translated": "在不断变化的网络安全环境中，利用网络欺骗作为一种针对复杂攻击的主动防御战略已经变得突出起来。本文提出了一个全面的调查，研究关键的网络需求必不可少的成功实施有效的网络欺骗技术。针对不同的网络结构和拓扑，我们深入研究了网络特征和欺骗机制部署之间的复杂关系。这项调查对现行的网络欺骗框架进行了深入分析，强调了这些框架在满足最佳效能要求方面的优势和局限性。通过综合理论和实践两方面的见解，我们有助于全面了解网络的先决条件，这些先决条件对于实现可靠和适应性强的网络欺骗战略至关重要。"
    },
    {
        "title": "Black-Box Attacks against Signed Graph Analysis via Balance Poisoning",
        "url": "http://arxiv.org/abs/2309.02396v1",
        "pub_date": "2023-09-05",
        "summary": "Signed graphs are well-suited for modeling social networks as they capture\nboth positive and negative relationships. Signed graph neural networks (SGNNs)\nare commonly employed to predict link signs (i.e., positive and negative) in\nsuch graphs due to their ability to handle the unique structure of signed\ngraphs. However, real-world signed graphs are vulnerable to malicious attacks\nby manipulating edge relationships, and existing adversarial graph attack\nmethods do not consider the specific structure of signed graphs. SGNNs often\nincorporate balance theory to effectively model the positive and negative\nlinks. Surprisingly, we find that the balance theory that they rely on can\nironically be exploited as a black-box attack. In this paper, we propose a\nnovel black-box attack called balance-attack that aims to decrease the balance\ndegree of the signed graphs. We present an efficient heuristic algorithm to\nsolve this NP-hard optimization problem. We conduct extensive experiments on\nfive popular SGNN models and four real-world datasets to demonstrate the\neffectiveness and wide applicability of our proposed attack method. By\naddressing these challenges, our research contributes to a better understanding\nof the limitations and resilience of robust models when facing attacks on\nSGNNs. This work contributes to enhancing the security and reliability of\nsigned graph analysis in social network modeling. Our PyTorch implementation of\nthe attack is publicly available on GitHub:\nhttps://github.com/JialongZhou666/Balance-Attack.git.",
        "translated": "签名图表非常适合于建模社会网络，因为它们捕捉了正面和负面的关系。符号图神经网络(SGNN)由于能够处理符号图的独特结构，因此常被用来预测这类图中的链接符号(即正符号和负符号)。然而，现实世界中的有符号图通过操纵边关系容易受到恶意攻击，现有的对抗图攻击方法没有考虑有符号图的具体结构。SGNN 往往结合平衡理论，有效地建模积极和消极的环节。令人惊讶的是，我们发现他们所依赖的平衡理论可以讽刺地被用作黑盒子攻击。在本文中，我们提出了一种新的黑盒攻击，称为平衡攻击，旨在降低平衡度的签名图。我们提出了一个有效的启发式算法来解决这个 NP 难最佳化问题。我们在五个流行的 SGNN 模型和四个真实世界的数据集上进行了广泛的实验，以验证我们提出的攻击方法的有效性和广泛的适用性。通过解决这些挑战，我们的研究有助于更好地理解在面对 SGNN 攻击时健壮模型的局限性和弹性。这项工作有助于提高社会网络建模中符号图分析的安全性和可靠性。我们的 PyTorch 攻击实现可以在 GitHub 上公开获得:  https://GitHub.com/jialongzhou666/balance-attack.git。"
    },
    {
        "title": "Empirical Review of Smart Contract and DeFi Security: Vulnerability\n  Detection and Automated Repair",
        "url": "http://arxiv.org/abs/2309.02391v2",
        "pub_date": "2023-09-05",
        "summary": "Decentralized Finance (DeFi) is emerging as a peer-to-peer financial\necosystem, enabling participants to trade products on a permissionless\nblockchain. Built on blockchain and smart contracts, the DeFi ecosystem has\nexperienced explosive growth in recent years. Unfortunately, smart contracts\nhold a massive amount of value, making them an attractive target for attacks.\nSo far, attacks against smart contracts and DeFi protocols have resulted in\nbillions of dollars in financial losses, severely threatening the security of\nthe entire DeFi ecosystem. Researchers have proposed various security tools for\nsmart contracts and DeFi protocols as countermeasures. However, a comprehensive\ninvestigation of these efforts is still lacking, leaving a crucial gap in our\nunderstanding of how to enhance the security posture of the smart contract and\nDeFi landscape.\n  To fill the gap, this paper reviews the progress made in the field of smart\ncontract and DeFi security from the perspective of both vulnerability detection\nand automated repair. First, we analyze the DeFi smart contract security issues\nand challenges. Specifically, we lucubrate various DeFi attack incidents and\nsummarize the attacks into six categories. Then, we present an empirical study\nof 42 state-of-the-art techniques that can detect smart contract and DeFi\nvulnerabilities. In particular, we evaluate the effectiveness of traditional\nsmart contract bug detection tools in analyzing complex DeFi protocols.\nAdditionally, we investigate 8 existing automated repair tools for smart\ncontracts and DeFi protocols, providing insight into their advantages and\ndisadvantages. To make this work useful for as wide of an audience as possible,\nwe also identify several open issues and challenges in the DeFi ecosystem that\nshould be addressed in the future.",
        "translated": "分散金融(DeFi)正在成为一个 P2P 金融生态系统，使参与者能够在无许可的区块链上交易产品。建立在区块链和智能合同，DeFi 生态系统经历了爆炸性增长，在最近几年。不幸的是，聪明的合同拥有巨大的价值，使它们成为攻击的有吸引力的目标。到目前为止，针对智能合同和 DeFi 协议的攻击已造成数十亿美元的财务损失，严重威胁到整个 DeFi 生态系统的安全。研究人员提出了各种智能合同和 DeFi 协议的安全工具作为对策。然而，我们仍然缺乏对这些努力的全面调查，在我们了解如何加强智能合同和 DeFi 环境的安全态势方面存在重大差距。为了填补这一空白，本文从漏洞检测和自动修复两个方面综述了智能合同和 DeFi 安全领域的研究进展。首先，我们分析了 DeFi 智能合同的安全问题和挑战。具体来说，我们深入研究了各种 DeFi 攻击事件，并将其归纳为六类。然后，我们提出了一个实证研究的42个国家的最先进的技术，可以检测智能合同和 DeFi 漏洞。特别是，我们评估了传统的智能合同漏洞检测工具在分析复杂 DeFi 协议中的有效性。此外，我们调查了8个现有的智能合同和 DeFi 协议的自动修复工具，深入分析了它们的优缺点。为了使这项工作对尽可能广泛的受众有用，我们还确定了几个开放的问题和挑战，在 DeFi 生态系统中，应该在未来解决。"
    },
    {
        "title": "Hybrid Design of Multiplicative Watermarking for Defense Against\n  Malicious Parameter Identification",
        "url": "http://arxiv.org/abs/2309.02385v1",
        "pub_date": "2023-09-05",
        "summary": "Watermarking is a promising active diagnosis technique for detection of\nhighly sophisticated attacks, but is vulnerable to malicious agents that use\neavesdropped data to identify and then remove or replicate the watermark. In\nthis work, we propose a hybrid multiplicative watermarking (HMWM) scheme, where\nthe watermark parameters are periodically updated, following the dynamics of\nthe unobservable states of specifically designed piecewise affine (PWA) hybrid\nsystems. We provide a theoretical analysis of the effects of this scheme on the\nclosed-loop performance, and prove that stability properties are preserved.\nAdditionally, we show that the proposed approach makes it difficult for an\neavesdropper to reconstruct the watermarking parameters, both in terms of the\nassociated computational complexity and from a systems theoretic perspective.",
        "translated": "水印是一种很有前途的主动诊断技术，用于检测高度复杂的攻击，但容易受到恶意代理的攻击，恶意代理使用窃听数据来识别，然后删除或复制水印。本文提出了一种混合乘性水印(HMWM)方案，该方案根据特定设计的分段仿射混合系统的不可观测状态的动态特性，对水印参数进行周期性更新。从理论上分析了该方案对闭环性能的影响，证明了该方案保持了系统的稳定性。此外，我们还发现，无论是从相关的计算复杂度还是从系统理论的角度来看，该方法都使得窃听者难以重建水印参数。"
    },
    {
        "title": "Smoothening block rewards: How much should miners pay for mining pools?",
        "url": "http://arxiv.org/abs/2309.02297v1",
        "pub_date": "2023-09-05",
        "summary": "The rewards a blockchain miner earns vary with time. Most of the time is\nspent mining without receiving any rewards, and only occasionally the miner\nwins a block and earns a reward. Mining pools smoothen the stochastic flow of\nrewards, and in the ideal case, provide a steady flow of rewards over time.\nSmooth block rewards allow miners to choose an optimal mining power growth\nstrategy that will result in a higher reward yield for a given investment. We\nquantify the economic advantage for a given miner of having smooth rewards, and\nuse this to define a maximum percentage of rewards that a miner should be\nwilling to pay for the mining pool services.",
        "translated": "区块链矿工的回报随着时间的推移而变化。大多数时间都花在采矿上，没有得到任何奖励，只有偶尔矿工赢得一个街区，并获得奖励。矿业资源池平滑了随机的报酬流，在理想的情况下，随着时间的推移提供了稳定的报酬流。平滑块奖励允许矿商选择一个最优的矿业电力增长策略，这将导致一个给定的投资更高的回报率。我们量化给定矿工的平稳回报的经济优势，并用它来定义矿工应该愿意为矿池服务支付的回报的最大百分比。"
    },
    {
        "title": "MAFIA: Protecting the Microarchitecture of Embedded Systems Against\n  Fault Injection Attacks",
        "url": "http://arxiv.org/abs/2309.02255v1",
        "pub_date": "2023-09-05",
        "summary": "Fault injection attacks represent an effective threat to embedded systems.\nRecently, Laurent et al. have reported that fault injection attacks can\nleverage faults inside the microarchitecture. However, state-of-the-art\ncounter-measures, hardwareonly or with hardware support, do not consider the\nintegrity of microarchitecture control signals that are the target of these\nfaults.\n  We present MAFIA, a microarchitecture protection against fault injection\nattacks. MAFIA ensures integrity of pipeline control signals through a\nsignature-based mechanism, and ensures fine-grained control-flow integrity with\na complete indirect branch support and code authenticity. We analyse the\nsecurity properties of two different implementations with different\nsecurity/overhead trade-offs: one with a CBC-MAC/Prince signature function, and\nanother one with a CRC32. We present our implementation of MAFIA in a RISC-V\nprocessor, supported by a dedicated compiler toolchain based on LLVM/Clang. We\nreport a hardware area overhead of 23.8 % and 6.5 % for the CBC-MAC/Prince and\nCRC32 respectively. The average code size and execution time overheads are 29.4\n% and 18.4 % respectively for the CRC32 implementation and are 50 % and 39 %\nfor the CBC-MAC/Prince.",
        "translated": "故障注入攻击是对嵌入式系统的有效威胁。最近，Laurent 等人报道了错误注入攻击可以利用微架构内部的错误。然而，最先进的应对措施，无论是硬件还是硬件支持，都没有考虑这些故障的目标微架构控制信号的完整性。我们介绍了 MAFIA，一种针对错误注入攻击的微架构保护。MAFIA 通过基于签名的机制保证了流水线控制信号的完整性，并通过完整的间接分支支持和代码真实性保证了细粒度的控制流完整性。我们分析了具有不同安全/开销权衡的两个不同实现的安全性能: 一个具有 CBC-MAC/Prince 签名函数，另一个具有 CRC32。我们介绍了我们的 MAFIA 在 RISC-V 处理器中的实现，该处理器由一个基于 LLVM/Clang 的专用编译器工具链支持。我们报告了 CBC-MAC/Prince 和 CRC32的硬件面积开销分别为23.8% 和6.5% 。CRC32实现的平均代码大小和执行时间开销分别为29.4% 和18.4% ，CBC-MAC/Prince 实现的平均代码大小和执行时间分别为50% 和39% 。"
    },
    {
        "title": "Second International Workshop on Adaptive Cyber Defense, 2023",
        "url": "http://arxiv.org/abs/2309.02247v1",
        "pub_date": "2023-09-05",
        "summary": "Recently, reinforcement and deep reinforcement learning (RL/DRL) have been\napplied to develop autonomous agents for cyber network operations(CyOps), where\nthe agents are trained in a representative environment using RL and\nparticularly DRL algorithms. The training environment must simulate CyOps with\nhigh fidelity, which the agent aims to learn and accomplish. A good simulator\nis hard to achieve due to the extreme complexity of the cyber environment. The\ntrained agent must also be generalizable to network variations because\noperational cyber networks change constantly. The red agent case is taken to\ndiscuss these two issues in this work. We elaborate on their essential\nrequirements and potential solution options, illustrated by some preliminary\nexperimentations in a Cyber Gym for Intelligent Learning (CyGIL) testbed.",
        "translated": "最近，强化和深度强化学习(rL/DRL)已被应用于开发网络操作的自主代理(CyOps) ，其中代理在一个具有代表性的环境中使用 rL，特别是 DRL 算法进行训练。训练环境必须以高逼真度模拟 CyOps，而代理的目标是学习和完成。由于网络环境的极端复杂性，一个好的模拟器是很难实现的。训练有素的代理还必须能够推广到网络的变化，因为操作性的网络网络不断变化。本文采用红色代理案例对这两个问题进行了探讨。我们阐述了它们的基本要求和潜在的解决方案选择，说明了一些初步实验的网络健身房智能学习(CyGIL)试验台。"
    },
    {
        "title": "On the Complexity of Differentially Private Best-Arm Identification with\n  Fixed Confidence",
        "url": "http://arxiv.org/abs/2309.02202v1",
        "pub_date": "2023-09-05",
        "summary": "Best Arm Identification (BAI) problems are progressively used for\ndata-sensitive applications, such as designing adaptive clinical trials, tuning\nhyper-parameters, and conducting user studies to name a few. Motivated by the\ndata privacy concerns invoked by these applications, we study the problem of\nBAI with fixed confidence under $\\epsilon$-global Differential Privacy (DP).\nFirst, to quantify the cost of privacy, we derive a lower bound on the sample\ncomplexity of any $\\delta$-correct BAI algorithm satisfying $\\epsilon$-global\nDP. Our lower bound suggests the existence of two privacy regimes depending on\nthe privacy budget $\\epsilon$. In the high-privacy regime (small $\\epsilon$),\nthe hardness depends on a coupled effect of privacy and a novel\ninformation-theoretic quantity, called the Total Variation Characteristic Time.\nIn the low-privacy regime (large $\\epsilon$), the sample complexity lower bound\nreduces to the classical non-private lower bound. Second, we propose AdaP-TT,\nan $\\epsilon$-global DP variant of the Top Two algorithm. AdaP-TT runs in\narm-dependent adaptive episodes and adds Laplace noise to ensure a good\nprivacy-utility trade-off. We derive an asymptotic upper bound on the sample\ncomplexity of AdaP-TT that matches with the lower bound up to multiplicative\nconstants in the high-privacy regime. Finally, we provide an experimental\nanalysis of AdaP-TT that validates our theoretical results.",
        "translated": "最佳手臂识别(BAI)问题逐渐被用于数据敏感的应用程序，例如设计自适应临床试验、调整超参数以及进行用户研究等。基于这些应用程序引发的数据隐私问题，我们研究了在 $epsilon $- global 差分隐私(DP)下带有固定置信度的 BAI 问题。首先，为了量化隐私成本，我们得到了满足 $epsilon $- global DP 的任何 $delta $- 纠正 BAI 算法的样本复杂度的下界。我们的下限表明存在两个隐私制度取决于隐私预算 $epsilon $。在高隐私条件下，硬度取决于隐私的耦合效应和一个新的信息理论量，称为总变化特征时间。在低隐私条件下，样本复杂度下界降低到经典的非隐私下界。其次，我们提出了 AdaP-TT，它是 Top Two 算法的一个 $epsilon $- global DP 变体。Adap-TT 运行在依赖手臂的自适应情节中，并增加了拉普拉斯噪音，以确保良好的隐私-效用权衡。在高保密性情况下，我们得到了与乘法常数上界相匹配的 AdaP-TT 样本复杂度的渐近上界。最后，我们提供了一个 AdaP-TT 的实验分析，验证了我们的理论结果。"
    },
    {
        "title": "The Adversarial Implications of Variable-Time Inference",
        "url": "http://arxiv.org/abs/2309.02159v1",
        "pub_date": "2023-09-05",
        "summary": "Machine learning (ML) models are known to be vulnerable to a number of\nattacks that target the integrity of their predictions or the privacy of their\ntraining data. To carry out these attacks, a black-box adversary must typically\npossess the ability to query the model and observe its outputs (e.g., labels).\nIn this work, we demonstrate, for the first time, the ability to enhance such\ndecision-based attacks. To accomplish this, we present an approach that\nexploits a novel side channel in which the adversary simply measures the\nexecution time of the algorithm used to post-process the predictions of the ML\nmodel under attack. The leakage of inference-state elements into algorithmic\ntiming side channels has never been studied before, and we have found that it\ncan contain rich information that facilitates superior timing attacks that\nsignificantly outperform attacks based solely on label outputs. In a case\nstudy, we investigate leakage from the non-maximum suppression (NMS) algorithm,\nwhich plays a crucial role in the operation of object detectors. In our\nexamination of the timing side-channel vulnerabilities associated with this\nalgorithm, we identified the potential to enhance decision-based attacks. We\ndemonstrate attacks against the YOLOv3 detector, leveraging the timing leakage\nto successfully evade object detection using adversarial examples, and perform\ndataset inference. Our experiments show that our adversarial examples exhibit\nsuperior perturbation quality compared to a decision-based attack. In addition,\nwe present a new threat model in which dataset inference based solely on timing\nleakage is performed. To address the timing leakage vulnerability inherent in\nthe NMS algorithm, we explore the potential and limitations of implementing\nconstant-time inference passes as a mitigation strategy.",
        "translated": "机器学习(ML)模型容易受到攻击，这些攻击的目标是其预测的完整性或其训练数据的隐私。要执行这些攻击，黑盒对手通常必须具备查询模型和观察其输出(例如，标签)的能力。在这项工作中，我们首次展示了增强这种基于决策的攻击的能力。为了实现这一点，我们提出了一种方法，利用一种新的边通道，其中对手只是测量算法的执行时间，用于后处理的预测的机器学习模型受到攻击。推理状态元素泄漏到算法计时侧信道的研究以前从未进行过，我们发现它可以包含丰富的信息，促进优越的计时攻击，明显优于仅仅基于标签输出的攻击。在一个实例中，我们研究了非最大抑制(NMS)算法的泄漏问题，该算法在目标检测器的运行中起着至关重要的作用。在我们检查与此算法相关的定时侧信道漏洞时，我们发现了增强基于决策的攻击的潜力。我们演示了对 YOLOv3检测器的攻击，利用时间泄漏成功地避开了目标检测，使用对手的例子，并执行数据集推断。我们的实验表明，与基于决策的攻击相比，我们的对手例子表现出更好的扰动质量。此外，我们提出了一个新的威胁模型，其中数据集推断完全基于时间泄漏。为了解决 NMS 算法固有的时间泄漏漏洞，我们探讨了将实现恒定时间推理传递作为一种缓解策略的潜力和局限性。"
    },
    {
        "title": "Efficient Query-Based Attack against ML-Based Android Malware Detection\n  under Zero Knowledge Setting",
        "url": "http://arxiv.org/abs/2309.01866v2",
        "pub_date": "2023-09-05",
        "summary": "The widespread adoption of the Android operating system has made malicious\nAndroid applications an appealing target for attackers. Machine learning-based\n(ML-based) Android malware detection (AMD) methods are crucial in addressing\nthis problem; however, their vulnerability to adversarial examples raises\nconcerns. Current attacks against ML-based AMD methods demonstrate remarkable\nperformance but rely on strong assumptions that may not be realistic in\nreal-world scenarios, e.g., the knowledge requirements about feature space,\nmodel parameters, and training dataset. To address this limitation, we\nintroduce AdvDroidZero, an efficient query-based attack framework against\nML-based AMD methods that operates under the zero knowledge setting. Our\nextensive evaluation shows that AdvDroidZero is effective against various\nmainstream ML-based AMD methods, in particular, state-of-the-art such methods\nand real-world antivirus solutions.",
        "translated": "Android 操作系统的广泛采用使恶意的 Android 应用程序成为攻击者的诱人目标。基于机器学习(ML-based)的 Android 恶意软件检测(AMD)方法对于解决这个问题至关重要; 然而，它们在敌对实例面前的脆弱性引起了人们的关注。目前针对基于机器学习的 AMD 方法的攻击表现出显著的性能，但是依赖于在真实场景中可能不现实的强烈假设，例如，关于特征空间、模型参数和训练数据集的知识需求。为了解决这个限制，我们引入了 AdvDroidZero，一个针对基于 ML 的 AMD 方法的高效的基于查询的攻击框架，它在零知识设置下运行。我们广泛的评估表明，AdvDroidZero 对各种主流的基于机器学习的 AMD 方法是有效的，特别是最先进的方法和现实世界的防病毒解决方案。"
    },
    {
        "title": "Designing a Security System Administration Course for Cybersecurity with\n  a Companion Project",
        "url": "http://arxiv.org/abs/2309.01839v1",
        "pub_date": "2023-09-04",
        "summary": "In the past few years, an incident response-oriented cybersecurity program\nhas been constructed at University of Central Oklahoma. As a core course in the\nnewly-established curricula, Secure System Administration focuses on the\nessential knowledge and skill set for system administration. To enrich students\nwith hands-on experience, we also develop a companion coursework project, named\nPowerGrader. In this paper, we present the course structure as well as the\ncompanion project design. Additionally, we survey the pertinent criterion and\ncurriculum requirements from the widely recognized accreditation units. By this\nmeans, we demonstrate the importance of a secure system administration course\nwithin the context of cybersecurity education",
        "translated": "在过去几年中，一个以事件响应为导向的网络安全计划已经在中央俄克拉何马大学建立起来。作为新课程的核心课程，安全系统管理侧重于系统管理的基本知识和技能。为了丰富学生的实践经验，我们还开发了一个配套的课程作业项目，名为 PowerGrader。在本文中，我们介绍了课程结构以及配套项目的设计。此外，我们调查的相关标准和课程要求，从广泛认可的认证单位。通过这种方式，我们证明了在网络安全教育的背景下安全系统管理课程的重要性"
    },
    {
        "title": "Blink: Link Local Differential Privacy in Graph Neural Networks via\n  Bayesian Estimation",
        "url": "http://arxiv.org/abs/2309.03190v2",
        "pub_date": "2023-09-06",
        "summary": "Graph neural networks (GNNs) have gained an increasing amount of popularity\ndue to their superior capability in learning node embeddings for various graph\ninference tasks, but training them can raise privacy concerns. To address this,\nwe propose using link local differential privacy over decentralized nodes,\nenabling collaboration with an untrusted server to train GNNs without revealing\nthe existence of any link. Our approach spends the privacy budget separately on\nlinks and degrees of the graph for the server to better denoise the graph\ntopology using Bayesian estimation, alleviating the negative impact of LDP on\nthe accuracy of the trained GNNs. We bound the mean absolute error of the\ninferred link probabilities against the ground truth graph topology. We then\npropose two variants of our LDP mechanism complementing each other in different\nprivacy settings, one of which estimates fewer links under lower privacy\nbudgets to avoid false positive link estimates when the uncertainty is high,\nwhile the other utilizes more information and performs better given relatively\nhigher privacy budgets. Furthermore, we propose a hybrid variant that combines\nboth strategies and is able to perform better across different privacy budgets.\nExtensive experiments show that our approach outperforms existing methods in\nterms of accuracy under varying privacy budgets.",
        "translated": "图神经网络(GNNs)由于在学习各种图推理任务的节点嵌入方面具有优越的能力而越来越受到人们的欢迎，但是对它们进行训练会引起人们对隐私问题的关注。为了解决这个问题，我们建议在分散的节点上使用链路本地差分隐私，允许与不受信任的服务器协作来训练 GNN，而不会暴露任何链路的存在。该方法将隐私预算分别花费在服务器的图的链路和度上，利用贝叶斯估计对图的拓扑结构进行去噪处理，减轻了 LDP 对训练后的 GNN 精度的负面影响。我们将推断链路概率的平均绝对误差与地面真值图的拓扑结构联系起来。然后，我们提出了我们的 LDP 机制的两个变体，在不同的隐私设置中相互补充，其中一个在较低的隐私预算下估计较少的链接，以避免在不确定性较高时的错误正面链接估计，而另一个利用更多的信息，并在相对较高的隐私预算下表现更好。此外，我们提出了一个混合变体，结合两种策略，能够更好地执行不同的隐私预算。大量的实验表明，在不同的隐私预算下，我们的方法在准确性方面优于现有的方法。"
    },
    {
        "title": "Provably Unlinkable Smart Card-based Payments",
        "url": "http://arxiv.org/abs/2309.03128v1",
        "pub_date": "2023-09-06",
        "summary": "The most prevalent smart card-based payment method, EMV, currently offers no\nprivacy to its users. Transaction details and the card number are sent in\ncleartext, enabling the profiling and tracking of cardholders. Since public\nawareness of privacy issues is growing and legislation, such as GDPR, is\nemerging, we believe it is necessary to investigate the possibility of making\npayments anonymous and unlinkable without compromising essential security\nguarantees and functional properties of EMV. This paper draws attention to\ntrade-offs between functional and privacy requirements in the design of such a\nprotocol. We present the UTX protocol - an enhanced payment protocol satisfying\nsuch requirements, and we formally certify key security and privacy properties\nusing techniques based on the applied pi-calculus.",
        "translated": "最流行的基于智能卡的支付方式，EMV，目前对其用户没有提供任何隐私。交易详情和卡号以明文形式发送，以便对持卡人进行分析和跟踪。由于公众对隐私问题的认识日益提高，而且 GDPR 等立法正在出现，我们认为有必要调查在不损害 EMV 的基本安全保障和功能特性的情况下匿名支付和不可链接支付的可能性。本文提请注意在设计这样一个协议时，功能需求和隐私需求之间的权衡。我们提出了 UTX 协议——一种满足这种要求的增强型支付协议，并且利用基于应用 pi 演算的技术对密钥安全性和隐私性进行了正式认证。"
    },
    {
        "title": "ORL-AUDITOR: Dataset Auditing in Offline Deep Reinforcement Learning",
        "url": "http://arxiv.org/abs/2309.03081v1",
        "pub_date": "2023-09-06",
        "summary": "Data is a critical asset in AI, as high-quality datasets can significantly\nimprove the performance of machine learning models. In safety-critical domains\nsuch as autonomous vehicles, offline deep reinforcement learning (offline DRL)\nis frequently used to train models on pre-collected datasets, as opposed to\ntraining these models by interacting with the real-world environment as the\nonline DRL. To support the development of these models, many institutions make\ndatasets publicly available with opensource licenses, but these datasets are at\nrisk of potential misuse or infringement. Injecting watermarks to the dataset\nmay protect the intellectual property of the data, but it cannot handle\ndatasets that have already been published and is infeasible to be altered\nafterward. Other existing solutions, such as dataset inference and membership\ninference, do not work well in the offline DRL scenario due to the diverse\nmodel behavior characteristics and offline setting constraints. In this paper,\nwe advocate a new paradigm by leveraging the fact that cumulative rewards can\nact as a unique identifier that distinguishes DRL models trained on a specific\ndataset. To this end, we propose ORL-AUDITOR, which is the first\ntrajectory-level dataset auditing mechanism for offline RL scenarios. Our\nexperiments on multiple offline DRL models and tasks reveal the efficacy of\nORL-AUDITOR, with auditing accuracy over 95% and false positive rates less than\n2.88%. We also provide valuable insights into the practical implementation of\nORL-AUDITOR by studying various parameter settings. Furthermore, we demonstrate\nthe auditing capability of ORL-AUDITOR on open-source datasets from Google and\nDeepMind, highlighting its effectiveness in auditing published datasets.\nORL-AUDITOR is open-sourced at https://github.com/link-zju/ORL-Auditor.",
        "translated": "数据是人工智能的重要资产，因为高质量的数据集可以显著提高机器学习模型的性能。在诸如自动驾驶汽车这样的安全关键领域，离线深度强化学习(线下 DRL)经常被用来在预收集的数据集上训练模型，而不是像线上 DRL 那样通过与现实环境交互来训练这些模型。为了支持这些模型的发展，许多机构使用开源许可证公开数据集，但这些数据集存在潜在的滥用或侵权风险。向数据集中注入水印可能会保护数据的知识产权，但是它不能处理已经发布的数据集，以后也不可能进行更改。其他现有的解决方案，比如数据集推断和成员推断，由于模型行为特征和离线设置约束的不同，在离线 DRL 场景中不能很好地工作。在本文中，我们提倡一种新的范式，利用累积奖励可以作为一种唯一标识符，区分在特定数据集上训练的 DRL 模型。为此，我们提出了 ORL-Auditor，这是第一个针对离线 RL 场景的轨迹级数据集审计机制。我们在多个离线 DRL 模型和任务上的实验揭示了 ORL-Auditor 的有效性，其审计准确率超过95% ，假阳性率小于2.88% 。我们还通过研究各种参数设置，为 ORL-Auditor 的实际应用提供了有价值的见解。此外，我们展示了 ORL-Auditor 对来自 Google 和 DeepMind 的开源数据集的审计能力，突出了它在审计已发布数据集方面的有效性。ORL-auditor 在 https://github.com/link-zju/ORL-AUDITOR 是开源的。"
    },
    {
        "title": "Disarming Steganography Attacks Inside Neural Network Models",
        "url": "http://arxiv.org/abs/2309.03071v1",
        "pub_date": "2023-09-06",
        "summary": "Similar to the revolution of open source code sharing, Artificial\nIntelligence (AI) model sharing is gaining increased popularity. However, the\nfast adaptation in the industry, lack of awareness, and ability to exploit the\nmodels make them significant attack vectors. By embedding malware in neurons,\nthe malware can be delivered covertly, with minor or no impact on the neural\nnetwork's performance. The covert attack will use the Least Significant Bits\n(LSB) weight attack since LSB has a minimal effect on the model accuracy, and\nas a result, the user will not notice it. Since there are endless ways to hide\nthe attacks, we focus on a zero-trust prevention strategy based on AI model\nattack disarm and reconstruction. We proposed three types of model\nsteganography weight disarm defense mechanisms. The first two are based on\nrandom bit substitution noise, and the other on model weight quantization. We\ndemonstrate a 100\\% prevention rate while the methods introduce a minimal\ndecrease in model accuracy based on Qint8 and K-LRBP methods, which is an\nessential factor for improving AI security.",
        "translated": "类似于开源代码共享的革命，人工智能(AI)模型共享越来越受欢迎。然而，行业中的快速适应性、缺乏意识以及利用模型的能力使得它们成为重要的攻击载体。通过在神经元中嵌入恶意软件，恶意软件可以秘密传递，对神经网络的性能影响很小或没有影响。隐蔽攻击采用最小有效位(LSB)权重攻击，因为 LSB 对模型精度的影响很小，用户不会注意到。由于隐藏攻击的方法层出不穷，本文提出了一种基于人工智能模型的攻击解除与重构的零信任防御策略。我们提出了三种类型的模型隐写权裁军防御机制。前两种是基于随机比特替换噪声的，另一种是基于模型权量化的。在 Qint8和 K-LRBP 方法的基础上，我们证明了100% 的预防率，而该方法在模型精度方面的最小降低，这是提高人工智能安全性的一个重要因素。"
    },
    {
        "title": "Hide and Seek (HaS): A Lightweight Framework for Prompt Privacy\n  Protection",
        "url": "http://arxiv.org/abs/2309.03057v1",
        "pub_date": "2023-09-06",
        "summary": "Numerous companies have started offering services based on large language\nmodels (LLM), such as ChatGPT, which inevitably raises privacy concerns as\nusers' prompts are exposed to the model provider. Previous research on secure\nreasoning using multi-party computation (MPC) has proven to be impractical for\nLLM applications due to its time-consuming and communication-intensive nature.\nWhile lightweight anonymization techniques can protect private information in\nprompts through substitution or masking, they fail to recover sensitive data\nreplaced in the LLM-generated results. In this paper, we expand the application\nscenarios of anonymization techniques by training a small local model to\nde-anonymize the LLM's returned results with minimal computational overhead. We\nintroduce the HaS framework, where \"H(ide)\" and \"S(eek)\" represent its two core\nprocesses: hiding private entities for anonymization and seeking private\nentities for de-anonymization, respectively. To quantitatively assess HaS's\nprivacy protection performance, we propose both black-box and white-box\nadversarial models. Furthermore, we conduct experiments to evaluate HaS's\nusability in translation and classification tasks. The experimental findings\ndemonstrate that the HaS framework achieves an optimal balance between privacy\nprotection and utility.",
        "translated": "许多公司已经开始提供基于大型语言模型(LLM)的服务，比如 ChatGPT，这不可避免地引起了隐私问题，因为用户的提示会暴露给模型提供商。多方计算(MPC)安全推理的研究由于其耗时和通信密集的特性，已被证明对 LLM 应用是不切实际的。虽然轻量级匿名化技术可以通过替换或屏蔽来保护提示中的私有信息，但它们无法恢复 LLM 生成的结果中替换的敏感数据。在本文中，我们通过训练一个小的局部模型来扩展匿名化技术的应用场景，以最小的计算开销去匿名化 LLM 的返回结果。我们介绍了 HaS 框架，其中“ H (ide)”和“ S (eek)”分别代表了它的两个核心过程: 隐藏私有实体进行匿名化和寻找私有实体进行去匿名化。为了定量评估 HaS 的隐私保护性能，我们提出了黑盒和白盒对抗模型。此外，我们还进行了实验来评估 HaS 在翻译和分类任务中的可用性。实验结果表明，HaS 框架实现了隐私保护和实用性之间的最佳平衡。"
    },
    {
        "title": "Automated CVE Analysis for Threat Prioritization and Impact Prediction",
        "url": "http://arxiv.org/abs/2309.03040v1",
        "pub_date": "2023-09-06",
        "summary": "The Common Vulnerabilities and Exposures (CVE) are pivotal information for\nproactive cybersecurity measures, including service patching, security\nhardening, and more. However, CVEs typically offer low-level, product-oriented\ndescriptions of publicly disclosed cybersecurity vulnerabilities, often lacking\nthe essential attack semantic information required for comprehensive weakness\ncharacterization and threat impact estimation. This critical insight is\nessential for CVE prioritization and the identification of potential\ncountermeasures, particularly when dealing with a large number of CVEs. Current\nindustry practices involve manual evaluation of CVEs to assess their attack\nseverities using the Common Vulnerability Scoring System (CVSS) and mapping\nthem to Common Weakness Enumeration (CWE) for potential mitigation\nidentification. Unfortunately, this manual analysis presents a major bottleneck\nin the vulnerability analysis process, leading to slowdowns in proactive\ncybersecurity efforts and the potential for inaccuracies due to human errors.\nIn this research, we introduce our novel predictive model and tool (called\nCVEDrill) which revolutionizes CVE analysis and threat prioritization. CVEDrill\naccurately estimates the CVSS vector for precise threat mitigation and priority\nranking and seamlessly automates the classification of CVEs into the\nappropriate CWE hierarchy classes. By harnessing CVEDrill, organizations can\nnow implement cybersecurity countermeasure mitigation with unparalleled\naccuracy and timeliness, surpassing in this domain the capabilities of\nstate-of-the-art tools like ChaptGPT.",
        "translated": "通用漏洞披露是主动采取网络安全措施的关键资料，包括修补服务、加强保安等。然而，CVE 通常提供低层次、面向产品的对公开披露的网络安全漏洞的描述，往往缺乏全面的弱点语义信息和威胁影响评估所需的必要攻击角色塑造。这种关键的洞察力对于 CVE 的优先级排序和确定潜在的对策是必不可少的，特别是在处理大量的 CVE 时。当前的行业实践包括使用通用漏洞评分系统(CVSS)手动评估 CVE，以评估它们的攻击严重程度，并将它们映射到公共弱点枚举(CWE) ，以便进行潜在的缓解识别。遗憾的是，这种手工分析是脆弱性分析过程中的一个主要瓶颈，导致积极主动的网络安全努力放缓，并可能因人为错误而出现不准确的情况。在这项研究中，我们介绍了我们的新型预测模型和工具(称为 CVEDrill) ，它彻底改革了 CVE 分析和威胁优先级排序。CVEDrill 精确地估计 CVSS 向量，以便精确地缓解威胁和优先级排序，并且无缝地自动将 CVE 分类到适当的 CWE 层次类中。通过利用 CVEDrill，组织现在可以以无与伦比的准确性和及时性实施网络安全对策缓解，在这个领域超越了像 ChaptGPT 这样的最先进工具的能力。"
    },
    {
        "title": "Fuzz on the Beach: Fuzzing Solana Smart Contracts",
        "url": "http://arxiv.org/abs/2309.03006v1",
        "pub_date": "2023-09-06",
        "summary": "Solana has quickly emerged as a popular platform for building decentralized\napplications (DApps), such as marketplaces for non-fungible tokens (NFTs). A\nkey reason for its success are Solana's low transaction fees and high\nperformance, which is achieved in part due to its stateless programming model.\nAlthough the literature features extensive tooling support for smart contract\nsecurity, current solutions are largely tailored for the Ethereum Virtual\nMachine. Unfortunately, the very stateless nature of Solana's execution\nenvironment introduces novel attack patterns specific to Solana requiring a\nrethinking for building vulnerability analysis methods.\n  In this paper, we address this gap and propose FuzzDelSol, the first\nbinary-only coverage-guided fuzzing architecture for Solana smart contracts.\nFuzzDelSol faithfully models runtime specifics such as smart contract\ninteractions. Moreover, since source code is not available for the large\nmajority of Solana contracts, FuzzDelSol operates on the contract's binary\ncode. Hence, due to the lack of semantic information, we carefully extracted\nlow-level program and state information to develop a diverse set of bug oracles\ncovering all major bug classes in Solana. Our extensive evaluation on 6049\nsmart contracts shows that FuzzDelSol's bug oracles find bugs with a high\nprecision and recall. To the best of our knowledge, this is the largest\nevaluation of the security landscape on the Solana mainnet.",
        "translated": "Solana 已经迅速成为构建分散式应用程序(DApps)的流行平台，比如不可替换令牌(NFT)的市场。其成功的一个关键原因是 Solana 的低交易费用和高性能，这在一定程度上得益于其无状态编程模型。虽然文献的特点是广泛的工具支持智能合同安全，目前的解决方案主要是为以太虚拟机定制。不幸的是，Solana 执行环境的无状态特性引入了特定于 Solana 的新的攻击模式，需要重新思考构建漏洞分析方法。在本文中，我们针对这一差距，并提出了 FuzzDelSol，第一个二进制只覆盖率引导的模糊索拉纳智能合同体系结构。FuzzDelSol 忠实地建模运行时细节，比如智能契约交互。此外，由于绝大多数 Solana 合同的源代码不可用，FuzzDelSol 使用合同的二进制代码进行操作。因此，由于缺乏语义信息，我们小心地提取了低级程序和状态信息，以开发一套涵盖 Solana 中所有主要 bug 类的多样化 bug 预言。我们对6049个智能合同的广泛评估表明，FuzzdelSol 的 bug 预言家能够高准确率召回率地发现 bug。据我们所知，这是对索拉纳内网安全状况的最大评估。"
    },
    {
        "title": "Demystifying RCE Vulnerabilities in LLM-Integrated Apps",
        "url": "http://arxiv.org/abs/2309.02926v1",
        "pub_date": "2023-09-06",
        "summary": "In recent years, Large Language Models (LLMs) have demonstrated remarkable\npotential across various downstream tasks. LLM-integrated frameworks, which\nserve as the essential infrastructure, have given rise to many LLM-integrated\nweb apps. However, some of these frameworks suffer from Remote Code Execution\n(RCE) vulnerabilities, allowing attackers to execute arbitrary code on apps'\nservers remotely via prompt injections. Despite the severity of these\nvulnerabilities, no existing work has been conducted for a systematic\ninvestigation of them. This leaves a great challenge on how to detect\nvulnerabilities in frameworks as well as LLM-integrated apps in real-world\nscenarios.\n  To fill this gap, we present two novel strategies, including 1) a static\nanalysis-based tool called LLMSmith to scan the source code of the framework to\ndetect potential RCE vulnerabilities and 2) a prompt-based automated testing\napproach to verify the vulnerability in LLM-integrated web apps. We discovered\n13 vulnerabilities in 6 frameworks, including 12 RCE vulnerabilities and 1\narbitrary file read/write vulnerability. 11 of them are confirmed by the\nframework developers, resulting in the assignment of 7 CVE IDs. After testing\n51 apps, we found vulnerabilities in 17 apps, 16 of which are vulnerable to RCE\nand 1 to SQL injection. We responsibly reported all 17 issues to the\ncorresponding developers and received acknowledgments. Furthermore, we amplify\nthe attack impact beyond achieving RCE by allowing attackers to exploit other\napp users (e.g. app responses hijacking, user API key leakage) without direct\ninteraction between the attacker and the victim. Lastly, we propose some\nmitigating strategies for improving the security awareness of both framework\nand app developers, helping them to mitigate these risks effectively.",
        "translated": "近年来，大型语言模型(LLM)在各种下游任务中显示出了巨大的潜力。作为基础设施的 LLM 集成框架已经产生了许多 LLM 集成的 Web 应用程序。然而，其中一些框架存在远程代码执行(Remote Code Execution，RCE)漏洞，允许攻击者通过提示注入在应用服务器上远程执行任意代码。尽管这些漏洞非常严重，但目前还没有对它们进行系统调查的工作。这就给如何在现实场景中检测框架以及 LLM 集成的应用程序中的漏洞带来了巨大的挑战。为了填补这个空白，我们提出了两个新的策略，包括1)一个基于静态分析的工具称为 LLMSmith 扫描源代码的框架，以检测潜在的 RCE 漏洞和2)一个基于提示的自动化测试方法，以验证 LLM 集成的 Web 应用程序的漏洞。我们在6个框架中发现了13个漏洞，包括12个 RCE 漏洞和1个任意文件读/写漏洞。其中11个由框架开发人员确认，导致了7个 CVE ID 的分配。在测试了51个应用程序之后，我们发现了17个应用程序的漏洞，其中16个容易受到 RCE 攻击，1个容易受到 SQL 注入攻击。我们负责地向相应的开发人员报告了所有17个问题，并收到了确认。此外，我们通过允许攻击者利用其他应用程序用户(例如应用程序响应劫持，用户 API 密钥泄露)而不需要攻击者和受害者之间的直接交互，扩大了攻击的影响。最后，我们提出了一些缓解策略，以提高框架和应用程序开发人员的安全意识，帮助他们有效地减轻这些风险。"
    },
    {
        "title": "Autonomous and Collaborative Smart Home Security System (ACSHSS)",
        "url": "http://arxiv.org/abs/2309.02899v1",
        "pub_date": "2023-09-06",
        "summary": "Firstly, the proposed solution provides remotely accessible integrated IoT\nresources for the safety and security of the building. By using Sha ort\nMessaging System (SMS), the age is sent to the user by the Global System for\nMobile (GSM) system. An SMS alert is sent to the user in case any sensor\ndetects an abnormality in their operation. Secondly, an authentication\nmechanism is deployed to enable only authorized users to access resources.\nThirdly, in case of a malicious approach in accessing IoT resources, a timely\nalert should be received by the owner. A Network Intrusion Detection System\n(NIDS) is deployed to detect and real-time information in case of any\nsuspicious activity while accessing the Internet of Things network.",
        "translated": "首先，提出的解决方案提供远程访问的综合物联网资源的安全和保障建设。通过使用短消息系统(SMS) ，年龄由全球移动通信系统(GSM)系统发送给用户。如果任何传感器检测到操作中的异常，则向用户发送 SMS 警报。其次，部署一种身份验证机制，只允许经过授权的用户访问资源。第三，如果有人恶意访问物联网资源，用户应该及时收到提醒。网络入侵预防系统(nIDS)是用来侦测及实时资讯，以防在物联网上发生任何可疑活动。"
    },
    {
        "title": "Roulette: A Semantic Privacy-Preserving Device-Edge Collaborative\n  Inference Framework for Deep Learning Classification Tasks",
        "url": "http://arxiv.org/abs/2309.02820v1",
        "pub_date": "2023-09-06",
        "summary": "Deep learning classifiers are crucial in the age of artificial intelligence.\nThe device-edge-based collaborative inference has been widely adopted as an\nefficient framework for promoting its applications in IoT and 5G/6G networks.\nHowever, it suffers from accuracy degradation under non-i.i.d. data\ndistribution and privacy disclosure. For accuracy degradation, direct use of\ntransfer learning and split learning is high cost and privacy issues remain.\nFor privacy disclosure, cryptography-based approaches lead to a huge overhead.\nOther lightweight methods assume that the ground truth is non-sensitive and can\nbe exposed. But for many applications, the ground truth is the user's crucial\nprivacy-sensitive information. In this paper, we propose a framework of\nRoulette, which is a task-oriented semantic privacy-preserving collaborative\ninference framework for deep learning classifiers. More than input data, we\ntreat the ground truth of the data as private information. We develop a novel\nparadigm of split learning where the back-end DNN is frozen and the front-end\nDNN is retrained to be both a feature extractor and an encryptor. Moreover, we\nprovide a differential privacy guarantee and analyze the hardness of ground\ntruth inference attacks. To validate the proposed Roulette, we conduct\nextensive performance evaluations using realistic datasets, which demonstrate\nthat Roulette can effectively defend against various attacks and meanwhile\nachieve good model accuracy. In a situation where the non-i.i.d. is very\nsevere, Roulette improves the inference accuracy by 21\\% averaged over\nbenchmarks, while making the accuracy of discrimination attacks almost\nequivalent to random guessing.",
        "translated": "在人工智能时代，深度学习分类器至关重要。基于设备边缘的协同推理作为一种有效的框架，在物联网和5G/6G 网络中得到了广泛的应用。然而，在非 ID 数据分发和隐私公开的情况下，它的准确性会下降。为了降低准确性，直接使用迁移学习和分割学习成本较高，并且隐私问题依然存在。对于隐私公开，基于密码学的方法导致了巨大的开销。其他轻量级方法假设地面真相是不敏感的，可以被揭露。但是对于许多应用程序来说，真相就是用户至关重要的隐私敏感信息。本文提出了一种面向任务的深度学习分类器语义隐私保护协作推理框架 Roulette。除了输入数据之外，我们还将数据的基本真相视为私有信息。我们提出了一种新的分裂学习模式，即将后端 DNN 冻结，并将前端 DNN 重新训练为特征提取器和加密器。此外，我们还提供了差分隐私保证，并分析了地面真相推理攻击的难度。为了验证所提出的轮盘赌算法的有效性，我们利用真实的数据集进行了广泛的性能评估，结果表明轮盘赌算法能够有效地抵御各种攻击，同时具有良好的模型精度。在非身份识别非常严重的情况下，轮盘赌的推断准确率比基准平均提高了21% ，而歧视攻击的准确率几乎等同于随机猜测。"
    },
    {
        "title": "Private Membership Aggregation",
        "url": "http://arxiv.org/abs/2309.03872v1",
        "pub_date": "2023-09-07",
        "summary": "We consider the problem of private membership aggregation (PMA), in which a\nuser counts the number of times a certain element is stored in a system of\nindependent parties that store arbitrary sets of elements from a universal\nalphabet. The parties are not allowed to learn which element is being counted\nby the user. Further, neither the user nor the other parties are allowed to\nlearn the stored elements of each party involved in the process. PMA is a\ngeneralization of the recently introduced problem of $K$ private set\nintersection ($K$-PSI). The $K$-PSI problem considers a set of $M$ parties\nstoring arbitrary sets of elements, and a user who wants to determine if a\ncertain element is repeated at least at $K$ parties out of the $M$ parties\nwithout learning which party has the required element and which party does not.\nTo solve the general problem of PMA, we dissect it into four categories based\non the privacy requirement and the collusions among databases/parties. We map\nthese problems into equivalent private information retrieval (PIR) problems. We\npropose achievable schemes for each of the four variants of the problem based\non the concept of cross-subspace alignment (CSA). The proposed schemes achieve\n\\emph{linear} communication complexity as opposed to the state-of-the-art\n$K$-PSI scheme that requires \\emph{exponential} complexity even though our PMA\nproblems contain more security and privacy constraints.",
        "translated": "我们考虑私有成员聚合(PMA)问题，在这个问题中，用户计算某个元素存储在一个由独立方组成的系统中的次数，该系统从通用字母表中存储任意集合的元素。不允许各方了解用户计算的是哪个元素。此外，用户和其他各方都不允许学习流程中涉及的每个各方的存储元素。PMA 是最近引入的 $K $私有集交集($K $- PSI)问题的推广。$K $- PSI 问题考虑一组存储任意元素集的 $M $方，以及一个用户，该用户希望确定某个元素是否至少在 $M $方中重复 $K $方，而不知道哪一方具有所需的元素，哪一方没有。为了解决 PMA 的一般问题，我们根据隐私需求和数据库/当事方之间的共谋将其分为四类。我们将这些问题映射为等效的私有信息检索(PIR)问题。基于跨子空间比对(CSA)的概念，我们为问题的四个变体中的每一个提出了可实现的方案。所提出的方案具有相对于最先进的 $K $PSI 方案所要求的相对复杂度{指数}的线性}通信复杂度，即使我们的 PMA 问题包含更多的安全性和隐私约束。"
    },
    {
        "title": "The complexity of solving a random polynomial system",
        "url": "http://arxiv.org/abs/2309.03855v1",
        "pub_date": "2023-09-07",
        "summary": "A multivariate cryptograpic instance in practice is a multivariate polynomial\nsystem. So the security of a protocol rely on the complexity of solving a\nmultivariate polynomial system. In this paper there is an overview on a general\nalgorithm used to solve a multivariate system and the quantity to which the\ncomplexity of this algorithm depends on: the solving degree. Unfortunately, it\nis hard to compute. For this reason, it is introduced an invariant: the degree\nof regularity. This invariant, under certain condition, give us an upper bound\non the solving degree. Then we speak about random polynomial systems and in\nparticular what \"random\" means to us. Finally, we give an upper bound on both\nthe degree of regularity and the solving degree of such random systems.",
        "translated": "实际中的多元密码实例是一个多元多项式系统。因此，协议的安全性依赖于求解多元多项式系统的复杂性。本文概述了求解多变量系统的一般算法及其复杂度所依赖的量: 求解度。不幸的是，这很难计算。为此，引入了一个不变量: 正则度。在一定条件下，这个不变量给出了求解度的一个上界。然后我们谈论随机多项式系统，特别是“随机”对我们意味着什么。最后，我们给出了这类随机系统的正则度和解度的上界。"
    },
    {
        "title": "Mixtures of Gaussians are Privately Learnable with a Polynomial Number\n  of Samples",
        "url": "http://arxiv.org/abs/2309.03847v1",
        "pub_date": "2023-09-07",
        "summary": "We study the problem of estimating mixtures of Gaussians under the constraint\nof differential privacy (DP). Our main result is that $\\tilde{O}(k^2 d^4\n\\log(1/\\delta) / \\alpha^2 \\varepsilon)$ samples are sufficient to estimate a\nmixture of $k$ Gaussians up to total variation distance $\\alpha$ while\nsatisfying $(\\varepsilon, \\delta)$-DP. This is the first finite sample\ncomplexity upper bound for the problem that does not make any structural\nassumptions on the GMMs.\n  To solve the problem, we devise a new framework which may be useful for other\ntasks. On a high level, we show that if a class of distributions (such as\nGaussians) is (1) list decodable and (2) admits a \"locally small'' cover\n[BKSW19] with respect to total variation distance, then the class of its\nmixtures is privately learnable. The proof circumvents a known barrier\nindicating that, unlike Gaussians, GMMs do not admit a locally small cover\n[AAL21].",
        "translated": "我们研究了在差分隐私约束下高斯混合模型的估计问题。我们的主要结果是 $tilde { O }(k ^ 2 d ^ 4 log (1/delta)/alpha ^ 2 varepsilon) $样本足以估计 $k $Gaussian 到总变异距离 $alpha $的混合，同时满足 $(varepsilon，delta) $- DP。这是该问题的第一个有限样本复杂度上界，该上界不对 GMM 进行任何结构假设。为了解决这个问题，我们设计了一个新的框架，可能对其他任务有用。在高层次上，我们表明，如果一类分布(如高斯分布)是(1)列表可解码的，并且(2)允许“局部小”覆盖[ BKSW19]相对于总变差距离，那么它的混合类是私人可学的。该证明绕过了一个已知的障碍，表明与高斯不同，GMM 不允许局部小覆盖[ AAL21]。"
    },
    {
        "title": "Detecting unknown HTTP-based malicious communication behavior via\n  generated adversarial flows and hierarchical traffic features",
        "url": "http://arxiv.org/abs/2309.03739v1",
        "pub_date": "2023-09-07",
        "summary": "Malicious communication behavior is the network communication behavior\ngenerated by malware (bot-net, spyware, etc.) after victim devices are\ninfected. Experienced adversaries often hide malicious information in HTTP\ntraffic to evade detection. However, related detection methods have inadequate\ngeneralization ability because they are usually based on artificial feature\nengineering and outmoded datasets. In this paper, we propose an HTTP-based\nMalicious Communication traffic Detection Model (HMCD-Model) based on generated\nadversarial flows and hierarchical traffic features. HMCD-Model consists of two\nparts. The first is a generation algorithm based on WGAN-GP to generate\nHTTP-based malicious communication traffic for data enhancement. The second is\na hybrid neural network based on CNN and LSTM to extract hierarchical\nspatial-temporal features of traffic. In addition, we collect and publish a\ndataset, HMCT-2020, which consists of large-scale malicious and benign traffic\nduring three years (2018-2020). Taking the data in HMCT-2020(18) as the\ntraining set and the data in other datasets as the test set, the experimental\nresults show that the HMCD-Model can effectively detect unknown HTTP-based\nmalicious communication traffic. It can reach F1 = 98.66% in the dataset\nHMCT-2020(19-20), F1 = 90.69% in the public dataset CIC-IDS-2017, and F1 =\n83.66% in the real traffic, which is 20+% higher than other representative\nmethods on average. This validates that HMCD-Model has the ability to discover\nunknown HTTP-based malicious communication behavior.",
        "translated": "恶意通信行为是恶意软件(bot-net、间谍软件等)感染受害设备后产生的网络通信行为。有经验的对手经常在 HTTP 流量中隐藏恶意信息以躲避检测。然而，由于相关检测方法通常基于人工特征工程和过时的数据集，因此其泛化能力不足。在本文中，我们提出了一个基于 HTTP 的恶意通信流量检测模型(HMCD-Model) ，该模型基于生成的敌对流量和分层流量特征。模型由两部分组成。第一种是基于 WGAN-GP 的恶意通信生成算法，该算法生成基于 HTTP 的恶意通信流量，用于数据增强。第二种是基于 CNN 和 LSTM 的混合神经网络，提取交通流的分层时空特征。此外，我们还收集并发布了一个名为 HMCT-2020的数据集，该数据集包含三年(2018-2020)期间的大规模恶意和良性流量。实验结果表明，以 HMCT-2020(18)中的数据作为训练集，以其他数据集中的数据作为测试集，HMCD 模型能够有效地检测未知的基于 HTTP 的恶意通信流量。在 HMCT-2020(19-20)数据集中 F1 = 98.66% ，在 CIC-IDS-2017公共数据集中 F1 = 90.69% ，在实际流量中 F1 = 83.66% ，比其他代表性方法平均高出20% 以上。这验证了 HMCD-Model 具有发现未知的基于 HTTP 的恶意通信行为的能力。"
    },
    {
        "title": "Adjacency Sketches in Adversarial Environments",
        "url": "http://arxiv.org/abs/2309.03728v1",
        "pub_date": "2023-09-07",
        "summary": "An adjacency sketching or implicit labeling scheme for a family $\\cal F$ of\ngraphs is a method that defines for any $n$ vertex $G \\in \\cal F$ an assignment\nof labels to each vertex in $G$, so that the labels of two vertices tell you\nwhether or not they are adjacent. The goal is to come up with labeling schemes\nthat use as few bits as possible to represent the labels. By using randomness\nwhen assigning labels, it is sometimes possible to produce adjacency sketches\nwith much smaller label sizes, but this comes at the cost of introducing some\nprobability of error. Both deterministic and randomized labeling schemes have\nbeen extensively studied, as they have applications for distributed data\nstructures and deeper connections to universal graphs and communication\ncomplexity. The main question of interest is which graph families have schemes\nusing short labels, usually $O(\\log n)$ in the deterministic case or constant\nfor randomized sketches.\n  In this work we consider the resilience of probabilistic adjacency sketches\nagainst an adversary making adaptive queries to the labels. This differs from\nthe previously analyzed probabilistic setting which is ``one shot\". We show\nthat in the adaptive adversarial case the size of the labels is tightly related\nto the maximal degree of the graphs in $\\cal F$. This results in a stronger\ncharacterization compared to what is known in the non-adversarial setting. In\nmore detail, we construct sketches that fail with probability $\\varepsilon$ for\ngraphs with maximal degree $d$ using $2d\\log (1/\\varepsilon)$ bit labels and\nshow that this is roughly the best that can be done for any specific graph of\nmaximal degree $d$, e.g.\\ a $d$-ary tree.",
        "translated": "图族 $cal F $的邻接草图或隐式标记方案是一种方法，它定义了任意 $n $顶点 $G 在 cal F $中的标记赋值给 $G $中的每个顶点，以便两个顶点的标记告诉你它们是否相邻。我们的目标是提出标签方案，使用尽可能少的位来表示标签。通过在分配标签时使用随机性，有时可以生成标签尺寸小得多的邻接草图，但这是以引入一些错误概率为代价的。确定性标记和随机标记两种方案都得到了广泛的研究，因为它们适用于分布式数据结构，并且与通用图和通信复杂性有着更深的联系。主要的问题是哪个图族有使用短标签的方案，在确定性情况下通常是 $O (log n) $，对于随机草图则是常数。在这项工作中，我们考虑的弹性概率相邻草图对对手作出自适应查询的标签。这不同于以前分析的概率设置是“一枪”。我们证明了在自适应对手情况下，标签的大小与图的最大度紧密相关。与非对抗性环境相比，这种角色塑造更强。更详细地说，我们使用 $2d log (1/varepsilon) $bit 标签来构造失败概率为 $varepsilon $的最大度图 $d $的草图，并且表明对于任何特定的最大度图 $d $，例如 $d $- ary 树，这大致是可以做到的最好的草图。"
    },
    {
        "title": "DiffDefense: Defending against Adversarial Attacks via Diffusion Models",
        "url": "http://arxiv.org/abs/2309.03702v1",
        "pub_date": "2023-09-07",
        "summary": "This paper presents a novel reconstruction method that leverages Diffusion\nModels to protect machine learning classifiers against adversarial attacks, all\nwithout requiring any modifications to the classifiers themselves. The\nsusceptibility of machine learning models to minor input perturbations renders\nthem vulnerable to adversarial attacks. While diffusion-based methods are\ntypically disregarded for adversarial defense due to their slow reverse\nprocess, this paper demonstrates that our proposed method offers robustness\nagainst adversarial threats while preserving clean accuracy, speed, and\nplug-and-play compatibility. Code at:\nhttps://github.com/HondamunigePrasannaSilva/DiffDefence.",
        "translated": "提出了一种利用扩散模型保护机器学习分类器免受攻击的重构方法，该方法不需要对分类器本身进行任何修改。机器学习模型易受微小输入扰动的影响，容易受到敌对攻击。尽管基于扩散的方法由于其反向过程缓慢而通常被忽视，但本文证明了我们提出的方法在保持干净的准确性、速度和即插即用兼容性的同时，提供了对抗对手威胁的鲁棒性。密码:  https://github.com/hondamunigeprasannasilva/diffdefence。"
    },
    {
        "title": "Learning from Limited Heterogeneous Training Data: Meta-Learning for\n  Unsupervised Zero-Day Web Attack Detection across Web Domains",
        "url": "http://arxiv.org/abs/2309.03660v1",
        "pub_date": "2023-09-07",
        "summary": "Recently unsupervised machine learning based systems have been developed to\ndetect zero-day Web attacks, which can effectively enhance existing Web\nApplication Firewalls (WAFs). However, prior arts only consider detecting\nattacks on specific domains by training particular detection models for the\ndomains. These systems require a large amount of training data, which causes a\nlong period of time for model training and deployment. In this paper, we\npropose RETSINA, a novel meta-learning based framework that enables zero-day\nWeb attack detection across different domains in an organization with limited\ntraining data. Specifically, it utilizes meta-learning to share knowledge\nacross these domains, e.g., the relationship between HTTP requests in\nheterogeneous domains, to efficiently train detection models. Moreover, we\ndevelop an adaptive preprocessing module to facilitate semantic analysis of Web\nrequests across different domains and design a multi-domain representation\nmethod to capture semantic correlations between different domains for\ncross-domain model training. We conduct experiments using four real-world\ndatasets on different domains with a total of 293M Web requests. The\nexperimental results demonstrate that RETSINA outperforms the existing\nunsupervised Web attack detection methods with limited training data, e.g.,\nRETSINA needs only 5-minute training data to achieve comparable detection\nperformance to the existing methods that train separate models for different\ndomains using 1-day training data. We also conduct real-world deployment in an\nInternet company. RETSINA captures on average 126 and 218 zero-day attack\nrequests per day in two domains, respectively, in one month.",
        "translated": "最近开发了基于非监督式学习的系统来检测零日网络攻击，这可以有效地增强现有的网络应用防火墙(waf)。然而，现有技术只考虑通过训练特定域的特定检测模型来检测对特定域的攻击。这些系统需要大量的训练数据，导致模型训练和部署时间长。在本文中，我们提出了一个新的基于元学习的框架 RETSINA，它能够在一个训练数据有限的组织中跨不同领域检测零日 Web 攻击。具体来说，它利用元学习来跨这些领域共享知识，例如异构领域中 HTTP 请求之间的关系，以有效地训练检测模型。此外，我们开发了一个自适应预处理模块，以方便跨不同领域的 Web 请求语义分析，并设计了一个多领域表示方法，以捕获跨领域模型训练的不同领域之间的语义相关性。我们使用四个不同领域的真实世界数据集进行实验，总共有293M 个 Web 请求。实验结果表明，在训练数据有限的情况下，RETSINA 的检测性能优于现有的无监督 Web 攻击检测方法，例如，RETSINA 只需要5分钟的训练数据就可以达到与现有方法相当的检测性能，现有方法使用1天的训练数据为不同的领域训练单独的模型。我们还在一家互联网公司进行真实世界的部署。RETSINA 在一个月内平均每天在两个域中分别捕获126和218个零日攻击请求。"
    },
    {
        "title": "ProvG-Searcher: A Graph Representation Learning Approach for Efficient\n  Provenance Graph Search",
        "url": "http://arxiv.org/abs/2309.03647v1",
        "pub_date": "2023-09-07",
        "summary": "We present ProvG-Searcher, a novel approach for detecting known APT behaviors\nwithin system security logs. Our approach leverages provenance graphs, a\ncomprehensive graph representation of event logs, to capture and depict data\nprovenance relations by mapping system entities as nodes and their interactions\nas edges. We formulate the task of searching provenance graphs as a subgraph\nmatching problem and employ a graph representation learning method. The central\ncomponent of our search methodology involves embedding of subgraphs in a vector\nspace where subgraph relationships can be directly evaluated. We achieve this\nthrough the use of order embeddings that simplify subgraph matching to\nstraightforward comparisons between a query and precomputed subgraph\nrepresentations. To address challenges posed by the size and complexity of\nprovenance graphs, we propose a graph partitioning scheme and a\nbehavior-preserving graph reduction method. Overall, our technique offers\nsignificant computational efficiency, allowing most of the search computation\nto be performed offline while incorporating a lightweight comparison step\nduring query execution. Experimental results on standard datasets demonstrate\nthat ProvG-Searcher achieves superior performance, with an accuracy exceeding\n99% in detecting query behaviors and a false positive rate of approximately\n0.02%, outperforming other approaches.",
        "translated": "我们提出了一种检测系统安全日志中已知 APT 行为的新方法 ProvG-Searcher。我们的方法利用起源图(事件日志的全面图表示) ，通过将系统实体映射为节点，将它们的交互映射为边，来捕获和描述数据起源关系。将物种起源图的搜索任务表示为子图匹配问题，并采用图表示学习方法。我们的搜索方法的中心组成部分涉及子图嵌入在向量空间，其中子图关系可以直接评估。我们通过使用顺序嵌入来实现这一点，顺序嵌入将子图匹配简化为查询和预计算子图表示之间的直接比较。为了解决起源图的大小和复杂性所带来的挑战，我们提出了一种图划分方案和一种行为保持的图约简方法。总的来说，我们的技术提供了显著的计算效率，允许大多数搜索计算离线执行，同时在查询执行期间结合一个轻量级比较步骤。在标准数据集上的实验结果表明，ProvG-Searcher 在检测查询行为方面取得了较好的性能，准确率超过99% ，假阳性率约为0.02% ，优于其他方法。"
    },
    {
        "title": "NeuroCodeBench: a plain C neural network benchmark for software\n  verification",
        "url": "http://arxiv.org/abs/2309.03617v1",
        "pub_date": "2023-09-07",
        "summary": "Safety-critical systems with neural network components require strong\nguarantees. While existing neural network verification techniques have shown\ngreat progress towards this goal, they cannot prove the absence of software\nfaults in the network implementation. This paper presents NeuroCodeBench - a\nverification benchmark for neural network code written in plain C. It contains\n32 neural networks with 607 safety properties divided into 6 categories: maths\nlibrary, activation functions, error-correcting networks, transfer function\napproximation, probability density estimation and reinforcement learning. Our\npreliminary evaluation shows that state-of-the-art software verifiers struggle\nto provide correct verdicts, due to their incomplete support of the standard C\nmathematical library and the complexity of larger neural networks.",
        "translated": "具有神经网络组件的安全关键系统需要强有力的保证。虽然现有的神经网络验证技术已经朝着这个目标取得了很大的进展，但它们不能证明在网络实现中没有软件故障。本文介绍了 NeuroCodeBench ——一个用普通 C 语言编写的神经网络代码的验证基准。它包括32个具有607种安全特性的神经网络，分为6类: 数学库、激活函数、纠错网络、传输函数逼近、概率密度估计和强化学习。我们的初步评估表明，最先进的软件验证人员很难提供正确的结论，因为他们不完全支持标准的 C 数学库和更大的神经网络的复杂性。"
    },
    {
        "title": "Your Battery Is a Blast! Safeguarding Against Counterfeit Batteries with\n  Authentication",
        "url": "http://arxiv.org/abs/2309.03607v1",
        "pub_date": "2023-09-07",
        "summary": "Lithium-ion (Li-ion) batteries are the primary power source in various\napplications due to their high energy and power density. Their market was\nestimated to be up to 48 billion U.S. dollars in 2022. However, the widespread\nadoption of Li-ion batteries has resulted in counterfeit cell production, which\ncan pose safety hazards to users. Counterfeit cells can cause explosions or\nfires, and their prevalence in the market makes it difficult for users to\ndetect fake cells. Indeed, current battery authentication methods can be\nsusceptible to advanced counterfeiting techniques and are often not adaptable\nto various cells and systems. In this paper, we improve the state of the art on\nbattery authentication by proposing two novel methodologies, DCAuth and\nEISthentication, which leverage the internal characteristics of each cell\nthrough Machine Learning models. Our methods automatically authenticate\nlithium-ion battery models and architectures using data from their regular\nusage without the need for any external device. They are also resilient to the\nmost common and critical counterfeit practices and can scale to several\nbatteries and devices. To evaluate the effectiveness of our proposed\nmethodologies, we analyze time-series data from a total of 20 datasets that we\nhave processed to extract meaningful features for our analysis. Our methods\nachieve high accuracy in battery authentication for both architectures (up to\n0.99) and models (up to 0.96). Moreover, our methods offer comparable\nidentification performances. By using our proposed methodologies, manufacturers\ncan ensure that devices only use legitimate batteries, guaranteeing the\noperational state of any system and safety measures for the users.",
        "translated": "锂离子电池由于其高能量和高功率密度而成为各种应用中的主要电源。据估计，到2022年，它们的市场规模将达到480亿美元。然而，锂离子电池的广泛使用导致了假冒电池的生产，这可能会对用户造成安全隐患。假电池可以引起爆炸或火灾，而且它们在市场上的流行使用户很难发现假电池。事实上，目前的电池认证方法很容易受到先进的伪造技术的影响，而且往往不能适用于各种电池和系统。在本文中，我们提出了两种新的方法，DCAuth 和 EIS 认证，它们通过机器学习模型利用每个单元的内部特性，改善了电池认证的现状。我们的方法使用常规使用的数据自动认证锂离子电池模型和架构，而不需要任何外部设备。它们还能抵御最常见和最严重的假冒行为，并可扩展到多个电池和设备。为了评估我们提出的方法的有效性，我们分析来自总共20个数据集的时间序列数据，我们已经处理，以提取我们的分析有意义的特征。我们的方法在电池认证方面实现了高精度，无论是体系结构(高达0.99)还是模型(高达0.96)。此外，我们的方法提供了可比较的识别性能。通过使用我们提出的方法，制造商可以确保设备只使用合法的电池，保证任何系统的运行状态和用户的安全措施。"
    },
    {
        "title": "Robust Representation Learning for Privacy-Preserving Machine Learning:\n  A Multi-Objective Autoencoder Approach",
        "url": "http://arxiv.org/abs/2309.04427v1",
        "pub_date": "2023-09-08",
        "summary": "Several domains increasingly rely on machine learning in their applications.\nThe resulting heavy dependence on data has led to the emergence of various laws\nand regulations around data ethics and privacy and growing awareness of the\nneed for privacy-preserving machine learning (ppML). Current ppML techniques\nutilize methods that are either purely based on cryptography, such as\nhomomorphic encryption, or that introduce noise into the input, such as\ndifferential privacy. The main criticism given to those techniques is the fact\nthat they either are too slow or they trade off a model s performance for\nimproved confidentiality. To address this performance reduction, we aim to\nleverage robust representation learning as a way of encoding our data while\noptimizing the privacy-utility trade-off. Our method centers on training\nautoencoders in a multi-objective manner and then concatenating the latent and\nlearned features from the encoding part as the encoded form of our data. Such a\ndeep learning-powered encoding can then safely be sent to a third party for\nintensive training and hyperparameter tuning. With our proposed framework, we\ncan share our data and use third party tools without being under the threat of\nrevealing its original form. We empirically validate our results on unimodal\nand multimodal settings, the latter following a vertical splitting system and\nshow improved performance over state-of-the-art.",
        "translated": "有几个领域在其应用中越来越依赖于机器学习。由此产生的对数据的严重依赖导致了围绕数据伦理和隐私的各种法律法规的出现，以及对保护隐私的机器学习(ppML)需求的日益关注。目前的 ppML 技术使用的方法要么纯粹基于密码学，如同态加密，要么在输入中引入噪声，如差分隐私。对这些技术的主要批评是，它们要么太慢，要么为了提高保密性而牺牲模型的性能。为了解决这种性能下降的问题，我们的目标是利用健壮的表示学习作为一种编码我们的数据的方式，同时优化隐私-效用的权衡。该方法以多目标训练自动编码器为中心，将编码部分的潜在特征和学习特征连接起来作为数据的编码形式。这种深度学习驱动的编码可以安全地发送给第三方进行强化训练和超参数调整。通过我们提出的框架，我们可以共享我们的数据并使用第三方工具，而不会受到暴露其原始形式的威胁。我们经验验证了我们的结果单峰和多峰设置，后者遵循垂直分裂系统，并显示改进的性能超过了最先进的状态。"
    },
    {
        "title": "Graded Modal Types for Integrity and Confidentiality",
        "url": "http://arxiv.org/abs/2309.04324v1",
        "pub_date": "2023-09-08",
        "summary": "Graded type systems, such as the one underlying the Granule programming\nlanguage, allow various different properties of a program's behaviour to be\ntracked via annotating types with additional information, which we call grades.\nOne example of such a property, often used as a case study in prior work on\ngraded types, is information flow control, in which types are graded by a\nlattice of security levels allowing noninterference properties to be\nautomatically verified and enforced. These typically focus on one particular\naspect of security, however, known as confidentiality; public outputs are\nprohibited from depending on private inputs. Integrity, a property specifying\nthat trusted outputs must not depend on untrusted inputs, has not been examined\nin this context.\n  This short paper aims to remedy this omission. It is well-known that\nconfidentiality and integrity are in some sense dual properties, but simply\nreversing the ordering of the security lattice turns out to be unsatisfactory\nfor the purpose of combining both kinds of property in a single system, at\nleast in our setting. We analogize the situation to recent work on embedding\nboth linear and uniqueness types in a graded framework, and use this framing to\ndemonstrate that we can enforce both integrity and confidentiality alongside\none another. The main idea is to add an additional flavour of modality\nannotated for integrity, such that the existing graded comonad for tracking\nconfidentiality now also acts as a relative monad over the new modality, with\nrules allowing information to flow from trusted to public to private.",
        "translated": "分级类型系统，例如 Granule 编程语言的底层系统，允许通过附加信息注释类型来跟踪程序行为的各种不同属性，我们称之为分级。这种属性的一个例子是信息流控制，这种控制通常用作分级类型的前期工作中的案例研究。在信息流控制中，类型通过安全级别的格子进行分级，允许自动验证和强制执行不干扰属性。然而，这些通常关注于安全性的一个特定方面，称为机密性; 公共输出被禁止依赖于私有输入。完整性(Integrity)是指定可信输出不能依赖于不可信输入的属性，尚未在此上下文中进行检查。本文旨在弥补这一遗漏。众所周知，保密性和完整性在某种意义上是双重属性，但是仅仅颠倒安全格的顺序，对于将这两种属性合并在一个单一系统中的目的来说是不令人满意的，至少在我们的设置中是如此。我们将这种情况与最近在分级框架中嵌入线性和唯一性类型的工作进行了类比，并使用这种框架来证明我们可以同时强制完整性和保密性。主要想法是增加一种注释为完整性的模式，这样现有的用于追踪机密性的分级鸡现在也作为一种相对于新模式的单体，其规则允许信息从受信任的流向公共的流向私人的流。"
    },
    {
        "title": "Two-Dimensional Dynamic Fusion for Continuous Authentication",
        "url": "http://arxiv.org/abs/2309.04128v1",
        "pub_date": "2023-09-08",
        "summary": "Continuous authentication has been widely studied to provide high security\nand usability for mobile devices by continuously monitoring and authenticating\nusers. Recent studies adopt multibiometric fusion for continuous authentication\nto provide high accuracy even when some of captured biometric data are of a low\nquality. However, existing continuous fusion approaches are resource-heavy as\nthey rely on all classifiers being activated all the time and may not be\nsuitable for mobile devices.\n  In this paper, we propose a new approach to multibiometric continuous\nauthentication: two-dimensional dynamic fusion. Our key insight is that\nmultibiometric continuous authentication calculates two-dimensional matching\nscores over classifiers and over time. Based on this, we dynamically select a\nset of classifiers based on the context in which authentication is taking\nplace, and fuse matching scores by multi-classifier fusion and multi-sample\nfusion. Through experimental evaluation, we show that our approach provides a\nbetter balance between resource usage and accuracy than the existing fusion\nmethods. In particular, we show that our approach provides higher accuracy than\nthe existing methods with the same number of score calculations by adopting\nmulti-sample fusion.",
        "translated": "连续认证通过对用户的持续监控和认证，为移动设备提供高安全性和可用性，已经得到了广泛的研究。最近的研究采用多生物特征融合技术进行连续认证，即使在一些生物特征数据质量低下的情况下也能提供高准确度。然而，现有的连续融合方法资源量很大，因为它们依赖于所有的分类器一直处于激活状态，并且可能不适合移动设备。本文提出了一种新的多生物特征连续认证方法: 二维动态融合。我们的主要见解是，多生物统计连续认证计算分类器和随着时间的推移的二维匹配分数。在此基础上，根据认证环境动态选择一组分类器，通过多分类器融合和多样本融合对匹配得分进行融合。通过实验评估，我们发现我们的方法比现有的融合方法在资源使用和准确性之间提供了更好的平衡。特别是，我们表明，我们的方法提供了更高的准确性比现有的方法与相同数量的分数计算采用多样本融合。"
    },
    {
        "title": "Blockchain-enabled Data Governance for Privacy-Preserved Sharing of\n  Confidential Data",
        "url": "http://arxiv.org/abs/2309.04125v1",
        "pub_date": "2023-09-08",
        "summary": "In a traditional cloud storage system, users benefit from the convenience it\nprovides but also take the risk of certain security and privacy issues. To\nensure confidentiality while maintaining data sharing capabilities, the\nCiphertext-Policy Attribute-based Encryption (CP-ABE) scheme can be used to\nachieve fine-grained access control in cloud services. However, existing\napproaches are impaired by three critical concerns: illegal authorization, key\ndisclosure, and privacy leakage.\n  To address these, we propose a blockchain-based data governance system that\nemploys blockchain technology and attribute-based encryption to prevent privacy\nleakage and credential misuse. First, our ABE encryption system can handle\nmulti-authority use cases while protecting identity privacy and hiding access\npolicy, which also protects data sharing against corrupt authorities. Second,\napplying the Advanced Encryption Standard (AES) for data encryption makes the\nwhole system efficient and responsive to real-world conditions. Furthermore,\nthe encrypted data is stored in a decentralized storage system such as IPFS,\nwhich does not rely on any centralized service provider and is, therefore,\nresilient against single-point failures. Third, illegal authorization activity\ncan be readily identified through the logged on-chain data. Besides the system\ndesign, we also provide security proofs to demonstrate the robustness of the\nproposed system.",
        "translated": "在传统的云存储系统中，用户受益于它提供的便利性，但也承担了某些安全和隐私问题的风险。为了在保证数据共享能力的同时保证数据的机密性，可以使用基于密文策略属性的加密(CP-ABE)方案来实现云服务的细粒度访问控制。然而，现有的方法受到三个关键问题的损害: 非法授权、密钥公开和隐私泄露。为了解决这些问题，我们提出了一个基于区块链的数据治理系统，该系统采用区块链技术和基于属性的加密来防止隐私泄露和证书滥用。首先，我们的 ABE 加密系统可以处理多权限用例，同时保护身份隐私和隐藏访问策略，这也保护数据共享，防止腐败的权限。其次，应用高级加密标准(AES)进行数据加密，可使整个系统有效率，并对现实环境作出响应。此外，加密的数据存储在一个分散的存储系统，如 IPFS，这不依赖于任何集中的服务提供商，因此，对单点故障的弹性。第三，可以通过登录的上链数据轻松地识别非法授权活动。除了系统设计，我们还提供了安全性证明，以证明所提出的系统的健壮性。"
    },
    {
        "title": "Penetrating Shields: A Systematic Analysis of Memory Corruption\n  Mitigations in the Spectre Era",
        "url": "http://arxiv.org/abs/2309.04119v1",
        "pub_date": "2023-09-08",
        "summary": "This paper provides the first systematic analysis of a synergistic threat\nmodel encompassing memory corruption vulnerabilities and microarchitectural\nside-channel vulnerabilities. We study speculative shield bypass attacks that\nleverage speculative execution attacks to leak secrets that are critical to the\nsecurity of memory corruption mitigations (i.e., the shields), and then use the\nleaked secrets to bypass the mitigation mechanisms and successfully conduct\nmemory corruption exploits, such as control-flow hijacking. We start by\nsystematizing a taxonomy of the state-of-the-art memory corruption mitigations\nfocusing on hardware-software co-design solutions. The taxonomy helps us to\nidentify 10 likely vulnerable defense schemes out of 20 schemes that we\nanalyze. Next, we develop a graph-based model to analyze the 10 likely\nvulnerable defenses and reason about possible countermeasures. Finally, we\npresent three proof-of-concept attacks targeting an already-deployed mitigation\nmechanism and two state-of-the-art academic proposals.",
        "translated": "本文首次系统地分析了包含内存损坏漏洞和微架构侧通道漏洞的协同威胁模型。我们研究推测性的盾牌绕过攻击，利用 Speculative_execution 攻击来泄露对缓解内存损坏的安全性至关重要的机密(比如盾牌) ，然后利用泄露的机密绕过缓解机制，成功地进行内存损坏利用，如控制流劫持。我们首先对最先进的内存损坏缓解方案进行系统化分类，重点放在硬件-软件协同设计解决方案上。分类法帮助我们从我们分析的20个方案中识别出10个可能的脆弱防御方案。接下来，我们开发了一个基于图表的模型来分析10个可能的脆弱防御和可能的对策的原因。最后，我们提出了三种针对已经部署的缓解机制的概念验证攻击和两种最先进的学术建议。"
    },
    {
        "title": "One-to-Multiple Clean-Label Image Camouflage (OmClic) based Backdoor\n  Attack on Deep Learning",
        "url": "http://arxiv.org/abs/2309.04036v1",
        "pub_date": "2023-09-07",
        "summary": "Image camouflage has been utilized to create clean-label poisoned images for\nimplanting backdoor into a DL model. But there exists a crucial limitation that\none attack/poisoned image can only fit a single input size of the DL model,\nwhich greatly increases its attack budget when attacking multiple commonly\nadopted input sizes of DL models. This work proposes to constructively craft an\nattack image through camouflaging but can fit multiple DL models' input sizes\nsimultaneously, namely OmClic. Thus, through OmClic, we are able to always\nimplant a backdoor regardless of which common input size is chosen by the user\nto train the DL model given the same attack budget (i.e., a fraction of the\npoisoning rate). With our camouflaging algorithm formulated as a\nmulti-objective optimization, M=5 input sizes can be concurrently targeted with\none attack image, which artifact is retained to be almost visually\nimperceptible at the same time. Extensive evaluations validate the proposed\nOmClic can reliably succeed in various settings using diverse types of images.\nFurther experiments on OmClic based backdoor insertion to DL models show that\nhigh backdoor performances (i.e., attack success rate and clean data accuracy)\nare achievable no matter which common input size is randomly chosen by the user\nto train the model. So that the OmClic based backdoor attack budget is reduced\nby M$\\times$ compared to the state-of-the-art camouflage based backdoor attack\nas a baseline. Significantly, the same set of OmClic based poisonous attack\nimages is transferable to different model architectures for backdoor implant.",
        "translated": "图像伪装已被用来创建干净的标签中毒图像植入后门到 DL 模型。但是存在一个关键的局限性，即一个攻击/中毒图像只能适应 DL 模型的单一输入大小，这就大大增加了攻击多个常用的 DL 模型输入大小时的攻击预算。这项工作提出建设性地制造一个攻击图像通过伪装，但可以适应多个 DL 模型的输入大小同时，即 OmClic。因此，通过 OmClic，我们能够总是植入一个后门，不管用户选择哪个公共输入大小来训练 DL 模型，给定相同的攻击预算(即，中毒率的一小部分)。将伪装算法转化为多目标优化算法，使得 M = 5的输入大小可以与一幅攻击图像同时瞄准，同时保留了几乎视觉无法感知的伪装。广泛的评估验证了所提议的 OmClic 可以在使用不同类型的图像的各种设置中可靠地成功。对基于 OmClic 的后门插入 DL 模型的进一步实验表明，无论用户随机选择哪种常见的输入大小来训练模型，高后门性能(即攻击成功率和干净数据的准确性)都是可以实现的。因此，基于 OmClic 的后门攻击预算比基于最先进伪装技术的后门攻击预算减少了 M 美元的几倍。值得注意的是，同一组基于 OmClic 的有毒攻击图像可以转移到不同的后门植入模型架构。"
    },
    {
        "title": "A Novel Supervised Deep Learning Solution to Detect Distributed Denial\n  of Service (DDoS) attacks on Edge Systems using Convolutional Neural Networks\n  (CNN)",
        "url": "http://arxiv.org/abs/2309.05646v1",
        "pub_date": "2023-09-11",
        "summary": "Cybersecurity attacks are becoming increasingly sophisticated and pose a\ngrowing threat to individuals, and private and public sectors. Distributed\nDenial of Service attacks are one of the most harmful of these threats in\ntoday's internet, disrupting the availability of essential services. This\nproject presents a novel deep learning-based approach for detecting DDoS\nattacks in network traffic using the industry-recognized DDoS evaluation\ndataset from the University of New Brunswick, which contains packet captures\nfrom real-time DDoS attacks, creating a broader and more applicable model for\nthe real world. The algorithm employed in this study exploits the properties of\nConvolutional Neural Networks (CNN) and common deep learning algorithms to\nbuild a novel mitigation technique that classifies benign and malicious\ntraffic. The proposed model preprocesses the data by extracting packet flows\nand normalizing them to a fixed length which is fed into a custom architecture\ncontaining layers regulating node dropout, normalization, and a sigmoid\nactivation function to out a binary classification. This allows for the model\nto process the flows effectively and look for the nodes that contribute to DDoS\nattacks while dropping the \"noise\" or the distractors. The results of this\nstudy demonstrate the effectiveness of the proposed algorithm in detecting DDOS\nattacks, achieving an accuracy of .9883 on 2000 unseen flows in network\ntraffic, while being scalable for any network environment.",
        "translated": "网络安全攻击正变得越来越复杂，对个人、私人和公共部门构成越来越大的威胁。分布式分布式拒绝服务攻击攻击是当今互联网上最有害的威胁之一，它破坏了基本服务的可用性。该项目提出了一种基于深度学习的新方法，利用业界公认的 DDoS 评估数据集来检测网络流量中的 DDoS 攻击，该新不伦瑞克大学包含实时 DDoS 攻击的数据包捕获，为现实世界创建了一个更广泛、更适用的模型。该算法利用了卷积神经网络(CNN)的特性和常用的深度学习算法，构建了一种新的分类良性和恶意流量的缓解技术。提出的模型通过提取数据包流并将其标准化为一个固定长度来预处理数据，这个长度被输入到一个自定义体系结构中，该体系结构包含调节节点丢失、标准化的层，以及一个用于输出二进制分类的乙状结构激活函数。这使得模型能够有效地处理流，并在丢弃“噪声”或干扰物的同时寻找导致 DDoS 攻击的节点。研究结果表明，该算法能够有效地检测 DDOS 攻击，对2000个网络流量中的无形流量的检测精度达到0.9883，并且适用于任何网络环境。"
    },
    {
        "title": "Privacy Side Channels in Machine Learning Systems",
        "url": "http://arxiv.org/abs/2309.05610v1",
        "pub_date": "2023-09-11",
        "summary": "Most current approaches for protecting privacy in machine learning (ML)\nassume that models exist in a vacuum, when in reality, ML models are part of\nlarger systems that include components for training data filtering, output\nmonitoring, and more. In this work, we introduce privacy side channels: attacks\nthat exploit these system-level components to extract private information at\nfar higher rates than is otherwise possible for standalone models. We propose\nfour categories of side channels that span the entire ML lifecycle (training\ndata filtering, input preprocessing, output post-processing, and query\nfiltering) and allow for either enhanced membership inference attacks or even\nnovel threats such as extracting users' test queries. For example, we show that\ndeduplicating training data before applying differentially-private training\ncreates a side-channel that completely invalidates any provable privacy\nguarantees. Moreover, we show that systems which block language models from\nregenerating training data can be exploited to allow exact reconstruction of\nprivate keys contained in the training set -- even if the model did not\nmemorize these keys. Taken together, our results demonstrate the need for a\nholistic, end-to-end privacy analysis of machine learning.",
        "translated": "目前大多数用于保护机器学习隐私的方法都假设模型存在于真空中，而实际上，机器学习模型是更大系统的一部分，包括用于训练数据过滤、输出监控等的组件。在这项工作中，我们引入了隐私侧通道: 利用这些系统级组件以比独立模型更高的速率提取隐私信息的攻击。我们提出了四类跨越整个机器学习生命周期的边通道(训练数据过滤、输入预处理、输出后处理和查询过滤) ，并允许增强的成员资格推断攻击，甚至新的威胁，如提取用户的测试查询。例如，我们表明，在应用差异私有训练之前删除训练数据会创建一个边通道，使任何可证明的隐私保障完全失效。此外，我们表明，阻止语言模型再生训练数据的系统可以被用来允许包含在训练集中的私有密钥的精确重建——即使模型没有记住这些密钥。综上所述，我们的研究结果表明需要对机器学习进行全面的、端到端的隐私分析。"
    },
    {
        "title": "Multiplierless Design of High-Speed Very Large Constant Multiplications",
        "url": "http://arxiv.org/abs/2309.05550v2",
        "pub_date": "2023-09-11",
        "summary": "In cryptographic algorithms, the constants to be multiplied by a variable can\nbe very large due to security requirements. Thus, the hardware complexity of\nsuch algorithms heavily depends on the design architecture handling large\nconstants. In this paper, we introduce an electronic design automation tool,\ncalled LEIGER, which can automatically generate the realizations of very large\nconstant multiplications for low-complexity and high-speed applications,\ntargeting the ASIC design platform. LEIGER can utilize the shift-adds\narchitecture and use 3-input operations, i.e., carry-save adders (CSAs), where\nthe number of CSAs is reduced using a prominent optimization algorithm. It can\nalso generate constant multiplications under a hybrid design architecture,\nwhere 2-and 3-input operations are used at different stages. Moreover, it can\ndescribe constant multiplications under a design architecture using compressor\ntrees. As a case study, high-speed Montgomery multiplication, which is a\nfundamental operation in cryptographic algorithms, is designed with its\nconstant multiplication block realized under the proposed architectures.\nExperimental results indicate that LEIGER enables a designer to explore the\ntrade-off between area and delay of the very large constant and Montgomery\nmultiplications and leads to designs with area-delay product, latency, and\nenergy consumption values significantly better than those obtained by a\nrecently proposed algorithm.",
        "translated": "在加密算法中，由于安全需求，要乘以变量的常数可能非常大。因此，这些算法的硬件复杂度很大程度上取决于处理大型常量的设计体系结构。在这篇文章中，我们介绍了一个名为 LEIGER 的电子设计自动化工具，它可以针对 ASIC 设计平台自动生成用于低复杂度和高速应用的非常大的常数乘法的实现。LEIGER 可以利用移位加法结构和使用3输入操作，即进位保存加法器(CSA) ，其中 CSA 的数量减少使用一个突出的优化算法。它还可以在混合设计架构下产生常数乘法，其中在不同阶段使用2和3输入操作。此外，它可以在设计体系结构下使用压缩树描述常数乘法。以密码算法中的基本运算——高速蒙哥马利乘法为例，设计了在该体系结构下实现常数乘法块的高速蒙哥马利乘法。实验结果表明，LEIGER 使设计人员能够探索非常大的常数乘法和蒙哥马利乘法的面积和延迟之间的平衡，并导致面积延迟积、延迟和能量消耗值的设计明显优于最近提出的算法。"
    },
    {
        "title": "D2WFP: A Novel Protocol for Forensically Identifying, Extracting, and\n  Analysing Deep and Dark Web Browsing Activities",
        "url": "http://arxiv.org/abs/2309.05537v1",
        "pub_date": "2023-09-11",
        "summary": "The use of the un-indexed web, commonly known as the deep web and dark web,\nto commit or facilitate criminal activity has drastically increased over the\npast decade. The dark web is an in-famously dangerous place where all kinds of\ncriminal activities take place [1-2], despite advances in web forensics\ntechniques, tools, and methodologies, few studies have formally tackled the\ndark and deep web forensics and the technical differences in terms of\ninvestigative techniques and artefacts identification and extraction. This\nresearch proposes a novel and comprehensive protocol to guide and assist\ndigital forensics professionals in investigating crimes committed on or via the\ndeep and dark web, The protocol named D2WFP establishes a new sequential\napproach for performing investigative activities by observing the order of\nvolatility and implementing a systemic approach covering all browsing related\nhives and artefacts which ultimately resulted into improv-ing the accuracy and\neffectiveness. Rigorous quantitative and qualitative research has been\nconducted by assessing D2WFP following a scientifically-sound and comprehensive\nprocess in different scenarios and the obtained results show an apparent\nincrease in the number of artefacts re-covered when adopting D2WFP which\noutperform any current industry or opensource browsing forensics tools. The\nsecond contribution of D2WFP is the robust formulation of artefact correlation\nand cross-validation within D2WFP which enables digital forensics professionals\nto better document and structure their analysis of host-based deep and dark web\nbrowsing artefacts.",
        "translated": "在过去十年中，使用未编制索引的网络(通常称为深网和暗网)从事或协助犯罪活动的情况急剧增加。尽管网络取证技术、工具和方法取得了进步，但很少有研究正式解决黑暗和深度网络取证以及在调查技术和人工物品识别和提取方面的技术差异。这项研究提出了一个新颖而全面的协议，以指导和协助数位鑑识专业人员调查犯罪或通过深和暗网，该协议命名为 D2粮食计划署建立了一个新的顺序方法进行调查活动，通过观察波动的顺序和实施一个系统的方法，涵盖所有浏览相关的蜂巢和人工制品，最终导致提高准确性和有效性。通过在不同情景下采用科学合理和全面的过程对 D2WFP 进行严格的定量和质性研究评估，得到的结果显示，在采用比当前任何行业或开源浏览取证工具表现更好的 D2WFP 时，发现的文物数量明显增加。D2WFP 的第二个贡献是在 D2WFP 内部对人工制品的相关性和交叉验证进行了有力的描述，这使得数位鑑识专业人员能够更好地记录和构建他们对基于主机的深度和暗度网络浏览人工制品的分析。"
    },
    {
        "title": "Classification of Quantum Computer Fault Injection Attacks",
        "url": "http://arxiv.org/abs/2309.05478v1",
        "pub_date": "2023-09-11",
        "summary": "The rapid growth of interest in quantum computing has brought about the need\nto secure these powerful machines against a range of physical attacks. As qubit\ncounts increase and quantum computers achieve higher levels of fidelity, their\npotential to execute novel algorithms and generate sensitive intellectual\nproperty becomes more promising. However, there is a significant gap in our\nunderstanding of the vulnerabilities these computers face in terms of security\nand privacy attacks. Among the potential threats are physical attacks,\nincluding those orchestrated by malicious insiders within data centers where\nthe quantum computers are located, which could compromise the integrity of\ncomputations and resulting data. This paper presents an exploration of\nfault-injection attacks as one class of physical attacks on quantum computers.\nThis work first introduces a classification of fault-injection attacks and\nstrategies, including the domain of fault-injection attacks, the fault targets,\nand fault manifestations in quantum computers. The resulting classification\nhighlights the potential threats that exist. By shedding light on the\nvulnerabilities of quantum computers to fault-injection attacks, this work\ncontributes to the development of robust security measures for this emerging\ntechnology.",
        "translated": "随着人们对量子计算兴趣的迅速增长，人们需要保护这些强大的机器免受一系列物理攻击。随着量子位计数的增加和量子计算机达到更高的保真度水平，它们执行新算法和生成敏感知识产权的潜力变得更有希望。然而，我们对这些计算机在安全和隐私攻击方面所面临的漏洞的理解还有很大的差距。潜在的威胁包括物理攻击，包括由量子计算机所在的数据中心内部的恶意内部人员精心策划的攻击，这可能会损害计算和结果数据的完整性。本文将故障注入攻击作为量子计算机的一类物理攻击进行了探讨。本文首先介绍了故障注入攻击的分类和策略，包括故障注入攻击的领域、故障目标和量子计算机中的故障表现。由此产生的分类突出显示了存在的潜在威胁。通过揭示量子计算机容易受到故障注入攻击的弱点，这项工作有助于为这一新兴技术开发强有力的安全措施。"
    },
    {
        "title": "Unveiling the Sentinels: Assessing AI Performance in Cybersecurity Peer\n  Review",
        "url": "http://arxiv.org/abs/2309.05457v1",
        "pub_date": "2023-09-11",
        "summary": "Peer review is the method employed by the scientific community for evaluating\nresearch advancements. In the field of cybersecurity, the practice of\ndouble-blind peer review is the de-facto standard. This paper touches on the\nholy grail of peer reviewing and aims to shed light on the performance of AI in\nreviewing for academic security conferences. Specifically, we investigate the\npredictability of reviewing outcomes by comparing the results obtained from\nhuman reviewers and machine-learning models. To facilitate our study, we\nconstruct a comprehensive dataset by collecting thousands of papers from\nrenowned computer science conferences and the arXiv preprint website. Based on\nthe collected data, we evaluate the prediction capabilities of ChatGPT and a\ntwo-stage classification approach based on the Doc2Vec model with various\nclassifiers. Our experimental evaluation of review outcome prediction using the\nDoc2Vec-based approach performs significantly better than the ChatGPT and\nachieves an accuracy of over 90%. While analyzing the experimental results, we\nidentify the potential advantages and limitations of the tested ML models. We\nexplore areas within the paper-reviewing process that can benefit from\nautomated support approaches, while also recognizing the irreplaceable role of\nhuman intellect in certain aspects that cannot be matched by state-of-the-art\nAI techniques.",
        "translated": "同行评议是科学界用来评价研究进展的方法。在网络安全领域，双盲同行评议的实践是事实上的标准。本文触及了同行评议的圣杯，旨在揭示人工智能在学术安全会议评议中的表现。具体来说，我们通过比较人类评审者和机器学习模型的结果来研究评审结果的可预测性。为了方便我们的研究，我们收集了来自著名的计算机科学会议和 arXiv 预印本网站的数千篇论文，从而构建了一个全面的数据集。基于收集到的数据，我们评估了 ChatGPT 和基于 Doc2Vec 模型和不同分类器的两阶段分类方法的预测能力。我们使用基于 Doc2Vec 的方法对评审结果预测的实验性评估比 ChatGPT 的性能要好得多，并且达到了90% 以上的准确率。在分析实验结果的同时，我们发现了测试的 ML 模型的潜在优势和局限性。我们探索了论文审阅过程中可以从自动化支持方法中受益的领域，同时也认识到人类智能在某些方面的不可替代的作用，这是最先进的人工智能技术所无法比拟的。"
    },
    {
        "title": "Practical Homomorphic Aggregation for Byzantine ML",
        "url": "http://arxiv.org/abs/2309.05395v1",
        "pub_date": "2023-09-11",
        "summary": "Due to the large-scale availability of data, machine learning (ML) algorithms\nare being deployed in distributed topologies, where different nodes collaborate\nto train ML models over their individual data by exchanging model-related\ninformation (e.g., gradients) with a central server. However, distributed\nlearning schemes are notably vulnerable to two threats. First, Byzantine nodes\ncan single-handedly corrupt the learning by sending incorrect information to\nthe server, e.g., erroneous gradients. The standard approach to mitigate such\nbehavior is to use a non-linear robust aggregation method at the server.\nSecond, the server can violate the privacy of the nodes. Recent attacks have\nshown that exchanging (unencrypted) gradients enables a curious server to\nrecover the totality of the nodes' data. The use of homomorphic encryption\n(HE), a gold standard security primitive, has extensively been studied as a\nprivacy-preserving solution to distributed learning in non-Byzantine scenarios.\nHowever, due to HE's large computational demand especially for high-dimensional\nML models, there has not yet been any attempt to design purely homomorphic\noperators for non-linear robust aggregators. In this work, we present SABLE,\nthe first completely homomorphic and Byzantine robust distributed learning\nalgorithm. SABLE essentially relies on a novel plaintext encoding method that\nenables us to implement the robust aggregator over batching-friendly BGV.\nMoreover, this encoding scheme also accelerates state-of-the-art homomorphic\nsorting with larger security margins and smaller ciphertext size. We perform\nextensive experiments on image classification tasks and show that our algorithm\nachieves practical execution times while matching the ML performance of its\nnon-private counterpart.",
        "translated": "由于数据的大规模可用性，机器学习(ML)算法正被部署在分布式拓扑中，不同的节点通过与中央服务器交换模型相关信息(例如，梯度) ，协作训练机器学习模型。然而，分布式学习方案明显容易受到两种威胁的影响。首先，拜占庭节点可以单独通过向服务器发送错误信息(例如错误的梯度)破坏学习。缓解此类行为的标准方法是在服务器上使用非线性鲁棒聚合方法。其次，服务器可能侵犯节点的隐私。最近的攻击表明，交换(未加密的)梯度可以使奇怪的服务器恢复节点的全部数据。使用同态加密(HE) ，一个黄金标准的安全原语，已被广泛研究作为一个保护隐私的解决方案，分布式学习在非拜占庭情况下。然而，由于 HE 的大量计算需求，尤其是对高维 ML 模型的需求，目前还没有任何尝试为非线性鲁棒聚集器设计纯同态算子。本文提出了第一个完全同态的拜占庭鲁棒分布式学习算法 SABLE。SABLE 本质上依赖于一种新的明文编码方法，使我们能够在批处理友好的 BGV 上实现鲁棒的聚合器。此外，该编码方案还加速了最先进的同态排序，具有更大的安全边界和更小的密文大小。我们对图像分类任务进行了广泛的实验，结果表明该算法在匹配其非私有对应物的最大似然性能的同时，达到了实际的执行时间。"
    },
    {
        "title": "FuzzLLM: A Novel and Universal Fuzzing Framework for Proactively\n  Discovering Jailbreak Vulnerabilities in Large Language Models",
        "url": "http://arxiv.org/abs/2309.05274v1",
        "pub_date": "2023-09-11",
        "summary": "Jailbreak vulnerabilities in Large Language Models (LLMs), which exploit\nmeticulously crafted prompts to elicit content that violates service\nguidelines, have captured the attention of research communities. While model\nowners can defend against individual jailbreak prompts through safety training\nstrategies, this relatively passive approach struggles to handle the broader\ncategory of similar jailbreaks. To tackle this issue, we introduce FuzzLLM, an\nautomated fuzzing framework designed to proactively test and discover jailbreak\nvulnerabilities in LLMs. We utilize templates to capture the structural\nintegrity of a prompt and isolate key features of a jailbreak class as\nconstraints. By integrating different base classes into powerful combo attacks\nand varying the elements of constraints and prohibited questions, FuzzLLM\nenables efficient testing with reduced manual effort. Extensive experiments\ndemonstrate FuzzLLM's effectiveness and comprehensiveness in vulnerability\ndiscovery across various LLMs.",
        "translated": "大型语言模型(LLM)中的越狱漏洞，利用精心设计的提示来引出违反服务指南的内容，已经引起了研究团体的注意。虽然模型车主可以通过安全培训策略来抵御个人越狱提示，但这种相对被动的方法难以处理更广泛类型的类似越狱事件。为了解决这个问题，我们引入了 FuzzLLM，这是一个自动化的模糊框架，旨在主动测试和发现 LLM 中的越狱漏洞。我们利用模板来捕获提示符的结构完整性，并将越狱类的关键特性作为约束隔离。通过将不同的基类集成到强大的组合攻击中，并改变约束和禁止问题的元素，FuzzLLM 可以减少人工操作，从而实现有效的测试。大量的实验证明了 FuzzLLM 在各种 LLM 中的漏洞发现的有效性和全面性。"
    },
    {
        "title": "A quantum tug of war between randomness and symmetries on homogeneous\n  spaces",
        "url": "http://arxiv.org/abs/2309.05253v1",
        "pub_date": "2023-09-11",
        "summary": "We explore the interplay between symmetry and randomness in quantum\ninformation. Adopting a geometric approach, we consider states as\n$H$-equivalent if related by a symmetry transformation characterized by the\ngroup $H$. We then introduce the Haar measure on the homogeneous space\n$\\mathbb{U}/H$, characterizing true randomness for $H$-equivalent systems.\nWhile this mathematical machinery is well-studied by mathematicians, it has\nseen limited application in quantum information: we believe our work to be the\nfirst instance of utilizing homogeneous spaces to characterize symmetry in\nquantum information. This is followed by a discussion of approximations of true\nrandomness, commencing with $t$-wise independent approximations and defining\n$t$-designs on $\\mathbb{U}/H$ and $H$-equivalent states. Transitioning further,\nwe explore pseudorandomness, defining pseudorandom unitaries and states within\nhomogeneous spaces. Finally, as a practical demonstration of our findings, we\nstudy the expressibility of quantum machine learning ansatze in homogeneous\nspaces. Our work provides a fresh perspective on the relationship between\nrandomness and symmetry in the quantum world.",
        "translated": "我们探讨了量子信息中对称性和随机性之间的相互作用。采用几何的方法，我们认为状态是等价的，如果通过对称变换关联的话，拥有属性群是等价的。然后，我们在 $mathbb { U }/h $上引入 Haar 测度，表征了 $h $等价系统的真随机齐性空间。虽然数学家们对这种数学机制进行了深入的研究，但它在量子信息中的应用却十分有限: 我们相信我们的工作是利用齐次空间来表征量子信息中的对称性的第一个实例。随后讨论了真随机性的近似，从 $t $- 独立近似开始，并在 $mathbb { U }/H $和 $H $- 等价状态上定义 $t $- 设计。在进一步的过渡中，我们探索了伪随机数，定义了齐次空间中的伪随机酉和状态。最后，作为我们发现的一个实际证明，我们研究了量子机器学习在齐次空间中的可表达性。我们的工作为量子世界中随机性和对称性之间的关系提供了一个新的视角。"
    },
    {
        "title": "Serberus: Protecting Cryptographic Code from Spectres at Compile-Time",
        "url": "http://arxiv.org/abs/2309.05174v1",
        "pub_date": "2023-09-11",
        "summary": "We present Serberus, the first comprehensive mitigation for hardening\nconstant-time (CT) code against Spectre attacks (involving the PHT, BTB, RSB,\nSTL and/or PSF speculation primitives) on existing hardware. Serberus is based\non three insights. First, some hardware control-flow integrity (CFI)\nprotections restrict transient control-flow to the extent that it may be\ncomprehensively considered by software analyses. Second, conformance to the\naccepted CT code discipline permits two code patterns that are unsafe in the\npost-Spectre era. Third, once these code patterns are addressed, all Spectre\nleakage of secrets in CT programs can be attributed to one of four classes of\ntaint primitives--instructions that can transiently assign a secret value to a\npublicly-typed register. We evaluate Serberus on cryptographic primitives in\nthe OpenSSL, Libsodium, and HACL* libraries. Serberus introduces 21.3% runtime\noverhead on average, compared to 24.9% for the next closest state-of-the-art\nsoftware mitigation, which is less secure.",
        "translated": "我们介绍了 Serberus，第一个针对 Spectre 攻击(包括 PHT、 BTB、 RSB、 STL 和/或 PSF 推测原语)加固常时(CT)代码的全面缓解。Serberus 基于三个观点。首先，一些硬件控制流完整性(CFI)保护对暂态控制流的限制可以通过软件分析综合考虑。其次，遵守公认的 CT 代码规则允许两种代码模式，这两种代码模式在后幽灵时代是不安全的。第三，一旦解决了这些代码模式，CT 程序中所有 Spectre 泄露的秘密都可以归因于四类污染原语中的一类——这些指令可以瞬时地为公开类型的寄存器分配一个秘密值。我们使用 OpenSSL、 Libsodium 和 HACL * 库中的加密原语对 Serberus 进行评估。Serberus 平均引入了21.3% 的运行时开销，相比之下，下一个最接近最先进的软件缓解版本的运行时开销为24.9% ，这个版本的安全性较差。"
    },
    {
        "title": "Preliminary Results from a U.S. Demographic Analysis of SMiSh\n  Susceptibility",
        "url": "http://arxiv.org/abs/2309.06322v1",
        "pub_date": "2023-09-12",
        "summary": "As adoption of mobile phones has skyrocketed, so have scams involving them.\nThe text method is called SMiShing, (aka SMShing, or smishing) in which a\nfraudster sends a phishing link via Short Message Service (SMS) text to a\nphone. However, no data exists on who is most vulnerable to SMiShing. Prior\nwork in phishing (its e-mail cousin) indicates that this is likely to vary by\ndemographic and contextual factors. In our study, we collect this data from\nN=1007 U.S. adult mobile phone users. Younger people and college students\nemerge in this sample as the most vulnerable. Participants struggled to\ncorrectly identify legitimate messages and were easily misled when they knew\nthey had an account with the faked message entity. Counterintuitively,\nparticipants with higher levels of security training and awareness were less\ncorrect in rating possible SMiSH. We recommend next steps for researchers,\nregulators and telecom providers.",
        "translated": "随着移动电话的普及率急剧上升，涉及手机的诈骗案件也随之增多。这种文本方法被称为 SMishing (又名 SMishing) ，欺诈者通过短信服务(SMS)向电话发送一个钓鱼链接。然而，没有数据表明谁最容易受到 SMiSing 的攻击。之前在网络钓鱼(它的电子邮件同类)方面的工作表明，这可能会因人口统计学和上下文因素而有所不同。在我们的研究中，我们收集了1007名美国成年手机用户的数据。在这个样本中，年轻人和大学生是最脆弱的。参与者很难正确识别合法的信息，当他们知道他们有一个假信息实体的帐户时，他们很容易被误导。与直觉相反，受过更高级别安全培训和意识的参与者对可能的 SMISH 的评分不太正确。我们建议研究人员、监管机构和电信供应商采取下一步措施。"
    },
    {
        "title": "Systematic Evaluation of Geolocation Privacy Mechanisms",
        "url": "http://arxiv.org/abs/2309.06263v1",
        "pub_date": "2023-09-12",
        "summary": "Location data privacy has become a serious concern for users as Location\nBased Services (LBSs) have become an important part of their life. It is\npossible for malicious parties having access to geolocation data to learn\nsensitive information about the user such as religion or political views.\nLocation Privacy Preserving Mechanisms (LPPMs) have been proposed by previous\nworks to ensure the privacy of the shared data while allowing the users to use\nLBSs. But there is no clear view of which mechanism to use according to the\nscenario in which the user makes use of a LBS. The scenario is the way the user\nis using a LBS (frequency of reports, number of reports). In this paper, we\nstudy the sensitivity of LPPMs on the scenario on which they are used. We\npropose a framework to systematically evaluate LPPMs by considering an\nexhaustive combination of LPPMs, attacks and metrics. Using our framework we\ncompare a selection of LPPMs including an improved mechanism that we introduce.\nBy evaluating over a variety of scenarios, we find that the efficacy (privacy,\nutility, and robustness) of the studied mechanisms is dependent on the\nscenario: for example the privacy of Planar Laplace geo-indistinguishability is\ngreatly reduced in a continuous scenario. We show that the scenario is\nessential to consider when choosing an obfuscation mechanism for a given\napplication.",
        "translated": "随着基于位置的服务(LBS)已经成为用户生活中的重要组成部分，位置数据隐私已经成为用户严重关注的问题。恶意当事方有可能获取地理位置数据，以获取有关用户的敏感信息，如宗教或政治观点。位置隐私保护机制(LPPM)是前人提出的一种既能保护共享数据的隐私，又能允许用户使用 LBS 的机制。但是对于根据用户使用 LBS 的场景使用哪种机制，目前还没有明确的看法。场景是用户使用 LBS 的方式(报告的频率，报告的数量)。在本文中，我们研究了 LPPM 对其使用场景的敏感性。我们提出了一个框架来系统地评估 LPPM，通过考虑一个详尽的组合 LPPM，攻击和度量。使用我们的框架，我们比较了 LPPM 的选择，包括我们引入的一个改进的机制。通过对各种情景的评估，我们发现所研究机制的功效(隐私性、实用性和稳健性)取决于情景，例如 Planar Laplace 的隐私性——在一个连续的情景中，地理不可分辨性大大降低。我们展示了在为给定的应用程序选择模糊处理机制时必须考虑的场景。"
    },
    {
        "title": "Unveiling Signle-Bit-Flip Attacks on DNN Executables",
        "url": "http://arxiv.org/abs/2309.06223v1",
        "pub_date": "2023-09-12",
        "summary": "Recent research has shown that bit-flip attacks (BFAs) can manipulate deep\nneural networks (DNNs) via DRAM Rowhammer exploitations. Existing attacks are\nprimarily launched over high-level DNN frameworks like PyTorch and flip bits in\nmodel weight files. Nevertheless, DNNs are frequently compiled into low-level\nexecutables by deep learning (DL) compilers to fully leverage low-level\nhardware primitives. The compiled code is usually high-speed and manifests\ndramatically distinct execution paradigms from high-level DNN frameworks.\n  In this paper, we launch the first systematic study on the attack surface of\nBFA specifically for DNN executables compiled by DL compilers. We design an\nautomated search tool to identify vulnerable bits in DNN executables and\nidentify practical attack vectors that exploit the model structure in DNN\nexecutables with BFAs (whereas prior works make likely strong assumptions to\nattack model weights). DNN executables appear more \"opaque\" than models in\nhigh-level DNN frameworks. Nevertheless, we find that DNN executables contain\nextensive, severe (e.g., single-bit flip), and transferrable attack surfaces\nthat are not present in high-level DNN models and can be exploited to deplete\nfull model intelligence and control output labels. Our finding calls for\nincorporating security mechanisms in future DNN compilation toolchains.",
        "translated": "最近的研究表明，位翻转攻击(BFA)可以通过 DRAM RowHammer 的开发来操纵深度神经网络(DNN)。现有的攻击主要是通过类似 PyTorch 这样的高级 DNN 框架和模型权重文件中的翻转位发起的。然而，深度学习(DL)编译器经常将 DNN 编译成低级可执行文件，以充分利用低级硬件原语。编译后的代码通常是高速的，并显示出与高级 DNN 框架截然不同的执行范例。本文针对 DL 编译器编译的 DNN 可执行文件，首次对 BFA 的攻击面进行了系统的研究。我们设计了一个自动搜索工具来识别 DNN 可执行文件中易受攻击的位，并识别实际的攻击向量，这些攻击向量使用 BFA 来利用 DNN 可执行文件中的模型结构(而之前的工作对攻击模型权重做出了强有力的假设)。DNN 可执行程序看起来比高级 DNN 框架中的模型更“不透明”。尽管如此，我们发现 DNN 可执行文件包含广泛的，严重的(例如，单位翻转)和可转移的攻击表面，这些攻击表面不存在于高级 DNN 模型中，并且可以被用来耗尽完整的模型智能和控制输出标签。我们的发现呼吁将安全机制纳入未来 DNN 编译工具链。"
    },
    {
        "title": "HoneyEVSE: An Honeypot to emulate Electric Vehicle Supply Equipments",
        "url": "http://arxiv.org/abs/2309.06077v1",
        "pub_date": "2023-09-12",
        "summary": "To fight climate change, new \"green\" technology are emerging, most of them\nusing electricity as a power source. Among the solutions, Electric Vehicles\n(EVs) represent a central asset in the future transport system. EVs require a\ncomplex infrastructure to enable the so-called Vehicle-to-Grid (V2G) paradigm\nto manage the charging process between the smart grid and the EV. In this\nparadigm, the Electric Vehicle Supply Equipment (EVSE), or charging station, is\nthe end device that authenticates the vehicle and delivers the power to charge\nit. However, since an EVSE is publicly exposed and connected to the Internet,\nrecent works show how an attacker with physical tampering and remote access can\ntarget an EVSE, exposing the security of the entire infrastructure and the\nfinal user. For this reason, it is important to develop novel strategies to\nsecure such infrastructures. In this paper we present HoneyEVSE, the first\nhoneypot conceived to simulate an EVSE. HoneyEVSE can simulate with high\nfidelity the EV charging process and, at the same time, enables a user to\ninteract with it through a dashboard. Furthermore, based on other charging\ncolumns exposed on the Internet, we emulate the login and device information\npages to increase user engagement. We exposed HoneyEVSE for 30 days to the\nInternet to assess its capability and measured the interaction received with\nits Shodan Honeyscore. Results show that HoneyEVSE can successfully evade the\nShodan honeyscore metric while attracting a high number of interactions on the\nexposed services.",
        "translated": "为了应对气候变化，新的“绿色”技术正在出现，其中大部分使用电力作为能源。在这些解决方案中，电动汽车是未来交通系统的核心资产。电动汽车需要一个复杂的基础设施来支持所谓的车辆到电网(V2G)范式来管理智能电网和电动汽车之间的充电过程。在这个范例中，电动汽车供应设备(EVSE) ，或充电站，是认证车辆和提供电力充电的终端设备。然而，由于 EVSE 是公开的并且连接到 Internet，最近的工作显示了具有物理篡改和远程访问的攻击者如何能够瞄准 EVSE，从而暴露整个基础设施和最终用户的安全性。出于这个原因，重要的是制定新的战略，以确保这些基础设施的安全。本文介绍了 HoneyEVSE，它是第一个模拟 EVSE 的蜜罐。HoneyEVSE 可以高保真地模拟电动汽车充电过程，同时使用户能够通过仪表板与之交互。此外，基于 Internet 上公开的其他收费列，我们模拟登录和设备信息页面，以增加用户参与度。我们将 HoneyEVSE 暴露在互联网上30天，以评估其能力，并测量与其 Shodan Honeyscore 的交互。结果表明，HoneyEVSE 能够成功地规避 Shodan 蜜分度量，同时在公开的服务上吸引大量的交互。"
    },
    {
        "title": "Verifiable Fairness: Privacy-preserving Computation of Fairness for\n  Machine Learning Systems",
        "url": "http://arxiv.org/abs/2309.06061v1",
        "pub_date": "2023-09-12",
        "summary": "Fair machine learning is a thriving and vibrant research topic. In this\npaper, we propose Fairness as a Service (FaaS), a secure, verifiable and\nprivacy-preserving protocol to computes and verify the fairness of any machine\nlearning (ML) model. In the deisgn of FaaS, the data and outcomes are\nrepresented through cryptograms to ensure privacy. Also, zero knowledge proofs\nguarantee the well-formedness of the cryptograms and underlying data. FaaS is\nmodel--agnostic and can support various fairness metrics; hence, it can be used\nas a service to audit the fairness of any ML model. Our solution requires no\ntrusted third party or private channels for the computation of the fairness\nmetric. The security guarantees and commitments are implemented in a way that\nevery step is securely transparent and verifiable from the start to the end of\nthe process. The cryptograms of all input data are publicly available for\neveryone, e.g., auditors, social activists and experts, to verify the\ncorrectness of the process. We implemented FaaS to investigate performance and\ndemonstrate the successful use of FaaS for a publicly available data set with\nthousands of entries.",
        "translated": "公平机器学习是一个蓬勃发展的研究课题。在本文中，我们提出了公平作为服务(FaaS) ，一个安全的，可验证的和保护隐私的协议来计算和验证任何机器学习(ML)模型的公平性。在 FaaS 的设计中，数据和结果通过密码表示以保证隐私。此外，零知识证明保证了密码和底层数据的良好格式。FaaS 是模型不可知的，并且可以支持各种公平性指标; 因此，它可以作为一种服务来审计任何机器学习模型的公平性。我们的解决方案不需要可信的第三方或私有通道来计算公平性度量。执行安全保证和承诺的方式是，每一步骤从进程开始到结束都是安全透明和可核查的。所有输入数据的密码都对所有人公开，例如审计员、社会活动家和专家，以验证过程的正确性。我们实现 FaaS 是为了调查性能，并演示如何成功地将 FaaS 用于具有数千条目的公开可用数据集。"
    },
    {
        "title": "Backdoor Attacks and Countermeasures in Natural Language Processing\n  Models: A Comprehensive Security Review",
        "url": "http://arxiv.org/abs/2309.06055v2",
        "pub_date": "2023-09-12",
        "summary": "Deep Neural Networks (DNNs) have led to unprecedented progress in various\nnatural language processing (NLP) tasks. Owing to limited data and computation\nresources, using third-party data and models has become a new paradigm for\nadapting various tasks. However, research shows that it has some potential\nsecurity vulnerabilities because attackers can manipulate the training process\nand data source. Such a way can set specific triggers, making the model exhibit\nexpected behaviors that have little inferior influence on the model's\nperformance for primitive tasks, called backdoor attacks. Hence, it could have\ndire consequences, especially considering that the backdoor attack surfaces are\nbroad.\n  To get a precise grasp and understanding of this problem, a systematic and\ncomprehensive review is required to confront various security challenges from\ndifferent phases and attack purposes. Additionally, there is a dearth of\nanalysis and comparison of the various emerging backdoor countermeasures in\nthis situation. In this paper, we conduct a timely review of backdoor attacks\nand countermeasures to sound the red alarm for the NLP security community.\nAccording to the affected stage of the machine learning pipeline, the attack\nsurfaces are recognized to be wide and then formalized into three\ncategorizations: attacking pre-trained model with fine-tuning (APMF) or\nprompt-tuning (APMP), and attacking final model with training (AFMT), where\nAFMT can be subdivided into different attack aims. Thus, attacks under each\ncategorization are combed. The countermeasures are categorized into two general\nclasses: sample inspection and model inspection. Overall, the research on the\ndefense side is far behind the attack side, and there is no single defense that\ncan prevent all types of backdoor attacks. An attacker can intelligently bypass\nexisting defenses with a more invisible attack. ......",
        "translated": "深度神经网络(DNN)在各种自然语言处理(NLP)任务中取得了前所未有的进展。由于数据和计算资源有限，利用第三方数据和模型已成为适应各种任务的新范式。然而，研究表明，由于攻击者可以操纵训练过程和数据源，因此它具有一些潜在的安全漏洞。这种方法可以设置特定的触发器，使模型表现出预期的行为，这些行为对模型执行原始任务(称为后门攻击)的性能影响不大。因此，它可能有可怕的后果，特别是考虑到后门攻击的表面是广泛的。为了准确把握和理解这个问题，需要从不同的阶段和攻击目的对各种安全挑战进行系统和全面的审查。此外，在这种情况下出现的各种后门对策缺乏分析和比较。在本文中，我们进行了一个及时的后门攻击和对策，以响应红色警报的 NLP 安全社区。根据机器学习流水线的受影响阶段，将攻击面识别为宽度较大的攻击面，然后将攻击面形式化为三种攻击目标: 用微调(APMF)或提示调整(APMP)攻击预训练模型和用训练(AFMT)攻击最终模型，其中 AFMT 可以细分为不同的攻击目标。因此，每个分类下的攻击都会被梳理。对策分为两大类: 样品检验和模型检验。总的来说，防御方面的研究远远落后于攻击方面，没有单一的防御能够防止所有类型的后门攻击。攻击者可以通过更隐形的攻击智能地绕过现有的防御。......"
    },
    {
        "title": "CToMP: A Cycle-task-oriented Memory Protection Scheme for Unmanned\n  Systems",
        "url": "http://arxiv.org/abs/2309.05978v1",
        "pub_date": "2023-09-12",
        "summary": "Memory corruption attacks (MCAs) refer to malicious behaviors of system\nintruders that modify the contents of a memory location to disrupt the normal\noperation of computing systems, causing leakage of sensitive data or\nperturbations to ongoing processes. Unlike general-purpose systems, unmanned\nsystems cannot deploy complete security protection schemes, due to their\nlimitations in size, cost and performance. MCAs in unmanned systems are\nparticularly difficult to defend against. Furthermore, MCAs have diverse and\nunpredictable attack interfaces in unmanned systems, severely impacting digital\nand physical sectors. In this paper, we first generalize, model and taxonomize\nMCAs found in unmanned systems currently, laying the foundation for designing a\nportable and general defense approach. According to different attack\nmechanisms, we found that MCAs are mainly categorized into two\ntypes--return2libc and return2shellcode. To tackle return2libc attacks, we\nmodel the erratic operation of unmanned systems with cycles and then propose a\ncycle-task-oriented memory protection (CToMP) approach to protect control flows\nfrom tampering. To defend against return2shellcode attacks, we introduce a\nsecure process stack with a randomized memory address by leveraging the memory\npool to prevent Shellcode from being executed. Moreover, we discuss the\nmechanism by which CToMP resists the ROP attack, a novel variant of return2libc\nattacks. Finally, we implement CToMP on CUAV V5+ with Ardupilot and Crazyflie.\nThe evaluation and security analysis results demonstrate that the proposed\napproach CToMP is resilient to various MCAs in unmanned systems with low\nfootprints and system overhead.",
        "translated": "内存损坏攻击(MCA)是指系统入侵者的恶意行为，它修改内存位置的内容，以破坏计算系统的正常运行，从而导致敏感数据泄漏或正在进行的进程受到干扰。与通用系统不同，无人系统由于其规模、成本和性能的限制，无法部署完整的安全保护方案。无人系统中的 MCA 尤其难以防御。此外，MCA 在无人系统中具有多样化和不可预测的攻击接口，严重影响了数字和物理扇区。本文首先对目前无人系统中存在的 MCA 进行了归纳、建模和分类，为设计一种便携的通用防御方法奠定了基础。根据不同的攻击机制，我们发现 MCA 主要分为两种类型—— return 2libc 和 return 2shell 代码。为了解决 return 2libc 攻击问题，我们建立了无人系统的周期不稳定运行模型，然后提出了一种面向周期任务的内存保护(CTMP)方法来保护控制流不被篡改。为了防御 return 2shell 代码攻击，我们引入了一个具有随机内存地址的安全进程堆栈，利用内存池来防止执行 Shell 代码。此外，我们还讨论了 CTOMP 抵抗 ROP 攻击的机制，ROP 攻击是 return 2libc 攻击的一种新的变体。最后，我们使用 Ardupilot 和 Crazyflie 在 CUAV V5 + 上实现了 CTOMP。评估和安全性分析结果表明，该方法在低占用率和系统开销的无人值守系统中对各种 MCA 具有较好的适应性。"
    },
    {
        "title": "Random Segmentation: New Traffic Obfuscation against Packet-Size-Based\n  Side-Channel Attacks",
        "url": "http://arxiv.org/abs/2309.05941v1",
        "pub_date": "2023-09-12",
        "summary": "Despite encryption, the packet size is still visible, enabling observers to\ninfer private information in the Internet of Things (IoT) environment (e.g.,\nIoT device identification). Packet padding obfuscates packet-length\ncharacteristics with a high data overhead because it relies on adding noise to\nthe data. This paper proposes a more data-efficient approach that randomizes\npacket sizes without adding noise. We achieve this by splitting large TCP\nsegments into random-sized chunks; hence, the packet length distribution is\nobfuscated without adding noise data. Our client-server implementation using\nTCP sockets demonstrates the feasibility of our approach at the application\nlevel. We realize our packet size control by adjusting two local\nsocket-programming parameters. First, we enable the TCP_NODELAY option to send\nout each packet with our specified length. Second, we downsize the sending\nbuffer to prevent the sender from pushing out more data than can be received,\nwhich could disable our control of the packet sizes. We simulate our defense on\na network trace of four IoT devices and show a reduction in device\nclassification accuracy from 98% to 63%, close to random guessing. Meanwhile,\nthe real-world data transmission experiments show that the added latency is\nreasonable, less than 21%, while the added packet header overhead is only about\n5%.",
        "translated": "尽管进行了加密，数据包的大小仍然可见，使观察者能够在物联网(IoT)环境中推断出私人信息(例如，物联网设备识别)。数据包填充使数据包长度的特征变得模糊，数据开销很大，因为它依赖于向数据中添加噪声。本文提出了一种数据效率更高的方法，在不增加噪声的情况下随机分组大小。我们通过将大的 TCP 段分割成随机大小的块来实现这一点; 因此，在不增加噪声数据的情况下，数据包长度分布被混淆了。我们使用 TCP 套接字的客户机-服务器实现证明了我们的方法在应用程序级别的可行性。我们通过调整两个本地套接字编程参数来实现数据包大小的控制。首先，我们启用 TCP _ NODELAY 选项来发送指定长度的每个数据包。其次，我们缩小发送缓冲区，以防止发送方推出超过可以接收的数据，这可能会使我们失去对数据包大小的控制。我们在四个物联网设备的网络跟踪上模拟我们的防御，并显示设备分类准确率从98% 降低到63% ，接近随机猜测。与此同时，现实世界的数据传输实验表明，增加的延迟是合理的，小于21% ，而增加的数据包报头开销只有5% 左右。"
    },
    {
        "title": "Catch You Everything Everywhere: Guarding Textual Inversion via Concept\n  Watermarking",
        "url": "http://arxiv.org/abs/2309.05940v1",
        "pub_date": "2023-09-12",
        "summary": "AIGC (AI-Generated Content) has achieved tremendous success in many\napplications such as text-to-image tasks, where the model can generate\nhigh-quality images with diverse prompts, namely, different descriptions in\nnatural languages. More surprisingly, the emerging personalization techniques\neven succeed in describing unseen concepts with only a few personal images as\nreferences, and there have been some commercial platforms for sharing the\nvaluable personalized concept. However, such an advanced technique also\nintroduces a severe threat, where malicious users can misuse the target concept\nto generate highly-realistic illegal images. Therefore, it becomes necessary\nfor the platform to trace malicious users and hold them accountable.\n  In this paper, we focus on guarding the most popular lightweight\npersonalization model, ie, Textual Inversion (TI). To achieve it, we propose\nthe novel concept watermarking, where watermark information is embedded into\nthe target concept and then extracted from generated images based on the\nwatermarked concept. Specifically, we jointly train a watermark encoder and a\nwatermark decoder with the sampler in the loop.\n  It shows great resilience to different diffusion sampling processes possibly\nchosen by malicious users, meanwhile preserving utility for normal use. In\npractice, the concept owner can upload his concept with different watermarks\n(ie, serial numbers) to the platform, and the platform allocates different\nusers with different serial numbers for subsequent tracing and forensics.",
        "translated": "AIGC (AI-Generated Content)在许多应用领域取得了巨大的成功，例如文本到图像的任务，该模型可以通过不同的提示(即自然语言中的不同描述)生成高质量的图像。更令人惊讶的是，新兴的个性化技术甚至成功地描述了看不见的概念，只有少量的个人图像作为参考，并已有一些商业平台分享有价值的个性化概念。然而，这种先进的技术也带来了严重的威胁，恶意用户可以误用目标概念来生成高度逼真的非法图像。因此，平台有必要跟踪恶意用户并追究他们的责任。在本文中，我们重点讨论了最流行的轻量级个性化模型，即文本反转(TI)。为此，提出了一种新的概念水印算法，将水印信息嵌入到目标概念中，然后基于水印概念从生成的图像中提取水印信息。具体来说，我们在循环中联合训练一个水印编码器和一个水印解码器。它对恶意用户可能选择的不同扩散采样过程表现出很强的弹性，同时为正常使用保留了实用性。在实践中，概念所有者可以上传不同水印(即，序列号)的概念到平台，平台分配不同的用户与不同的序列号，以便随后跟踪和取证。"
    },
    {
        "title": "Public key cryptosystems based on Iterated Functions Systems",
        "url": "http://arxiv.org/abs/2309.05917v1",
        "pub_date": "2023-09-12",
        "summary": "Let $f=(f_0,f_1,\\dots, f_{\\nu-1})$ be a collection of one-to-one functions\nfrom some space~$X$ into itself such that the sets $f_j(X)$ are disjoint. If\n$w=w_1w_2\\cdots w_k$ is a word on the alphabet $\\{0,1,\\dots,\\nu-1\\}$, let\n$\\Phi_{f,w} = f_{w_1}\\circ f_{w_2}\\circ\\cdots\\circ f_{w_k}$. Given a\nfunction~$F$ of which we know that it can be written as $\\Phi_{f,w}$, it is\neasy to recover~$w$. We give some examples of this situation where everything\ncan be scrambled up by using some private key to get a new system\n$g=(g_1,g_2,\\dots,g_{\\nu-1})$ on another set~$Y$ in such a way that the images\nof the $g_j$ are no longer disjoint. We define a cryptosystem whose public key\nis~$g$. The message to be encrypted is a word~$w$ and the associated cryptogram\nis $\\Phi_{g,w}$. The private key allows to recover $\\Phi_{f,w}$ from\n$\\Phi_{g,w}$.",
        "translated": "设 $f = (f _ 0，f _ 1，圆点，f _ { nu-1}) $是从某个空间 ~ $X $到自身的一对一函数的集合，这样集合 $f _ j (X) $是不相交的。如果 $w = w _ 1w _ 2 cdot w _ k $是字母表上的一个单词 ${0,1，dot，nu-1} $，让 $Phi _ { f，w } = f _ { w _ 1} circ f _ { w _ 2} circ cdot circ f _ { w _ k } $。给定一个函数 ~ $F $，我们知道它可以写成 $Phi _ { f，w } $，很容易恢复 ~ $w $。我们给出了一些例子，在这种情况下，一切都可以通过使用一些私钥来获得一个新的系统 $g = (g _ 1，g _ 2，dot，g _ { nu-1}) $在另一个集合上 $Y $，这样 $g _ j $的图像就不再是不相交的。我们定义了一个公钥为 ~ $g $的密码系统。要加密的消息是一个单词 ~ $w $，相关的密码是 $Phi _ { g，w } $。私钥允许从 $Phi _ { g，w } $恢复 $Phi _ { f，w } $。"
    },
    {
        "title": "Hardening RGB-D Object Recognition Systems against Adversarial Patch\n  Attacks",
        "url": "http://arxiv.org/abs/2309.07106v1",
        "pub_date": "2023-09-13",
        "summary": "RGB-D object recognition systems improve their predictive performances by\nfusing color and depth information, outperforming neural network architectures\nthat rely solely on colors. While RGB-D systems are expected to be more robust\nto adversarial examples than RGB-only systems, they have also been proven to be\nhighly vulnerable. Their robustness is similar even when the adversarial\nexamples are generated by altering only the original images' colors. Different\nworks highlighted the vulnerability of RGB-D systems; however, there is a\nlacking of technical explanations for this weakness. Hence, in our work, we\nbridge this gap by investigating the learned deep representation of RGB-D\nsystems, discovering that color features make the function learned by the\nnetwork more complex and, thus, more sensitive to small perturbations. To\nmitigate this problem, we propose a defense based on a detection mechanism that\nmakes RGB-D systems more robust against adversarial examples. We empirically\nshow that this defense improves the performances of RGB-D systems against\nadversarial examples even when they are computed ad-hoc to circumvent this\ndetection mechanism, and that is also more effective than adversarial training.",
        "translated": "RGB-D 物体识别系统通过融合颜色和深度信息来提高其预测性能，优于单纯依赖颜色的神经网络体系结构。虽然 RGB-D 系统预计将更强大的对抗性的例子比 RGB-only 系统，他们也已被证明是高度脆弱的。即使是通过改变原始图像的颜色来生成对立的例子，它们的鲁棒性也是相似的。不同的著作强调了 RGB-D 系统的脆弱性，然而，缺乏对这一弱点的技术解释。因此，在我们的工作中，我们通过研究 RGB-D 系统的学习深度表示，发现颜色特征使得网络学习的功能更加复杂，因此，对小的扰动更加敏感。为了缓解这个问题，我们提出了一个基于检测机制的防御，使 RGB-D 系统更加健壮的对抗对手的例子。我们的实验表明，这种防御提高了 RGB-D 系统对抗对手的性能，即使他们是特别计算规避这种检测机制，这也是更有效的比对手训练。"
    },
    {
        "title": "A Comprehensive Analysis of the Role of Artificial Intelligence and\n  Machine Learning in Modern Digital Forensics and Incident Response",
        "url": "http://arxiv.org/abs/2309.07064v1",
        "pub_date": "2023-09-13",
        "summary": "In the dynamic landscape of digital forensics, the integration of Artificial\nIntelligence (AI) and Machine Learning (ML) stands as a transformative\ntechnology, poised to amplify the efficiency and precision of digital forensics\ninvestigations. However, the use of ML and AI in digital forensics is still in\nits nascent stages. As a result, this paper gives a thorough and in-depth\nanalysis that goes beyond a simple survey and review. The goal is to look\nclosely at how AI and ML techniques are used in digital forensics and incident\nresponse. This research explores cutting-edge research initiatives that cross\ndomains such as data collection and recovery, the intricate reconstruction of\ncybercrime timelines, robust big data analysis, pattern recognition,\nsafeguarding the chain of custody, and orchestrating responsive strategies to\nhacking incidents. This endeavour digs far beneath the surface to unearth the\nintricate ways AI-driven methodologies are shaping these crucial facets of\ndigital forensics practice. While the promise of AI in digital forensics is\nevident, the challenges arising from increasing database sizes and evolving\ncriminal tactics necessitate ongoing collaborative research and refinement\nwithin the digital forensics profession. This study examines the contributions,\nlimitations, and gaps in the existing research, shedding light on the potential\nand limitations of AI and ML techniques. By exploring these different research\nareas, we highlight the critical need for strategic planning, continual\nresearch, and development to unlock AI's full potential in digital forensics\nand incident response. Ultimately, this paper underscores the significance of\nAI and ML integration in digital forensics, offering insights into their\nbenefits, drawbacks, and broader implications for tackling modern cyber\nthreats.",
        "translated": "在动态的数位鑑识领域，人工智能(AI)和机器学习(ML)的结合是一种变革性的技术，有望提高数位鑑识调查的效率和精确度。然而，机器学习和人工智能在数位鑑识领域的应用仍处于初级阶段。因此，本文在简单的调查和回顾的基础上，进行了深入而全面的分析。我们的目标是密切关注人工智能和机器学习技术如何应用于数位鑑识和事件响应。这项研究探讨了跨领域的尖端研究举措，如数据收集和恢复，复杂的重建网络犯罪时间表，强大的大数据分析，模式识别，保护监管链，并策划黑客事件的反应策略。这项努力深入挖掘了人工智能驱动的方法论在塑造数位鑑识实践这些关键方面的错综复杂的方式。虽然人工智能在数位鑑识领域的前景是显而易见的，但随着数据库规模的不断扩大和犯罪手法的不断演变，面临的挑战需要数位鑑识内部持续合作研究和完善。本研究考察了现有研究的贡献、局限性和差距，揭示了人工智能和机器学习技术的潜力和局限性。通过探索这些不同的研究领域，我们强调了战略规划、持续研究和开发的关键需要，以充分发挥人工智能在数位鑑识和事件应对方面的潜力。最后，本文强调了人工智能和机器学习整合在数位鑑识领域的重要性，深入分析了它们在应对现代网络威胁方面的优缺点和更广泛的影响。"
    },
    {
        "title": "Cryptography: Against AI and QAI Odds",
        "url": "http://arxiv.org/abs/2309.07022v1",
        "pub_date": "2023-09-13",
        "summary": "Artificial Intelligence (AI) presents prodigious technological prospects for\ndevelopment, however, all that glitters is not gold! The cyber-world faces the\nworst nightmare with the advent of AI and quantum computers. Together with\nQuantum Artificial Intelligence (QAI), they pose a catastrophic threat to\nmodern cryptography. It would also increase the capability of cryptanalysts\nmanifold, with its built-in persistent and extensive predictive intelligence.\nThis prediction ability incapacitates the constrained message space in device\ncryptography. With the comparison of these assumptions and the intercepted\nciphertext, the code-cracking process will considerably accelerate. Before the\nvigorous and robust developments in AI, we have never faced and never had to\nprepare for such a plaintext-originating attack. The supremacy of AI can be\nchallenged by creating ciphertexts that would give the AI attacker erroneous\nresponses stymied by randomness and misdirect them. AI threat is deterred by\ndeviating from the conventional use of small, known-size keys and\npattern-loaded ciphers. The strategy is vested in implementing larger secret\nsize keys, supplemented by ad-hoc unilateral randomness of unbound limitations\nand a pattern-devoid technique. The very large key size can be handled with low\nprocessing and computational burden to achieve desired unicity distances. The\nstrategy against AI odds is feasible by implementing non-algorithmic\nrandomness, large and inexpensive memory chips, and wide-area communication\nnetworks. The strength of AI, i.e., randomness and pattern detection can be\nused to generate highly optimized ciphers and algorithms. These pattern-devoid,\nrandomness-rich ciphers also provide a timely and plausible solution for NIST's\nproactive approach toward the quantum challenge.",
        "translated": "人工智能(AI)展示了巨大的技术发展前景，然而，所有闪闪发光的不是黄金！随着人工智能和量子计算机的出现，网络世界面临着最可怕的噩梦。它们与量子人工智能(QAI)一起对现代密码学构成了灾难性的威胁。它还将提高密码分析人员的能力，其内置的持久性和广泛的预测智能。这种预测能力使设备加密中受限制的消息空间丧失能力。通过比较这些假设和截获的密文，密码破解过程将大大加快。在人工智能蓬勃发展之前，我们从来没有面对过，也从来没有必要为这种明文攻击做准备。人工智能的至高无上地位可以通过创建密文来挑战，这些密文会给人工智能攻击者错误的反应，使其受到随机性的阻碍并误导它们。人工智能的威胁是通过偏离传统的使用小的，已知大小的密钥和模式加载密码来阻止的。该策略被赋予实现更大的秘密大小密钥，辅之以无约束限制的临时单边随机性和无模式技术。非常大的密钥大小可以处理与低处理和计算负担，以实现所需的唯一性距离。通过实现非算法随机性、大而廉价的存储芯片以及广域通信网络，该方法是可行的。人工智能的优势，即随机性和模式检测可以用来生成高度优化的密码和算法。这些模式缺乏，随机性丰富的密码也提供了一个及时和合理的解决方案 NIST 的前瞻性方法对量子挑战。"
    },
    {
        "title": "Two Timin': Repairing Smart Contracts With A Two-Layered Approach",
        "url": "http://arxiv.org/abs/2309.07841v1",
        "pub_date": "2023-09-14",
        "summary": "Due to the modern relevance of blockchain technology, smart contracts present\nboth substantial risks and benefits. Vulnerabilities within them can trigger a\ncascade of consequences, resulting in significant losses. Many current papers\nprimarily focus on classifying smart contracts for malicious intent, often\nrelying on limited contract characteristics, such as bytecode or opcode. This\npaper proposes a novel, two-layered framework: 1) classifying and 2) directly\nrepairing malicious contracts. Slither's vulnerability report is combined with\nsource code and passed through a pre-trained RandomForestClassifier (RFC) and\nLarge Language Models (LLMs), classifying and repairing each suggested\nvulnerability. Experiments demonstrate the effectiveness of fine-tuned and\nprompt-engineered LLMs. The smart contract repair models, built from\npre-trained GPT-3.5-Turbo and fine-tuned Llama-2-7B models, reduced the overall\nvulnerability count by 97.5% and 96.7% respectively. A manual inspection of\nrepaired contracts shows that all retain functionality, indicating that the\nproposed method is appropriate for automatic batch classification and repair of\nvulnerabilities in smart contracts.",
        "translated": "由于区块链技术的现代相关性，智能合同既带来了巨大的风险，也带来了巨大的收益。它们内部的漏洞可能引发一系列后果，导致重大损失。当前的许多论文主要关注于对恶意意图的智能合同进行分类，通常依赖于有限的合同特征，如字节码或操作码。本文提出了一个新颖的两层框架: 1)分类和2)直接修复恶意合同。Slither 的漏洞报告与源代码相结合，并通过一个预先训练好的随机森林分类器(RFC)和大语言模型(LLM) ，对每个建议的漏洞进行分类和修复。实验证明了微调和快速工程 LLM 的有效性。智能合同修复模型，建立在预先训练的 GPT-3.5-Turbo 和微调的 Llama-2-7B 模型，分别减少了97.5% 和96.7% 的整体脆弱性计数。对修复合同的人工检查表明，所有合同都保留了功能，表明拟议的方法适用于智能合同中的脆弱性的自动批量分类和修复。"
    },
    {
        "title": "Communication Efficient Private Federated Learning Using Dithering",
        "url": "http://arxiv.org/abs/2309.07809v1",
        "pub_date": "2023-09-14",
        "summary": "The task of preserving privacy while ensuring efficient communication is a\nfundamental challenge in federated learning. In this work, we tackle this\nchallenge in the trusted aggregator model, and propose a solution that achieves\nboth objectives simultaneously. We show that employing a quantization scheme\nbased on subtractive dithering at the clients can effectively replicate the\nnormal noise addition process at the aggregator. This implies that we can\nguarantee the same level of differential privacy against other clients while\nsubstantially reducing the amount of communication required, as opposed to\ntransmitting full precision gradients and using central noise addition. We also\nexperimentally demonstrate that the accuracy of our proposed approach matches\nthat of the full precision gradient method.",
        "translated": "在联邦学习中，保护隐私的同时保证有效的通信是一个基本的挑战。在这项工作中，我们解决了这个挑战，在可信聚合器模型，并提出了一个解决方案，实现两个目标同时进行。结果表明，在客户端采用基于减抖动的量化方案可以有效地复制聚合器的正常噪声加成过程。这意味着我们可以保证与其他客户相同的差分隐私水平，同时大大减少所需的通信量，而不是传输完全精确的梯度和使用中央噪声附加。实验结果表明，该方法的精度与全精度梯度法相当。"
    },
    {
        "title": "The Nonce-nce of Web Security: an Investigation of CSP Nonces Reuse",
        "url": "http://arxiv.org/abs/2309.07782v1",
        "pub_date": "2023-09-14",
        "summary": "Content Security Policy (CSP) is an effective security mechanism that\nprevents the exploitation of Cross-Site Scripting (XSS) vulnerabilities on\nwebsites by specifying the sources from which their web pages can load\nresources, such as scripts and styles. CSP nonces enable websites to allow the\nexecution of specific inline scripts and styles without relying on a whitelist.\nIn this study, we measure and analyze the use of CSP nonces in the wild,\nspecifically looking for nonce reuse, short nonces, and invalid nonces. We find\nthat, of the 2271 sites that deploy a nonce-based policy, 598 of them reuse the\nsame nonce value in more than one response, potentially enabling attackers to\nbypass protection offered by the CSP against XSS attacks. We analyze the causes\nof the nonce reuses to identify whether they are introduced by the server-side\ncode or if the nonces are being cached by web caches. Moreover, we investigate\nwhether nonces are only reused within the same session or for different\nsessions, as this impacts the effectiveness of CSP in preventing XSS attacks.\nFinally, we discuss the possibilities for attackers to bypass the CSP and\nachieve XSS in different nonce reuse scenarios.",
        "translated": "内容保安政策是一项有效的保安机制，透过指定网页载入资源(例如脚本和样式)的来源，防止网站利用跨网站脚本保安漏洞。CSP nonce 允许网站在不依赖白名单的情况下执行特定的内联脚本和样式。在这项研究中，我们测量和分析了 CSP nonce 在野外的使用，特别是寻找一次重用、短期 nonce 和无效 nonce。我们发现，在部署基于 nonce 策略的2271个站点中，有598个站点在多个响应中重用了相同的 nonce 值，这可能使攻击者能够绕过 CSP 提供的针对 XSS 攻击的保护。我们分析 nonce 重用的原因，以确定它们是由服务器端代码引入的，还是由 Web 缓存缓存的 nonce。此外，我们调查 nonce 是否只在同一个会话或不同的会话中重用，因为这会影响 CSP 在防止 XSS 攻击方面的有效性。最后，我们讨论了攻击者在不同的重用场景中绕过 CSP 实现 XSS 的可能性。"
    },
    {
        "title": "TGh: A TEE/GC Hybrid Enabling Confidential FaaS Platforms",
        "url": "http://arxiv.org/abs/2309.07764v1",
        "pub_date": "2023-09-14",
        "summary": "Trusted Execution Environments (TEEs) suffer from performance issues when\nexecuting certain management instructions, such as creating an enclave, context\nswitching in and out of protected mode, and swapping cached pages. This is\nespecially problematic for short-running, interactive functions in\nFunction-as-a-Service (FaaS) platforms, where existing techniques to address\nenclave overheads are insufficient. We find FaaS functions can spend more time\nmanaging the enclave than executing application instructions. In this work, we\npropose a TEE/GC hybrid (TGh) protocol to enable confidential FaaS platforms.\nTGh moves computation out of the enclave onto the untrusted host using garbled\ncircuits (GC), a cryptographic construction for secure function evaluation. Our\napproach retains the security guarantees of enclaves while avoiding the\nperformance issues associated with enclave management instructions.",
        "translated": "可信执行环境(Trusted Execution Environment，TEE)在执行某些管理指令时会遇到性能问题，例如创建飞地、进出受保护模式的上下文切换以及交换缓存页面。这对于功能即服务(Functional-as-a-Service，FaaS)平台中短期运行的交互式功能尤其成问题，因为现有的技术不足以解决飞地开销问题。我们发现 FaaS 函数管理飞地的时间比执行应用程序指令的时间更长。在这项工作中，我们提出了一个 TEE/GC 混合(TGh)协议来支持机密的 FaaS 平台。TGh 使用乱码电路(GC)将计算从飞地移到不可信主机上，这是一种用于安全功能评估的加密结构。我们的做法保留了飞地的安全保障，同时避免了与飞地管理指示有关的业绩问题。"
    },
    {
        "title": "RIS-Assisted Physical Layer Authentication for 6G Endogenous Security",
        "url": "http://arxiv.org/abs/2309.07736v1",
        "pub_date": "2023-09-14",
        "summary": "The physical layer authentication (PLA) is a promising technology which can\nenhance the access security of a massive number of devices in the near future.\nIn this paper, we propose a reconfigurable intelligent surface (RIS)-assisted\nPLA system, in which the legitimate transmitter can customize the channel\nfingerprints during PLA by controlling the ON-OFF state of the RIS. Without\nloss of generality, we use the received signal strength (RSS) based spoofing\ndetection approach to analyze the feasibility of the proposed architecture.\nSpecifically, based on the RSS, we derive the statistical properties of PLA and\ngive some interesting insights, which showcase that the RIS-assisted PLA is\ntheoretically feasible. Then, we derive the optimal detection threshold to\nmaximize the performance in the context of the presented performance metrics.\nNext, the actual feasibility of the proposed system is verified via\nproof-of-concept experiments on a RIS-assisted PLA prototype platform. The\nexperiment results show that there are 3.5% and 76% performance improvements\nwhen the transmission sources are at different locations and at the same\nlocation, respectively.",
        "translated": "物理层认证(PLA)技术是一种很有前途的技术，可以在不久的将来提高大量设备的访问安全性。本文提出了一种可重构智能表面(RIS)辅助 PLA 系统，其中合法的发射机可以通过控制 RIS 的开关状态来定制 PLA 期间的信道指纹。不失一般性，我们使用基于接收信号强度(RSS)的欺骗检测方法来分析建议架构的可行性。具体地说，我们基于 RSS，推导了 PLA 的统计特性，并给出了一些有趣的见解，说明 RIS 辅助 PLA 在理论上是可行的。然后，在给出的性能指标上下文中，我们推导出最佳的检测阈值来使性能最大化。接下来，通过在 RIS 辅助 PLA 原型平台上的概念验证实验，验证了该系统的实际可行性。实验结果表明，当传输源位于不同的位置和相同的位置时，系统的性能分别提高了3.5% 和76% 。"
    },
    {
        "title": "AIDPS:Adaptive Intrusion Detection and Prevention System for Underwater\n  Acoustic Sensor Networks",
        "url": "http://arxiv.org/abs/2309.07730v1",
        "pub_date": "2023-09-14",
        "summary": "Underwater Acoustic Sensor Networks (UW-ASNs) are predominantly used for\nunderwater environments and find applications in many areas. However, a lack of\nsecurity considerations, the unstable and challenging nature of the underwater\nenvironment, and the resource-constrained nature of the sensor nodes used for\nUW-ASNs (which makes them incapable of adopting security primitives) make the\nUW-ASN prone to vulnerabilities. This paper proposes an Adaptive decentralised\nIntrusion Detection and Prevention System called AIDPS for UW-ASNs. The\nproposed AIDPS can improve the security of the UW-ASNs so that they can\nefficiently detect underwater-related attacks (e.g., blackhole, grayhole and\nflooding attacks). To determine the most effective configuration of the\nproposed construction, we conduct a number of experiments using several\nstate-of-the-art machine learning algorithms (e.g., Adaptive Random Forest\n(ARF), light gradient-boosting machine, and K-nearest neighbours) and concept\ndrift detection algorithms (e.g., ADWIN, kdqTree, and Page-Hinkley). Our\nexperimental results show that incremental ARF using ADWIN provides optimal\nperformance when implemented with One-class support vector machine (SVM)\nanomaly-based detectors. Furthermore, our extensive evaluation results also\nshow that the proposed scheme outperforms state-of-the-art bench-marking\nmethods while providing a wider range of desirable features such as scalability\nand complexity.",
        "translated": "水声传感器网络(UW-ASN)主要用于水下环境，在许多领域有着广泛的应用。然而，由于缺乏安全考虑，水下环境的不稳定性和挑战性，以及用于 UW-ASN 的传感器节点的资源受限性(这使得它们无法采用安全原语) ，使得 UW-ASN 容易受到攻击。本文提出了一种称为 AIDPS 的自适应分散式入侵预防系统，用于 UW-ASN。提出的 AIDPS 可以提高 UW-ASN 的安全性，使其能够有效地检测与水下相关的攻击(如黑洞、灰洞和洪水攻击)。为了确定建议结构的最有效配置，我们使用几种最先进的机器学习算法(例如，自适应随机森林(ARF) ，光梯度增强机器和 k 最近邻)和概念漂移检测算法(例如，ADWIN，kdqTree 和 Page-Hinkley)进行了大量的实验。我们的实验结果表明，使用 ADWIN 的增量 ARF 在使用基于一类支持向量机(SVM)的异常检测器时提供了最佳的性能。此外，我们广泛的评估结果还表明，该方案优于最先进的基准标记方法，同时提供了更广泛的可扩展性和复杂性等理想特性。"
    },
    {
        "title": "From Compliance to Impact: Tracing the Transformation of an\n  Organizational Security Awareness Program",
        "url": "http://arxiv.org/abs/2309.07724v1",
        "pub_date": "2023-09-14",
        "summary": "There is a growing recognition of the need for a transformation from\norganizational security awareness programs focused on compliance -- measured by\ntraining completion rates -- to those resulting in behavior change. However,\nfew prior studies have begun to unpack the organizational practices of the\nsecurity awareness teams tasked with executing program transformation. We\nconducted a year-long case study of a security awareness program in a United\nStates (U.S.) government agency, collecting data via field observations,\ninterviews, and documents. Our findings reveal the challenges and practices\ninvolved in the progression of a security awareness program from being\ncompliance-focused to emphasizing impact on workforce attitudes and behaviors.\nWe uniquely capture transformational organizational security awareness\npractices in action via a longitudinal study involving multiple workforce\nperspectives. Our study insights can serve as a resource for other security\nawareness programs and workforce development initiatives aimed at better\ndefining the security awareness work role.",
        "translated": "人们越来越认识到，需要从注重遵守的组织安全意识项目(通过培训完成率来衡量)转变为那些导致行为改变的项目。然而，很少有先前的研究开始揭示负责执行程序转换的安全意识团队的组织实践。我们对美国政府机构的一个安全意识项目进行了为期一年的案例研究，通过实地观察、访谈和文件收集数据。我们的研究结果揭示了安全意识项目从以遵从为中心到强调对员工态度和行为的影响的过程中所涉及的挑战和实践。我们通过一个涉及多种劳动力视角的追踪研究，独特地捕获了转型组织安全意识实践。我们的研究见解可以作为其他安全意识方案和劳动力发展计划的资源，旨在更好地定义安全意识工作的作用。"
    },
    {
        "title": "Sync+Sync: A Covert Channel Built on fsync with Storage",
        "url": "http://arxiv.org/abs/2309.07657v1",
        "pub_date": "2023-09-14",
        "summary": "Scientists have built a variety of covert channels for secretive information\ntransmission with CPU cache and main memory. In this paper, we turn to a lower\nlevel in the memory hierarchy, i.e., persistent storage. Most programs store\nintermediate or eventual results in the form of files and some of them call\nfsync to synchronously persist a file with storage device for orderly\npersistence. Our quantitative study shows that one program would undergo\nsignificantly longer response time for fsync call if the other program is\nconcurrently calling fsync, although they do not share any data. We further\nfind that, concurrent fsync calls contend at multiple levels of storage stack\ndue to sharing software structures (e.g., Ext4's journal) and hardware\nresources (e.g., disk's I/O dispatch queue).\n  We accordingly build a covert channel named Sync+Sync. Sync+Sync delivers a\ntransmission bandwidth of 20,000 bits per second at an error rate of about\n0.40% with an ordinary solid-state drive. Sync+Sync can be conducted in\ncross-disk partition, cross-file system, cross-container, cross-virtual\nmachine, and even cross-disk drive fashions, without sharing data between\nprograms. Next, we launch side-channel attacks with Sync+Sync and manage to\nprecisely detect operations of a victim database (e.g., insert/update and\nB-Tree node split). We also leverage Sync+Sync to distinguish applications and\nwebsites with high accuracy by detecting and analyzing their fsync frequencies\nand flushed data volumes. These attacks are useful to support further\nfine-grained information leakage.",
        "translated": "科学家们利用 CPU 高速缓存和主存建立了多种秘密信息传输的隐蔽通道。在本文中，我们将讨论一个较低的内存阶层，即持久存储。大多数程序以文件的形式存储中间结果或最终结果，其中一些程序调用 fsync 来同步持久化一个带有存储设备的文件，以实现有序持久化。我们的定量研究表明，如果另一个程序同时调用 fsync，那么一个程序对 fsync 的响应时间会显著延长，尽管它们不共享任何数据。我们进一步发现，由于共享软件结构(例如，Ext4的日志)和硬件资源(例如，磁盘的 I/O 调度队列) ，并发 fsync 调用在多级存储堆栈中竞争。因此，我们构建了一个名为 Sync + Sync 的隐蔽通道。“同步 + 同步”传输带宽为每秒20,000比特，对于普通固态硬盘，错误率约为0.40% 。Sync + Sync 可以以跨磁盘分区、跨文件系统、跨容器、跨虚拟机甚至跨磁盘驱动器的方式进行，而不需要在程序之间共享数据。接下来，我们使用 Sync + Sync 启动侧信道攻击，并设法精确检测受害者数据库的操作(例如，插入/更新和 B-Tree 节点拆分)。我们还利用 Sync + Sync 通过检测和分析应用程序和网站的 fsync 频率和刷新的数据量来高精度地区分它们。这些攻击有助于支持进一步的细粒度信息泄露。"
    },
    {
        "title": "Do Not Give Away My Secrets: Uncovering the Privacy Issue of Neural Code\n  Completion Tools",
        "url": "http://arxiv.org/abs/2309.07639v1",
        "pub_date": "2023-09-14",
        "summary": "Neural Code Completion Tools (NCCTs) have reshaped the field of software\ndevelopment, which accurately suggest contextually-relevant code snippets\nbenefiting from language modeling techniques. However, language models may emit\nthe training data verbatim during inference with appropriate prompts. This\nmemorization property raises privacy concerns of commercial NCCTs about the\nhard-coded credential leakage, leading to unauthorized access to systems.\nTherefore, to answer whether NCCTs will inadvertently emit the hard-coded\ncredential, we propose an evaluation tool called Hard-coded Credential Revealer\n(HCR). HCR effectively constructs test prompts from GitHub code files with\ncredentials to trigger memorization phenomenon of commercial NCCTs. Then, HCR\nextracts credentials with pre-defined format from the responses by four\ndesigned filters. We apply HCR to evaluate two representative commercial NCCTs:\nGitHub Copilot and Amazon CodeWhisperer and successfully extracted 2,702\nhard-coded credentials from Copilot and 129 secrets from CodeWhisper under the\nblack-box setting, among which at least 3.6% and 5.4% secrets are real strings\nfrom GitHub repositories. Moreover, two operational credentials were\nidentified. The experimental results raise the severe privacy concern of the\npotential leakage of hard-coded credentials in the training data of commercial\nNCCTs.",
        "translated": "神经代码完成工具(NCCTs)已经重塑了软件开发领域，它准确地建议了与上下文相关的代码片段，这些代码片段受益于语言建模技术。但是，语言模型可以在推理过程中使用适当的提示语逐字地发出训练数据。这种记忆特性引起了商业 NCCTs 对硬编码凭证泄漏的隐私担忧，从而导致未经授权访问系统。因此，为了回答 NCCTs 是否会不经意地发出硬编码的凭证，我们提出了一个名为 Hard-code Credential Reveaer 的评估工具。ROC 有效地从 GitHub 代码文件中构建带有凭证的测试提示，从而触发商业 NCCTs 的记忆现象。然后，ROC 通过四个设计的过滤器从响应中提取具有预定义格式的凭证。我们申请 ROC 来评估两个代表性的商业 NCCTs: GitHub Copilot 和 Amazon CodeWhisperer，并成功地从 Copilot 提取了2,702个硬编码凭证，在黑盒设置下从 CodeWhisper 提取了129个秘密，其中至少有3.6% 和5.4% 的秘密是来自 GitHub 知识库的真实字符串。此外，还确定了两个操作凭据。实验结果提出了严重的隐私问题的潜在泄漏的硬编码凭证的训练数据的商业中心。"
    },
    {
        "title": "Keep your Identity Small: Privacy-preserving Client-side Fingerprinting",
        "url": "http://arxiv.org/abs/2309.07563v1",
        "pub_date": "2023-09-14",
        "summary": "Device fingerprinting is a widely used technique that allows a third party to\nidentify a particular device. Applications of device fingerprinting include\nauthentication, attacker identification, or software license binding.\n  Device fingerprinting is also used on the web as a method for identifying\nusers. Unfortunately, one of its most widespread uses is to identify users\nvisiting different websites and thus build their browsing history. This\nconstitutes a specific type of web tracking that poses a threat to users'\nprivacy. While many anti-tracking solutions have been proposed, all of them\nblock or tamper with device fingerprinting techniques rather than just blocking\ntheir web tracking application. Therefore, users may be limited in their\nexperience while using a website.\n  In this paper, we propose \\textit{Privacy-preserving Client-side\nFingerprinting} (PCF), a new method that allows device fingerprinting on the\nweb, while blocks the possibility of performing web tracking. To this end, PCF\nis built upon fingerprinting transparency: any website ought to declare its\nfingerprinting scripts while users will compute them in a privacy-preserving\nmanner, limiting the resultant fingerprints for each different domain and,\ntherefore, making web tracking not feasible.",
        "translated": "设备指纹识别是一种广泛使用的技术，它允许第三方识别特定的设备。设备指纹识别的应用程序包括身份验证、攻击者识别或软件许可证绑定。设备指纹识别也被用在网络上作为识别用户的一种方法。不幸的是，它最广泛的用途之一是识别访问不同网站的用户，从而建立他们的浏览历史。这构成了一种特定类型的网络跟踪，对用户的隐私构成了威胁。虽然许多反跟踪解决方案已经提出，所有这些阻塞或篡改设备指纹技术，而不仅仅是阻止他们的网络跟踪应用程序。因此，当用户使用网站时，他们的体验可能会受到限制。在本文中，我们提出了文本{隐私保护客户端指纹}(PCF) ，一种新的方法，允许在网络上的设备指纹，同时阻止执行网络跟踪的可能性。为此，PCF 建立在指纹识别透明度的基础上: 任何网站都应声明其指纹识别脚本，而用户将以保护隐私的方式计算这些脚本，限制每个不同域的指纹识别结果，从而使网络跟踪不可行。"
    },
    {
        "title": "DP-PQD: Privately Detecting Per-Query Gaps In Synthetic Data Generated\n  By Black-Box Mechanisms",
        "url": "http://arxiv.org/abs/2309.08574v1",
        "pub_date": "2023-09-15",
        "summary": "Synthetic data generation methods, and in particular, private synthetic data\ngeneration methods, are gaining popularity as a means to make copies of\nsensitive databases that can be shared widely for research and data analysis.\nSome of the fundamental operations in data analysis include analyzing\naggregated statistics, e.g., count, sum, or median, on a subset of data\nsatisfying some conditions. When synthetic data is generated, users may be\ninterested in knowing if their aggregated queries generating such statistics\ncan be reliably answered on the synthetic data, for instance, to decide if the\nsynthetic data is suitable for specific tasks. However, the standard data\ngeneration systems do not provide \"per-query\" quality guarantees on the\nsynthetic data, and the users have no way of knowing how much the aggregated\nstatistics on the synthetic data can be trusted. To address this problem, we\npresent a novel framework named DP-PQD (differentially-private per-query\ndecider) to detect if the query answers on the private and synthetic datasets\nare within a user-specified threshold of each other while guaranteeing\ndifferential privacy. We give a suite of private algorithms for per-query\ndeciders for count, sum, and median queries, analyze their properties, and\nevaluate them experimentally.",
        "translated": "合成数据生成方法，特别是私有合成数据生成方法，作为一种复制敏感数据库的手段，正在日益普及，这些数据库可以广泛共享，用于研究和数据分析。数据分析中的一些基本操作包括对满足某些条件的数据子集进行聚合统计分析，例如计数、和或中位数。当生成合成数据时，用户可能有兴趣知道他们生成此类统计数据的聚合查询是否可以在合成数据上得到可靠的回答，例如，确定合成数据是否适合特定任务。但是，标准的数据生成系统不能保证合成数据的“每次查询”质量，用户无法知道合成数据的聚合统计信息可信度。为了解决这个问题，我们提出了一个新的框架 DP-PQD (差分私有每个查询决策者)来检测私有和合成数据集上的查询答案是否在一个用户指定的阈值内，同时保证差分隐私。我们为每个查询决策者提供了一套用于计数、求和和中值查询的私有算法，分析了它们的性质，并对它们进行了实验评估。"
    },
    {
        "title": "Local Differential Privacy in Graph Neural Networks: a Reconstruction\n  Approach",
        "url": "http://arxiv.org/abs/2309.08569v1",
        "pub_date": "2023-09-15",
        "summary": "Graph Neural Networks have achieved tremendous success in modeling complex\ngraph data in a variety of applications. However, there are limited studies\ninvestigating privacy protection in GNNs. In this work, we propose a learning\nframework that can provide node privacy at the user level, while incurring low\nutility loss. We focus on a decentralized notion of Differential Privacy,\nnamely Local Differential Privacy, and apply randomization mechanisms to\nperturb both feature and label data at the node level before the data is\ncollected by a central server for model training. Specifically, we investigate\nthe application of randomization mechanisms in high-dimensional feature\nsettings and propose an LDP protocol with strict privacy guarantees. Based on\nfrequency estimation in statistical analysis of randomized data, we develop\nreconstruction methods to approximate features and labels from perturbed data.\nWe also formulate this learning framework to utilize frequency estimates of\ngraph clusters to supervise the training procedure at a sub-graph level.\nExtensive experiments on real-world and semi-synthetic datasets demonstrate the\nvalidity of our proposed model.",
        "translated": "图形神经网络在复杂图形数据建模方面取得了巨大的成功。然而，目前对 GNN 中隐私保护的研究还很有限。在这项工作中，我们提出了一个学习框架，可以在用户层面上提供节点隐私，同时承担低效用损失。我们专注于一个分散的差分隐私概念，即本地差分隐私，并应用随机化机制，在数据被中央服务器收集用于模型训练之前，在节点级别扰乱特征和标签数据。具体来说，我们研究了随机化机制在高维特征设置中的应用，并提出了一种具有严格隐私保护的 LDP 协议。基于随机数据统计分析中的频率估计，我们提出了从扰动数据中逼近特征和标记的重构方法。我们还建立了这个学习框架，利用图簇的频率估计来监督子图级别的训练过程。在真实世界和半合成数据集上的大量实验证明了我们提出的模型的有效性。"
    },
    {
        "title": "HINT: Healthy Influential-Noise based Training to Defend against Data\n  Poisoning Attacks",
        "url": "http://arxiv.org/abs/2309.08549v1",
        "pub_date": "2023-09-15",
        "summary": "While numerous defense methods have been proposed to prohibit potential\npoisoning attacks from untrusted data sources, most research works only defend\nagainst specific attacks, which leaves many avenues for an adversary to\nexploit. In this work, we propose an efficient and robust training approach to\ndefend against data poisoning attacks based on influence functions, named\nHealthy Influential-Noise based Training. Using influence functions, we craft\nhealthy noise that helps to harden the classification model against poisoning\nattacks without significantly affecting the generalization ability on test\ndata. In addition, our method can perform effectively when only a subset of the\ntraining data is modified, instead of the current method of adding noise to all\nexamples that has been used in several previous works. We conduct comprehensive\nevaluations over two image datasets with state-of-the-art poisoning attacks\nunder different realistic attack scenarios. Our empirical results show that\nHINT can efficiently protect deep learning models against the effect of both\nuntargeted and targeted poisoning attacks.",
        "translated": "虽然已经提出了许多防御方法来禁止来自不可信数据源的潜在中毒攻击，但大多数研究工作只针对特定攻击进行防御，这给对手留下了许多可以利用的途径。在这项工作中，我们提出了一个有效和健壮的训练方法来抵御数据中毒攻击的影响函数为基础，称为健康的影响噪声为基础的训练。利用影响函数构造健康噪声，在不影响测试数据泛化能力的前提下，加强了中毒攻击分类模型。此外，我们的方法可以有效地执行时，只有一个子集的训练数据进行修改，而不是目前的方法，加入噪声的所有例子，已经在几个以前的工作中使用。我们对两个具有最先进中毒攻击的图像数据集在不同的真实攻击场景下进行了综合评估。我们的实验结果表明，HINT 能够有效地保护深度学习模型免受非靶向和靶向中毒攻击的影响。"
    },
    {
        "title": "XFedHunter: An Explainable Federated Learning Framework for Advanced\n  Persistent Threat Detection in SDN",
        "url": "http://arxiv.org/abs/2309.08485v1",
        "pub_date": "2023-09-15",
        "summary": "Advanced Persistent Threat (APT) attacks are highly sophisticated and employ\na multitude of advanced methods and techniques to target organizations and\nsteal sensitive and confidential information. APT attacks consist of multiple\nstages and have a defined strategy, utilizing new and innovative techniques and\ntechnologies developed by hackers to evade security software monitoring. To\neffectively protect against APTs, detecting and predicting APT indicators with\nan explanation from Machine Learning (ML) prediction is crucial to reveal the\ncharacteristics of attackers lurking in the network system. Meanwhile,\nFederated Learning (FL) has emerged as a promising approach for building\nintelligent applications without compromising privacy. This is particularly\nimportant in cybersecurity, where sensitive data and high-quality labeling play\na critical role in constructing effective machine learning models for detecting\ncyber threats. Therefore, this work proposes XFedHunter, an explainable\nfederated learning framework for APT detection in Software-Defined Networking\n(SDN) leveraging local cyber threat knowledge from many training collaborators.\nIn XFedHunter, Graph Neural Network (GNN) and Deep Learning model are utilized\nto reveal the malicious events effectively in the large number of normal ones\nin the network system. The experimental results on NF-ToN-IoT and DARPA TCE3\ndatasets indicate that our framework can enhance the trust and accountability\nof ML-based systems utilized for cybersecurity purposes without privacy\nleakage.",
        "translated": "进阶持续性渗透攻击攻击(APT)是一种高度复杂的攻击方式，它采用了大量先进的方法和技术来瞄准组织，窃取敏感和机密的信息。APT 攻击由多个阶段组成，具有明确的策略，利用黑客开发的新的和创新的技术和技术来规避安全软件监控。为了有效地防范 APT 攻击，检测和预测 APT 指标，并结合机器学习(ML)预测方法对其进行解释，对于揭示网络系统中潜伏的攻击者的特征至关重要。与此同时，联邦学习(FL)已经成为在不损害隐私的情况下构建智能应用程序的一种有前途的方法。这在网络安全方面尤其重要，因为在网络安全领域，敏感数据和高质量标签在建立有效的机器学习模型以侦测网络威胁方面发挥着关键作用。因此，这项工作提出了 XFedHunter，一个可解释的联邦学习框架，用于在软件定义网络中检测 APT (SDN) ，利用来自许多培训合作者的本地网络威胁知识。在 XFedHunter 中，利用图形神经网络(GNN)和深度学习(Deep Learning)模型有效地揭示了网络系统中大量正常事件中的恶意事件。在 NF-ToN-IoT 和 DARPA TCE3数据集上的实验结果表明，我们的框架可以增强用于网络安全目的的基于机器学习的系统的信任和可靠性，而不会泄露隐私。"
    },
    {
        "title": "VulnSense: Efficient Vulnerability Detection in Ethereum Smart Contracts\n  by Multimodal Learning with Graph Neural Network and Language Model",
        "url": "http://arxiv.org/abs/2309.08474v1",
        "pub_date": "2023-09-15",
        "summary": "This paper presents VulnSense framework, a comprehensive approach to\nefficiently detect vulnerabilities in Ethereum smart contracts using a\nmultimodal learning approach on graph-based and natural language processing\n(NLP) models. Our proposed framework combines three types of features from\nsmart contracts comprising source code, opcode sequences, and control flow\ngraph (CFG) extracted from bytecode. We employ Bidirectional Encoder\nRepresentations from Transformers (BERT), Bidirectional Long Short-Term Memory\n(BiLSTM) and Graph Neural Network (GNN) models to extract and analyze these\nfeatures. The final layer of our multimodal approach consists of a fully\nconnected layer used to predict vulnerabilities in Ethereum smart contracts.\nAddressing limitations of existing vulnerability detection methods relying on\nsingle-feature or single-model deep learning techniques, our method surpasses\naccuracy and effectiveness constraints. We assess VulnSense using a collection\nof 1.769 smart contracts derived from the combination of three datasets:\nCurated, SolidiFI-Benchmark, and Smartbugs Wild. We then make a comparison with\nvarious unimodal and multimodal learning techniques contributed by GNN, BiLSTM\nand BERT architectures. The experimental outcomes demonstrate the superior\nperformance of our proposed approach, achieving an average accuracy of 77.96\\%\nacross all three categories of vulnerable smart contracts.",
        "translated": "本文介绍了 VulnSense 框架，这是一个全面的方法，有效地检测 Ethereum 智能合同的漏洞，使用基于图形和自然语言处理(nLP)模型的多模式学习方法。我们提出的框架结合了智能契约的三种特性，包括源代码、操作码序列和从字节码中提取的控制流程图(CFG)。采用变压器双向编码表示(BERT)、双向长短期存储器(BiLSTM)和图形神经网络(GNN)模型对这些特征进行提取和分析。我们的多式联运方法的最后一层是一个完全连接的层，用于预测 Ethereum 智能合同的漏洞。针对现有的基于单特征或单模型深度学习技术的漏洞检测方法存在的局限性，该方法超越了准确性和有效性的限制。我们使用从三个数据集(Curated，SolidiFI-Benchmark 和 Smartbug Wild)中获得的1.769个智能契约的集合来评估 VulnSense。然后对 GNN、 BiLSTM 和 BERT 体系结构提供的各种单模式和多模式学习技术进行了比较。实验结果表明，我们提出的方法的优越性能，实现了在所有三类脆弱的智能合同的平均准确率为77.96% 。"
    },
    {
        "title": "Lattice attack on group ring NTRU: The case of the dihedral group",
        "url": "http://arxiv.org/abs/2309.08304v1",
        "pub_date": "2023-09-15",
        "summary": "Group ring NTRU (GR-NTRU) provides a general structure to design different\nvariants of NTRU-like schemes by employing different groups. Although, most of\nthe schemes in literature are built over cyclic groups, nonabelian groups can\nalso be used. Coppersmith and Shamir in 1997 have suggested that\nnoncommutativity may result in better security against some lattice attacks for\nsome groups. Lattice attacks on the public key of NTRU-like cryptosystems try\nto retrieve the private key by solving the shortest vector problem (SVP) or its\napproximation in a lattice of a certain dimension, assuming the knowledge of\nthe public key only. This paper shows that dihedral groups do not guarantee\nbetter security against this class of attacks. We prove that retrieving the\nprivate key is possible by solving the SVP in two lattices with half the\ndimension of the original lattice generated for GR-NTRU based on dihedral\ngroups. The possibility of such an attack was mentioned by Yasuda et\nal.(IACR/2015/1170). In contrast to their proposed approach, we explicitly\nprovide the lattice reduction without any structure theorem from the\nrepresentation theory for finite groups. Furthermore, we demonstrate the\neffectiveness of our technique with experimental results.",
        "translated": "群环 NTRU (GR-NTRU)提供了一种通用结构，可以通过使用不同的群来设计类 NTRU 格式的不同变体。虽然文献中的大多数格式都是建立在循环群上的，但是非交换群也可以使用。Coppersmith 和 Shamir 在1997年提出，对于某些群，非交换性可能导致对某些格攻击的更好的安全性。针对类 NTRU 密码体制公钥的格式攻击试图在只知道公钥的情况下，通过求解一定维度格中的最短向量问题(SVP)或其近似来获取私钥。本文证明了二面体群不能保证更好的安全性来抵抗这类攻击。通过求解两个格中的 SVP 问题，证明了基于二面体群的 GR-NTRU 的私钥检索是可行的。Yasuda 等人提到了这种攻击的可能性(IACR/2015/1170)。与他们提出的方法相反，我们明确地提供了格规约，而没有从有限群的表示论中得到任何结构定理。通过实验结果验证了该方法的有效性。"
    },
    {
        "title": "Verifiable Privacy-Preserving Computing",
        "url": "http://arxiv.org/abs/2309.08248v1",
        "pub_date": "2023-09-15",
        "summary": "Privacy-enhancing technologies (PETs), such as secure multi-party computation\n(MPC) and homomorphic encryption (HE), are deployed increasingly often to\nguarantee data confidentiality in computations over private, distributed data.\nSimilarly, we observe a steep increase in the adoption of zero-knowledge proofs\n(ZKPs) to guarantee (public) verifiability of locally executed computations. We\nproject that applications that are data intensive and require strong privacy\nguarantees, are also likely to require correctness guarantees. While the\ncombination of methods for (public) verifiability and privacy protection has\nclear significance, many attempts are far from practical adoption.\n  In this work, we analyze existing solutions that add (public) verifiability\nto privacy-preserving computations over distributed data, in order to preserve\nconfidentiality and guarantee correctness. To determine the required security\nand usability properties and whether these are satisfied, we look at various\napplication areas including verifiable outsourcing, distributed ledger\ntechnology (DLT), and genomics. We then classify the solutions and describe\nfrequently used approaches as well as efficiency metrics. Last but not least,\nwe identify open challenges and discuss directions for future research that\nmake verifiable, privacy-preserving computations more secure, efficient, and\napplicable in the real world.",
        "translated": "隐私增强技术(PET) ，如安全多方计算(MPC)和同态加密(HE) ，被越来越多地用于保证私有分布式数据计算中的数据机密性。类似地，我们观察到采用零知识证明(ZKP)来保证本地执行计算的(公共)可验证性的急剧增加。我们预计，数据密集型、需要强大隐私保障的应用程序也可能需要正确性保障。虽然(公共)可验证性方法和隐私保护方法的结合具有明显的意义，但许多尝试都远未得到实际采用。在这项工作中，我们分析了现有的解决方案，增加(公共)可验证的隐私保护计算的分布式数据，以保持机密性和保证正确性。为了确定所需的安全性和可用性属性以及这些属性是否得到满足，我们研究了各种应用程序领域，包括可验证外包、分布式分类帐技术(DLT)和基因组学。然后，我们对解决方案进行分类，并描述常用的方法以及效率指标。最后但并非最不重要的是，我们确定开放的挑战，并讨论未来的研究方向，使可验证的，保护隐私的计算更安全，高效，并适用于现实世界。"
    },
    {
        "title": "A Duty to Forget, a Right to be Assured? Exposing Vulnerabilities in\n  Machine Unlearning Services",
        "url": "http://arxiv.org/abs/2309.08230v1",
        "pub_date": "2023-09-15",
        "summary": "The right to be forgotten requires the removal or \"unlearning\" of a user's\ndata from machine learning models. However, in the context of Machine Learning\nas a Service (MLaaS), retraining a model from scratch to fulfill the unlearning\nrequest is impractical due to the lack of training data on the service\nprovider's side (the server). Furthermore, approximate unlearning further\nembraces a complex trade-off between utility (model performance) and privacy\n(unlearning performance). In this paper, we try to explore the potential\nthreats posed by unlearning services in MLaaS, specifically over-unlearning,\nwhere more information is unlearned than expected. We propose two strategies\nthat leverage over-unlearning to measure the impact on the trade-off balancing,\nunder black-box access settings, in which the existing machine unlearning\nattacks are not applicable. The effectiveness of these strategies is evaluated\nthrough extensive experiments on benchmark datasets, across various model\narchitectures and representative unlearning approaches. Results indicate\nsignificant potential for both strategies to undermine model efficacy in\nunlearning scenarios. This study uncovers an underexplored gap between\nunlearning and contemporary MLaaS, highlighting the need for careful\nconsiderations in balancing data unlearning, model utility, and security.",
        "translated": "被遗忘的权利要求从机器学习模型中删除或“取消学习”用户的数据。然而，在机器学习即服务(MLaaS)的背景下，由于缺乏服务提供者(服务器)方面的训练数据，重新训练一个模型来满足非学习请求是不切实际的。此外，近似忘却进一步包括效用(模型性能)和隐私(忘却性能)之间的复杂权衡。在本文中，我们试图探讨在 MLaaS 中忘却服务所带来的潜在威胁，特别是过度忘却，因为在这种情况下，未学习到的信息比预期的要多。我们提出了两种策略，利用过度忘却来衡量在黑盒访问设置下对权衡平衡的影响，在这种情况下，现有的机器忘却攻击是不适用的。这些策略的有效性是通过对基准数据集、各种模型架构和代表性的忘记学习方法进行广泛的实验来评估的。研究结果表明，这两种策略都有显著的潜力在非学习情境中破坏模型效能。这项研究揭示了忘却和当代 MLaaS 之间的一个未被充分探索的差距，强调了在平衡数据忘却、模型实用性和安全性方面需要谨慎考虑的必要性。"
    },
    {
        "title": "HM-Conformer: A Conformer-based audio deepfake detection system with\n  hierarchical pooling and multi-level classification token aggregation methods",
        "url": "http://arxiv.org/abs/2309.08208v1",
        "pub_date": "2023-09-15",
        "summary": "Audio deepfake detection (ADD) is the task of detecting spoofing attacks\ngenerated by text-to-speech or voice conversion systems. Spoofing evidence,\nwhich helps to distinguish between spoofed and bona-fide utterances, might\nexist either locally or globally in the input features. To capture these, the\nConformer, which consists of Transformers and CNN, possesses a suitable\nstructure. However, since the Conformer was designed for sequence-to-sequence\ntasks, its direct application to ADD tasks may be sub-optimal. To tackle this\nlimitation, we propose HM-Conformer by adopting two components: (1)\nHierarchical pooling method progressively reducing the sequence length to\neliminate duplicated information (2) Multi-level classification token\naggregation method utilizing classification tokens to gather information from\ndifferent blocks. Owing to these components, HM-Conformer can efficiently\ndetect spoofing evidence by processing various sequence lengths and aggregating\nthem. In experimental results on the ASVspoof 2021 Deepfake dataset,\nHM-Conformer achieved a 15.71% EER, showing competitive performance compared to\nrecent systems.",
        "translated": "音频深度伪造检测(ADD)是检测由文本到语音或语音转换系统产生的欺骗攻击的任务。欺骗证据，有助于区分欺骗和真实的话语，可能存在于局部或全局的输入特征。为了捕获这些，变形金刚，其中包括变形金刚和 CNN，拥有一个合适的结构。然而，由于 Conformer 是为序列到序列任务而设计的，因此它直接应用于 ADD 任务可能是次优的。针对这一局限性，提出了 HM-Conformer 算法，该算法采用了两个组件: (1)逐步减少序列长度以消除重复信息; (2)利用分类标记从不同块中收集信息的多级分类标记聚合方法。由于这些组件的存在，HM-Conformer 可以通过处理各种序列长度并聚合它们来有效地检测欺骗证据。在 ASVspoof 2021 Deepfalse 数据集上的实验结果显示，HM-Conformer 达到了15.71% 的 EER，与最近的系统相比显示出了竞争性能。"
    },
    {
        "title": "Scaling up prime factorization with self-organizing gates: A\n  memcomputing approach",
        "url": "http://arxiv.org/abs/2309.08198v1",
        "pub_date": "2023-09-15",
        "summary": "We report preliminary results on using the MEMCPU\\texttrademark{} Platform to\ncompute the prime factorization of large biprimes. The first approach, the\ndirect model, directly returns the factors of a given biprime. The second\napproach, the congruence model, returns smooth congruences to address the\nbottleneck of standard sieve methods. The models have size-dependent structure,\nand the MEMCPU Platform requires structure-dependent tuning for optimal\nperformance. Therefore, for both models, we tuned the platform on sample\nproblems up to a given size according to available resources. Then we generated\nRSA-like benchmark biprimes to perform rigorous scaling analysis. The MEMCPU\ntimings over the tuned range followed low degree polynomials in the number of\nbits, markedly different than other tested methods including general number\nfield sieve. MEMCPU's congruence model was the most promising, which was scaled\nup to 300-bit factorization problems while following a $2^{nd}$ degree\npolynomial fit. We also discuss the approach to tuning the MEMCPU Platform for\nproblems beyond the reach of today's most advanced methods. Finally, basic\nanalysis of the acceleration expected from an ASIC implementation is provided\nand suggests the possibility of real time factorization of large biprimes.",
        "translated": "我们报告了使用 MEMCPU 文本商标{}平台计算大双质数的整数分解的初步结果。第一种方法是直接模型，它直接返回给定双素数的因子。第二种方法是同余模型，它返回平滑同余来解决标准筛选方法的瓶颈问题。这些模型具有依赖于大小的结构，并且 MEMCPU 平台需要依赖于结构的调整以获得最佳性能。因此，对于这两个模型，我们根据可用资源对样例问题的平台进行了调优，使其达到给定的大小。然后我们生成类 RSA 的基准双素数来执行严格的标度分析。在调谐范围内的 MEMCPU 计时遵循低度多项式的比特数，明显不同于其他测试方法，包括普通数域筛选法。MEMCPU 的同余模型是最有前途的，它在遵循 $2 ^ { nd } $度多项式拟合的情况下可以扩展到300位因式分解问题。我们还讨论了针对当今最先进的方法无法解决的问题调优 MEMCPU 平台的方法。最后，对 ASIC 实现的加速性能进行了基本分析，提出了大型双素数实时分解的可能性。"
    },
    {
        "title": "Model-Based Generation of Attack-Fault Trees",
        "url": "http://arxiv.org/abs/2309.09941v1",
        "pub_date": "2023-09-18",
        "summary": "Joint safety and security analysis of cyber-physical systems is a necessary\nstep to correctly capture inter-dependencies between these properties.\nAttack-Fault Trees represent a combination of dynamic Fault Trees and Attack\nTrees and can be used to model and model-check a holistic view on both safety\nand security. Manually creating a complete AFT for the whole system is,\nhowever, a daunting task. It needs to span multiple abstraction layers, e.g.,\nabstract application architecture and data flow as well as system and library\ndependencies that are affected by various vulnerabilities. We present an AFT\ngeneration tool-chain that facilitates this task using partial Fault and Attack\nTrees that are either manually created or mined from vulnerability databases.\nWe semi-automatically create two system models that provide the necessary\ninformation to automatically combine these partial Fault and Attack Trees into\ncomplete AFTs using graph transformation rules.",
        "translated": "网络物理系统的联合安全性和安全性分析是正确捕获这些属性之间相互依赖关系的必要步骤。攻击-故障树表示动态故障树和攻击树的组合，可以用来对安全性和安全性的整体视图进行建模和模型检查。然而，为整个系统手动创建一个完整的 AFT 是一项艰巨的任务。它需要跨越多个抽象层，例如，抽象的应用程序架构和数据流，以及受各种漏洞影响的系统和库依赖。我们提出了一个 AFT 生成工具链，它使用部分故障和攻击树来实现这一任务，这些树或者是手动创建的，或者是从漏洞数据库中挖掘的。我们使用图形转换规则半自动地创建两个系统模型，提供必要的信息来自动地将这些部分故障树和攻击树组合成完整的 AFT。"
    },
    {
        "title": "Efficient Avoidance of Vulnerabilities in Auto-completed Smart Contract\n  Code Using Vulnerability-constrained Decoding",
        "url": "http://arxiv.org/abs/2309.09826v1",
        "pub_date": "2023-09-18",
        "summary": "Auto-completing code enables developers to speed up coding significantly.\nRecent advances in transformer-based large language model (LLM) technologies\nhave been applied to code synthesis. However, studies show that many of such\nsynthesized codes contain vulnerabilities. We propose a novel\nvulnerability-constrained decoding approach to reduce the amount of vulnerable\ncode generated by such models. Using a small dataset of labeled vulnerable\nlines of code, we fine-tune an LLM to include vulnerability labels when\ngenerating code, acting as an embedded classifier. Then, during decoding, we\ndeny the model to generate these labels to avoid generating vulnerable code. To\nevaluate the method, we chose to automatically complete Ethereum Blockchain\nsmart contracts (SCs) as the case study due to the strict requirements of SC\nsecurity. We first fine-tuned the 6-billion-parameter GPT-J model using 186,397\nEthereum SCs after removing the duplication from 2,217,692 SCs. The fine-tuning\ntook more than one week using ten GPUs. The results showed that our fine-tuned\nmodel could synthesize SCs with an average BLEU (BiLingual Evaluation\nUnderstudy) score of 0.557. However, many codes in the auto-completed SCs were\nvulnerable. Using the code before the vulnerable line of 176 SCs containing\ndifferent types of vulnerabilities to auto-complete the code, we found that\nmore than 70% of the auto-completed codes were insecure. Thus, we further\nfine-tuned the model on other 941 vulnerable SCs containing the same types of\nvulnerabilities and applied vulnerability-constrained decoding. The fine-tuning\ntook only one hour with four GPUs. We then auto-completed the 176 SCs again and\nfound that our approach could identify 62% of the code to be generated as\nvulnerable and avoid generating 67% of them, indicating the approach could\nefficiently and effectively avoid vulnerabilities in the auto-completed code.",
        "translated": "自动完成代码使开发人员能够显著加快编码速度。基于变换器的大语言模型(LLM)技术的最新进展已经被应用到代码合成中。然而，研究表明，许多这样的合成代码包含漏洞。我们提出了一种新的漏洞约束译码方法，以减少这类模型产生的漏洞代码量。我们使用一个标有易受攻击代码行的小数据集，作为嵌入式分类器，在生成代码时对 LLM 进行微调，使其包含易受攻击的标签。然后，在解码过程中，我们拒绝生成这些标签的模型，以避免生成易受攻击的代码。为了评价该方法，我们选择自动完成以太区块链智能合同(SC)作为案例研究，因为供应链安全的严格要求。我们首先在去除2,217,692个 SC 的重复后，使用186,397个以太粒子对60亿参数的 GPT-J 模型进行了微调。使用10个 GPU 进行微调花了一个多星期的时间。结果表明，我们的微调模型能够合成平均 BLEU (双语评估替代学习)分数为0.557的 SC。然而，自动完成的 SC 中的许多代码是易受攻击的。通过对176个包含不同漏洞类型的 SC 的漏洞行之前的代码进行自动补全，我们发现70% 以上的自动补全代码是不安全的。因此，我们进一步微调了模型的其他941个脆弱的 SC 包含相同类型的脆弱性和应用的脆弱性约束解码。对四个 GPU 的微调只花了一个小时。然后，我们再次自动完成了176个 SC，发现我们的方法可以确定62% 的代码生成为脆弱的，并避免生成其中的67% ，这表明该方法可以有效地避免自动完成的代码中的脆弱性。"
    },
    {
        "title": "Efficient Concept Drift Handling for Batch Android Malware Detection\n  Models",
        "url": "http://arxiv.org/abs/2309.09807v1",
        "pub_date": "2023-09-18",
        "summary": "The rapidly evolving nature of Android apps poses a significant challenge to\nstatic batch machine learning algorithms employed in malware detection systems,\nas they quickly become obsolete. Despite this challenge, the existing\nliterature pays limited attention to addressing this issue, with many advanced\nAndroid malware detection approaches, such as Drebin, DroidDet and MaMaDroid,\nrelying on static models. In this work, we show how retraining techniques are\nable to maintain detector capabilities over time. Particularly, we analyze the\neffect of two aspects in the efficiency and performance of the detectors: 1)\nthe frequency with which the models are retrained, and 2) the data used for\nretraining. In the first experiment, we compare periodic retraining with a more\nadvanced concept drift detection method that triggers retraining only when\nnecessary. In the second experiment, we analyze sampling methods to reduce the\namount of data used to retrain models. Specifically, we compare fixed sized\nwindows of recent data and state-of-the-art active learning methods that select\nthose apps that help keep the training dataset small but diverse. Our\nexperiments show that concept drift detection and sample selection mechanisms\nresult in very efficient retraining strategies which can be successfully used\nto maintain the performance of the static Android malware state-of-the-art\ndetectors in changing environments.",
        "translated": "Android 应用程序的快速发展对恶意软件检测系统中使用的静态批处理机学习算法提出了重大挑战，因为它们很快就会过时。尽管存在这个挑战，现有的文献对这个问题的关注有限，许多先进的 Android 恶意软件检测方法，如 Drebin，DroidDet 和 MaMaDroid，都依赖于静态模型。在这项工作中，我们展示了如何再培训技术能够随着时间的推移保持检测器的能力。特别地，我们分析了两个方面对检测器效率和性能的影响: 1)模型再训练的频率，2)用于再训练的数据。在第一个实验中，我们比较了周期性再训练和更先进的概念漂移检测方法，后者只在必要时触发再训练。在第二个实验中，我们分析了抽样方法以减少用于再训练模型的数据量。具体来说，我们比较最近数据的固定大小窗口和最先进的主动学习方法，这些方法选择那些帮助保持训练数据集小但多样化的应用程序。我们的实验表明，概念漂移检测和样本选择机制产生了非常有效的再训练策略，可以成功地用于维护静态 Android 恶意软件在不断变化的环境中的性能。"
    },
    {
        "title": "Modulation to the Rescue: Identifying Sub-Circuitry in the Transistor\n  Morass for Targeted Analysis",
        "url": "http://arxiv.org/abs/2309.09782v1",
        "pub_date": "2023-09-18",
        "summary": "Physical attacks form one of the most severe threats against secure computing\nplatforms. Their criticality arises from their corresponding threat model: By,\ne.g., passively measuring an integrated circuit's (IC's) environment during a\nsecurity-related operation, internal secrets may be disclosed. Furthermore, by\nactively disturbing the physical runtime environment of an IC, an adversary can\ncause a specific, exploitable misbehavior. The set of physical attacks consists\nof techniques that apply either globally or locally. When compared to global\ntechniques, local techniques exhibit a much higher precision, hence having the\npotential to be used in advanced attack scenarios. However, using physical\ntechniques with additional spatial dependency expands the parameter search\nspace exponentially. In this work, we present and compare two techniques,\nnamely laser logic state imaging (LLSI) and lock-in thermography (LIT), that\ncan be used to discover sub-circuitry of an entirely unknown IC based on\noptical and thermal principles. We show that the time required to identify\nspecific regions can be drastically reduced, thus lowering the complexity of\nphysical attacks requiring positional information. Our case study on an Intel\nH610 Platform Controller Hub showcases that, depending on the targeted voltage\nrail, our technique reduces the search space by around 90 to 98 percent.",
        "translated": "物理攻击是对安全计算平台的最严重威胁之一。它们的危险性来自于它们相应的威胁模型: 例如，在与安全相关的操作中被动地测量集成电路(IC)的环境，内部秘密可能被泄露。此外，通过主动干扰集成电路的物理执行期函式库，对手可以导致特定的、可利用的不当行为。这组物理攻击包括应用于全局或局部的技术。与全局技术相比，局部技术具有更高的精度，因此有可能用于高级攻击场景。然而，使用具有附加空间依赖性的物理技术将参数搜索空间指数级地扩展。在这项工作中，我们提出和比较两种技术，即激光逻辑状态成像(LLSI)和锁定热成像(LIT) ，可以用来发现子电路的完全未知的集成电路的光学和热原理。我们表明，识别特定区域所需的时间可以大大减少，从而降低了需要位置信息的物理攻击的复杂性。我们对英特尔 H610 PCH 的案例研究表明，根据目标电压轨道，我们的技术将搜索空间减少了大约90% 到98% 。"
    },
    {
        "title": "Securing Fixed Neural Network Steganography",
        "url": "http://arxiv.org/abs/2309.09700v1",
        "pub_date": "2023-09-18",
        "summary": "Image steganography is the art of concealing secret information in images in\na way that is imperceptible to unauthorized parties. Recent advances show that\nis possible to use a fixed neural network (FNN) for secret embedding and\nextraction. Such fixed neural network steganography (FNNS) achieves high\nsteganographic performance without training the networks, which could be more\nuseful in real-world applications. However, the existing FNNS schemes are\nvulnerable in the sense that anyone can extract the secret from the\nstego-image. To deal with this issue, we propose a key-based FNNS scheme to\nimprove the security of the FNNS, where we generate key-controlled\nperturbations from the FNN for data embedding. As such, only the receiver who\npossesses the key is able to correctly extract the secret from the stego-image\nusing the FNN. In order to improve the visual quality and undetectability of\nthe stego-image, we further propose an adaptive perturbation optimization\nstrategy by taking the perturbation cost into account. Experimental results\nshow that our proposed scheme is capable of preventing unauthorized secret\nextraction from the stego-images. Furthermore, our scheme is able to generate\nstego-images with higher visual quality than the state-of-the-art FNNS scheme,\nespecially when the FNN is a neural network for ordinary learning tasks.",
        "translated": "图像隐写术是将秘密信息隐藏在图像中，使未经授权的人无法察觉的一种技术。最近的研究表明，使用固定神经网络(FNN)进行秘密嵌入和提取是可行的。这种固定神经网络隐写(FNNS)在不需要训练网络的情况下实现了高性能的隐写，在实际应用中可能更有用。然而，现有的 FNNS 方案是脆弱的，因为任何人都可以从隐写图像中提取秘密。为了解决这一问题，我们提出了一种基于密钥的 FNNS 方案来提高 FNNS 的安全性，该方案从 FNN 中产生用于数据嵌入的密钥控制扰动。因此，只有拥有密钥的接收机才能使用模糊神经网络正确地从隐写图像中提取秘密。为了提高隐写图像的视觉质量和不可检测性，进一步提出了一种考虑摄动代价的自适应摄动优化策略。实验结果表明，该方案能够有效地防止图像中未经授权的秘密提取。此外，与现有的 FNNS 方法相比，该方法能够生成具有更高视觉质量的隐写图像，尤其是当 FNN 是用于普通学习任务的神经网络时。"
    },
    {
        "title": "Towards Model Co-evolution Across Self-Adaptation Steps for Combined\n  Safety and Security Analysis",
        "url": "http://arxiv.org/abs/2309.09653v1",
        "pub_date": "2023-09-18",
        "summary": "Self-adaptive systems offer several attack surfaces due to the communication\nvia different channels and the different sensors required to observe the\nenvironment. Often, attacks cause safety to be compromised as well, making it\nnecessary to consider these two aspects together. Furthermore, the approaches\ncurrently used for safety and security analysis do not sufficiently take into\naccount the intermediate steps of an adaptation. Current work in this area\nignores the fact that a self-adaptive system also reveals possible\nvulnerabilities (even if only temporarily) during the adaptation. To address\nthis issue, we propose a modeling approach that takes into account the\ndifferent relevant aspects of a system, its adaptation process, as well as\nsafety hazards and security attacks. We present several models that describe\ndifferent aspects of a self-adaptive system and we outline our idea of how\nthese models can then be combined into an Attack-Fault Tree. This allows\nmodeling aspects of the system on different levels of abstraction and co-evolve\nthe models using transformations according to the adaptation of the system.\nFinally, analyses can then be performed as usual on the resulting Attack-Fault\nTree.",
        "translated": "自适应系统由于通过不同的信道和观察环境所需的不同传感器进行通信，提供了多种攻击表面。通常，攻击也会导致安全性受到损害，因此有必要同时考虑这两个方面。此外，目前用于安全和安保分析的方法没有充分考虑到适应的中间步骤。目前在这方面的工作忽略了一个事实，即自适应系统也揭示了可能的脆弱性(即使只是暂时的)在适应期间。为了解决这个问题，我们提出了一种建模方法，该方法考虑到系统的不同相关方面、其适应过程以及安全风险和安全攻击。我们提出了几个模型，描述了一个自适应系统的不同方面，我们概述了我们的想法，这些模型然后可以组合成一个攻击-故障树。这允许在不同抽象层次上对系统的各个方面进行建模，并根据系统的适应性使用转换共同演化模型。最后，可以像通常一样对得到的攻击故障树进行分析。"
    },
    {
        "title": "VULNERLIZER: Cross-analysis Between Vulnerabilities and Software\n  Libraries",
        "url": "http://arxiv.org/abs/2309.09649v1",
        "pub_date": "2023-09-18",
        "summary": "The identification of vulnerabilities is a continuous challenge in software\nprojects. This is due to the evolution of methods that attackers employ as well\nas the constant updates to the software, which reveal additional issues. As a\nresult, new and innovative approaches for the identification of vulnerable\nsoftware are needed. In this paper, we present VULNERLIZER, which is a novel\nframework for cross-analysis between vulnerabilities and software libraries. It\nuses CVE and software library data together with clustering algorithms to\ngenerate links between vulnerabilities and libraries. In addition, the training\nof the model is conducted in order to reevaluate the generated associations.\nThis is achieved by updating the assigned weights. Finally, the approach is\nthen evaluated by making the predictions using the CVE data from the test set.\nThe results show that the VULNERLIZER has a great potential in being able to\npredict future vulnerable libraries based on an initial input CVE entry or a\nsoftware library. The trained model reaches a prediction accuracy of 75% or\nhigher.",
        "translated": "在软件项目中，漏洞的识别是一个持续的挑战。这是由于攻击者使用的方法的演变以及软件的不断更新，这揭示了额外的问题。因此，需要新的和创新的方法来识别易受攻击的软件。在本文中，我们提出了 VULNERLIZER，这是一个新的框架，用于交叉分析之间的漏洞和软件库。它使用 CVE 和软件库数据以及聚类算法来生成漏洞和库之间的链接。此外，模型的训练是为了重新评估生成的关联。这是通过更新分配的权重来实现的。最后，通过使用测试集中的 CVE 数据进行预测，对该方法进行评估。结果表明，VULNERLIZER 在基于初始输入 CVE 条目或软件库预测未来易受攻击的图书馆方面具有很大的潜力。训练后的模型预测精度达到75% 以上。"
    },
    {
        "title": "Simulation of Sensor Spoofing Attacks on Unmanned Aerial Vehicles Using\n  the Gazebo Simulator",
        "url": "http://arxiv.org/abs/2309.09648v1",
        "pub_date": "2023-09-18",
        "summary": "Conducting safety simulations in various simulators, such as the Gazebo\nsimulator, became a very popular means of testing vehicles against potential\nsafety risks (i.e. crashes). However, this was not the case with security\ntesting. Performing security testing in a simulator is very difficult because\nsecurity attacks are performed on a different abstraction level. In addition,\nthe attacks themselves are becoming more sophisticated, which directly\ncontributes to the difficulty of executing them in a simulator. In this paper,\nwe attempt to tackle the aforementioned gap by investigating possible attacks\nthat can be simulated, and then performing their simulations. The presented\napproach shows that attacks targeting the LiDAR and GPS components of unmanned\naerial vehicles can be simulated. This is achieved by exploiting\nvulnerabilities of the ROS and MAVLink protocol and injecting malicious\nprocesses into an application. As a result, messages with arbitrary values can\nbe spoofed to the corresponding topics, which allows attackers to update\nrelevant parameters and cause a potential crash of a vehicle. This was tested\nin multiple scenarios, thereby proving that it is indeed possible to simulate\ncertain attack types, such as spoofing and jamming.",
        "translated": "在各种模拟器中进行安全模拟，例如在 Gazebo 模拟器中进行安全模拟，已经成为一种非常流行的测试车辆潜在安全风险(即撞车)的方法。然而，安全性测试并非如此。在模拟器中执行安全测试非常困难，因为安全攻击是在不同的抽象层上执行的。此外，攻击本身正变得越来越复杂，这直接导致了在模拟器中执行它们的困难。在本文中，我们试图通过研究可以模拟的可能的攻击，然后执行他们的模拟来解决上述缺口。该方法可以模拟对无人机激光雷达和 GPS 组件的攻击。这是通过利用 ROS 和 MAVLink 协议的漏洞和向应用程序中注入恶意进程来实现的。因此，具有任意值的消息可以被欺骗到相应的主题，从而允许攻击者更新相关参数并导致潜在的车辆碰撞。这在多个场景中进行了测试，从而证明确实可以模拟某些攻击类型，如欺骗和干扰。"
    },
    {
        "title": "Applying Security Testing Techniques to Automotive Engineering",
        "url": "http://arxiv.org/abs/2309.09647v1",
        "pub_date": "2023-09-18",
        "summary": "The openness of modern IT systems and their permanent change make it\nchallenging to keep these systems secure. A combination of regression and\nsecurity testing called security regression testing, which ensures that changes\nmade to a system do not harm its security, are therefore of high significance\nand the interest in such approaches has steadily increased. In this article we\npresent a systematic classification of available security regression testing\napproaches based on a solid study of background and related work to sketch\nwhich parts of the research area seem to be well understood and evaluated, and\nwhich ones require further research. For this purpose we extract approaches\nrelevant to security regression testing from computer science digital libraries\nbased on a rigorous search and selection strategy. Then, we provide a\nclassification of these according to security regression approach criteria:\nabstraction level, security issue, regression testing techniques, and tool\nsupport, as well as evaluation criteria, for instance evaluated system,\nmaturity of the system, and evaluation measures. From the resulting\nclassification we derive observations with regard to the abstraction level,\nregression testing techniques, tool support as well as evaluation, and finally\nidentify several potential directions of future research.",
        "translated": "现代 IT 系统的开放性及其永久性的变化使得保证这些系统的安全性变得具有挑战性。因此，回归测试和称为「保安回归测试」的保安测试相结合，确保系统的更改不会损害系统的保安，具有重要意义，而且人们对这些方法的兴趣已稳步增加。在这篇文章中，我们对现有的安全回归测试方法进行了系统的分类，这些分类是基于对背景和相关工作的深入研究，从而勾勒出研究领域中哪些部分似乎已得到很好的理解和评估，以及哪些部分需要进一步的研究。为此，我们根据严谨的搜寻和选择策略，从电脑科学数码图书馆中提取与保安回归测试有关的方法。然后，我们根据安全回归方法标准对这些抽象层进行分类: 安全问题、回归测试技术和工具支持，以及评估标准，例如评估系统、系统的成熟度和评估措施。根据分类结果，我们得出了关于抽象层、回归测试技术、工具支持以及评估的观察结果，并最终确定了未来研究的几个潜在方向。"
    },
    {
        "title": "Spoofing attack augmentation: can differently-trained attack models\n  improve generalisation?",
        "url": "http://arxiv.org/abs/2309.09586v1",
        "pub_date": "2023-09-18",
        "summary": "A reliable deepfake detector or spoofing countermeasure (CM) should be robust\nin the face of unpredictable spoofing attacks. To encourage the learning of\nmore generaliseable artefacts, rather than those specific only to known\nattacks, CMs are usually exposed to a broad variety of different attacks during\ntraining. Even so, the performance of deep-learning-based CM solutions are\nknown to vary, sometimes substantially, when they are retrained with different\ninitialisations, hyper-parameters or training data partitions. We show in this\npaper that the potency of spoofing attacks, also deep-learning-based, can\nsimilarly vary according to training conditions, sometimes resulting in\nsubstantial degradations to detection performance. Nevertheless, while a\nRawNet2 CM model is vulnerable when only modest adjustments are made to the\nattack algorithm, those based upon graph attention networks and self-supervised\nlearning are reassuringly robust. The focus upon training data generated with\ndifferent attack algorithms might not be sufficient on its own to ensure\ngeneraliability; some form of spoofing attack augmentation at the algorithm\nlevel can be complementary.",
        "translated": "一个可靠的深度伪造检测器或欺骗对策(CM)在面对不可预测的欺骗攻击时应该是健壮的。为了鼓励学习更具普遍性的工件，而不是那些只针对已知攻击的工件，CM 通常在训练期间暴露于各种不同的攻击。即便如此，当使用不同的初始化、超参数或训练数据分区进行重新训练时，基于深度学习的 CM 解决方案的性能已知会有所不同，有时甚至会有很大的差异。我们在本文中展示了欺骗攻击的效力，也是基于深度学习，同样可以根据训练条件的不同而变化，有时会导致检测性能的大幅度下降。然而，当 RawNet2 CM 模型只对攻击算法进行适当的调整时，该模型是脆弱的，而基于图注意网络和自监督学习的模型具有可靠的鲁棒性。重点关注使用不同攻击算法生成的训练数据本身可能不足以确保通用性; 在算法级别的某种形式的欺骗攻击增强可以相互补充。"
    },
    {
        "title": "DeFi composability as MEV non-interference",
        "url": "http://arxiv.org/abs/2309.10781v1",
        "pub_date": "2023-09-19",
        "summary": "Complex DeFi services are usually constructed by composing a variety of\nsimpler smart contracts. The permissionless nature of the blockchains where\nthese smart contracts are executed makes DeFi services exposed to security\nrisks, since adversaries can target any of the underlying contracts to\neconomically damage the compound service. We introduce a new notion of secure\ncomposability of smart contracts, which ensures that adversaries cannot\neconomically harm the compound contract by interfering with its dependencies.",
        "translated": "复杂的 DeFi 服务通常通过组合各种更简单的智能合同来构建。执行这些智能合同的区块链的无许可性质使得 DeFi 服务面临安全风险，因为对手可以针对任何基础合同从经济上破坏复合服务。我们引入了智能契约安全可组合的新概念，保证了对手不会通过干扰复合契约的依赖关系而对复合契约造成经济损害。"
    },
    {
        "title": "SPFL: A Self-purified Federated Learning Method Against Poisoning\n  Attacks",
        "url": "http://arxiv.org/abs/2309.10607v1",
        "pub_date": "2023-09-19",
        "summary": "While Federated learning (FL) is attractive for pulling privacy-preserving\ndistributed training data, the credibility of participating clients and\nnon-inspectable data pose new security threats, of which poisoning attacks are\nparticularly rampant and hard to defend without compromising privacy,\nperformance or other desirable properties of FL. To tackle this problem, we\npropose a self-purified FL (SPFL) method that enables benign clients to exploit\ntrusted historical features of locally purified model to supervise the training\nof aggregated model in each iteration. The purification is performed by an\nattention-guided self-knowledge distillation where the teacher and student\nmodels are optimized locally for task loss, distillation loss and\nattention-based loss simultaneously. SPFL imposes no restriction on the\ncommunication protocol and aggregator at the server. It can work in tandem with\nany existing secure aggregation algorithms and protocols for augmented security\nand privacy guarantee. We experimentally demonstrate that SPFL outperforms\nstate-of-the-art FL defenses against various poisoning attacks. The attack\nsuccess rate of SPFL trained model is at most 3$\\%$ above that of a clean\nmodel, even if the poisoning attack is launched in every iteration with all but\none malicious clients in the system. Meantime, it improves the model quality on\nnormal inputs compared to FedAvg, either under attack or in the absence of an\nattack.",
        "translated": "虽然联邦学习(FL)在提取保护隐私的分布式训练数据方面具有吸引力，但是参与客户端的可信度和不可检查的数据构成了新的安全威胁，其中中毒攻击尤其猖獗，难以在不损害隐私、性能或 FL 其他理想属性的情况下进行防御。为了解决这个问题，我们提出了一种自净化 FL (SPFL)方法，使良性客户能够利用局部净化模型的可信历史特征来监督每次迭代中聚合模型的训练。该方法采用注意引导的自我知识提取方法，对教师和学生模型进行局部优化，同时考虑任务损失、提取损失和注意损失。SPFL 对服务器上的通信协议和聚合器没有限制。它可以与任何现有的安全聚合算法和协议协同工作，以增强安全性和保护隐私。我们通过实验证明 SPFL 在抵御各种中毒攻击方面优于最先进的 FL 防御系统。SPFL 训练模型的攻击成功率最多比一个干净模型的攻击成功率高3% ，即使在系统中除了一个恶意客户端之外的每个迭代中都发起了中毒攻击。同时，与 FedAvg 相比，无论是在受到攻击还是没有受到攻击的情况下，它都提高了正常输入下的模型质量。"
    },
    {
        "title": "Adversarial Attacks Against Uncertainty Quantification",
        "url": "http://arxiv.org/abs/2309.10586v1",
        "pub_date": "2023-09-19",
        "summary": "Machine-learning models can be fooled by adversarial examples, i.e.,\ncarefully-crafted input perturbations that force models to output wrong\npredictions. While uncertainty quantification has been recently proposed to\ndetect adversarial inputs, under the assumption that such attacks exhibit a\nhigher prediction uncertainty than pristine data, it has been shown that\nadaptive attacks specifically aimed at reducing also the uncertainty estimate\ncan easily bypass this defense mechanism. In this work, we focus on a different\nadversarial scenario in which the attacker is still interested in manipulating\nthe uncertainty estimate, but regardless of the correctness of the prediction;\nin particular, the goal is to undermine the use of machine-learning models when\ntheir outputs are consumed by a downstream module or by a human operator.\nFollowing such direction, we: \\textit{(i)} design a threat model for attacks\ntargeting uncertainty quantification; \\textit{(ii)} devise different attack\nstrategies on conceptually different UQ techniques spanning for both\nclassification and semantic segmentation problems; \\textit{(iii)} conduct a\nfirst complete and extensive analysis to compare the differences between some\nof the most employed UQ approaches under attack. Our extensive experimental\nanalysis shows that our attacks are more effective in manipulating uncertainty\nquantification measures than attacks aimed to also induce misclassifications.",
        "translated": "机器学习模型可能被敌对的例子愚弄，例如，精心设计的输入扰动迫使模型输出错误的预测。虽然最近提出了不确定性量化来检测对手的输入，但假设这种攻击表现出比原始数据更高的预测不确定性，已经显示专门旨在减少不确定性估计的自适应攻击可以轻松绕过这种防御机制。在这项工作中，我们集中在一个不同的对抗场景中，攻击者仍然对操纵不确定性估计感兴趣，但不管预测的正确性; 特别是，目标是破坏机器学习模型的使用，当他们的输出被下游模块或人类操作员消耗。遵循这样的方向，我们: texttit {(i)}设计了一个针对不确定性量化的攻击威胁模型; texttit {(ii)}针对分类和语义分割问题，针对概念上不同的 UQ 技术设计了不同的攻击策略; texttit {(iii)}进行了第一次完整而广泛的分析，以比较一些最常用的受攻击的 UQ 方法之间的差异。我们广泛的实验分析表明，我们的攻击是更有效地操纵不确定性量化措施比攻击的目的也导致错误分类。"
    },
    {
        "title": "A Neighbourhood-Aware Differential Privacy Mechanism for Static Word\n  Embeddings",
        "url": "http://arxiv.org/abs/2309.10551v1",
        "pub_date": "2023-09-19",
        "summary": "We propose a Neighbourhood-Aware Differential Privacy (NADP) mechanism\nconsidering the neighbourhood of a word in a pretrained static word embedding\nspace to determine the minimal amount of noise required to guarantee a\nspecified privacy level. We first construct a nearest neighbour graph over the\nwords using their embeddings, and factorise it into a set of connected\ncomponents (i.e. neighbourhoods). We then separately apply different levels of\nGaussian noise to the words in each neighbourhood, determined by the set of\nwords in that neighbourhood. Experiments show that our proposed NADP mechanism\nconsistently outperforms multiple previously proposed DP mechanisms such as\nLaplacian, Gaussian, and Mahalanobis in multiple downstream tasks, while\nguaranteeing higher levels of privacy.",
        "translated": "我们提出一个邻域感知差分隐私(NADP)机制，在预先训练好的静态词嵌入空间中考虑词的邻域，以确定保证指定隐私级别所需的最小噪声量。我们首先利用词语的嵌入构造一个最近邻图，并将其分解为一组连通的组件(即邻域)。然后，我们分别应用不同水平的高斯噪声的字在每个邻里，由该邻里的字集决定。实验表明，我们提出的 NADP 机制在多个下游任务中始终优于多个先前提出的 DP 机制，如 Laplacian，Gaussian 和 Mahalanobis，同时保证了更高的隐私水平。"
    },
    {
        "title": "Model Leeching: An Extraction Attack Targeting LLMs",
        "url": "http://arxiv.org/abs/2309.10544v1",
        "pub_date": "2023-09-19",
        "summary": "Model Leeching is a novel extraction attack targeting Large Language Models\n(LLMs), capable of distilling task-specific knowledge from a target LLM into a\nreduced parameter model. We demonstrate the effectiveness of our attack by\nextracting task capability from ChatGPT-3.5-Turbo, achieving 73% Exact Match\n(EM) similarity, and SQuAD EM and F1 accuracy scores of 75% and 87%,\nrespectively for only $50 in API cost. We further demonstrate the feasibility\nof adversarial attack transferability from an extracted model extracted via\nModel Leeching to perform ML attack staging against a target LLM, resulting in\nan 11% increase to attack success rate when applied to ChatGPT-3.5-Turbo.",
        "translated": "模型水蛭是一种针对大语言模型(LLM)的新型提取攻击，它能够将目标 LLM 中的任务特定知识提取到一个简化的参数模型中。我们通过从 ChatGPT-3.5-Turbo 中提取任务能力来证明我们的攻击的有效性，实现73% 的精确匹配(EM)相似性，以及分别为75% 和87% 的 SQuAD EM 和 F1准确度分数，API 成本仅为50美元。我们进一步证明了来自通过 Model Leeching 提取的提取模型的敌对攻击可转移性的可行性，以针对目标 LLM 进行 ML 攻击分期，导致当应用于 ChatGPT-3.5-Turbo 时攻击成功率增加11% 。"
    },
    {
        "title": "Love or Hate? Share or Split? Privacy-Preserving Training Using Split\n  Learning and Homomorphic Encryption",
        "url": "http://arxiv.org/abs/2309.10517v1",
        "pub_date": "2023-09-19",
        "summary": "Split learning (SL) is a new collaborative learning technique that allows\nparticipants, e.g. a client and a server, to train machine learning models\nwithout the client sharing raw data. In this setting, the client initially\napplies its part of the machine learning model on the raw data to generate\nactivation maps and then sends them to the server to continue the training\nprocess. Previous works in the field demonstrated that reconstructing\nactivation maps could result in privacy leakage of client data. In addition to\nthat, existing mitigation techniques that overcome the privacy leakage of SL\nprove to be significantly worse in terms of accuracy. In this paper, we improve\nupon previous works by constructing a protocol based on U-shaped SL that can\noperate on homomorphically encrypted data. More precisely, in our approach, the\nclient applies homomorphic encryption on the activation maps before sending\nthem to the server, thus protecting user privacy. This is an important\nimprovement that reduces privacy leakage in comparison to other SL-based works.\nFinally, our results show that, with the optimum set of parameters, training\nwith HE data in the U-shaped SL setting only reduces accuracy by 2.65% compared\nto training on plaintext. In addition, raw training data privacy is preserved.",
        "translated": "分割学习(Split learning，SL)是一种新的合作学习学习技术，它允许参与者，例如客户端和服务器，在没有客户端共享原始数据的情况下，训练机器学习模型。在这个设置中，客户机最初将其机器学习模型的一部分应用于原始数据以生成激活映射，然后将它们发送到服务器以继续培训过程。该领域以前的工作表明，重建激活映射可能导致客户数据的隐私泄露。除此之外，克服 SL 隐私泄漏的现有缓解技术在准确性方面被证明是非常糟糕的。本文在前人工作的基础上，提出了一种基于 U 形 SL 的同态加密数据操作协议。更确切地说，在我们的方法中，客户端在将激活映射发送到服务器之前应用同态加密，从而保护用户隐私。与其他基于 SL 的工作相比，这是一个重要的改进，可以减少隐私泄漏。最后，我们的结果表明，在最佳的参数设置下，在 U 形 SL 环境下用 HE 数据进行训练，与纯文本训练相比，训练的准确率仅降低了2.65% 。此外，原始训练数据的隐私性也得到了保护。"
    },
    {
        "title": "Steganography for Neural Radiance Fields by Backdooring",
        "url": "http://arxiv.org/abs/2309.10503v1",
        "pub_date": "2023-09-19",
        "summary": "The utilization of implicit representation for visual data (such as images,\nvideos, and 3D models) has recently gained significant attention in computer\nvision research. In this letter, we propose a novel model steganography scheme\nwith implicit neural representation. The message sender leverages Neural\nRadiance Fields (NeRF) and its viewpoint synthesis capabilities by introducing\na viewpoint as a key. The NeRF model generates a secret viewpoint image, which\nserves as a backdoor. Subsequently, we train a message extractor using\noverfitting to establish a one-to-one mapping between the secret message and\nthe secret viewpoint image. The sender delivers the trained NeRF model and the\nmessage extractor to the receiver over the open channel, and the receiver\nutilizes the key shared by both parties to obtain the rendered image in the\nsecret view from the NeRF model, and then obtains the secret message through\nthe message extractor. The inherent complexity of the viewpoint information\nprevents attackers from stealing the secret message accurately. Experimental\nresults demonstrate that the message extractor trained in this letter achieves\nhigh-capacity steganography with fast performance, achieving a 100\\% accuracy\nin message extraction. Furthermore, the extensive viewpoint key space of NeRF\nensures the security of the steganography scheme.",
        "translated": "对视觉数据(如图像、视频和三维模型)进行隐式表示是近年来计算机视觉研究的热点。在这封信中，我们提出了一种新的模型隐写方案与隐式神经元表示。消息发送者利用神经辐射场(NeRF)及其视点综合功能，通过引入视点作为关键字。NERF 模型生成一个秘密视点图像，作为后门。然后，我们训练一个使用过拟合的信息提取器来建立秘密信息和秘密视点图像之间的一对一映射。发送方通过开放信道向接收方提供经过训练的 NERF 模型和消息提取器，接收方利用双方共享的密钥从 NERF 模型获得秘密视图中的渲染图像，然后通过消息提取器获得秘密消息。视点信息固有的复杂性使攻击者无法准确地窃取秘密消息。实验结果表明，该信函训练的消息提取器实现了高性能的大容量隐写，提取准确率达到100% 。此外，NERF 的扩展视点关键空间保证了隐写方案的安全性。"
    },
    {
        "title": "Exploring the Dark Side of AI: Advanced Phishing Attack Design and\n  Deployment Using ChatGPT",
        "url": "http://arxiv.org/abs/2309.10463v1",
        "pub_date": "2023-09-19",
        "summary": "This paper explores the possibility of using ChatGPT to develop advanced\nphishing attacks and automate their large-scale deployment. We make ChatGPT\ngenerate the following parts of a phishing attack: i) cloning a targeted\nwebsite, ii) integrating code for stealing credentials, iii) obfuscating code,\niv) automating website deployment on a hosting provider, v) registering a\nphishing domain name, and vi) integrating the website with a reverse proxy. The\ninitial assessment of the automatically generated phishing kits highlights\ntheir rapid generation and deployment process as well as the close resemblance\nof the resulting pages to the target website. More broadly, we demonstrate that\nrecent advances in AI underscore the potential risks of its misuse in phishing\nattacks, which can lead to their increased prevalence and severity. This\nhighlights the necessity for enhanced countermeasures within AI systems.",
        "translated": "本文探讨了使用 ChatGPT 开发高级网络钓鱼攻击并自动化其大规模部署的可能性。我们让 ChatGPT 生成钓鱼攻击的以下部分: i)克隆一个目标网站，ii)整合用于窃取凭证的代码，iii)混淆代码，iv)在托管提供商上自动部署网站，v)注册一个钓鱼域名，vi)将网站与反向代理整合。对自动生成的网络钓鱼工具包的初步评估突出表明，这些工具包的生成和部署过程非常迅速，所生成的网页与目标网站非常相似。更广泛地说，我们表明，最近在 AI 方面的进展强调了在网络钓鱼攻击中滥用 AI 的潜在风险，这可能导致其流行程度和严重性的增加。这突出了人工智能系统中增强对策的必要性。"
    },
    {
        "title": "The supersingular Endomorphism Ring and One Endomorphism problems are\n  equivalent",
        "url": "http://arxiv.org/abs/2309.10432v1",
        "pub_date": "2023-09-19",
        "summary": "The supersingular Endomorphism Ring problem is the following: given a\nsupersingular elliptic curve, compute all of its endomorphisms. The presumed\nhardness of this problem is foundational for isogeny-based cryptography. The\nOne Endomorphism problem only asks to find a single non-scalar endomorphism. We\nprove that these two problems are equivalent, under probabilistic polynomial\ntime reductions. We prove a number of consequences. First, assuming the\nhardness of the endomorphism ring problem, the Charles--Goren--Lauter hash\nfunction is collision resistant, and the SQIsign identification protocol is\nsound. Second, the endomorphism ring problem is equivalent to the problem of\ncomputing arbitrary isogenies between supersingular elliptic curves, a result\npreviously known only for isogenies of smooth degree. Third, there exists an\nunconditional probabilistic algorithm to solve the endomorphism ring problem in\ntime O~(sqrt(p)), a result that previously required to assume the generalized\nRiemann hypothesis. To prove our main result, we introduce a flexible framework\nfor the study of isogeny graphs with additional information. We prove a general\nand easy-to-use rapid mixing theorem.",
        "translated": "超奇异自同态环问题是这样的: 给定一条超奇异椭圆曲线，计算它的所有自同态。这个问题的假定难度是基于等基因的密码学的基础。一个自同态问题只要求找到一个单一的非标量自同态。证明了这两个问题在概率多项式时间缩减下是等价的。我们证明了一些后果。首先，假设自同态环问题的难度很大，Charles-Goren-Lauter 散列函数是抗冲突的，而 SQIsign 识别协议是可靠的。其次，自同态环问题等同于计算超奇异椭圆曲线之间的任意等同性的问题，这个结果以前只知道光滑度的等同性。第三，存在一个无条件的概率算法来解决时间为 O ~ (sqrt (p))的自同态环问题，这个结果以前需要假设广义黎曼猜想。为了证明我们的主要结果，我们引入了一个灵活的框架来研究带有附加信息的等同图。我们证明了一个通用的、易于使用的快速混合定理。"
    },
    {
        "title": "Poster: Control-Flow Integrity in Low-end Embedded Devices",
        "url": "http://arxiv.org/abs/2309.10396v2",
        "pub_date": "2023-09-19",
        "summary": "Embedded, smart, and IoT devices are increasingly popular in numerous\neveryday settings. Since lower-end devices have the most strict cost\nconstraints, they tend to have few, if any, security features. This makes them\nattractive targets for exploits and malware. Prior research proposed various\nsecurity architectures for enforcing security properties for\nresource-constrained devices, e.g., via Remote Attestation (RA). Such\ntechniques can (statically) verify software integrity of a remote device and\ndetect compromise. However, run-time (dynamic) security, e.g., via Control-Flow\nIntegrity (CFI), is hard to achieve. This work constructs an architecture that\nensures integrity of software execution against run-time attacks, such as\nReturn-Oriented Programming (ROP). It is built atop a recently proposed CASU --\na low-cost active Root-of-Trust (RoT) that guarantees software immutability. We\nextend CASU to support a shadow stack and a CFI monitor to mitigate run-time\nattacks. This gives some confidence that CFI can indeed be attained even on\nlow-end devices, with minimal hardware overhead.",
        "translated": "嵌入式、智能和物联网设备在许多日常设置中越来越受欢迎。由于低端设备有最严格的成本限制，它们往往没有什么安全特性。这使得他们成为攻击和恶意软件的有吸引力的目标。先前的研究提出了各种安全体系结构，用于强制资源受限设备的安全属性，例如，通过远程认证(Remote Attation，RA)。这种技术可以(静态地)验证远程设备的软件完整性并检测危害。然而，运行时(动态)安全，例如通过控制流完整性(Control-Flow Integrity，CFI) ，是很难实现的。这项工作构建了一个架构，以确保针对运行时攻击(如 Return-to-libc 攻击(ROP))的软件执行的完整性。它构建在最近提出的 CASU 之上——一个低成本的活动根信任(Root-of-Trust，RoT) ，它保证了软件的不可变性。我们扩展 CASU 以支持影子堆栈和 CFI 监视器以减轻运行时攻击。这给予了一些信心，即使在低端设备上，也可以以最小的硬件开销实现 CFI。"
    },
    {
        "title": "CellSecure: Securing Image Data in Industrial Internet-of-Things via\n  Cellular Automata and Chaos-Based Encryption",
        "url": "http://arxiv.org/abs/2309.11476v1",
        "pub_date": "2023-09-20",
        "summary": "In the era of Industrial IoT (IIoT) and Industry 4.0, ensuring secure data\ntransmission has become a critical concern. Among other data types, images are\nwidely transmitted and utilized across various IIoT applications, ranging from\nsensor-generated visual data and real-time remote monitoring to quality control\nin production lines. The encryption of these images is essential for\nmaintaining operational integrity, data confidentiality, and seamless\nintegration with analytics platforms. This paper addresses these critical\nconcerns by proposing a robust image encryption algorithm tailored for IIoT and\nCyber-Physical Systems (CPS). The algorithm combines Rule-30 cellular automata\nwith chaotic scrambling and substitution. The Rule 30 cellular automata serves\nas an efficient mechanism for generating pseudo-random sequences that enable\nfast encryption and decryption cycles suitable for real-time sensor data in\nindustrial settings. Most importantly, it induces non-linearity in the\nencryption algorithm. Furthermore, to increase the chaotic range and keyspace\nof the algorithm, which is vital for security in distributed industrial\nnetworks, a hybrid chaotic map, i.e., logistic-sine map is utilized. Extensive\nsecurity analysis has been carried out to validate the efficacy of the proposed\nalgorithm. Results indicate that our algorithm achieves close-to-ideal values,\nwith an entropy of 7.99 and a correlation of 0.002. This enhances the\nalgorithm's resilience against potential cyber-attacks in the industrial\ndomain.",
        "translated": "在工业物联网(IIoT)和工业4.0时代，确保安全的数据传输已成为一个关键问题。在其他数据类型中，图像被广泛地传输和利用到各种 IIoT 应用程序中，从传感器生成的可视数据和实时远程监控到生产线的质量控制。这些图像的加密对于维护操作完整性、数据保密性和与分析平台的无缝集成是必不可少的。针对这些关键问题，本文提出了一种针对 IIoT 和网络物理系统(CPS)的鲁棒图像加密算法。该算法将 Rule-30元胞自动机与混沌置乱和置换相结合。该规则30细胞自动机作为一个有效的机制，生成伪随机序列，使快速加密和解密周期适合实时传感器数据在工业设置。最重要的是，它在加密算法中引入了非线性。此外，为了提高算法的混沌范围和密钥空间，利用混合混沌映射即逻辑正弦映射，提高了算法在分布式工业网络中的安全性。通过广泛的安全性分析，验证了该算法的有效性。结果表明，该算法的熵值为7.99，相关系数为0.002，达到了接近理想的值。这增强了算法对工业领域潜在网络攻击的弹性。"
    },
    {
        "title": "Noise-Crypt: Image Encryption with Non-linear Noise, Hybrid Chaotic\n  Maps, and Hashing",
        "url": "http://arxiv.org/abs/2309.11471v1",
        "pub_date": "2023-09-20",
        "summary": "To secure the digital images over insecure transmission channels, a new image\nencryption algorithm Noise-Crypt is proposed in this paper. Noise-Crypt\nintegrates non-linear random noise, hybrid chaotic maps, and SHA-256 hashing\nalgorithm. The utilized hybrid chaotic maps are the logistic-tent and the\nlogistic-sine-cosine map. The hybrid chaotic maps enhance the pseudorandom\nsequence generation and selection of substitution boxes, while the\nlogistic-sine-cosine map induces non-linearity in the algorithm through random\nnoise. This deliberate inclusion of noise contributes to increased resistance\nagainst cryptanalysis. The proposed scheme has been evaluated for several\nsecurity parameters, such as differential attacks, entropy, correlation, etc.\nExtensive evaluation demonstrates the efficacy of the proposed scheme, with\nalmost ideal values of entropy of 7.99 and correlation of -0.0040. Results of\nthe security analysis validate the potency of the proposed scheme in achieving\nrobust image encryption.",
        "translated": "为了在不安全的传输信道上保护数字图像，提出了一种新的图像加密算法: 噪声加密算法。噪声密码集成了非线性随机噪声，混合混沌映射和 SHA-256散列算法。所利用的混合混沌映射是逻辑帐和逻辑正弦余弦映射。混合混沌映射增强了伪随机序列的产生和替换盒的选择，而逻辑正弦-余弦映射通过随机噪声引入算法的非线性。这种故意包含噪音有助于增加对密码分析的抵抗力。针对差分攻击、熵、相关性等安全参数对该方案进行了评估。广泛的评估表明了该方案的有效性，几乎理想的熵值为7.99，相关系数为 -0.0040。安全性分析结果验证了该方案在实现鲁棒图像加密方面的有效性。"
    },
    {
        "title": "AudioFool: Fast, Universal and synchronization-free Cross-Domain Attack\n  on Speech Recognition",
        "url": "http://arxiv.org/abs/2309.11462v1",
        "pub_date": "2023-09-20",
        "summary": "Automatic Speech Recognition systems have been shown to be vulnerable to\nadversarial attacks that manipulate the command executed on the device. Recent\nresearch has focused on exploring methods to create such attacks, however, some\nissues relating to Over-The-Air (OTA) attacks have not been properly addressed.\nIn our work, we examine the needed properties of robust attacks compatible with\nthe OTA model, and we design a method of generating attacks with arbitrary such\ndesired properties, namely the invariance to synchronization, and the\nrobustness to filtering: this allows a Denial-of-Service (DoS) attack against\nASR systems. We achieve these characteristics by constructing attacks in a\nmodified frequency domain through an inverse Fourier transform. We evaluate our\nmethod on standard keyword classification tasks and analyze it in OTA, and we\nanalyze the properties of the cross-domain attacks to explain the efficiency of\nthe approach.",
        "translated": "自动语音识别系统已被证明是易受敌对攻击，操纵命令执行的设备。最近的研究集中在探索制造这种攻击的方法，然而，与 OTA 攻击有关的一些问题还没有得到适当的解决。在我们的工作中，我们研究了与 OTA 模型兼容的鲁棒攻击所需要的特性，并且我们设计了一种方法来产生具有任意这种特性的攻击，即同步不变性和过滤的鲁棒性: 这允许对 ASR 系统的拒绝服务(DoS)攻击。我们通过反向傅里叶变换在一个修改后的频率域中构建攻击来实现这些特性。我们评估了我们的方法在标准关键字分类任务中的应用，并在 OTA 中对其进行了分析，同时分析了跨域攻击的特点，说明了该方法的有效性。"
    },
    {
        "title": "Software Compartmentalization Trade-Offs with Hardware Capabilities",
        "url": "http://arxiv.org/abs/2309.11332v2",
        "pub_date": "2023-09-20",
        "summary": "Compartmentalization is a form of defensive software design in which an\napplication is broken down into isolated but communicating components.\nRetrofitting compartmentalization into existing applications is often thought\nto be expensive from the engineering effort and performance overhead points of\nview. Still, recent years have seen proposals of compartmentalization methods\nwith promises of low engineering efforts and reduced performance impact. ARM\nMorello combines a modern ARM processor with an implementation of Capability\nHardware Enhanced RISC Instructions (CHERI) aiming to provide efficient and\nsecure compartmentalization. Past works exploring CHERI-based\ncompartmentalization were restricted to emulated/FPGA prototypes.\n  In this paper, we explore possible compartmentalization schemes with CHERI on\nthe Morello chip. We propose two approaches representing different trade-offs\nin terms of engineering effort, security, scalability, and performance impact.\nWe describe and implement these approaches on a prototype OS running bare metal\non the Morello chip, compartmentalize two popular applications, and investigate\nthe performance overheads. Furthermore, we show that compartmentalization can\nbe achieved with an engineering cost that can be quite low if one is willing to\ntrade off on scalability and security, and that performance overheads are\nsimilar to other intra-address space isolation mechanisms.",
        "translated": "防火分区是防御性软件设计的一种形式，在这种设计中，应用程序被分解为孤立的、但可以通信的组件。从工程和性能开销的角度来看，将防火分区改造成现有的应用程序通常被认为是昂贵的。尽管如此，近年来还是出现了一些防火分区方法的提议，承诺降低工程效率和性能影响。ARM Morello 结合了现代 ARM 处理器和能力硬件增强的 RISC 指令(CHERI)的实现，旨在提供高效和安全的防火分区。过去探索基于 CHERI 的防火分区的工作仅限于仿真/FPGA 原型。在这篇文章中，我们探讨了在莫雷洛芯片上使用 CHERI 的可能的防火分区方案。我们提出了两种方法，它们在工程努力、安全性、可伸缩性和性能影响方面表现出不同的权衡。我们描述和实现这些方法的原型操作系统上运行裸金属莫雷洛芯片，划分两个流行的应用程序，并研究性能开销。此外，我们还表明，如果愿意在可扩展性和安全性之间进行权衡，那么防火分区的工程成本可以非常低，而且性能开销与其他内部地址空间隔离机制类似。"
    },
    {
        "title": "Lazy Contracts: Alleviating High Gas Costs by Secure and Trustless\n  Off-chain Execution of Smart Contracts",
        "url": "http://arxiv.org/abs/2309.11317v1",
        "pub_date": "2023-09-20",
        "summary": "Smart contracts are programs that are executed on the blockchain and can\nhold, manage and transfer assets in the form of cryptocurrencies. The\ncontract's execution is then performed on-chain and is subject to consensus,\ni.e. every node on the blockchain network has to run the function calls and\nkeep track of their side-effects. In most programmable blockchains, such as\nEthereum, the notion of gas is introduced to prevent DoS attacks by malicious\nparties who might try to slow down the network by performing heavy\ncomputations. A fixed cost to each atomic operation, and the initiator of a\nfunction call pays the total gas cost as a transaction fee. This helps prevent\nDoS attacks, but the resulting fees are extremely high. For example, in 2022,\non Ethereum alone, there has been a total gas usage of 1.77 Million ETH ~ 4.3\nBillion USD. This thesis proposes \"lazy contracts\" as a solution to alleviate\nthese costs. Our solution moves most of the computation off-chain, ensuring\nthat each function call incurs only a tiny amount of gas usage, while\npreserving enough data on-chain to guarantee an implicit consensus about the\nstate of the contract variables and ownership of funds. A complete on-chain\nexecution of the functions will only be triggered in case two parties to the\ncontract are in disagreement about the current state, which in turn can only\nhappen if at least one party is dishonest. In such cases, our protocol can\nidentify the dishonest party and penalize them by having them pay for the\nentire gas usage. Hence, no rational party has an incentive to act dishonestly.\nFinally, we perform extensive experiments over 160,735 real-world Solidity\ncontracts that were involved in 9,055,492 transactions in January 2022--January\n2023 on Ethereum and show that our approach reduces the overall gas usage by\n55.4%, which amounts to an astounding saving of 109.9 Million USD in gas fees.",
        "translated": "智能合同是在区块链上执行的程序，可以以加密货币的形式持有、管理和转移资产。然后合同的执行是在链上进行的，并且受制于共识，即区块链网络上的每个节点都必须运行函数调用并跟踪它们的副作用。在大多数可编程的区块链中，例如以太，引入了气体的概念，以防止恶意方的 DoS 攻击，这些恶意方可能试图通过执行大量计算来减慢网络速度。每个原子操作的固定成本，函数调用的发起者支付总的气体成本作为交易费用。这有助于防止 DoS 攻击，但由此产生的费用非常高。例如，在2022年，仅以太坊一项，天然气的总使用量就达到了177万瑞典盾 ~ 43亿美元。本文提出“惰性契约”作为降低这些成本的一种解决方案。我们的解决方案将大部分计算转移出链，确保每个函数调用只引起极少量的气体使用，同时保留足够的链上数据，以保证对合同变量的状态和资金所有权的隐式共识。只有在合同双方对当前状态存在分歧的情况下，才能启动职能的完全上链执行，而这种情况只有在至少一方不诚实的情况下才会发生。在这种情况下，我们的协议可以识别出不诚实的一方，并通过让他们支付整个煤气使用量来惩罚他们。因此，任何理性的一方都没有不诚实行为的动机。最后，我们在2022年1月至2023年1月的9055.492笔交易中，对160735份现实世界中的 Soliity 合同进行了广泛的实验，结果显示我们的方法将总体天然气使用量减少了55.4% ，相当于惊人地节省了1.099亿美元的天然气费用。"
    },
    {
        "title": "Policy Patterns for Usage Control in Data Spaces",
        "url": "http://arxiv.org/abs/2309.11289v1",
        "pub_date": "2023-09-20",
        "summary": "Data-driven technologies have the potential to initiate a transportation\nrelated revolution in the way we travel, commute and navigate within cities. As\na major effort of this transformation relies on Mobility Data Spaces for the\nexchange of mobility data, the necessity to protect valuable data and formulate\nconditions for data exchange arises. This paper presents key contributions to\nthe development of automated contract negotiation and data usage policies in\nthe Mobility Data Space. A comprehensive listing of policy patterns for usage\ncontrol is provided, addressing common requirements and scenarios in data\nsharing and governance. The use of the Open Digital Rights Language (ODRL) is\nproposed to formalize the collected policies, along with an extension of the\nODRL vocabulary for data space-specific properties.",
        "translated": "数据驱动技术有可能引发一场与交通相关的革命，改变我们在城市中旅行、通勤和导航的方式。由于这一转变的一项主要努力依赖于移动数据空间交换移动数据，因此有必要保护宝贵的数据并为数据交换规定条件。本文介绍了在移动数据空间中开发自动合同谈判和数据使用策略的主要贡献。提供了用于使用控制的策略模式的全面清单，解决了数据共享和治理中的常见需求和场景。建议使用开放数字版权语言(ODRL)来形式化收集到的策略，同时扩展 ODRL 词汇表，用于特定于数据空间的属性。"
    },
    {
        "title": "Tropical cryptography III: digital signatures",
        "url": "http://arxiv.org/abs/2309.11256v1",
        "pub_date": "2023-09-20",
        "summary": "We use tropical algebras as platforms for a very efficient digital signature\nprotocol. Security relies on computational hardness of factoring one-variable\ntropical polynomials; this problem is known to be NP-hard.",
        "translated": "我们使用热带代数作为一个非常有效的数字签名协议的平台。安全性依赖于分解一元热带多项式的计算硬度，这个问题被认为是 NP 难的。"
    },
    {
        "title": "A Game-theoretic Approach for Provably-Uniform Random Number Generation\n  in Decentralized Networks",
        "url": "http://arxiv.org/abs/2309.11250v1",
        "pub_date": "2023-09-20",
        "summary": "Many protocols in distributed computing rely on a source of randomness,\nusually called a random beacon, both for their applicability and security. This\nis especially true for proof-of-stake blockchain protocols in which the next\nminer or set of miners have to be chosen randomly and each party's likelihood\nto be selected is in proportion to their stake in the cryptocurrency.\n  Current random beacons used in proof-of-stake protocols, such as Ouroboros\nand Algorand, have two fundamental limitations: Either (i)~they rely on\npseudorandomness, e.g.~assuming that the output of a hash function is uniform,\nwhich is a widely-used but unproven assumption, or (ii)~they generate their\nrandomness using a distributed protocol in which several participants are\nrequired to submit random numbers which are then used in the generation of a\nfinal random result. However, in this case, there is no guarantee that the\nnumbers provided by the parties are uniformly random and there is no incentive\nfor the parties to honestly generate uniform randomness. Most random beacons\nhave both limitations.\n  In this thesis, we provide a protocol for distributed generation of\nrandomness. Our protocol does not rely on pseudorandomness at all. Similar to\nsome of the previous approaches, it uses random inputs by different\nparticipants to generate a final random result. However, the crucial difference\nis that we provide a game-theoretic guarantee showing that it is in everyone's\nbest interest to submit uniform random numbers. Hence, our approach is the\nfirst to incentivize honest behavior instead of just assuming it. Moreover, the\napproach is trustless and generates unbiased random numbers. It is also\ntamper-proof and no party can change the output or affect its distribution.\nFinally, it is designed with modularity in mind and can be easily plugged into\nexisting distributed protocols such as proof-of-stake blockchains.",
        "translated": "分布式计算中的许多协议都依赖于随机性的来源，通常称为随机信标，这既是为了它们的适用性，也是为了它们的安全性。对于证明利益的区块链协议来说尤其如此，在这种协议中，必须随机选择下一个采矿者或下一组采矿者，而且每一方被选中的可能性与他们在加密货币中的利益成比例。目前在证明协议中使用的随机信标，如 Ouroboros 和 Algorand，有两个基本的限制: (i) ~ 它们依赖于伪随机数，例如 ~ 假设散列函数的输出是均匀的，这是一个广泛使用但未经证实的假设，或者(ii) ~ 它们使用分布式协议产生随机性，其中几个参与者被要求提交随机数，然后用于生成最终的随机结果。然而，在这种情况下，不能保证当事方提供的数字是一致随机的，也没有激励当事方诚实地产生一致随机性。大多数随机信标都有这两个限制。在这篇论文中，我们提供了一个随机分散式发电的协议。我们的协议根本不依赖伪随机数。与以前的一些方法类似，它使用不同参与者的随机输入来生成最终的随机结果。然而，关键的区别在于，我们提供了一个博弈论保证，表明提交统一的随机数符合每个人的最佳利益。因此，我们的方法是第一个激励诚实的行为，而不是仅仅假设它。此外，该方法是不可信的，并产生无偏随机数。它也是防篡改的，任何一方都不能改变输出或影响其分布。最后，它的设计考虑到了模块性，可以很容易地插入到现有的分布式协议中，如木桩证明块链。"
    },
    {
        "title": "Two generalizations of ideal matrices and their applications",
        "url": "http://arxiv.org/abs/2309.11240v1",
        "pub_date": "2023-09-20",
        "summary": "In this paper, two kinds of generalizations of ideal matrices, generalized\nideal matrices and double ideal matrices. are obtained and studied, The\nconcepts of generalized ideal matrices and double ideal matrices are proposed,\nand their ranks and maxima.linearly independent groups are verified.The initial\nmotivation to study double cyclic matrices is to study the quasi cyclic codes\nof the fractional index. In this paper, the generalized form of the quasi\ncyclic codes, i.e. the {\\phi}-quasi cyclic codes. and the construction of the\ngenerated matrix are given by the double ideal matrix.",
        "translated": "本文讨论了理想矩阵的两种推广形式: 广义理想矩阵和双理想矩阵。给出了广义理想矩阵和双理想矩阵的概念，并证明了它们的秩和极大值。研究双循环矩阵的最初动机是研究分数指数的拟循环码。本文给出了拟循环码的广义形式，即{ phi }-拟循环码。生成矩阵的构造由双理想矩阵给出。"
    },
    {
        "title": "Trace Monomial Boolean Functions with Large High-Order Nonlinearities",
        "url": "http://arxiv.org/abs/2309.11229v1",
        "pub_date": "2023-09-20",
        "summary": "Exhibiting an explicit Boolean function with a large high-order nonlinearity\nis an important problem in cryptography, coding theory, and computational\ncomplexity. We prove lower bounds on the second-order, third-order, and\nhigher-order nonlinearities of some trace monomial Boolean functions.\n  We prove lower bounds on the second-order nonlinearities of functions\n$\\mathrm{tr}_n(x^7)$ and $\\mathrm{tr}_n(x^{2^r+3})$ where $n=2r$. Among all\ntrace monomials, our bounds match the best second-order nonlinearity lower\nbounds by \\cite{Car08} and \\cite{YT20} for odd and even $n$ respectively. We\nprove a lower bound on the third-order nonlinearity for functions\n$\\mathrm{tr}_n(x^{15})$, which is the best third-order nonlinearity lower\nbound. For any $r$, we prove that the $r$-th order nonlinearity of\n$\\mathrm{tr}_n(x^{2^{r+1}-1})$ is at least\n$2^{n-1}-2^{(1-2^{-r})n+\\frac{r}{2^{r-1}}-1}- O(2^{\\frac{n}{2}})$. For $r \\ll\n\\log_2 n$, this is the best lower bound among all explicit functions.",
        "translated": "显式布尔函数具有大的高阶非线性，这是密码学、编码理论和计算复杂性中的一个重要问题。证明了一些迹单项布尔函数的二阶、三阶和高阶非线性项的下界。证明了函数 $mathrm { tr } _ n (x ^ 7) $和 $mathrm { tr } _ n (x ^ {2 ^ r + 3}) $的二阶非线性的下界，其中 $n = 2r $。在所有的迹单项式中，我们的界分别用{ Car08}和{ YT20}来表示奇数和偶数 $n $来匹配最佳的二阶非线性下界。证明了函数 $mathrm { tr } _ n (x ^ {15}) $的三阶非线性的一个下界，这是最佳的三阶非线性下界。对于任意 $r $，我们证明了 $mathrm { tr } _ n (x ^ {2 ^ { r + 1}-1}) $的 $r $th 阶非线性至少是 $2 ^ { n-1}-2 ^ {(1-2 ^ {-r }) n + frac { r }{2 ^ { r-1}-1}-O (2 ^ { frac { n }{2}}) $。对于 $r ll log _ 2 n $，这是所有显式函数中最好的下界。"
    },
    {
        "title": "Enabling Quartile-based Estimated-Mean Gradient Aggregation As Baseline\n  for Federated Image Classifications",
        "url": "http://arxiv.org/abs/2309.12267v1",
        "pub_date": "2023-09-21",
        "summary": "Federated Learning (FL) has revolutionized how we train deep neural networks\nby enabling decentralized collaboration while safeguarding sensitive data and\nimproving model performance. However, FL faces two crucial challenges: the\ndiverse nature of data held by individual clients and the vulnerability of the\nFL system to security breaches. This paper introduces an innovative solution\nnamed Estimated Mean Aggregation (EMA) that not only addresses these challenges\nbut also provides a fundamental reference point as a $\\mathsf{baseline}$ for\nadvanced aggregation techniques in FL systems. EMA's significance lies in its\ndual role: enhancing model security by effectively handling malicious outliers\nthrough trimmed means and uncovering data heterogeneity to ensure that trained\nmodels are adaptable across various client datasets. Through a wealth of\nexperiments, EMA consistently demonstrates high accuracy and area under the\ncurve (AUC) compared to alternative methods, establishing itself as a robust\nbaseline for evaluating the effectiveness and security of FL aggregation\nmethods. EMA's contributions thus offer a crucial step forward in advancing the\nefficiency, security, and versatility of decentralized deep learning in the\ncontext of FL.",
        "translated": "联邦学习(FL)已经彻底改变了我们如何训练深层神经网络，通过实现分散协作，同时保护敏感数据和提高模型性能。然而，外联面临着两个关键的挑战: 个人客户持有的数据的多样性和外联系统对数字证书认证机构的脆弱性。本文介绍了一种新颖的解决方案——估计平均聚合(EMA) ，它不仅解决了这些问题，而且为 FL 系统中的高级聚合技术提供了一个基本的参考点。EMA 的重要性在于它的双重作用: 通过裁剪手段有效地处理恶意离群值来增强模型安全性，并揭示数据异构性，以确保经过训练的模型能够适应各种客户数据集。通过大量的实验，与其他方法相比，EMA 始终表现出较高的精度和曲线下面积(AUC) ，为评价 FL 聚集方法的有效性和安全性建立了一个稳健的基线。因此，EMA 的贡献提供了一个关键的步骤，以提高效率，安全性和多功能的分散深度学习的背景下的外语。"
    },
    {
        "title": "t-EER: Parameter-Free Tandem Evaluation of Countermeasures and Biometric\n  Comparators",
        "url": "http://arxiv.org/abs/2309.12237v1",
        "pub_date": "2023-09-21",
        "summary": "Presentation attack (spoofing) detection (PAD) typically operates alongside\nbiometric verification to improve reliablity in the face of spoofing attacks.\nEven though the two sub-systems operate in tandem to solve the single task of\nreliable biometric verification, they address different detection tasks and are\nhence typically evaluated separately. Evidence shows that this approach is\nsuboptimal. We introduce a new metric for the joint evaluation of PAD solutions\noperating in situ with biometric verification. In contrast to the tandem\ndetection cost function proposed recently, the new tandem equal error rate\n(t-EER) is parameter free. The combination of two classifiers nonetheless leads\nto a \\emph{set} of operating points at which false alarm and miss rates are\nequal and also dependent upon the prevalence of attacks. We therefore introduce\nthe \\emph{concurrent} t-EER, a unique operating point which is invariable to\nthe prevalence of attacks. Using both modality (and even application) agnostic\nsimulated scores, as well as real scores for a voice biometrics application, we\ndemonstrate application of the t-EER to a wide range of biometric system\nevaluations under attack. The proposed approach is a strong candidate metric\nfor the tandem evaluation of PAD systems and biometric comparators.",
        "translated": "表示攻击(欺骗)检测(PAD)通常与生物特征验证一起工作，以提高面对欺骗攻击时的可靠性。尽管这两个子系统同时运作以解决可靠的生物特征验证这一单一任务，但它们处理不同的检测任务，因此通常分别进行评估。有证据表明，这种方法是次优的。我们介绍了一个新的度量联合评估的 PAD 解决方案在现场操作与生物特征验证。与最近提出的串联检测代价函数相比，新的串联等误码率(t-EER)是无参数的。尽管如此，两个分类器的组合导致了一个错误报警和漏报率相等的操作点集合，并且依赖于攻击的流行程度。因此，我们引入了并发 t-EER，这是一个唯一的操作点，它对攻击的普遍性是不变的。使用模态(甚至应用程序)不可知的模拟分数，以及语音生物特征识别应用程序的实际分数，我们演示了 t-EER 在受攻击的大范围生物特征识别系统评估中的应用。该方法是对 PAD 系统和生物特征比较器进行串联评估的有力候选指标。"
    },
    {
        "title": "De-authentication using Ambient Light Sensor",
        "url": "http://arxiv.org/abs/2309.12220v1",
        "pub_date": "2023-09-21",
        "summary": "While user authentication happens before initiating or resuming a login\nsession, de-authentication detects the absence of a previously-authenticated\nuser to revoke her currently active login session. The absence of proper\nde-authentication can lead to well-known lunchtime attacks, where a nearby\nadversary takes over a carelessly departed user's running login session. The\nexisting solutions for automatic de-authentication have distinct practical\nlimitations, e.g., extraordinary deployment requirements or high initial cost\nof external equipment.\n  In this paper, we propose \"DE-authentication using Ambient Light sensor\"\n(DEAL), a novel, inexpensive, fast, and user-friendly de-authentication\napproach. DEAL utilizes the built-in ambient light sensor of a modern computer\nto determine if the user is leaving her work-desk. DEAL, by design, is\nresilient to natural shifts in lighting conditions and can be configured to\nhandle abrupt changes in ambient illumination (e.g., due to toggling of room\nlights). We collected data samples from 4800 sessions with 120 volunteers in 4\ntypical workplace settings and conducted a series of experiments to evaluate\nthe quality of our proposed approach thoroughly. Our results show that DEAL can\nde-authenticate a departing user within 4 seconds with a hit rate of 89.15% and\na fall-out of 7.35%. Finally, bypassing DEAL to launch a lunchtime attack is\npractically infeasible as it requires the attacker to either take the user's\nposition within a few seconds or manipulate the sensor readings sophisticatedly\nin real-time.",
        "translated": "当用户身份验证发生在启动或恢复登录会话之前时，反身份验证检测到没有先前经过身份验证的用户来撤销其当前活动的登录会话。缺乏适当的反身份验证可能导致众所周知的午餐时间攻击，在这种攻击中，附近的一个对手接管了一个不小心离开的用户的正在运行的登录会话。现有的自动取消认证解决方案有明显的实际局限性，例如部署要求过高或外部设备的初始成本过高。本文提出了一种新颖、廉价、快速、用户友好的“环境光传感器反身份认证”(DEAL)方法。DEAL 利用现代计算机内置的环境光传感器来确定用户是否离开了她的办公桌。DEAL 在设计上对照明条件的自然变化具有弹性，并且可以配置为处理环境照明的突然变化(例如，由于房间灯光的切换)。我们收集了来自4个典型工作场所的120名志愿者的4800个会议的数据样本，并进行了一系列的实验来彻底评估我们提出的方法的质量。结果表明，DEAL 可以在4秒内解除离开用户的身份验证，命中率为89.15% ，退出率为7.35% 。最后，绕过 DEAL 发动午餐攻击实际上是不可行的，因为它要求攻击者要么在几秒钟内占据用户的位置，要么实时复杂地操纵传感器读数。"
    },
    {
        "title": "Information Forensics and Security: A quarter-century-long journey",
        "url": "http://arxiv.org/abs/2309.12159v1",
        "pub_date": "2023-09-21",
        "summary": "Information Forensics and Security (IFS) is an active R&amp;D area whose goal is\nto ensure that people use devices, data, and intellectual properties for\nauthorized purposes and to facilitate the gathering of solid evidence to hold\nperpetrators accountable. For over a quarter century since the 1990s, the IFS\nresearch area has grown tremendously to address the societal needs of the\ndigital information era. The IEEE Signal Processing Society (SPS) has emerged\nas an important hub and leader in this area, and the article below celebrates\nsome landmark technical contributions. In particular, we highlight the major\ntechnological advances on some selected focus areas in the field developed in\nthe last 25 years from the research community and present future trends.",
        "translated": "信息取证和安全(IFS)是一个活跃的研发领域，其目标是确保人们使用设备、数据和知识产权的授权目的，并促进收集确凿的证据，以追究肇事者的责任。自1990年代以来的四分之一个世纪以来，IFS 的研究领域已经取得了巨大的发展，以满足数字信息时代的社会需求。IEEE 信号处理协会(SPS)已经成为这一领域的重要枢纽和领导者，下面的文章赞扬了一些具有里程碑意义的技术贡献。特别是，我们强调了过去25年来研究界在该领域某些选定的重点领域取得的主要技术进步，并提出了未来的趋势。"
    },
    {
        "title": "S-GBDT: Frugal Differentially Private Gradient Boosting Decision Trees",
        "url": "http://arxiv.org/abs/2309.12041v1",
        "pub_date": "2023-09-21",
        "summary": "Privacy-preserving learning of gradient boosting decision trees (GBDT) has\nthe potential for strong utility-privacy tradeoffs for tabular data, such as\ncensus data or medical meta data: classical GBDT learners can extract\nnon-linear patterns from small sized datasets. The state-of-the-art notion for\nprovable privacy-properties is differential privacy, which requires that the\nimpact of single data points is limited and deniable. We introduce a novel\ndifferentially private GBDT learner and utilize four main techniques to improve\nthe utility-privacy tradeoff. (1) We use an improved noise scaling approach\nwith tighter accounting of privacy leakage of a decision tree leaf compared to\nprior work, resulting in noise that in expectation scales with $O(1/n)$, for\n$n$ data points. (2) We integrate individual R\\'enyi filters to our method to\nlearn from data points that have been underutilized during an iterative\ntraining process, which -- potentially of independent interest -- results in a\nnatural yet effective insight to learning streams of non-i.i.d. data. (3) We\nincorporate the concept of random decision tree splits to concentrate privacy\nbudget on learning leaves. (4) We deploy subsampling for privacy amplification.\nOur evaluation shows for the Abalone dataset ($&lt;4k$ training data points) a\n$R^2$-score of $0.39$ for $\\varepsilon=0.15$, which the closest prior work only\nachieved for $\\varepsilon=10.0$. On the Adult dataset ($50k$ training data\npoints) we achieve test error of $18.7\\,\\%$ for $\\varepsilon=0.07$ which the\nclosest prior work only achieved for $\\varepsilon=1.0$. For the Abalone dataset\nfor $\\varepsilon=0.54$ we achieve $R^2$-score of $0.47$ which is very close to\nthe $R^2$-score of $0.54$ for the nonprivate version of GBDT. For the Adult\ndataset for $\\varepsilon=0.54$ we achieve test error $17.1\\,\\%$ which is very\nclose to the test error $13.7\\,\\%$ of the nonprivate version of GBDT.",
        "translated": "梯度提升决策树(GBDT)的隐私保护学习对于表格数据(如人口普查数据或医学元数据)具有很强的效用-隐私权衡潜力: 经典的 GBDT 学习者可以从小型数据集中提取非线性模式。可证明的隐私属性的最新概念是差分隐私的，它要求单个数据点的影响是有限的和可否认的。我们介绍了一种新的差分私有 GBDT 学习器，并利用四种主要技术来改善效用-隐私权衡。(1)与之前的工作相比，我们使用了一种改进的噪音标度方法，更严格地计算决策树叶的隐私泄漏，导致噪音在 $O (1/n) $的预期标度下为 $n $数据点。(2)我们将单独的 R‘ enyi 过滤器集成到我们的方法中，以从迭代训练过程中未充分利用的数据点中学习，这可能具有独立的兴趣，从而产生对非 i.id 数据学习流的自然而有效的洞察力。(3)引入随机决策树分割的概念，将隐私预算集中在学习树上。(4)采用次采样方法进行隐私放大。我们的评估显示，对于鲍鱼数据集($< 4k $训练数据点) ，$varepsilon = 0.15 $的 $R ^ 2 $- 得分为0.39 $，这是之前最接近的工作仅为 $varepsilon = 10.0 $。在成人数据集($50k $训练数据点)上，我们实现了 $18.7的测试误差,% $为 $varepsilon = 0.07 $，这是之前最接近的工作仅为 $varepsilon = 1.0 $。对于 $varepsilon = 0.54 $的鲍鱼数据集，我们实现 $R ^ 2 $- 得分为 $0.47 $，非常接近 GBDT 的非私有版本的 $R ^ 2 $- 得分为 $0.54 $。对于 $varepsilon = 0.54 $的成人数据集，我们实现了测试错误 $17.1,% $，非常接近 GBDT 的非私有版本的测试错误 $13.7,% $。"
    },
    {
        "title": "A quaternary analogue of Tang-Ding codes",
        "url": "http://arxiv.org/abs/2309.12003v1",
        "pub_date": "2023-09-21",
        "summary": "In a recent paper, Tang and Ding introduced a class of binary cyclic codes of\nrate close to one half with a designed lower bound on their minimum distance.\nThe definition involves the base $2$ expansion of the integers in their\ndefining set. In this paper we propose an analogue for quaternary codes. In\naddition, the performances of the subfield subcode and of the trace code (two\nbinary cyclic codes) are investigated.",
        "translated": "在最近的一篇论文中，唐和丁介绍了一类速率接近二分之一的二进制循环码，它们的最小距离有一个设计的下界。该定义涉及定义集中整数的基 $2 $扩展。本文提出了四元码的一个类似形式。此外，还研究了子域子码和跟踪码(两个二进制循环码)的性能。"
    },
    {
        "title": "A New cryptanalysis model based on random and quantum walks",
        "url": "http://arxiv.org/abs/2309.11997v1",
        "pub_date": "2023-09-21",
        "summary": "Randomness plays a key role in the design of attacks on cryptographic systems\nand cyber security algorithms in general. Random walks and quantum walks are\npowerful tools for mastering random phenomena. In this article, I propose a\nprobabilistic attack model of a cryptographic system. This model is based on a\nrandom or quantum walk with the space of states being a space containing the\nsecret to be revealed. This space can be a subspace of keys, plain texts or\ncipher texts.",
        "translated": "随机性在密码系统攻击设计和网络安全算法设计中起着关键作用。随机游动和量子游动是掌握随机现象的有力工具。在本文中，我提出了一个密码系统的概率攻击模型。这个模型是基于随机或量子游走，其中的状态空间是一个包含秘密的空间。这个空间可以是密钥、纯文本或密码文本的子空间。"
    },
    {
        "title": "Generic Selfish Mining MDP for DAG Protocols",
        "url": "http://arxiv.org/abs/2309.11924v1",
        "pub_date": "2023-09-21",
        "summary": "Selfish Mining is strategic rule-breaking to maximize rewards in\nproof-of-work protocols [3] and Markov Decision Processes (MDPs) are the\npreferred tool for finding optimal strategies in Bitcoin [4, 10] and similar\nlinear chain protocols [12]. Protocols increasingly adopt non-sequential chain\nstructures [11], for which MDP analysis is more involved [2]. To date,\nresearchers have tailored specific attack spaces for each protocol [2, 4, 5, 7,\n10, 12]. Assumptions differ, and validating and comparing results is difficult.\nTo overcome this, we propose a generic attack space that supports the wide\nclass of DAG protocols that provide a total ordering of blocks [11], e. g.,\nEthereum, Fruitchains, and Parallel Proof-of-Work. Our approach is modular: we\nspecify each protocol as one program, and then derive the Selfish Mining MDPs\nautomatically.",
        "translated": "自私挖掘是战略性的规则破坏，以最大化工作证明协议中的奖励[3] ，马尔可夫决策过程(MDP)是在比特币[4,10]和类似的线性链协议[12]中找到最佳策略的首选工具。协议越来越多地采用非顺序链结构[11] ，其中 MDP 分析更为复杂[2]。迄今为止，研究人员已经为每个协议定制了特定的攻击空间[2,4,5,7,10,12]。假设不同，验证和比较结果是困难的。为了克服这一点，我们提出了一个通用的攻击空间，支持广泛类的 DAG 协议，提供一个块的总顺序[11] ，例如，Ethereum，Fruitchain 和并行工作证明。我们的方法是模块化的: 我们将每个协议指定为一个程序，然后自动获得自私挖掘 MDP。"
    },
    {
        "title": "The supersingular endomorphism ring problem given one endomorphism",
        "url": "http://arxiv.org/abs/2309.11912v1",
        "pub_date": "2023-09-21",
        "summary": "Given a supersingular elliptic curve E and a non-scalar endomorphism $\\alpha$\nof E, we prove that the endomorphism ring of E can be computed in classical\ntime about disc(Z[$\\alpha$])^1/4 , and in quantum subexponential time, assuming\nthe generalised Riemann hypothesis. Previous results either had higher\ncomplexities, or relied on heuristic assumptions. Along the way, we prove that\nthe Primitivisation problem can be solved in polynomial time (a problem\npreviously believed to be hard), and we prove that the action of smooth ideals\non oriented elliptic curves can be computed in polynomial time (previous\nresults of this form required the ideal to be powersmooth, i.e., not divisible\nby any large prime power). Following the attacks on SIDH, isogenies in high\ndimension are a central ingredient of our results.",
        "translated": "给定一条超奇异椭圆曲线 E 和 E 的非标量自同态 $alpha $，我们证明了 E 的自同态环可以在经典时间关于圆盘(Z [ $alpha $]) ^ 1/4和量子次指数时间中计算，假设广义黎曼猜想。以前的结果要么更加复杂，要么依赖于启发式假设。在这个过程中，我们证明了初等化问题可以在多项式时间内求解(这个问题以前被认为是困难的) ，并且我们证明了光滑理想对有向椭圆曲线的作用可以在多项式时间内计算(这种形式以前的结果要求理想是幂光滑的，即不能被任何大的初等幂整除)。在攻击 SIDH 之后，高维度的同源性是我们结果的一个核心成分。"
    },
    {
        "title": "Full mesh networking technology with peer to peer grid topology based on\n  variable parameter full dimensional space",
        "url": "http://arxiv.org/abs/2309.11903v1",
        "pub_date": "2023-09-21",
        "summary": "The continuous development of computer network technology has accelerated the\npace of informatization, and at the same time, network security issues are\nbecoming increasingly prominent. Networking technology with different network\ntopologies is one of the important means to solve network security problems.\nThe security of VPN is based on the division of geographical boundaries, but\nthe granularity is relatively coarse, which is difficult to cope with the\ndynamic changes of the security situation. Zero trust network solves the VPN\nproblem through peer to peer authorization and continuous verification, but\nmost of the solutions use a central proxy device, resulting in the central node\nbecoming the bottleneck of the network. This paper put forward the hard-Nat\ntraversal formula based on the birthday paradox, which solves the long-standing\nproblem of hard NAT traversal. A full mesh networking mechanism with variable\nparameter full-dimensional spatial peer-to-peer grid topology was proposed,\nwhich covers all types of networking schemes and achieve peer-2-peer resource\ninterconnection on both methodological and engineering level.",
        "translated": "计算机网络技术的不断发展加快了信息化的步伐，同时，网络安全问题也日益突出。采用不同网络拓扑结构的网络技术是解决网络安全问题的重要手段之一。VPN 的安全性基于地理边界的划分，但粒度较粗，难以适应安全态势的动态变化。零信任网络通过对等授权和连续验证解决了 VPN 问题，但大多数解决方案都使用中央代理设备，导致中央节点成为网络的瓶颈。本文提出了基于生日问题的硬 NAT 遍历公式，解决了长期存在的硬 NAT 遍历问题。提出了一种可变参数全维空间对等网格拓扑结构的全网格组网机制，该机制涵盖了各种组网方案，在方法学和工程层面上实现了对等资源的互连。"
    },
    {
        "title": "Understanding Deep Gradient Leakage via Inversion Influence Functions",
        "url": "http://arxiv.org/abs/2309.13016v1",
        "pub_date": "2023-09-22",
        "summary": "Deep Gradient Leakage (DGL) is a highly effective attack that recovers\nprivate training images from gradient vectors. This attack casts significant\nprivacy challenges on distributed learning from clients with sensitive data,\nwhere clients are required to share gradients. Defending against such attacks\nrequires but lacks an understanding of when and how privacy leakage happens,\nmostly because of the black-box nature of deep networks. In this paper, we\npropose a novel Inversion Influence Function (I$^2$F) that establishes a\nclosed-form connection between the recovered images and the private gradients\nby implicitly solving the DGL problem. Compared to directly solving DGL, I$^2$F\nis scalable for analyzing deep networks, requiring only oracle access to\ngradients and Jacobian-vector products. We empirically demonstrate that I$^2$F\neffectively approximated the DGL generally on different model architectures,\ndatasets, attack implementations, and noise-based defenses. With this novel\ntool, we provide insights into effective gradient perturbation directions, the\nunfairness of privacy protection, and privacy-preferred model initialization.\nOur codes are provided in\nhttps://github.com/illidanlab/inversion-influence-function.",
        "translated": "深度梯度泄漏(DGL)是一种从梯度向量中恢复私有训练图像的高效攻击方法。这种攻击对从客户端分布式学习敏感数据带来了重大隐私挑战，因为客户端需要共享渐变。防御此类攻击需要但缺乏对隐私泄露发生的时间和方式的理解，这主要是因为深度网络的黑盒特性。本文提出了一种新的反演影响函数(I $^ 2 $F) ，通过隐式求解 DGL 问题，在恢复后的图像和私有梯度之间建立一个闭合形式的连接。与直接求解 DGL 相比，I $^ 2 $F 可扩展用于分析深层网络，只需要 Oracle 访问梯度和 Jacobian 向量产品。我们通过经验证明，I $^ 2 $F 通常在不同的模型架构、数据集、攻击实现和基于噪声的防御上有效地接近 DGL。通过这个新颖的工具，我们提供了有效的梯度扰动方向，隐私保护的不公平性，以及隐私优先模型的初始化。我们的代码是 https://github.com/illidanlab/inversion-influence-function 提供的。"
    },
    {
        "title": "Expressive variational quantum circuits provide inherent privacy in\n  federated learning",
        "url": "http://arxiv.org/abs/2309.13002v1",
        "pub_date": "2023-09-22",
        "summary": "Federated learning has emerged as a viable distributed solution to train\nmachine learning models without the actual need to share data with the central\naggregator. However, standard neural network-based federated learning models\nhave been shown to be susceptible to data leakage from the gradients shared\nwith the server. In this work, we introduce federated learning with variational\nquantum circuit model built using expressive encoding maps coupled with\noverparameterized ans\\\"atze. We show that expressive maps lead to inherent\nprivacy against gradient inversion attacks, while overparameterization ensures\nmodel trainability. Our privacy framework centers on the complexity of solving\nthe system of high-degree multivariate Chebyshev polynomials generated by the\ngradients of quantum circuit. We present compelling arguments highlighting the\ninherent difficulty in solving these equations, both in exact and approximate\nscenarios. Additionally, we delve into machine learning-based attack strategies\nand establish a direct connection between overparameterization in the original\nfederated learning model and underparameterization in the attack model.\nFurthermore, we provide numerical scaling arguments showcasing that\nunderparameterization of the expressive map in the attack model leads to the\nloss landscape being swamped with exponentially many spurious local minima\npoints, thus making it extremely hard to realize a successful attack. This\nprovides a strong claim, for the first time, that the nature of quantum machine\nlearning models inherently helps prevent data leakage in federated learning.",
        "translated": "联合学习已经成为一种可行的分布式解决方案，用于训练机器学习模型，而无需与中央聚合器共享数据。然而，标准的基于神经网络的联邦学习模型已被证明容易受到与服务器共享的梯度数据泄漏的影响。在本文中，我们引入了联邦学习与变分量子电路模型建立使用表达式编码映射耦合过参数化的“ atze”。我们表明，表达映射导致对梯度反演攻击的固有隐私，而过参数化确保模型的可训练性。我们的隐私框架集中于解决由量子电路梯度产生的高度多元切比雪夫多项式系统的复杂性。我们提出令人信服的论据，强调解决这些方程固有的困难，在精确和近似的情况下。此外，我们深入研究了基于机器学习的攻击策略，并在原联邦学习模型中的过参数化和攻击模型中的欠参数化之间建立了直接的联系。此外，我们提供的数值比例参数表明，在攻击模型的表达式地图参数化不足，导致损失景观淹没指数许多虚假的局部极小点，从而使得极难实现成功的攻击。这首次提供了一个强有力的主张，即量子机器学习模型的本质在本质上有助于防止联邦学习中的数据泄漏。"
    },
    {
        "title": "Current file Overview PassViz: A Visualisation System for Analysing\n  Leaked Passwords",
        "url": "http://arxiv.org/abs/2309.12968v1",
        "pub_date": "2023-09-22",
        "summary": "Passwords remain the most widely used form of user authentication, despite\nadvancements in other methods. However, their limitations, such as\nsusceptibility to attacks, especially weak passwords defined by human users,\nare well-documented. The existence of weak human-defined passwords has led to\nrepeated password leaks from websites, many of which are of large scale. While\nsuch password leaks are unfortunate security incidents, they provide security\nresearchers and practitioners with good opportunities to learn valuable\ninsights from such leaked passwords, in order to identify ways to improve\npassword policies and other security controls on passwords. Researchers have\nproposed different data visualisation techniques to help analyse leaked\npasswords. However, many approaches rely solely on frequency analysis, with\nlimited exploration of distance-based graphs. This paper reports PassViz, a\nnovel method that combines the edit distance with the t-SNE (t-distributed\nstochastic neighbour embedding) dimensionality reduction algorithm for\nvisualising and analysing leaked passwords in a 2-D space. We implemented\nPassViz as an easy-to-use command-line tool for visualising large-scale\npassword databases, and also as a graphical user interface (GUI) to support\ninteractive visual analytics of small password databases. Using the\n\"000webhost\" leaked database as an example, we show how PassViz can be used to\nvisually analyse different aspects of leaked passwords and to facilitate the\ndiscovery of previously unknown password patterns. Overall, our approach\nempowers researchers and practitioners to gain valuable insights and improve\npassword security through effective data visualisation and analysis.",
        "translated": "尽管其他方法有所改进，但密码仍然是使用最广泛的用户身份验证形式。然而，它们的局限性，例如易受攻击，特别是由人类用户定义的弱密码，已经得到了充分的证明。人类定义的弱密码的存在导致了网站密码的反复泄露，其中许多是大规模的。虽然这类密码泄露是不幸的安全事件，但它们为安全研究人员和从业人员提供了良好的机会，从这类泄露的密码中学习有价值的见解，以确定改进密码政策和其他密码安全控制的方法。研究人员提出了不同的数据可视化技术，以帮助分析泄露的密码。然而，许多方法仅仅依赖于频率分析，对基于距离的图的探索有限。本文报道了 PassViz，一种将编辑距离与 t-SNE (t 分布随机邻居嵌入)降维算法相结合的新方法，用于在二维空间中可视化和分析泄漏的密码。我们实现 PassViz 作为一个易于使用的命令行工具，用于可视化大型密码数据库，也作为一个图形用户界面(GUI) ，以支持小型密码数据库的交互式可视化分析。以“000webhost”泄漏的数据库为例，我们展示了 PassViz 如何可以用来直观地分析泄漏的密码的不同方面，并促进发现以前未知的密码模式。总体而言，我们的方法使研究人员和从业人员能够通过有效的数据可视化和分析，获得有价值的见解和改善密码安全性。"
    },
    {
        "title": "On Data Fabrication in Collaborative Vehicular Perception: Attacks and\n  Countermeasures",
        "url": "http://arxiv.org/abs/2309.12955v1",
        "pub_date": "2023-09-22",
        "summary": "Collaborative perception, which greatly enhances the sensing capability of\nconnected and autonomous vehicles (CAVs) by incorporating data from external\nresources, also brings forth potential security risks. CAVs' driving decisions\nrely on remote untrusted data, making them susceptible to attacks carried out\nby malicious participants in the collaborative perception system. However,\nsecurity analysis and countermeasures for such threats are absent. To\nunderstand the impact of the vulnerability, we break the ground by proposing\nvarious real-time data fabrication attacks in which the attacker delivers\ncrafted malicious data to victims in order to perturb their perception results,\nleading to hard brakes or increased collision risks. Our attacks demonstrate a\nhigh success rate of over 86\\% on high-fidelity simulated scenarios and are\nrealizable in real-world experiments. To mitigate the vulnerability, we present\na systematic anomaly detection approach that enables benign vehicles to jointly\nreveal malicious fabrication. It detects 91.5% of attacks with a false positive\nrate of 3% in simulated scenarios and significantly mitigates attack impacts in\nreal-world scenarios.",
        "translated": "协同感知通过整合外部资源的数据，极大地提高了连通自主车辆(CAV)的感知能力，同时也带来了潜在的安全风险。CAV 的驾驶决策依赖于远程不可信数据，这使得它们容易受到协同感知系统中恶意参与者的攻击。然而，缺乏针对此类威胁的安全分析和对策。为了理解这个漏洞的影响，我们打破常规，提出了各种实时数据编造攻击，攻击者向受害者提供恶意数据，以扰乱他们的感知结果，导致硬刹车或增加碰撞风险。我们的攻击在高保真模拟场景中的成功率高达86% 以上，并且在现实世界的实验中是可以实现的。为了减低这个漏洞，我们提出了一个有系统的异常检测方法，使良性车辆能够共同揭露恶意捏造。在模拟场景中，它检测到91.5% 的攻击，误报率为3% ，并且在现实场景中显著减轻了攻击的影响。"
    },
    {
        "title": "A New Security Threat in MCUs -- SoC-wide timing side channels and how\n  to find them",
        "url": "http://arxiv.org/abs/2309.12925v1",
        "pub_date": "2023-09-22",
        "summary": "Microarchitectural timing side channels have been thoroughly investigated as\na security threat in hardware designs featuring shared buffers (e.g., caches)\nand/or parallelism between attacker and victim task execution. Contradicting\ncommon intuitions, recent activities demonstrate, however, that this threat is\nreal also in microcontroller SoCs without such features. In this paper, we\ndescribe SoC-wide timing side channels previously neglected by security\nanalysis and present a new formal method to close this gap. In a case study\nwith the RISC-V Pulpissimo SoC platform, our method found a vulnerability to a\nso far unknown attack variant that allows an attacker to obtain information\nabout a victim's memory access behavior. After implementing a conservative fix,\nwe were able to verify that the SoC is now secure w.r.t. timing side channels.",
        "translated": "微架构计时端通道作为一种安全威胁已经被深入研究，它在硬件设计中具有共享缓冲区(例如缓存)和/或攻击者和受害者任务执行之间的并行性。然而，与一般直觉相反，最近的活动表明，这种威胁在没有这些特性的微控制器系统芯片中也是真实存在的。在本文中，我们描述了以前被安全性分析忽略的片上系统时序侧通道，并提出了一种新的形式化的方法来弥补这一差距。在一个使用 RISC-V Pulpissimo SoC 平台的案例研究中，我们的方法发现了一个漏洞，该漏洞针对一个目前为止未知的攻击变体，该变体允许攻击者获取有关受害者的内存访问行为的信息。在实现了一个保守的修复之后，我们能够验证 SoC 现在是安全的 W.r.t 时间端通道。"
    },
    {
        "title": "How Automated Market Makers Approach the Thin Market Problem in\n  Cryptoeocnomic Systems",
        "url": "http://arxiv.org/abs/2309.12818v1",
        "pub_date": "2023-09-22",
        "summary": "The proper design of automated market makers (AMMs) is crucial to enable the\ncontinuous trading of assets represented as digital tokens on markets of\ncryptoeconomic systems. Improperly designed AMMs can make such markets suffer\nfrom the thin market problem (TMP), which can cause cryptoeconomic systems to\nfail their purposes. We developed an AMM taxonomy that showcases AMM design\ncharacteristics. Based on the AMM taxonomy, we devised AMM archetypes\nimplementing principal solution approaches for the TMP. The main purpose of\nthis article is to support practitioners and researchers in tackling the TMP\nthrough proper AMM designs.",
        "translated": "自动做市商(AMM)的正确设计对于在密码经济系统的市场上持续交易以数字代币表示的资产是至关重要的。设计不当的 AMM 会使这样的市场遭受稀缺市场问题(TMP)的困扰，这会导致密码经济系统失败。我们开发了一个显示 AMM 设计特征的 AMM 分类法。基于 AMM 分类法，我们设计了实现 TMP 主要解决方案方法的 AMM 原型。本文的主要目的是支持从业人员和研究人员通过正确的 AMM 设计来处理 TMP。"
    },
    {
        "title": "Optimal Dynamic Fees for Blockchain Resources",
        "url": "http://arxiv.org/abs/2309.12735v1",
        "pub_date": "2023-09-22",
        "summary": "We develop a general and practical framework to address the problem of the\noptimal design of dynamic fee mechanisms for multiple blockchain resources. Our\nframework allows to compute policies that optimally trade-off between adjusting\nresource prices to handle persistent demand shifts versus being robust to local\nnoise in the observed block demand. In the general case with more than one\nresource, our optimal policies correctly handle cross-effects (complementarity\nand substitutability) in resource demands. We also show how these cross-effects\ncan be used to inform resource design, i.e. combining resources into bundles\nthat have low demand-side cross-effects can yield simpler and more efficient\nprice-update rules. Our framework is also practical, we demonstrate how it can\nbe used to refine or inform the design of heuristic fee update rules such as\nEIP-1559 or EIP-4844 with two case studies. We then estimate a uni-dimensional\nversion of our model using real market data from the Ethereum blockchain and\nempirically compare the performance of our optimal policies to EIP-1559.",
        "translated": "为了解决多区块链资源动态收费机制的优化设计问题，我们提出了一个通用的、实用的框架。我们的框架允许计算在调整资源价格以处理持续的需求变化与对观察到的区块需求中的局部噪声具有鲁棒性之间的最佳平衡的政策。在一般情况下，我们的最优策略能够正确处理资源需求的交叉效应(互补性和可替代性)。我们还展示了如何利用这些交叉效应来指导资源设计，也就是说，将资源组合成具有低需求侧交叉效应的捆绑包可以产生更简单、更有效的价格更新规则。我们的框架也是实用的，我们通过两个案例说明了如何使用它来完善或通知启发式收费更新规则的设计，如 EIP-1559或 EIP-4844。然后，我们使用来自以太区块链的真实市场数据估计我们的模型的单维版本，并经验性地比较我们的最优策略与 EIP-1559的性能。"
    },
    {
        "title": "Towards a Near-real-time Protocol Tunneling Detector based on Machine\n  Learning Techniques",
        "url": "http://arxiv.org/abs/2309.12720v1",
        "pub_date": "2023-09-22",
        "summary": "In the very last years, cybersecurity attacks have increased at an\nunprecedented pace, becoming ever more sophisticated and costly. Their impact\nhas involved both private/public companies and critical infrastructures. At the\nsame time, due to the COVID-19 pandemic, the security perimeters of many\norganizations expanded, causing an increase of the attack surface exploitable\nby threat actors through malware and phishing attacks. Given these factors, it\nis of primary importance to monitor the security perimeter and the events\noccurring in the monitored network, according to a tested security strategy of\ndetection and response. In this paper, we present a protocol tunneling detector\nprototype which inspects, in near real time, a company's network traffic using\nmachine learning techniques. Indeed, tunneling attacks allow malicious actors\nto maximize the time in which their activity remains undetected. The detector\nmonitors unencrypted network flows and extracts features to detect possible\noccurring attacks and anomalies, by combining machine learning and deep\nlearning. The proposed module can be embedded in any network security\nmonitoring platform able to provide network flow information along with its\nmetadata. The detection capabilities of the implemented prototype have been\ntested both on benign and malicious datasets. Results show 97.1% overall\naccuracy and an F1-score equals to 95.6%.",
        "translated": "在最近几年，网络安全攻击以前所未有的速度增加，变得越来越复杂和昂贵。它们的影响既涉及私营/上市公司，也涉及关键基础设施。与此同时，由于2019冠状病毒疾病流行，许多组织的安全范围扩大了，导致威胁行为者通过恶意软件和网络钓鱼攻击可利用的攻击面增加。鉴于这些因素，根据经过测试的检测和响应安全战略，监测安全周界和监测网络中发生的事件至关重要。在本文中，我们提出了一个协议隧道检测器原型，使用机器学习技术近实时地检测一个公司的网络流量。事实上，隧道攻击允许恶意行为者最大限度地利用其活动不被发现的时间。该检测器通过机器学习和深度学习相结合的方法，监测未加密的网络流，提取特征以检测可能发生的攻击和异常。该模块可以嵌入到任何能够提供网络流量信息及其元数据的网络安全监控平台中。已实现的原型的检测能力已在良性和恶意数据集上进行了测试。结果显示97.1% 的整体准确率和 F1评分相当于95.6% 。"
    },
    {
        "title": "An Efficient and Secure Arbitrary N-Party Quantum Key Agreement Protocol\n  Using Bell States",
        "url": "http://arxiv.org/abs/2309.12719v1",
        "pub_date": "2023-09-22",
        "summary": "Two quantum key agreement protocols using Bell states and Bell measurement\nwere recently proposed by Shukla et al.(Quantum Inf. Process. 13(11),\n2391-2405, 2014). However, Zhu et al. pointed out that there are some security\nflaws and proposed an improved version (Quantum Inf. Process. 14(11),\n4245-4254, 2015). In this study, we will show Zhu et al.'s improvement still\nexists some security problems, and its efficiency is not high enough. For\nsolving these problems, we utilize four Pauli operations {I, Z, X, Y } to\nencode two bits instead of the original two operations {I,X} to encode one bit,\nand then propose an efficient and secure arbitrary N-party quantum key\nagreement protocol. In the protocol, the channel checking with decoy single\nphotons is introduced to avoid the eavesdropper's flip attack, and a\npost-measurement mechanism is used to prevent against the collusion attack. The\nsecurity analysis shows the present protocol can guarantee the correctness,\nsecurity, privacy and fairness of quantum key agreement.",
        "translated": "最近 Shukla 等人提出了两种使用 Bell 态和 Bell 测量的量子密钥协商协议。过程。13(11) ，2391-2405,2014).然而，朱先生等人指出，这里存在一些安全漏洞，并提出了一个改进的版本(Quantum Inf。过程。14(11) ，4245-4254,2015).在本研究中，我们将展示朱等人的改进仍然存在一些安全问题，而且其效率不够高。为了解决这些问题，我们利用四个泡利运算{ I，Z，X，Y }代替原来的两个运算{ I，X }编码两个位，编码一个位，然后提出了一个高效、安全的任意 N 方量子密钥协商协议。该协议引入了利用诱饵单光子进行信道检测以避免窃听者的翻转攻击，并采用了后测机制来防止合谋攻击。安全性分析表明，该协议能够保证量子密钥协议的正确性、安全性、隐私性和公平性。"
    },
    {
        "title": "Cuttlefish: Expressive Fast Path Blockchains with FastUnlock",
        "url": "http://arxiv.org/abs/2309.12715v1",
        "pub_date": "2023-09-22",
        "summary": "Cuttlefish addresses several limitations of existing consensus-less and\nconsensus-minimized decentralized ledgers, including restricted programmability\nand the risk of deadlocked assets. The key insight of Cuttlefish is that\nconsensus in blockchains is necessary due to contention, rather than multiple\nowners of an asset as suggested by prior work. Previous proposals proactively\nuse consensus to prevent contention from blocking assets, taking a pessimistic\napproach. In contrast, Cuttlefish introduces collective objects and multi-owner\ntransactions that can offer most of the functionality of classic blockchains\nwhen objects transacted on are not under contention. Additionally, in case of\ncontention, Cuttlefish proposes a novel `Unlock' protocol that significantly\nreduces the latency of unblocking contented objects. By leveraging these\nfeatures, Cuttlefish implements consensus-less protocols for a broader range of\ntransactions, including asset swaps and multi-signature transactions, which\nwere previously believed to require consensus.",
        "translated": "Cuttlefish 解决了现有缺乏共识和共识最小化的分散分类账的若干局限性，包括可编程性受到限制和资产陷入僵局的风险。Cuttlefish 的主要观点是，区块链中的共识是必要的，因为存在争议，而不是像以前的工作所建议的那样，一个资产的多个所有者。以前的提案主动利用共识，采取悲观的方法，防止争夺阻碍资产。相比之下，Cuttlefish 引入了集体对象和多所有者事务，当事务处理的对象不存在争用时，这些对象可以提供传统块链的大部分功能。此外，在出现争用的情况下，Cuttlefish 提出了一种新的“解锁”协议，该协议显著降低了解锁内容对象的延迟。通过利用这些特性，Cuttlefish 为更广泛的交易实现了缺乏共识的协议，包括资产互换和多重签名交易，以前认为这些交易需要共识。"
    },
    {
        "title": "LinGCN: Structural Linearized Graph Convolutional Network for\n  Homomorphically Encrypted Inference",
        "url": "http://arxiv.org/abs/2309.14331v1",
        "pub_date": "2023-09-25",
        "summary": "The growth of Graph Convolution Network (GCN) model sizes has revolutionized\nnumerous applications, surpassing human performance in areas such as personal\nhealthcare and financial systems. The deployment of GCNs in the cloud raises\nprivacy concerns due to potential adversarial attacks on client data. To\naddress security concerns, Privacy-Preserving Machine Learning (PPML) using\nHomomorphic Encryption (HE) secures sensitive client data. However, it\nintroduces substantial computational overhead in practical applications. To\ntackle those challenges, we present LinGCN, a framework designed to reduce\nmultiplication depth and optimize the performance of HE based GCN inference.\nLinGCN is structured around three key elements: (1) A differentiable structural\nlinearization algorithm, complemented by a parameterized discrete indicator\nfunction, co-trained with model weights to meet the optimization goal. This\nstrategy promotes fine-grained node-level non-linear location selection,\nresulting in a model with minimized multiplication depth. (2) A compact\nnode-wise polynomial replacement policy with a second-order trainable\nactivation function, steered towards superior convergence by a two-level\ndistillation approach from an all-ReLU based teacher model. (3) an enhanced HE\nsolution that enables finer-grained operator fusion for node-wise activation\nfunctions, further reducing multiplication level consumption in HE-based\ninference. Our experiments on the NTU-XVIEW skeleton joint dataset reveal that\nLinGCN excels in latency, accuracy, and scalability for homomorphically\nencrypted inference, outperforming solutions such as CryptoGCN. Remarkably,\nLinGCN achieves a 14.2x latency speedup relative to CryptoGCN, while preserving\nan inference accuracy of 75% and notably reducing multiplication depth.",
        "translated": "图形卷积网络(GCN)模型规模的增长已经彻底改变了许多应用，在个人医疗保健和金融系统等领域超越了人类的表现。由于对客户数据的潜在敌对攻击，在云中部署 GCNs 引起了隐私方面的担忧。为了解决安全问题，使用同态加密保护机器学习(PPML)保护敏感客户端数据。然而，它在实际应用中引入了大量的计算开销。为了应对这些挑战，我们提出了 LinGCN，一个旨在降低乘法深度和优化基于 HE 的 GCN 推理性能的框架。LinGCN 的结构主要有三个要素: (1)可微结构线性化算法，辅以参数化离散指示函数，并与模型权重共同训练，以达到优化目标。这种策略促进了细粒度节点级非线性位置选择，从而产生了乘法深度最小的模型。(2)紧凑的节点多项式替换策略，带有二阶可训练的激活函数，通过基于全 relU 的教师模型的两级精馏方法导向优收敛。(3)增强的 HE 解决方案，使节点激活函数的细粒度算子融合，进一步减少基于 HE 的推理中的乘法级消耗。我们在 NTU-XVIEW 骨架联合数据集上的实验表明，LinGCN 在同态加密推理的延迟、准确性和可伸缩性方面优于 CryptoGCN 等解决方案。值得注意的是，LinGCN 相对于 CryptoGCN 实现了14.2倍的延迟加速，同时保持了75% 的推断准确率，并显著降低了乘法深度。"
    },
    {
        "title": "Towards a Theory of Maximal Extractable Value II: Uncertainty",
        "url": "http://arxiv.org/abs/2309.14201v1",
        "pub_date": "2023-09-25",
        "summary": "Maximal Extractable Value (MEV) is value extractable by temporary monopoly\npower commonly found in decentralized systems. This extraction stems from a\nlack of user privacy upon transaction submission and the ability of a\nmonopolist validator to reorder, add, and/or censor transactions. There are two\nmain directions to reduce MEV: reduce the flexibility of the miner to reorder\ntransactions by enforcing ordering rules and/or introduce a competitive market\nfor the right to reorder, add, and/or censor transactions. In this work, we\nunify these approaches via \\emph{uncertainty principles}, akin to those found\nin harmonic analysis and physics. This provides a quantitative trade-off\nbetween the freedom to reorder transactions and the complexity of an economic\npayoff to a user in a decentralized network. This trade off is analogous to the\nNyquist-Shannon sampling theorem and demonstrates that sequencing rules in\nblockchains need to be application specific. Our results suggest that neither\nso-called fair ordering techniques nor economic mechanisms can individually\nmitigate MEV for arbitrary payoff functions.",
        "translated": "最大可提取价值(MEV)是分散系统中普遍存在的暂时性垄断权力所能提取的价值。这种提取源于事务提交时缺乏用户隐私，以及垄断验证程序重新排序、添加和/或审查事务的能力。减少 MEV 有两个主要方向: 通过强制执行订货规则和/或为重新订货、增加和/或审查交易的权利引入竞争性市场，减少矿商重新订货的灵活性。在这项工作中，我们通过 emph {不确定性原理}统一了这些方法，类似于在傅里叶分析和物理学中发现的方法。这在重新安排交易的自由和分散网络中用户的经济回报的复杂性之间提供了一个量化的平衡。这种权衡类似于 Nyquist-Shannon 的采样定理，表明区块链中的排序规则需要针对具体应用。我们的研究结果表明，所谓的公平排序技术或经济机制都不能单独减轻任意支付函数的 MEV。"
    },
    {
        "title": "Targeted Attacks: Redefining Spear Phishing and Business Email\n  Compromise",
        "url": "http://arxiv.org/abs/2309.14166v1",
        "pub_date": "2023-09-25",
        "summary": "In today's digital world, cybercrime is responsible for significant damage to\norganizations, including financial losses, operational disruptions, or\nintellectual property theft. Cyberattacks often start with an email, the major\nmeans of corporate communication. Some rare, severely damaging email threats -\nknown as spear phishing or Business Email Compromise - have emerged. However,\nthe literature disagrees on their definition, impeding security vendors and\nresearchers from mitigating targeted attacks. Therefore, we introduce targeted\nattacks. We describe targeted-attack-detection techniques as well as\nsocial-engineering methods used by fraudsters. Additionally, we present\ntext-based attacks - with textual content as malicious payload - and compare\nnon-targeted and targeted variants.",
        "translated": "在当今的数字世界中，网络犯罪对组织造成重大损害，包括经济损失、运营中断或知识产权盗窃。网络攻击通常始于电子邮件，而电子邮件是企业交流的主要手段。一些罕见的，严重破坏性的电子邮件威胁-称为鱼叉式网络钓鱼或商业电子邮件妥协-已经出现。然而，文献并不同意他们的定义，阻碍安全供应商和研究人员减轻有针对性的攻击。因此，我们引入有针对性的攻击。我们描述目标攻击检测技术以及欺诈者使用的社会工程方法。此外，我们还介绍了基于文本的攻击——将文本内容作为恶意有效载荷——并比较了非目标和目标变体。"
    },
    {
        "title": "One-Class Classification for Intrusion Detection on Vehicular Networks",
        "url": "http://arxiv.org/abs/2309.14134v1",
        "pub_date": "2023-09-25",
        "summary": "Controller Area Network bus systems within vehicular networks are not\nequipped with the tools necessary to ward off and protect themselves from\nmodern cyber-security threats. Work has been done on using machine learning\nmethods to detect and report these attacks, but common methods are not robust\ntowards unknown attacks. These methods usually rely on there being a sufficient\nrepresentation of attack data, which may not be available due to there either\nnot being enough data present to adequately represent its distribution or the\ndistribution itself is too diverse in nature for there to be a sufficient\nrepresentation of it. With the use of one-class classification methods, this\nissue can be mitigated as only normal data is required to train a model for the\ndetection of anomalous instances. Research has been done on the efficacy of\nthese methods, most notably One-Class Support Vector Machine and Support Vector\nData Description, but many new extensions of these works have been proposed and\nhave yet to be tested for injection attacks in vehicular networks. In this\npaper, we investigate the performance of various state-of-the-art one-class\nclassification methods for detecting injection attacks on Controller Area\nNetwork bus traffic. We investigate the effectiveness of these techniques on\nattacks launched on Controller Area Network buses from two different vehicles\nduring normal operation and while being attacked. We observe that the Subspace\nSupport Vector Data Description method outperformed all other tested methods\nwith a Gmean of about 85%.",
        "translated": "车辆网络中的控制器局域网路总线系统没有配备必要的工具来抵御和保护自己免受现代网络安全威胁。利用机器学习方法来检测和报告这些攻击已经取得了一定的成果，但是一般的方法对于未知的攻击并不具有鲁棒性。这些方法通常依赖于攻击数据有一个充分的表示，这可能是不可用的，因为要么没有足够的数据存在，以充分表示其分布或分布本身太多样化，以至于没有足够的表示。使用单类分类方法可以减轻这个问题，因为只需要正常数据就可以训练一个模型来检测异常实例。人们已经对这些方法的功效进行了研究，其中最引人注目的是一类支持向量机和支持向量数据描述，但是这些工作的许多新的扩展已经被提出，并且尚未测试在车辆网络中的注入攻击。在本文中，我们研究了各种最先进的一类分类方法在检测针对控制器局域网路总线交通的注入攻击方面的性能。我们研究了这些技术在正常运行和受到攻击时从两种不同的车辆对控制器局域网路发动攻击的有效性。我们观察到子空间支持向量数据描述方法优于所有其他测试方法，Gmean 为85% 左右。"
    },
    {
        "title": "SurrogatePrompt: Bypassing the Safety Filter of Text-To-Image Models via\n  Substitution",
        "url": "http://arxiv.org/abs/2309.14122v1",
        "pub_date": "2023-09-25",
        "summary": "Advanced text-to-image models such as DALL-E 2 and Midjourney possess the\ncapacity to generate highly realistic images, raising significant concerns\nregarding the potential proliferation of unsafe content. This includes adult,\nviolent, or deceptive imagery of political figures. Despite claims of rigorous\nsafety mechanisms implemented in these models to restrict the generation of\nnot-safe-for-work (NSFW) content, we successfully devise and exhibit the first\nprompt attacks on Midjourney, resulting in the production of abundant\nphotorealistic NSFW images. We reveal the fundamental principles of such prompt\nattacks and suggest strategically substituting high-risk sections within a\nsuspect prompt to evade closed-source safety measures. Our novel framework,\nSurrogatePrompt, systematically generates attack prompts, utilizing large\nlanguage models, image-to-text, and image-to-image modules to automate attack\nprompt creation at scale. Evaluation results disclose an 88% success rate in\nbypassing Midjourney's proprietary safety filter with our attack prompts,\nleading to the generation of counterfeit images depicting political figures in\nviolent scenarios. Both subjective and objective assessments validate that the\nimages generated from our attack prompts present considerable safety hazards.",
        "translated": "先进的文本到图像模型，如 dALL-e 2和 Midjourney，具有生成高度逼真图像的能力，引起了人们对不安全内容可能扩散的严重关切。这包括成人、暴力或欺骗性的政治人物形象。尽管声称在这些模型中实施了严格的安全机制来限制不安全工作(NSFW)内容的生成，但我们成功地设计并展示了对 Midtrip 的第一个迅速攻击，导致产生了大量逼真的 NSFW 图像。我们揭示了这种快速攻击的基本原理，并建议在可疑的快速攻击中战略性地替换高风险部分，以规避封闭来源的安全措施。我们的新框架 SurrogatePrompt 系统地生成攻击提示，利用大型语言模型、图像到文本和图像到图像模块来自动大规模创建攻击提示。评估结果显示，通过攻击提示绕过中途网专有安全过滤器的成功率为88% ，导致生成了描绘暴力场景中政治人物的伪造图像。主观和客观的评估都验证了从我们的攻击提示生成的图像存在相当大的安全隐患。"
    },
    {
        "title": "Random-Energy Secret Sharing via Extreme Synergy",
        "url": "http://arxiv.org/abs/2309.14047v1",
        "pub_date": "2023-09-25",
        "summary": "The random-energy model (REM), a solvable spin-glass model, has impacted an\nincredibly diverse set of problems, from protein folding to combinatorial\noptimization to many-body localization. Here, we explore a new connection to\nsecret sharing. We formulate a secret-sharing scheme, based on the REM, and\nanalyze its information-theoretic properties. Our analyses reveal that the\ncorrelations between subsystems of the REM are highly synergistic and form the\nbasis for secure secret-sharing schemes. We derive the ranges of temperatures\nand secret lengths over which the REM satisfies the requirement of secure\nsecret sharing. We show further that a special point in the phase diagram\nexists at which the REM-based scheme is optimal in its information encoding.\nOur analytical results for the thermodynamic limit are in good qualitative\nagreement with numerical simulations of finite systems, for which the strict\nsecurity requirement is replaced by a tradeoff between secrecy and\nrecoverability. Our work offers a further example of information theory as a\nunifying concept, connecting problems in statistical physics to those in\ncomputation.",
        "translated": "随机能量模型(REM) ，一个可解的自旋玻璃模型，已经影响了一系列令人难以置信的问题，从蛋白质折叠到组合优化到多体定位。在这里，我们探索一种与秘密共享的新联系。提出了一种基于 REM 的秘密共享方案，并分析了其信息理论性质。我们的分析表明，REM 子系统之间的相关性是高度协同的，并形成了安全秘密共享方案的基础。我们推导出 REM 满足安全秘密共享要求的温度范围和秘密长度。我们进一步证明了相图中存在一个特殊的点，在这个点上基于 REM 的方案的信息编码是最优的。我们对热力学极限的分析结果与有限系统的数值模拟在质量上非常吻合，对于有限系统，严格的安全要求被保密性和可恢复性之间的权衡所取代。我们的工作提供了信息论作为一个统一概念的进一步例子，将统计物理学中的问题与计算中的问题联系起来。"
    },
    {
        "title": "An Upper-Bound on the Decoding Failure Probability of the LRPC Decoder",
        "url": "http://arxiv.org/abs/2309.14028v1",
        "pub_date": "2023-09-25",
        "summary": "Low Rank Parity Check (LRPC) codes form a class of rank-metric\nerror-correcting codes that was purposely introduced to design public-key\nencryption schemes. An LRPC code is defined from a parity check matrix whose\nentries belong to a relatively low dimensional vector subspace of a large\nfinite field. This particular algebraic feature can then be exploited to\ncorrect with high probability rank errors when the parameters are appropriately\nchosen. In this paper, we present theoretical upper-bounds on the probability\nthat the LRPC decoding algorithm fails.",
        "translated": "低等级奇偶校验(LRPC)码是一类用于设计公钥加密方案的等级度量纠错码。LRPC 码由一个奇偶校验矩阵定义，该矩阵的条目属于一个大型有限域的相对低维向量子空间。这个特殊的代数特征，然后可以用来纠正高概率秩误差时，参数选择适当。本文给出了 LRPC 译码算法失败概率的理论上界。"
    },
    {
        "title": "Cryptanalysis of protocols using (Simultaneous) Conjugacy Search Problem\n  in certain Metabelian Platform Groups",
        "url": "http://arxiv.org/abs/2309.13928v1",
        "pub_date": "2023-09-25",
        "summary": "There are many group-based cryptosystems in which the security relies on the\ndifficulty of solving Conjugacy Search Problem (CSP) and Simultaneous Conjugacy\nSearch Problem (SCSP) in their underlying platform groups. In this paper we\ngive a cryptanalysis of these systems which use certain semidirect product of\nabelian groups.",
        "translated": "在许多基于组的密码体制中，其安全性依赖于在其底层平台组中求解共轭搜索问题(CSP)和同时共轭搜索问题(SCSP)的难度。在本文中，我们给出了这些系统的密码分析，它们使用了一定的交换群半直积。"
    },
    {
        "title": "PA-iMFL: Communication-Efficient Privacy Amplification Method against\n  Data Reconstruction Attack in Improved Multi-Layer Federated Learning",
        "url": "http://arxiv.org/abs/2309.13864v1",
        "pub_date": "2023-09-25",
        "summary": "Recently, big data has seen explosive growth in the Internet of Things (IoT).\nMulti-layer FL (MFL) based on cloud-edge-end architecture can promote model\ntraining efficiency and model accuracy while preserving IoT data privacy. This\npaper considers an improved MFL, where edge layer devices own private data and\ncan join the training process. iMFL can improve edge resource utilization and\nalso alleviate the strict requirement of end devices, but suffers from the\nissues of Data Reconstruction Attack (DRA) and unacceptable communication\noverhead. This paper aims to address these issues with iMFL. We propose a\nPrivacy Amplification scheme on iMFL (PA-iMFL). Differing from standard MFL, we\ndesign privacy operations in end and edge devices after local training,\nincluding three sequential components, local differential privacy with Laplace\nmechanism, privacy amplification subsample, and gradient sign reset.\nBenefitting from privacy operations, PA-iMFL reduces communication overhead and\nachieves privacy-preserving. Extensive results demonstrate that against\nState-Of-The-Art (SOTA) DRAs, PA-iMFL can effectively mitigate private data\nleakage and reach the same level of protection capability as the SOTA defense\nmodel. Moreover, due to adopting privacy operations in edge devices, PA-iMFL\npromotes up to 2.8 times communication efficiency than the SOTA compression\nmethod without compromising model accuracy.",
        "translated": "最近，物联网(IoT)中的大数据出现了爆炸性的增长。基于云端架构的多层 FL (MFL)可以在保护物联网数据隐私的同时提高模型训练效率和模型精度。本文考虑了一种改进的漏磁检测算法，其中边缘层设备拥有私有数据并可以加入训练过程。IMFL 可以提高边缘资源的利用率，减轻对终端设备的严格要求，但存在数据重构攻击(DRA)和不可接受的通信开销等问题。本文旨在用 iMFL 解决这些问题。提出了一种基于 iMFL (PA-iMFL)的隐私扩展方案。与标准漏磁检测不同的是，我们在本地培训后设计末端和边缘设备的隐私操作，包括三个顺序组件、拉普拉斯机制的本地差分隐私、隐私放大子样本和梯度标志重置。受益于隐私操作，PA-iMFL 减少了通信开销，实现了隐私保护。大量的实验结果表明，针对最先进的(SOTA) DRA，PA-iMFL 可以有效地减少私有数据泄漏，并达到与 SOTA 防御模型相同的保护能力水平。此外，由于在边缘设备中采用了隐私操作，PA-iMFL 在不影响模型精度的情况下，比 SOTA 压缩方法提高了高达2.8倍的通信效率。"
    },
    {
        "title": "On the Effectiveness of Adversarial Samples against Ensemble\n  Learning-based Windows PE Malware Detectors",
        "url": "http://arxiv.org/abs/2309.13841v1",
        "pub_date": "2023-09-25",
        "summary": "Recently, there has been a growing focus and interest in applying machine\nlearning (ML) to the field of cybersecurity, particularly in malware detection\nand prevention. Several research works on malware analysis have been proposed,\noffering promising results for both academic and practical applications. In\nthese works, the use of Generative Adversarial Networks (GANs) or Reinforcement\nLearning (RL) can aid malware creators in crafting metamorphic malware that\nevades antivirus software. In this study, we propose a mutation system to\ncounteract ensemble learning-based detectors by combining GANs and an RL model,\novercoming the limitations of the MalGAN model. Our proposed FeaGAN model is\nbuilt based on MalGAN by incorporating an RL model called the Deep Q-network\nanti-malware Engines Attacking Framework (DQEAF). The RL model addresses three\nkey challenges in performing adversarial attacks on Windows Portable Executable\nmalware, including format preservation, executability preservation, and\nmaliciousness preservation. In the FeaGAN model, ensemble learning is utilized\nto enhance the malware detector's evasion ability, with the generated\nadversarial patterns. The experimental results demonstrate that 100\\% of the\nselected mutant samples preserve the format of executable files, while certain\nsuccesses in both executability preservation and maliciousness preservation are\nachieved, reaching a stable success rate.",
        "translated": "近年来，机器学习(ML)在网络安全领域的应用越来越受到关注，尤其是在恶意软件的检测和预防方面。一些恶意软件分析的研究工作已经提出，提供了有希望的结果，为学术和实际应用。在这些工作中，使用生成对抗网络(gANs)或强化学习(RL)可以帮助恶意软件创造者制造变形恶意软件，以规避杀毒软件。在这项研究中，我们提出了一个突变系统来抵消集成学习的检测器相结合的 GAN 和 RL 模型，克服了 MalGAN 模型的局限性。我们提出的 FeaGAN 模型是建立在 MalGAN 的基础上，通过结合 RL 模型称为深 Q 网络反恶意软件引擎攻击框架(DQEAF)。RL 模型解决了对 Windows Portable Executable 恶意软件进行对抗性攻击的三个关键挑战，包括格式保护、可执行性保护和恶意保护。在 FeaGAN 模型中，集成学习被用来增强恶意软件检测器的逃避能力，并产生对抗模式。实验结果表明，所选择的突变样本100% 保持了可执行文件的格式，同时在可执行性保持和恶意性保持方面都取得了一定的成功，达到了稳定的成功率。"
    },
    {
        "title": "Privacy-preserving and Privacy-attacking Approaches for Speech and Audio\n  -- A Survey",
        "url": "http://arxiv.org/abs/2309.15087v1",
        "pub_date": "2023-09-26",
        "summary": "In contemporary society, voice-controlled devices, such as smartphones and\nhome assistants, have become pervasive due to their advanced capabilities and\nfunctionality. The always-on nature of their microphones offers users the\nconvenience of readily accessing these devices. However, recent research and\nevents have revealed that such voice-controlled devices are prone to various\nforms of malicious attacks, hence making it a growing concern for both users\nand researchers to safeguard against such attacks. Despite the numerous studies\nthat have investigated adversarial attacks and privacy preservation for images,\na conclusive study of this nature has not been conducted for the audio domain.\nTherefore, this paper aims to examine existing approaches for\nprivacy-preserving and privacy-attacking strategies for audio and speech. To\nachieve this goal, we classify the attack and defense scenarios into several\ncategories and provide detailed analysis of each approach. We also interpret\nthe dissimilarities between the various approaches, highlight their\ncontributions, and examine their limitations. Our investigation reveals that\nvoice-controlled devices based on neural networks are inherently susceptible to\nspecific types of attacks. Although it is possible to enhance the robustness of\nsuch models to certain forms of attack, more sophisticated approaches are\nrequired to comprehensively safeguard user privacy.",
        "translated": "在当今社会，智能手机和家庭助理等语音控制设备由于其先进的性能和功能而变得无处不在。他们的麦克风总是开着的特性为用户提供了方便，可以随时访问这些设备。然而，最近的研究和事件表明，这种语音控制设备容易受到各种形式的恶意攻击，因此使用户和研究人员越来越关注如何防范这种攻击。尽管许多研究已经调查了对抗性攻击和图像隐私保护，这种性质的结论性研究尚未进行的音频领域。因此，本文旨在研究现有的音频和语音隐私保护和隐私攻击策略。为了实现这个目标，我们将攻击和防御场景分为几类，并对每种方法进行了详细的分析。我们还解释了不同方法之间的差异，突出了它们的贡献，并检查了它们的局限性。我们的研究表明，基于神经网络的语音控制设备天生容易受到特定类型的攻击。虽然有可能增强此类模型对某些形式的攻击的稳健性，但需要更复杂的方法来全面保护用户隐私。"
    },
    {
        "title": "Logic Locking based Trojans: A Friend Turns Foe",
        "url": "http://arxiv.org/abs/2309.15067v1",
        "pub_date": "2023-09-26",
        "summary": "Logic locking and hardware Trojans are two fields in hardware security that\nhave been mostly developed independently from each other. In this paper, we\nidentify the relationship between these two fields. We find that a common\nstructure that exists in many logic locking techniques has desirable properties\nof hardware Trojans (HWT). We then construct a novel type of HWT, called\nTrojans based on Logic Locking (TroLL), in a way that can evade\nstate-of-the-art ATPG-based HWT detection techniques. In an effort to detect\nTroLL, we propose customization of existing state-of-the-art ATPG-based HWT\ndetection approaches as well as adapting the SAT-based attacks on logic locking\nto HWT detection. In our experiments, we use random sampling as reference. It\nis shown that the customized ATPG-based approaches are the best performing but\nonly offer limited improvement over random sampling. Moreover, their efficacy\nalso diminishes as TroLL's triggers become longer, i.e., have more bits\nspecified). We thereby highlight the need to find a scalable HWT detection\napproach for TroLL.",
        "translated": "逻辑锁和硬件木马是硬件安全的两个领域，它们大多是相互独立开发的。在本文中，我们确定了这两个领域之间的关系。我们发现，存在于许多逻辑锁技术中的一种常见结构具有硬件特洛伊木马(HWT)的理想特性。然后，我们构建了一种新型的 HWT，称为基于逻辑锁的木马(Trojans based of Logic Lock，TroLL) ，它可以避开基于 ATPG 的最先进的 HWT 检测技术。为了检测 TroLL，我们提出了定制现有的基于 ATPG 的 HWT 检测方法，并将基于 SAT 的逻辑锁攻击应用到 HWT 检测中。在我们的实验中，我们使用随机抽样作为参考。结果表明，定制的基于 ATPG 的方法是最好的，但只能提供有限的改进比随机抽样。此外，当 TroLL 的触发器变得更长，即指定的位数更多时，它们的功效也会减弱)。因此，我们强调需要为 TroLL 找到一种可伸缩的 HWT 检测方法。"
    },
    {
        "title": "A Quantitative Information Flow Analysis of the Topics API",
        "url": "http://arxiv.org/abs/2309.14746v1",
        "pub_date": "2023-09-26",
        "summary": "Third-party cookies have been a privacy concern since cookies were first\ndeveloped in the mid 1990s, but more strict cookie policies were only\nintroduced by Internet browser vendors in the early 2010s. More recently, due\nto regulatory changes, browser vendors have started to completely block\nthird-party cookies, with both Firefox and Safari already compliant.\n  The Topics API is being proposed by Google as an additional and less\nintrusive source of information for interest-based advertising (IBA), following\nthe upcoming deprecation of third-party cookies. Initial results published by\nGoogle estimate the probability of a correct re-identification of a random\nindividual would be below 3% while still supporting IBA.\n  In this paper, we analyze the re-identification risk for individual Internet\nusers introduced by the Topics API from the perspective of Quantitative\nInformation Flow (QIF), an information- and decision-theoretic framework. Our\nmodel allows a theoretical analysis of both privacy and utility aspects of the\nAPI and their trade-off, and we show that the Topics API does have better\nprivacy than third-party cookies. We leave the utility analyses for future\nwork.",
        "translated": "自从20世纪90年代中期第一次开发出 Cookie 以来，第三方 Cookie 一直是一个隐私问题，但是更严格的 Cookie 政策只是在2010年代初才由互联网浏览器供应商引入。最近，由于法规的改变，浏览器厂商已经开始完全屏蔽第三方 cookie，Firefox 和 Safari 都已经兼容了。随着第三方 cookie 即将退出市场，Google 提出将 Topics API 作为基于兴趣的广告(IBA)的额外信息来源，并减少其对用户的干扰。谷歌公布的最初结果估计，在支持 IBA 的情况下，对随机个体进行正确重新识别的概率将低于3% 。本文从定量信息流(QIF)这一信息决策理论框架出发，分析了 Topics API 引入的个人互联网用户重新识别风险。我们的模型允许对 API 的隐私和实用方面以及它们之间的权衡进行理论分析，并且我们证明了 Topics API 确实比第三方 cookie 具有更好的隐私性。我们将效用分析留给将来的工作。"
    },
    {
        "title": "SyzTrust: State-aware Fuzzing on Trusted OS Designed for IoT Devices",
        "url": "http://arxiv.org/abs/2309.14742v1",
        "pub_date": "2023-09-26",
        "summary": "Trusted Execution Environments (TEEs) embedded in IoT devices provide a\ndeployable solution to secure IoT applications at the hardware level. By\ndesign, in TEEs, the Trusted Operating System (Trusted OS) is the primary\ncomponent. It enables the TEE to use security-based design techniques, such as\ndata encryption and identity authentication. Once a Trusted OS has been\nexploited, the TEE can no longer ensure security. However, Trusted OSes for IoT\ndevices have received little security analysis, which is challenging from\nseveral perspectives: (1) Trusted OSes are closed-source and have an\nunfavorable environment for sending test cases and collecting feedback. (2)\nTrusted OSes have complex data structures and require a stateful workflow,\nwhich limits existing vulnerability detection tools. To address the challenges,\nwe present SyzTrust, the first state-aware fuzzing framework for vetting the\nsecurity of resource-limited Trusted OSes. SyzTrust adopts a hardware-assisted\nframework to enable fuzzing Trusted OSes directly on IoT devices as well as\ntracking state and code coverage non-invasively. SyzTrust utilizes composite\nfeedback to guide the fuzzer to effectively explore more states as well as to\nincrease the code coverage. We evaluate SyzTrust on Trusted OSes from three\nmajor vendors: Samsung, Tsinglink Cloud, and Ali Cloud. These systems run on\nCortex M23/33 MCUs, which provide the necessary abstraction for embedded TEEs.\nWe discovered 70 previously unknown vulnerabilities in their Trusted OSes,\nreceiving 10 new CVEs so far. Furthermore, compared to the baseline, SyzTrust\nhas demonstrated significant improvements, including 66% higher code coverage,\n651% higher state coverage, and 31% improved vulnerability-finding capability.\nWe report all discovered new vulnerabilities to vendors and open source\nSyzTrust.",
        "translated": "嵌入在物联网设备中的可信执行环境(Trusted Execution Environment，TEE)提供了一种可部署的解决方案，可以在硬件层面上保护物联网应用程序的安全。根据设计，在 TEE 中，可信操作系统(Trusted OS)是主要组件。它使得 TEE 能够使用基于安全的设计技术，例如数据加密和身份验证。一旦使用了受信任的操作系统，TEE 就不能再确保安全性。然而，用于物联网设备的可信操作系统几乎没有收到安全分析，这从几个方面提出了挑战: (1)可信操作系统是封闭源代码的，对于发送测试用例和收集反馈有一个不利的环境。(2)可信操作系统具有复杂的数据结构，需要有状态的工作流，限制了现有的漏洞检测工具。为了应对这些挑战，我们介绍了 SyzTrust，这是第一个国家意识的模糊框架，用于审查资源有限的可信操作系统的安全性。SyzTrust 采用了一个硬件辅助框架，可以直接在物联网设备上模糊可信操作系统，并且可以非侵入性地跟踪状态和代码覆盖率。SyzTrust 利用复合反馈来引导模糊器有效地探索更多的状态并增加代码覆盖率。我们对 SyzTrust 的可信操作系统进行评估，来自三大供应商: 三星、青岛云和阿里云。这些系统运行在 Cortex M23/33 MCU 上，为嵌入式 TEE 提供必要的抽象。我们在他们的可信操作系统中发现了70个以前未知的漏洞，到目前为止收到了10个新的 CVE。此外，与基线相比，SyzTrust 显示出了显著的改进，包括代码覆盖率提高了66% ，状态覆盖率提高了651% ，漏洞发现能力提高了31% 。我们报告所有发现的供应商和开源 SyzTrust 的新漏洞。"
    },
    {
        "title": "XGV-BERT: Leveraging Contextualized Language Model and Graph Neural\n  Network for Efficient Software Vulnerability Detection",
        "url": "http://arxiv.org/abs/2309.14677v1",
        "pub_date": "2023-09-26",
        "summary": "With the advancement of deep learning (DL) in various fields, there are many\nattempts to reveal software vulnerabilities by data-driven approach.\nNonetheless, such existing works lack the effective representation that can\nretain the non-sequential semantic characteristics and contextual relationship\nof source code attributes. Hence, in this work, we propose XGV-BERT, a\nframework that combines the pre-trained CodeBERT model and Graph Neural Network\n(GCN) to detect software vulnerabilities. By jointly training the CodeBERT and\nGCN modules within XGV-BERT, the proposed model leverages the advantages of\nlarge-scale pre-training, harnessing vast raw data, and transfer learning by\nlearning representations for training data through graph convolution. The\nresearch results demonstrate that the XGV-BERT method significantly improves\nvulnerability detection accuracy compared to two existing methods such as\nVulDeePecker and SySeVR. For the VulDeePecker dataset, XGV-BERT achieves an\nimpressive F1-score of 97.5%, significantly outperforming VulDeePecker, which\nachieved an F1-score of 78.3%. Again, with the SySeVR dataset, XGV-BERT\nachieves an F1-score of 95.5%, surpassing the results of SySeVR with an\nF1-score of 83.5%.",
        "translated": "随着深度学习(DL)在各个领域的发展，越来越多的研究试图通过数据驱动的方法来揭示软件的脆弱性。然而，现有的这些著作缺乏能够保留源代码属性的非顺序语义特征和上下文关系的有效表示。因此，在本研究中，我们提出 XGV-BERT，一个结合预先训练的 CodeBERT 模型与图形神经网路(GCN)来侦测软体漏洞的架构。通过在 XGV-BERT 中联合训练 CodeBERT 和 GCN 模块，提出的模型利用了大规模预训练、利用大量原始数据以及通过图卷积学习训练数据表示的转移学习的优势。研究结果表明，与 VulDeePecker 和 SySeVR 方法相比，XGV-BERT 方法显著提高了漏洞检测的准确性。对于 VulDeePecker 数据集，XGV-BERT 获得了令人印象深刻的97.5% 的 F1评分，明显优于 VulDeePecker，后者获得了78.3% 的 F1评分。同样，对于 SySeVR 数据集，XGV-BERT 达到了95.5% 的 F1评分，以83.5% 的 F1评分超过了 SySeVR 的结果。"
    },
    {
        "title": "A Public Key Infrastructure for 5G Service-Based Architecture",
        "url": "http://arxiv.org/abs/2309.14659v1",
        "pub_date": "2023-09-26",
        "summary": "The 3GPP 5G Service-based Architecture (SBA) security specifications leave\nseveral details on how to setup an appropriate Public Key Infrastructure (PKI)\nfor 5G SBA, unspecified. In this work, we propose 5G-SBA-PKI, a public key\ninfrastructure for secure inter-NF communication in 5G SBA core networks, where\nNF refers to Network Functions. 5G-SBA-PKI is designed to include multiple\ncertificate authorities (with different scopes of operation and capabilities)\nat different PLMN levels for certification operations and key exchange between\ncommunicating NFs, where PLMN refers to a Public Land Mobile Network. We\nconduct a formal analysis of 5G-SBA-PKI with respect to the desired security\nproperties using TAMARIN prover. Finally, we evaluate 5G-SBA-PKI's performance\nwith \"pre-quantum\" as well as quantum-safe cryptographic algorithms.",
        "translated": "3GPP 5G 基于服务的体系结构(SBA)安全规范对于如何为5G 基于服务的体系结构(SBA)设置适当的公钥基础设施(PKI)留下了一些细节，但没有具体说明。在这项工作中，我们提出了5G-SBA-PKI，一个用于在5G SBA 核心网络中保护 NF 间通信的公钥基础设施，其中 NF 指的是网络功能。5G-SBA-PKI 旨在包括不同 PLMN 级别的多个证书颁发机构(具有不同的业务范围和能力) ，用于认证业务和通信 NF 之间的密钥交换，PLMN 指的是公共陆地移动网络。我们使用 TAMARIN 验证器对5G-SBA-PKI 进行了正式的安全性分析。最后，我们评估了5G-SBA-PKI 的性能与“前量子”以及量子安全的密码算法。"
    },
    {
        "title": "Concept and Construction of Group Signature with self-proof capacity for\n  confirming and denying",
        "url": "http://arxiv.org/abs/2309.14635v1",
        "pub_date": "2023-09-26",
        "summary": "With privacy-preserving and traceability properties, group signature is a\ncryptosystem with central role in cryptography. And there are lots of\napplication scenarios. A new extension concept of group signature is presented,\nnamely group signature with self-proof capacity. For a legitimate group\nsignature, the real signer can prove that the signature is indeed signed by\nhim/her. While for the other members of the group, they can prove that the\nsignature is not signed by him/her. The former can be used for claiming money\nreward from the police, while the latter can be used for proving one's innocent\nin a criminal investigation.",
        "translated": "群签名是一个具有隐私保护和可追踪性的密码体制，在密码学中占有核心地位。还有很多应用场景。提出了一种新的群签名扩展概念，即具有自证明能力的群签名。对于一个合法的群签名，真正的签名人可以证明签名确实是由他/她签署的。至于团体的其他成员，他们可以证明签名并非由他/她签署。前者可用于向警方索取金钱报酬，而后者可用于在刑事侦查中证明自己的清白。"
    },
    {
        "title": "DifAttack: Query-Efficient Black-Box Attack via Disentangled Feature\n  Space",
        "url": "http://arxiv.org/abs/2309.14585v1",
        "pub_date": "2023-09-26",
        "summary": "This work investigates efficient score-based black-box adversarial attacks\nwith a high Attack Success Rate (ASR) and good generalizability. We design a\nnovel attack method based on a Disentangled Feature space, called DifAttack,\nwhich differs significantly from the existing ones operating over the entire\nfeature space. Specifically, DifAttack firstly disentangles an image's latent\nfeature into an adversarial feature and a visual feature, where the former\ndominates the adversarial capability of an image, while the latter largely\ndetermines its visual appearance. We train an autoencoder for the\ndisentanglement by using pairs of clean images and their Adversarial Examples\n(AEs) generated from available surrogate models via white-box attack methods.\nEventually, DifAttack iteratively optimizes the adversarial feature according\nto the query feedback from the victim model until a successful AE is generated,\nwhile keeping the visual feature unaltered. In addition, due to the avoidance\nof using surrogate models' gradient information when optimizing AEs for\nblack-box models, our proposed DifAttack inherently possesses better attack\ncapability in the open-set scenario, where the training dataset of the victim\nmodel is unknown. Extensive experimental results demonstrate that our method\nachieves significant improvements in ASR and query efficiency simultaneously,\nespecially in the targeted attack and open-set scenarios. The code will be\navailable at https://github.com/csjunjun/DifAttack.git soon.",
        "translated": "本文研究具有高攻击成功率(ASR)和良好推广性的基于分数的高效黑盒对抗攻击。我们设计了一种新的基于分离特征空间的攻击方法，称为分离攻击，它与现有的在整个特征空间上运行的攻击方法有很大的不同。具体来说，DifEvolution 首先将图像的潜在特征分解为对抗特征和视觉特征，前者主导图像的对抗能力，而后者在很大程度上决定了图像的视觉外观。我们训练一个自动编码器的解纠缠使用清洁的图像对和它们的对抗性示例(AE)生成可用的代理模型通过白盒攻击方法。最终，DifStrike 根据来自受害者模型的查询反馈迭代优化对抗特征，直到成功生成 AE，同时保持视觉特征不变。此外，由于在优化黑盒模型的 AE 时避免使用替代模型的梯度信息，所以我们提出的 DifAttack 在开放集场景中固有地具有更好的攻击能力，其中受害者模型的训练数据集是未知的。大量的实验结果表明，该方法在提高 ASR 和查询效率的同时，取得了显著的效果，特别是在目标攻击和开集场景中。密码很快就会在 https://github.com/csjunjun/difattack.git 公布。"
    },
    {
        "title": "Assessing Utility of Differential Privacy for RCTs",
        "url": "http://arxiv.org/abs/2309.14581v1",
        "pub_date": "2023-09-26",
        "summary": "Randomized control trials, RCTs, have become a powerful tool for assessing\nthe impact of interventions and policies in many contexts. They are considered\nthe gold-standard for inference in the biomedical fields and in many social\nsciences. Researchers have published an increasing number of studies that rely\non RCTs for at least part of the inference, and these studies typically include\nthe response data collected, de-identified and sometimes protected through\ntraditional disclosure limitation methods. In this paper, we empirically assess\nthe impact of strong privacy-preservation methodology (with \\ac{DP}\nguarantees), on published analyses from RCTs, leveraging the availability of\nreplication packages (research compendia) in economics and policy analysis. We\nprovide simulations studies and demonstrate how we can replicate the analysis\nin a published economics article on privacy-protected data under various\nparametrizations. We find that relatively straightforward DP-based methods\nallow for inference-valid protection of the published data, though\ncomputational issues may limit more complex analyses from using these methods.\nThe results have applicability to researchers wishing to share RCT data,\nespecially in the context of low- and middle-income countries, with strong\nprivacy protection.",
        "translated": "随机对照试验(RCT)已成为评估干预措施和政策在许多情况下的影响的有力工具。它们被认为是生物医学领域和许多社会科学推理的黄金标准。研究人员发表了越来越多的研究，这些研究至少部分依赖于随机对照试验，这些研究通常包括收集的反应数据，去识别，有时通过传统的披露限制方法保护。在本文中，我们利用经济学和政策分析中复制包(研究纲要)的可用性，实证评估了强大的隐私保护方法(ac { DP }保证)对随机对照试验发表的分析的影响。我们提供了模拟研究，并展示了我们如何在一篇发表的关于隐私保护数据的经济学文章中在各种参数化情况下复制分析。我们发现相对简单的基于 DP 的方法允许对已发表的数据进行推理有效的保护，尽管计算问题可能限制使用这些方法进行更复杂的分析。研究结果适用于希望共享随机对照试验数据的研究人员，特别是在具有强烈隐私保护的低收入和中等收入国家。"
    },
    {
        "title": "ADESS: A Proof-of-Work Protocol to Deter Double-Spend Attacks",
        "url": "http://arxiv.org/abs/2309.14551v1",
        "pub_date": "2023-09-25",
        "summary": "A principal vulnerability of a proof-of-work (\"PoW\") blockchain is that an\nattacker can re-write the history of transactions by forking a previously\npublished block and build a new chain segment containing a different sequence\nof transactions. If the attacker's chain has the most cumulative mining puzzle\ndifficulty, nodes will recognize it as canonical. We propose a modification to\nPoW protocols, called ADESS, that contains two novel features. The first\nmodification enables a node to identify the attacker chain by comparing the\ntemporal sequence of blocks on competing chains. The second modification\npenalizes the attacker by requiring it to apply exponentially increasing\nhashrate in order to make its chain canonical. We demonstrate two things; (i)\nthe expected cost of carrying out a double-spend attack is weakly higher under\nADESS compared to the current PoW protocols and (ii) for any value of\ntransaction, there is a penalty setting in ADESS that renders the expected\nprofit of a double-spend attack negative.",
        "translated": "工作证明(“ PoW”)区块链的一个主要漏洞是，攻击者可以通过分叉先前发布的区块来重写事务的历史，并构建一个包含不同事务序列的新链段。如果攻击者的链具有最大的累积挖掘难题，那么节点将把它识别为规范的。我们提议对 PoW 协议进行一个修改，称为 ADESS，其中包含两个新特性。第一个修改允许节点通过比较竞争链上的块的时间序列来识别攻击链。第二个修改通过要求攻击者应用指数增长的 hashrate 来惩罚攻击者，以使其链规范化。我们证明了两件事情: (i)与目前的 PoW 协议相比，在 ADESS 下执行双重花费攻击的预期成本稍高; (ii)对于任何交易的价值，在 ADESS 中有一个惩罚设置，使得双重花费攻击的预期利润为负。"
    },
    {
        "title": "Some Efficient and Optimal K-Norm Mechanisms",
        "url": "http://arxiv.org/abs/2309.15790v1",
        "pub_date": "2023-09-27",
        "summary": "A differentially private computation often begins with a bound on a\n$d$-dimensional statistic's $\\ell_p$ sensitivity. The $K$-norm mechanism can\nyield more accurate additive noise by using a statistic-specific (and possibly\nnon-$\\ell_p$) norm. However, sampling such mechanisms requires sampling from\nthe corresponding norm balls. These are $d$-dimensional convex polytopes, and\nthe fastest known general algorithm for approximately sampling such polytopes\ntakes time $\\tilde O(d^{3+\\omega})$, where $\\omega \\geq 2$ is the matrix\nmultiplication exponent. For the simple problems of sum and ranked vote, this\npaper constructs samplers that run in time $\\tilde O(d^2)$. More broadly, we\nsuggest that problem-specific $K$-norm mechanisms may be an overlooked\npractical tool for private additive noise.",
        "translated": "差异私有计算通常以 $d $- 维统计量的 $ell _ p $灵敏度的界开始。$K $规范机制可以通过使用统计特定的(可能是非 $ell _ p $)规范来产生更精确的加性噪声。然而，取样这样的机制需要从相应的标准球取样。这些是 $d $- 维凸多边形，已知最快的近似抽样通用算法需要时间 $tilde o (d ^ {3 + omega }) $，其中 $omega geq 2 $是矩阵乘法指数。对于和与排序表决的简单问题，本文构造了时间运行的采样器。更广泛地说，我们认为问题特定的 $K $- 规范机制可能是一个被忽视的实用工具，用于私有的附加噪声。"
    },
    {
        "title": "Breaking NoC Anonymity using Flow Correlation Attack",
        "url": "http://arxiv.org/abs/2309.15687v1",
        "pub_date": "2023-09-27",
        "summary": "Network-on-Chip (NoC) is widely used as the internal communication fabric in\ntoday's multicore System-on-Chip (SoC) designs. Security of the on-chip\ncommunication is crucial because exploiting any vulnerability in shared NoC\nwould be a goldmine for an attacker. NoC security relies on effective\ncountermeasures against diverse attacks. We investigate the security strength\nof existing anonymous routing protocols in NoC architectures. Specifically,\nthis paper makes two important contributions. We show that the existing\nanonymous routing is vulnerable to machine learning (ML) based flow correlation\nattacks on NoCs. We propose a lightweight anonymous routing that use traffic\nobfuscation techniques which can defend against ML-based flow correlation\nattacks. Experimental studies using both real and synthetic traffic reveal that\nour proposed attack is successful against state-of-the-art anonymous routing in\nNoC architectures with a high accuracy (up to 99%) for diverse traffic\npatterns, while our lightweight countermeasure can defend against ML-based\nattacks with minor hardware and performance overhead.",
        "translated": "片上网络(Network-on-Chip，NoC)作为当今多核系统芯片(System-on-Chip，SoC)设计中广泛使用的内部通信结构。片上通信的安全性至关重要，因为利用共享 NoC 中的任何漏洞都将是攻击者的金矿。NoC 安全依赖于针对不同攻击的有效对策。我们研究了 NoC 体系结构中现有匿名路由协议的安全强度。具体而言，本文做出了两个重要贡献。我们发现现有的匿名路由易受基于机器学习(ML)的流相关攻击。提出了一种轻量级匿名路由算法，该算法采用流量模糊技术，能够抵御基于 ML 的流量相关攻击。使用实际和合成流量的实验研究表明，我们提出的攻击能够成功地对抗 NoC 架构中最先进的匿名路由，对不同流量模式的准确率高达99% ，而我们的轻量级对策能够以较小的硬件和性能开销抵御基于 ML 的攻击。"
    },
    {
        "title": "Grain-128PLE: Generic Physical-Layer Encryption for IoT Networks",
        "url": "http://arxiv.org/abs/2309.15569v1",
        "pub_date": "2023-09-27",
        "summary": "Physical layer security (PLS) encompasses techniques proposed at the physical\nlayer to achieve information security objectives while requiring a minimal\nresource footprint. The channel coding-based secrecy and signal\nmodulation-based encryption approaches are reliant on certain channel\nconditions or a certain communications protocol stack to operate on, which\nprevents them from being a generic solution. This paper presents Grain-128PLE,\na lightweight physical layer encryption (PLE) scheme that is derived from the\nGrain-128AEAD v2 stream cipher. The Grain-128PLE stream cipher performs\nencryption and decryption at the physical layer, in between the channel coding\nand signal modulation processes. This placement, like that of the A5 stream\ncipher that had been used in the GSM communications standard, makes it a\ngeneric solution for providing data confidentiality in IoT networks. The design\nof Grain-128PLE maintains the structure of the main building blocks of the\noriginal Grain-128AEAD v2 stream cipher, evaluated for its security strength\nduring NIST's recent Lightweight Cryptography competition, and is therefore\nexpected to achieve similar levels of security.",
        "translated": "物理层安全(PLS)包括在物理层提出的技术，以实现信息安全目标，同时需要最小的资源占用。基于信道编码的保密和基于信号调制的加密方法依赖于特定的信道条件或特定的通信协议栈进行操作，这使得它们不能成为通用的解决方案。本文提出了一种基于粒子128AEAD v2流密码的轻量级物理层加密方案——粒子128PLE。在信道编码和信号调制过程之间的物理层上，粒度128PLE 流密码执行加密和解密。这种位置，就像在 GSM 通信标准中使用的 A5流密码一样，使它成为物联网中提供数据保密性的通用解决方案。128PLE 的设计保持了最初的128AEAD v2流密码的主要组成部分的结构，在 NIST 最近的轻量级密码比赛中评估了其安全强度，因此预计将达到类似的安全级别。"
    },
    {
        "title": "Cyber Security Requirements for Platforms Enhancing AI Reproducibility",
        "url": "http://arxiv.org/abs/2309.15525v1",
        "pub_date": "2023-09-27",
        "summary": "Scientific research is increasingly reliant on computational methods, posing\nchallenges for ensuring research reproducibility. This study focuses on the\nfield of artificial intelligence (AI) and introduces a new framework for\nevaluating AI platforms for reproducibility from a cyber security standpoint to\naddress the security challenges associated with AI research. Using this\nframework, five popular AI reproducibility platforms; Floydhub, BEAT, Codalab,\nKaggle, and OpenML were assessed. The analysis revealed that none of these\nplatforms fully incorporates the necessary cyber security measures essential\nfor robust reproducibility. Kaggle and Codalab, however, performed better in\nterms of implementing cyber security measures covering aspects like security,\nprivacy, usability, and trust. Consequently, the study provides tailored\nrecommendations for different user scenarios, including individual researchers,\nsmall laboratories, and large corporations. It emphasizes the importance of\nintegrating specific cyber security features into AI platforms to address the\nchallenges associated with AI reproducibility, ultimately advancing\nreproducibility in this field. Moreover, the proposed framework can be applied\nbeyond AI platforms, serving as a versatile tool for evaluating a wide range of\nsystems and applications from a cyber security perspective.",
        "translated": "科学研究越来越依赖于计算方法，为确保研究的可重复性提出了挑战。本文以人工智能领域为研究对象，从网络安全的角度出发，提出了一个评估人工智能平台可重复性的新框架，以解决人工智能研究面临的安全挑战。使用这个框架，我们评估了五个流行的人工智能再现性平台: Floydhub、 BEAT、 Codalab、 Kaggle 和 OpenML。分析表明，这些平台中没有一个完全包含对强大的可重复性至关重要的必要网络安全措施。然而，Kaggle 和 Codalab 在实施网络安全措施方面表现得更好，这些措施涵盖了安全、隐私、可用性和信任等方面。因此，该研究为不同的用户场景提供了量身定制的建议，包括个别研究人员、小型实验室和大型公司。报告强调，必须将具体的网络安全特征纳入人工智能平台，以应对与人工智能可重复性相关的挑战，最终推进这一领域的可重复性。此外，拟议的框架可应用于人工智能平台以外的领域，成为从网络安全角度评估各种系统和应用程序的通用工具。"
    },
    {
        "title": "Raijū: Reinforcement Learning-Guided Post-Exploitation for Automating\n  Security Assessment of Network Systems",
        "url": "http://arxiv.org/abs/2309.15518v1",
        "pub_date": "2023-09-27",
        "summary": "In order to assess the risks of a network system, it is important to\ninvestigate the behaviors of attackers after successful exploitation, which is\ncalled post-exploitation. Although there are various efficient tools supporting\npost-exploitation implementation, no application can automate this process.\nMost of the steps of this process are completed by experts who have profound\nknowledge of security, known as penetration testers or pen-testers. To this\nend, our study proposes the Raij\\=u framework, a Reinforcement Learning\n(RL)-driven automation approach that assists pen-testers in quickly\nimplementing the process of post-exploitation for security-level evaluation in\nnetwork systems. We implement two RL algorithms, Advantage Actor-Critic (A2C)\nand Proximal Policy Optimization (PPO), to train specialized agents capable of\nmaking intelligent actions, which are Metasploit modules to automatically\nlaunch attacks of privileges escalation, gathering hashdump, and lateral\nmovement. By leveraging RL, we aim to empower these agents with the ability to\nautonomously select and execute actions that can exploit vulnerabilities in\ntarget systems. This approach allows us to automate certain aspects of the\npenetration testing workflow, making it more efficient and responsive to\nemerging threats and vulnerabilities. The experiments are performed in four\nreal environments with agents trained in thousands of episodes. The agents\nautomatically select actions and launch attacks on the environments and achieve\nover 84\\% of successful attacks with under 55 attack steps given. Moreover, the\nA2C algorithm has proved extremely effective in the selection of proper actions\nfor automation of post-exploitation.",
        "translated": "为了评估网络系统的风险，研究攻击者在攻击成功后的行为，即后期攻击行为是非常重要的。尽管有各种有效的工具支持后期开发实现，但是没有应用程序能够自动化这个过程。这个过程的大部分步骤是由对安全性有深刻认识的专家完成的，这些专家被称为渗透测试人员或笔测试人员。为此，我们的研究提出了 Raij = u 框架，这是一种强化学习驱动的自动化方法，可以帮助笔测试者快速实现网络系统安全级别评估的后期开发过程。我们实现了两个 RL 算法，Advantage Actor-Critic (A2C)和 Proximal Policy Optimation (PPO) ，来训练能够进行智能操作的专门代理，这些代理是 Metasploit 模块，可以自动发起特权升级、收集 hashdump 和横向移动的攻击。通过利用 RL，我们的目标是赋予这些代理自主选择和执行能够利用目标系统漏洞的动作的能力。这种方法允许我们自动化渗透测试工作流的某些方面，使其更有效率，并对新出现的威胁和漏洞作出响应。这些实验在四个真实的环境中进行，实验人员经过数千集的训练。代理自动选择操作并发动对环境的攻击，在给定的55个攻击步骤下，实现了84% 以上的成功攻击。此外，A2C 算法已被证明是非常有效的选择适当的行动，以自动化的后期开发。"
    },
    {
        "title": "SOCI^+: An Enhanced Toolkit for Secure OutsourcedComputation on Integers",
        "url": "http://arxiv.org/abs/2309.15406v1",
        "pub_date": "2023-09-27",
        "summary": "Secure outsourced computation is critical for cloud computing to safeguard\ndata confidentiality and ensure data usability. Recently, secure outsourced\ncomputation schemes following a twin-server architecture based on partially\nhomomorphic cryptosystems have received increasing attention. The Secure\nOutsourced Computation on Integers (SOCI) [1] toolkit is the state-of-the-art\namong these schemes which can perform secure computation on integers without\nrequiring the costly bootstrapping operation as in fully homomorphic\nencryption; however, SOCI suffers from relatively large computation and\ncommunication overhead. In this paper, we propose SOCI+ which significantly\nimproves the performance of SOCI. Specifically, SOCI+ employs a novel (2,\n2)-threshold Paillier cryptosystem with fast encryption and decryption as its\ncryptographic primitive, and supports a suite of efficient secure arithmetic\ncomputation on integers protocols, including a secure multiplication protocol\n(SMUL), a secure comparison protocol (SCMP), a secure sign bit-acquisition\nprotocol (SSBA), and a secure division protocol (SDIV), all based on the (2,\n2)-threshold Paillier cryptosystem with fast encryption and decryption. In\naddition, SOCI+ incorporates an offline and online computation mechanism to\nfurther optimize its performance. We perform rigorous theoretical analysis to\nprove the correctness and security of SOCI+. Compared with SOCI, our\nexperimental evaluation shows that SOCI+ is up to 5.4 times more efficient in\ncomputation and 40% less in communication overhead.",
        "translated": "安全的外包计算对于云计算保护数据机密性和确保数据可用性至关重要。近年来，基于部分同态密码体制的双服务器安全外包计算方案受到越来越多的关注。安全外包整数计算(SOCI)[1]工具包是这些方案中最先进的，它可以对整数进行安全计算，而无需像完全同态加密那样进行昂贵的自举操作，然而，SOCI 承受着相对较大的计算和通信开销。在本文中，我们提出了 SOCI + ，显著提高了 SOCI 的性能。具体而言，SOCI + 采用了一种新的(2,2)门限 Paillier 密码体制，以快速加密和解密作为其密码原语，并支持一套高效的整数协议安全算术计算，包括安全乘法协议(SMUL)、安全比较协议(SCMP)、安全符号位获取协议(SSBA)和安全除法协议(SDIV) ，所有这些都基于(2,2)门限 Paillier 密码体制，具有快速加密和解密。此外，SOCI + 还结合了离线和在线计算机制，以进一步优化其性能。通过严格的理论分析，验证了 SOCI + 的正确性和安全性。实验结果表明，与 SOCI 相比，SOCI + 的计算效率提高了5.4倍，通信开销减少了40% 。"
    },
    {
        "title": "Multi-dimensional Data Quick Query for Blockchain-based Federated\n  Learning",
        "url": "http://arxiv.org/abs/2309.15348v1",
        "pub_date": "2023-09-27",
        "summary": "Due to the drawbacks of Federated Learning (FL) such as vulnerability of a\nsingle central server, centralized federated learning is shifting to\ndecentralized federated learning, a paradigm which takes the advantages of\nblockchain. A key enabler for adoption of blockchain-based federated learning\nis how to select suitable participants to train models collaboratively.\nSelecting participants by storing and querying the metadata of data owners on\nblockchain could ensure the reliability of selected data owners, which is\nhelpful to obtain high-quality models in FL. However, querying\nmulti-dimensional metadata on blockchain needs to traverse every transaction in\neach block, making the query time-consuming. An efficient query method for\nmulti-dimensional metadata in the blockchain for selecting participants in FL\nis absent and challenging. In this paper, we propose a novel data structure to\nimprove the query efficiency within each block named MerkleRB-Tree. In detail,\nwe leverage Minimal Bounding Rectangle(MBR) and bloom-filters for the query\nprocess of multi-dimensional continuous-valued attributes and discrete-valued\nattributes respectively. Furthermore, we migrate the idea of the skip list\nalong with an MBR and a bloom filter at the head of each block to enhance the\nquery efficiency for inter-blocks. The performance analysis and extensive\nevaluation results on the benchmark dataset demonstrate the superiority of our\nmethod in blockchain-based FL.",
        "translated": "由于联邦学习(FL)单一中心服务器的脆弱性等缺点，集中式联邦学习正在向分散式联邦学习转变，这是一种利用区块链优势的联邦学习模式。采用基于区块链的联邦学习的一个关键推动因素是如何选择合适的参与者来协同训练模型。通过区块链上数据所有者元数据的存储和查询来选择参与者，可以保证所选数据所有者的可靠性，有助于获得高质量的 FL 模型。然而，在区块链上查询多维元数据需要遍历每个区块中的每个事务，这使得查询非常耗时。区块链中多维元数据的有效查询方法用于选择外语学习的参与者是缺乏的和具有挑战性的。本文提出了一种新的数据结构 MerkleRB-Tree 来提高每个块的查询效率。具体来说，我们分别利用最小边界矩形(MBR)和布鲁姆滤波器来处理多维连续值属性和离散值属性的查询过程。此外，我们将跳过列表的思想与 MBR 和 Bloom 过滤器一起迁移到每个块的头部，以提高块间查询的效率。对基准数据集的性能分析和广泛的评估结果表明了该方法在基于区块链的 FL 中的优越性。"
    },
    {
        "title": "Evaluation and Analysis of Standard Security Technology in V2X\n  Communication -- Exploring ECQV Implicit Certificate Cracking",
        "url": "http://arxiv.org/abs/2309.15340v1",
        "pub_date": "2023-09-27",
        "summary": "In IEEE 1609.2 and IEEE 1609.2.1 standards for Vehicle-to-everything (V2X)\nsecure communication, various security algorithms based on Elliptic Curve\nCryptography (ECC) have been adopted and designed. To enhance the efficiency of\nthe Security Credential Management System (SCMS), this study evaluates the\ncomputational time for key generation, key expansion, signature generation, and\nsignature verification under different security strengths. This study discusses\nrelevant techniques based on Elliptic Curve Qu-Vanstone (ECQV) implicit\ncertificates, analyzes the length of uncompressed elliptic curve points,\ncompressed elliptic curve points, explicit certificates, and implicit\ncertificates. Furthermore, this study proposes mathematical models to\ndemonstrate the probability of ECQV cracking and provides suggestions for\nmitigating ECQV cracking risks.",
        "translated": "在 IEEE 1609.2和 IEEE 1609.2.1车辆到一切(V2X)安全通信标准中，采用和设计了各种基于椭圆曲线密码学(ECC)的安全算法。为了提高安全证书管理系统(SCMS)的效率，本研究评估了不同安全强度下密钥生成、密钥扩展、签名生成和签名验证的计算时间。本文讨论了基于椭圆曲线曲凡斯通(ECQV)隐式证书的相关技术，分析了未压缩椭圆曲线点的长度、压缩椭圆曲线点的长度、显式证书的长度和隐式证书的长度。此外，本研究提出数学模型来证明 ECQV 开裂的可能性，并提供减轻 ECQV 开裂风险的建议。"
    },
    {
        "title": "DefectHunter: A Novel LLM-Driven Boosted-Conformer-based Code\n  Vulnerability Detection Mechanism",
        "url": "http://arxiv.org/abs/2309.15324v1",
        "pub_date": "2023-09-27",
        "summary": "One of the most pressing threats to computing systems is software\nvulnerabilities, which can compromise both hardware and software components.\nExisting methods for vulnerability detection remain suboptimal. Traditional\ntechniques are both time-consuming and labor-intensive, while\nmachine-learning-based approaches often underperform when applied to complex\ndatasets, due to their inability to capture high-dimensional relationships.\nPrevious deep-learning strategies also fall short in capturing sufficient\nfeature information. Although self-attention mechanisms can process information\nover long distances, they fail to capture structural information. In this\npaper, we introduce DefectHunter, an innovative model for vulnerability\nidentification that employs the Conformer mechanism. This mechanism fuses\nself-attention with convolutional networks to capture both local, position-wise\nfeatures and global, content-based interactions. Furthermore, we optimize the\nself-attention mechanisms to mitigate the issue of excessive attention heads\nintroducing extraneous noise by adjusting the denominator. We evaluated\nDefectHunter against ten baseline methods using six industrial and two highly\ncomplex datasets. On the QEMU dataset, DefectHunter exhibited a 20.62\\%\nimprovement in accuracy over Pongo-70B, and for the CWE-754 dataset, its\naccuracy was 14.64\\% higher. To investigate how DefectHunter comprehends\nvulnerabilities, we conducted a case study, which revealed that our model\neffectively understands the mechanisms underlying vulnerabilities.",
        "translated": "计算系统面临的最紧迫的威胁之一是软件漏洞，它可能危及硬件和软件组件。现有的漏洞检测方法仍然是次优的。传统技术既费时又费力，而基于机器学习的方法在应用于复杂数据集时往往表现不佳，因为它们无法捕获高维关系。以往的深度学习策略在获取足够的特征信息方面也存在不足。尽管自我注意机制能够长距离处理信息，但是它们不能捕获结构信息。在本文中，我们介绍了 DefectHunter，这是一个创新的脆弱性识别模型，它采用了 Conform 机制。这种机制将自我注意与卷积网络结合起来，捕获局部的、位置明确的特征和全局的、基于内容的交互。此外，我们优化了自我注意机制，通过调整分母来缓解注意头部过度引入外部噪声的问题。我们使用六个工业数据集和两个高度复杂的数据集对比十个基线方法评估 DefectHunter。在 QEMU 数据集中，DefectHunter 比 Pongo-70B 的准确性提高了20.62% ，对于 CWE-754数据集，其准确性提高了14.64% 。为了研究 DefectHunter 如何理解漏洞，我们进行了一个案例研究，发现我们的模型有效地理解了潜在漏洞的机制。"
    },
    {
        "title": "Modifying twist algorithms for determining the key length of a\n  Vigenère cipher",
        "url": "http://arxiv.org/abs/2309.15240v1",
        "pub_date": "2023-09-26",
        "summary": "In this article, we analyze and improve upon the twist-based algorithms\nintroduced by Barr--Simoson and Park--Kim--Cho--Yum for determining the key\nlength of a Vigen\\`{e}re cipher. We provide an in-depth discussion on how the\ndomain of the twist index affects the accuracy of these algorithms along with\nsupporting experimental evidence. We also introduce a new twist-based\nalgorithm, the twist$^{++}$ algorithm, and show this algorithm is more accurate\nthan the twist$^{+}$ algorithm for a wide range of key lengths and text\nlengths.",
        "translated": "本文对 Barr-Simoson 和 Park-Kim-Cho-Yum 提出的基于扭曲的算法进行了分析和改进。我们提供了一个深入的讨论领域的扭曲指数如何影响这些算法的准确性以及支持的实验证据。我们还介绍了一个新的基于扭曲的算法，扭曲 $^ { + + } $算法，并表明该算法比扭曲 $^ { + } $算法更准确的关键字长度和文本长度的范围。"
    },
    {
        "title": "Revisiting Neural Program Smoothing for Fuzzing",
        "url": "http://arxiv.org/abs/2309.16618v1",
        "pub_date": "2023-09-28",
        "summary": "Testing with randomly generated inputs (fuzzing) has gained significant\ntraction due to its capacity to expose program vulnerabilities automatically.\nFuzz testing campaigns generate large amounts of data, making them ideal for\nthe application of machine learning (ML). Neural program smoothing (NPS), a\nspecific family of ML-guided fuzzers, aims to use a neural network as a smooth\napproximation of the program target for new test case generation.\n  In this paper, we conduct the most extensive evaluation of NPS fuzzers\nagainst standard gray-box fuzzers (&gt;11 CPU years and &gt;5.5 GPU years), and make\nthe following contributions: (1) We find that the original performance claims\nfor NPS fuzzers do not hold; a gap we relate to fundamental, implementation,\nand experimental limitations of prior works. (2) We contribute the first\nin-depth analysis of the contribution of machine learning and gradient-based\nmutations in NPS. (3) We implement Neuzz++, which shows that addressing the\npractical limitations of NPS fuzzers improves performance, but that standard\ngray-box fuzzers almost always surpass NPS-based fuzzers. (4) As a consequence,\nwe propose new guidelines targeted at benchmarking fuzzing based on machine\nlearning, and present MLFuzz, a platform with GPU access for easy and\nreproducible evaluation of ML-based fuzzers. Neuzz++, MLFuzz, and all our data\nare public.",
        "translated": "使用随机生成的输入(fuzzing)进行测试已经获得了显著的牵引力，因为它能够自动暴露程序漏洞。模糊测试活动产生大量的数据，使它们成为机器学习(ML)应用的理想选择。神经网络程序平滑(NPS)是一类特殊的 ML 引导的模糊控制器，其目的是利用神经网络作为程序目标的平滑逼近来生成新的测试用例。本文针对标准灰盒型(> 11 CPU 年和 > 5.5 GPU 年)的 NPS 模糊控制器进行了最广泛的评价，并做出了以下贡献: (1)发现 NPS 模糊控制器的原始性能要求不成立，与以往工作的基本原理、实施和实验局限性有一定的差距。(2)我们首次对机器学习和基于梯度的突变在 NPS 中的作用进行了深入的分析。(3)我们实现了 Neuzz + + ，这表明解决 NPS 模糊器的实际局限性可以提高性能，但是标准的灰盒模糊器几乎总是超过基于 NPS 的模糊器。(4)因此，我们提出了基于机器学习的模糊基准测试的新指导方针，并提出了 MLFuzz，一个具有 GPU 访问的平台，可以方便和可重复地评估基于 ML 的模糊。Neuzz + + ，MLFuzz，以及我们所有的数据都是公开的。"
    },
    {
        "title": "Efficient Hardware Implementation of Constant Time Sampling for HQC",
        "url": "http://arxiv.org/abs/2309.16493v1",
        "pub_date": "2023-09-28",
        "summary": "HQC is one of the code-based finalists in the last round of the NIST post\nquantum cryptography standardization process. In this process, security and\nimplementation efficiency are key metrics for the selection of the candidates.\nA critical compute kernel with respect to efficient hardware implementations\nand security in HQC is the sampling method used to derive random numbers. Due\nto its security criticality, recently an updated sampling algorithm was\npresented to increase its robustness against side-channel attacks.\n  In this paper, we pursue a cross layer approach to optimize this new sampling\nalgorithm to enable an efficient hardware implementation without comprising the\noriginal algorithmic security and side-channel attack robustness.\n  We compare our cross layer based implementation to a direct hardware\nimplementation of the original algorithm and to optimized implementations of\nthe previous sampler version. All implementations are evaluated using the\nXilinx Artix 7 FPGA. Our results show that our approach reduces the latency by\na factor of 24 compared to the original algorithm and by a factor of 28\ncompared to the previously used sampler with significantly less resources.",
        "translated": "HQC 是美国国家标准技术研究所(nIST)量子密码学标准化进程最后一轮的决赛选手之一。在这个过程中，安全性和实现效率是选择候选人的关键指标。在 HQC 中，关于有效的硬件实现和安全性的一个关键计算核心是用于推导随机数的抽样方法。由于其安全性的严重性，最近提出了一种更新的采样算法，以增强其对侧信道攻击的鲁棒性。本文采用了一种跨层优化的方法来优化这种新的采样算法，使其能够在不包含原始算法安全性和边信道攻击鲁棒性的情况下实现有效的硬件实现。我们将基于跨层的实现与原始算法的直接硬件实现以及前一个采样器版本的优化实现进行了比较。所有实现都使用 Xilinx Artix 7 FPGA 进行评估。我们的结果表明，与原始算法相比，我们的方法减少了24倍的延迟，与之前使用的资源明显减少的采样器相比，减少了28倍的延迟。"
    },
    {
        "title": "Cyber Sentinel: Exploring Conversational Agents in Streamlining Security\n  Tasks with GPT-4",
        "url": "http://arxiv.org/abs/2309.16422v1",
        "pub_date": "2023-09-28",
        "summary": "In an era where cyberspace is both a battleground and a backbone of modern\nsociety, the urgency of safeguarding digital assets against ever-evolving\nthreats is paramount. This paper introduces Cyber Sentinel, an innovative\ntask-oriented cybersecurity dialogue system that is effectively capable of\nmanaging two core functions: explaining potential cyber threats within an\norganization to the user, and taking proactive/reactive security actions when\ninstructed by the user. Cyber Sentinel embodies the fusion of artificial\nintelligence, cybersecurity domain expertise, and real-time data analysis to\ncombat the multifaceted challenges posed by cyber adversaries. This article\ndelves into the process of creating such a system and how it can interact with\nother components typically found in cybersecurity organizations. Our work is a\nnovel approach to task-oriented dialogue systems, leveraging the power of\nchaining GPT-4 models combined with prompt engineering across all sub-tasks. We\nalso highlight its pivotal role in enhancing cybersecurity communication and\ninteraction, concluding that not only does this framework enhance the system's\ntransparency (Explainable AI) but also streamlines the decision-making process\nand responding to threats (Actionable AI), therefore marking a significant\nadvancement in the realm of cybersecurity communication.",
        "translated": "在网络空间既是战场又是现代社会的支柱的时代，保护数字资产免受不断演变的威胁的紧迫性至关重要。本文介绍了 Cyber Sentinel，这是一个创新的面向任务的网络安全对话系统，它能够有效地管理两个核心功能: 向用户解释组织内潜在的网络威胁，以及在用户指示下采取主动/被动的安全行动。《网络哨兵》融合了人工智能、网络安全领域的专业知识和实时数据分析，以应对网络对手带来的多方面挑战。本文深入研究了创建这样一个系统的过程，以及它如何与网络安全组织中通常存在的其他组件进行交互。我们的工作是一种面向任务的对话系统的新方法，利用 GPT-4模型的链接能力，结合跨所有子任务的快速工程。我们亦强调该框架在加强网络安全通讯和互动方面的关键作用，认为该框架不但提高了系统的透明度(可解释的人工智能) ，而且简化了决策过程和应对威胁(可采取行动的人工智能) ，因此标志着网络安全通讯领域的重大进展。"
    },
    {
        "title": "Assessing the Solvency of Virtual Asset Service Providers: Are Current\n  Standards Sufficient?",
        "url": "http://arxiv.org/abs/2309.16408v1",
        "pub_date": "2023-09-28",
        "summary": "Entities like centralized cryptocurrency exchanges fall under the business\ncategory of virtual asset service providers (VASPs). As any other enterprise,\nthey can become insolvent. VASPs enable the exchange, custody, and transfer of\ncryptoassets organized in wallets across distributed ledger technologies\n(DLTs). Despite the public availability of DLT transactions, the cryptoasset\nholdings of VASPs are not yet subject to systematic auditing procedures. In\nthis paper, we propose an approach to assess the solvency of a VASP by\ncross-referencing data from three distinct sources: cryptoasset wallets,\nbalance sheets from the commercial register, and data from supervisory\nentities. We investigate 24 VASPs registered with the Financial Market\nAuthority in Austria and provide regulatory data insights such as who are the\ncustomers and where do they come from. Their yearly incoming and outgoing\ntransaction volume amount to 2 billion EUR for around 1.8 million users. We\ndescribe what financial services they provide and find that they are most\nsimilar to traditional intermediaries such as brokers, money exchanges, and\nfunds, rather than banks. Next, we empirically measure DLT transaction flows of\nfour VASPs and compare their cryptoasset holdings to balance sheet entries.\nData are consistent for two VASPs only. This enables us to identify gaps in the\ndata collection and propose strategies to address them. We remark that any\nentity in charge of auditing requires proof that a VASP actually controls the\nfunds associated with its on-chain wallets. It is also important to report fiat\nand cryptoasset and liability positions broken down by asset types at a\nreasonable frequency.",
        "translated": "像集中式加密货币交易所这样的实体属于虚拟资产服务提供商(VASP)的业务类别。与其它任何企业一样，它们也可能破产。VASP 支持跨分布式分类账技术(DLT)在钱包中组织的加密资产的交换、保管和转移。尽管 DLT 交易对公众开放，但 VASP 持有的加密资产尚未经过系统的审计程序。在本文中，我们提出了一种方法来评估 VASP 的偿付能力相互参照的数据来自三个不同的来源: 加密资产钱包，资产负债表来自商业登记和数据来自监管实体。我们调查了在奥地利金融市场管理局注册的24个 VASP，并提供了监管数据见解，如谁是客户，他们来自哪里。它们每年的进出交易量达到20亿欧元，用户约为180万。我们描述了它们提供的金融服务，发现它们与经纪商、货币交易所和基金等传统中介机构最为相似，而非银行。接下来，我们通过经验测量四个 VASP 的 DLT 事务流，并将它们的加密资产持有量与资产负债表条目进行比较。只有两个 VASP 的数据是一致的。这使我们能够查明数据收集方面的差距，并提出解决这些差距的战略。我们注意到，任何负责审计的实体都需要证明，VASP 实际上控制着与其链条上钱包相关的资金。同样重要的是，以合理的频率报告按资产类型分列的法定、加密资产和负债头寸。"
    },
    {
        "title": "Libertas: Privacy-Preserving Computation for Decentralised Personal Data\n  Stores",
        "url": "http://arxiv.org/abs/2309.16365v1",
        "pub_date": "2023-09-28",
        "summary": "Data-driven decision-making and AI applications present exciting new\nopportunities delivering widespread benefits. The rapid adoption of such\napplications triggers legitimate concerns about loss of privacy and misuse of\npersonal data. This leads to a growing and pervasive tension between harvesting\nubiquitous data on the Web and the need to protect individuals. Decentralised\npersonal data stores (PDS) such as Solid are frameworks designed to give\nindividuals ultimate control over their personal data. But current PDS\napproaches have limited support for ensuring privacy when computations combine\ndata spread across users. Secure Multi-Party Computation (MPC) is a well-known\nsubfield of cryptography, enabling multiple autonomous parties to\ncollaboratively compute a function while ensuring the secrecy of inputs (input\nprivacy). These two technologies complement each other, but existing practices\nfall short in addressing the requirements and challenges of introducing MPC in\na PDS environment. For the first time, we propose a modular design for\nintegrating MPC with Solid while respecting the requirements of\ndecentralisation in this context. Our architecture, Libertas, requires no\nprotocol level changes in the underlying design of Solid, and can be adapted to\nother PDS. We further show how this can be combined with existing differential\nprivacy techniques to also ensure output privacy. We use empirical benchmarks\nto inform and evaluate our implementation and design choices. We show the\ntechnical feasibility and scalability pattern of the proposed system in two\nnovel scenarios -- 1) empowering gig workers with aggregate computations on\ntheir earnings data; and 2) generating high-quality differentially-private\nsynthetic data without requiring a trusted centre. With this, we demonstrate\nthe linear scalability of Libertas, and gained insights about compute\noptimisations under such an architecture.",
        "translated": "数据驱动的决策和人工智能应用提供了令人兴奋的新机遇，带来了广泛的好处。此类应用程序的迅速采用引发了人们对隐私权丧失和个人数据滥用的合理担忧。这导致了收集网络上无处不在的数据与保护个人需求之间日益普遍的紧张关系。像 Solid 这样的分散式个人数据存储(PDS)是旨在让个人对自己的个人数据拥有最终控制权的框架。但是，当计算结合跨用户传播的数据时，目前的 PDS 方法对确保隐私的支持有限。安全多方计算(MPC)是一个众所周知的子领域的加密技术，使多个自治方协作计算一个函数，同时确保输入的保密性(输入隐私)。这两种技术相辅相成，但现有做法在解决在公共数据系统环境中引入 MPC 的要求和挑战方面存在不足。我们首次提出了一种模块化设计，将 MPC 与 Solid 集成在一起，同时尊重这种情况下的地方分权需求。我们的体系结构，Libertas，不需要对 Solid 的底层设计进行协议级别的更改，并且可以适用于其他 PDS。我们进一步展示了如何结合现有的差分隐私技术来确保输出的隐私性。我们使用经验性的基准来通知和评估我们的实现和设计选择。我们在两个新的场景中展示了该系统的技术可行性和可扩展性模式——1)授权零工对他们的收入数据进行总计算; 2)生成高质量的差别私有合成数据，而不需要一个可信的中心。通过这个，我们展示了 Libertas 的线性可伸缩性，并获得了在这种架构下计算优化的见解。"
    },
    {
        "title": "AgEncID: Aggregate Encryption Individual Decryption of Key for FPGA\n  Bitstream IP Cores in Cloud",
        "url": "http://arxiv.org/abs/2309.16282v1",
        "pub_date": "2023-09-28",
        "summary": "Cloud computing platforms are progressively adopting Field Programmable Gate\nArrays to deploy specialized hardware accelerators for specific computational\ntasks. However, the security of FPGA-based bitstream for Intellectual Property,\nIP cores from unauthorized interception in cloud environments remains a\nprominent concern. Existing methodologies for protection of such bitstreams\npossess several limitations, such as requiring a large number of keys, tying\nbitstreams to specific FPGAs, and relying on trusted third parties. This paper\nproposes Aggregate Encryption and Individual Decryption, a cryptosystem based\non key aggregation to enhance the security of FPGA-based bitstream for IP cores\nand to address the pitfalls of previous related works. In our proposed scheme,\nIP providers can encrypt their bitstreams with a single key for a set S of FPGA\nboards, with which the bitstreams can directly be decrypted on any of the FPGA\nboards in S. Aggregate encryption of the key is performed in a way which\nensures that the key can solely be obtained onboard through individual\ndecryption employing the board's private key, thus facilitating secure key\nprovisioning. The proposed cryptosystem is evaluated mainly on Zynq FPGAs. The\noutcomes demonstrate that our cryptosystem not only outperforms existing\ntechniques with respect to resource, time and energy significantly but also\nupholds robust security assurances.",
        "translated": "云计算平台正逐步采用现场可编程门阵列来为特定的计算任务部署专门的硬件加速器。然而，基于 FPGA 的知识产权比特流、 IP 核的安全性仍然是云环境中未经授权拦截的一个突出问题。现有的保护这些比特流的方法有一些局限性，比如需要大量的密钥，将比特流绑定到特定的 FPGA，以及依赖可信的第三方。为了提高基于 FPGA 的 IP 核位流的安全性，解决以往相关工作中存在的安全隐患，提出了一种基于密钥聚合的密码体制——聚合加密和个体解密。在我们提出的方案中，IP 供应商可以使用一组 FPGA 板的单一密钥对其比特流进行加密，这些比特流可以直接在任何 FPGA 板上解密。密钥的集合加密是通过一种方式进行的，这种方式确保密钥只能通过使用板上的私钥进行单独解密而在板上获得，从而促进安全密钥的提供。该密码体制主要在 Zynq 现场可编程门阵列(FPGA)上进行评估。结果表明，我们的密码系统不仅在资源、时间和能源方面明显优于现有的技术，而且保持了强大的安全保证。"
    },
    {
        "title": "On finding dense sub-lattices as low energy states of a quantum\n  Hamiltonian",
        "url": "http://arxiv.org/abs/2309.16256v1",
        "pub_date": "2023-09-28",
        "summary": "Lattice-based cryptography has emerged as one of the most prominent\ncandidates for post-quantum cryptography, projected to be secure against the\nimminent threat of large-scale fault-tolerant quantum computers. The Shortest\nVector Problem (SVP) is to find the shortest non-zero vector in a given\nlattice. It is fundamental to lattice-based cryptography and believed to be\nhard even for quantum computers. We study a natural generalization of the SVP\nknown as the $K$-Densest Sub-lattice Problem ($K$-DSP): to find the densest\n$K$-dimensional sub-lattice of a given lattice. We formulate $K$-DSP as finding\nthe first excited state of a Z-basis Hamiltonian, making $K$-DSP amenable to\ninvestigation via an array of quantum algorithms, including Grover search,\nquantum Gibbs sampling, adiabatic, and Variational Quantum Algorithms. The\ncomplexity of the algorithms depends on the basis through which the input\nlattice is presented. We present a classical polynomial-time algorithm that\ntakes an arbitrary input basis and preprocesses it into inputs suited to\nquantum algorithms. With preprocessing, we prove that $O(KN^2)$ qubits suffice\nfor solving $K$-DSP for $N$ dimensional input lattices. We empirically\ndemonstrate the performance of a Quantum Approximate Optimization Algorithm\n$K$-DSP solver for low dimensions, highlighting the influence of a good\npreprocessed input basis. We then discuss the hardness of $K$-DSP in relation\nto the SVP, to see if there is reason to build post-quantum cryptography on\n$K$-DSP. We devise a quantum algorithm that solves $K$-DSP with run-time\nexponent $(5KN\\log{N})/2$. Therefore, for fixed $K$, $K$-DSP is no more than\npolynomially harder than the SVP.",
        "translated": "基于格子的密码学已经成为后量子密码学最突出的候选者之一，预计将安全抵御大规模容错量子计算机的迫在眉睫的威胁。最短向量问题(SVP)就是在给定的格上找到最短的非零向量。它是基于格子的密码学的基础，甚至被认为对量子计算机来说也是很困难的。我们研究了 SVP 的一个自然推广，称为 $K $- 稠密子格问题($K $- DSP) : 寻找给定格的最稠密的 $K $- 维子格。我们制定 $K $- DSP 作为寻找 Z 基哈密顿量的第一激发态，使 $K $- DSP 适合通过一系列量子算法进行研究，包括 Grover 搜索，量子吉布斯采样，绝热和变分量子算法。算法的复杂度取决于输入格的表示基础。我们提出了一个经典的多项式时间算法，采取任意输入基，并预处理成适合量子算法的输入。通过预处理，证明了 $O (KN ^ 2) $量子比特足以解决 $K $- DSP 中的 $N 维输入格。实验证明了量子近似优化算法 $K $- DSP 求解器在低维情况下的性能，突出了良好的预处理输入基的影响。然后，我们讨论 $K $- DSP 相对于 SVP 的硬度，看看是否有理由在 $K $- DSP 上构建后量子密码。设计了一种基于运行时指数 $(5KN log { N })/2 $解决 $K $- DSP 问题的量子算法。因此，对于固定的 $K $，$K $- DSP 并不比高级副总裁更难。"
    },
    {
        "title": "Random and Safe Cache Architecture to Defeat Cache Timing Attacks",
        "url": "http://arxiv.org/abs/2309.16172v1",
        "pub_date": "2023-09-28",
        "summary": "Caches have been exploited to leak secret information due to the different\ntimes they take to handle memory accesses. Cache timing attacks include\nnon-speculative cache side and covert channel attacks and cache-based\nspeculative execution attacks. We first present a systematic view of the attack\nand defense space and show that no existing defense has addressed both\nspeculative and non-speculative cache timing attack families, which we do in\nthis paper. We propose Random and Safe (RaS) cache architectures to decorrelate\nthe cache state changes from memory requests. RaS fills the cache with ``safe''\ncache lines that are likely to be used in the future, rather than with\ndemand-fetched, security-sensitive lines. RaS captures a group of safe\naddresses during runtime and fetches addresses randomly displaced from these\naddresses. Our proposed RaS architecture is flexible to allow\nsecurity-performance trade-offs. We show different designs of RaS architectures\nthat can defeat cache side-channel attacks and cache-based speculative\nexecution attacks. The RaS variant against cache-based speculative execution\nattacks has 4.2% average performance overhead and other RaS variants against\nboth attack families have 7.9% to 45.2% average overhead. For some benchmarks,\nRaS defenses improve the performance while providing security.",
        "translated": "由于处理内存访问的时间不同，缓存被用来泄漏机密信息。缓存计时攻击包括非推测性缓存端攻击、隐蔽通道攻击和基于缓存的 Speculative_execution 攻击。我们首先对攻击和防御空间提出了一个系统的观点，并表明没有现有的防御同时涉及推测性和非推测性缓存定时攻击家族，我们在本文中就是这样做的。我们提出随机和安全(RaS)缓存体系结构，以解除缓存状态的变化与内存请求的关联。RaS 使用“安全”的缓存线路填充缓存，这些线路将来可能会使用，而不是使用需求获取的、安全敏感的线路。RaS 在运行时捕获一组安全地址，并从这些地址中随机抽取地址。我们提出的 RaS 体系结构是灵活的，允许在安全性能方面进行权衡。我们展示了不同的 RaS 架构设计，它们可以抵御缓存侧信道攻击和基于缓存的 Speculative_execution 攻击。针对基于缓存的 Speculative_execution 攻击的 RAS 变体的平均性能开销为4.2% ，针对两个攻击系列的其他 RAS 变体的平均性能开销为7.9% 至45.2% 。对于某些基准测试，RaS 防御在提供安全性的同时提高了性能。"
    },
    {
        "title": "UVL: A Unified Framework for Video Tampering Localization",
        "url": "http://arxiv.org/abs/2309.16126v1",
        "pub_date": "2023-09-28",
        "summary": "With the development of deep learning technology, various forgery methods\nemerge endlessly. Meanwhile, methods to detect these fake videos have also\nachieved excellent performance on some datasets. However, these methods suffer\nfrom poor generalization to unknown videos and are inefficient for new forgery\nmethods. To address this challenging problem, we propose UVL, a novel unified\nvideo tampering localization framework for synthesizing forgeries.\nSpecifically, UVL extracts common features of synthetic forgeries: boundary\nartifacts of synthetic edges, unnatural distribution of generated pixels, and\nnoncorrelation between the forgery region and the original. These features are\nwidely present in different types of synthetic forgeries and help improve\ngeneralization for detecting unknown videos. Extensive experiments on three\ntypes of synthetic forgery: video inpainting, video splicing and DeepFake show\nthat the proposed UVL achieves state-of-the-art performance on various\nbenchmarks and outperforms existing methods by a large margin on cross-dataset.",
        "translated": "随着深度学习技术的发展，各种伪造方法层出不穷。同时，检测这些假视频的方法在一些数据集上也取得了良好的性能。但是，这些方法对未知视频的泛化能力较差，对于新的伪造方法来说效率较低。为了解决这个具有挑战性的问题，我们提出了 UVL，一种新的统一的视频篡改定位框架，用于合成伪造文件。具体来说，UVL 提取合成伪造的共同特征: 合成边缘的边界伪影，生成像素的非自然分布，以及伪造区域与原始区域之间的非相关性。这些特征广泛存在于不同类型的合成伪造中，有助于提高检测未知视频的一般性。通过对视频修改、视频拼接和 DeepFake 三种合成伪造方法的大量实验表明，该方法在各种基准测试中都取得了最好的性能，并且在跨数据集方面优于现有方法。"
    },
    {
        "title": "Differentially Private Secure Multiplication: Hiding Information in the\n  Rubble of Noise",
        "url": "http://arxiv.org/abs/2309.16105v1",
        "pub_date": "2023-09-28",
        "summary": "We consider the problem of private distributed multi-party multiplication. It\nis well-established that Shamir secret-sharing coding strategies can enable\nperfect information-theoretic privacy in distributed computation via the\ncelebrated algorithm of Ben Or, Goldwasser and Wigderson (the \"BGW algorithm\").\nHowever, perfect privacy and accuracy require an honest majority, that is, $N\n\\geq 2t+1$ compute nodes are required to ensure privacy against any $t$\ncolluding adversarial nodes. By allowing for some controlled amount of\ninformation leakage and approximate multiplication instead of exact\nmultiplication, we study coding schemes for the setting where the number of\nhonest nodes can be a minority, that is $N&lt; 2t+1.$ We develop a tight\ncharacterization privacy-accuracy trade-off for cases where $N &lt; 2t+1$ by\nmeasuring information leakage using {differential} privacy instead of perfect\nprivacy, and using the mean squared error metric for accuracy. A novel\ntechnical aspect is an intricately layered noise distribution that merges ideas\nfrom differential privacy and Shamir secret-sharing at different layers.",
        "translated": "我们考虑了私有的分布式多方乘法问题。众所周知，Shamir 秘密共享编码策略可以通过著名的 Ben Or，Goldwasser 和 Wigderson 算法(“ BGW 算法”)在分布式计算中实现完美的信息理论隐私。然而，完美的隐私和准确性需要诚实的大多数，也就是说，$N geq 2t + 1 $计算节点需要确保对任何 $t $合谋的对手节点的隐私。通过考虑一些控制数量的信息泄露和近似乘法，而不是精确的乘法，我们研究了编码方案的设置，其中诚实节点的数量可以是少数，即 $N < 2t + 1。$我们开发了一个严格的角色塑造隐私-准确性权衡方案，对于那些 n < 2t + 1 $的情况，我们使用差别隐私而不是完美隐私来衡量信息泄露，并使用均方差度量来衡量准确性。一个新颖的技术方面是一个错综复杂的分层噪音分布，它融合了来自差分隐私和沙米尔秘密分享的不同层次的想法。"
    },
    {
        "title": "QR TPM in Programmable Low-Power Devices",
        "url": "http://arxiv.org/abs/2309.17414v1",
        "pub_date": "2023-09-29",
        "summary": "Trusted Platform Modules (TPMs), which serve as the root of trust in secure\nsystems, are secure crypto-processors that carry out cryptographic primitives.\nShould large-scale quantum computing become a reality, the cryptographic\nprimitives adopted in the TPM 2.0 standard will no longer be secure. Thus, the\ndesign of TPMs that provide Quantum Resistant (QR) primitives is of utmost\nimportance, in particular with the restrictions imposed by embedded systems. In\nthis paper, we investigate the deployment of QR primitives and protocols in the\nstandard TPM 2.0. Cryptographic algorithms that are already in the NIST QR\ncryptography standardization process, as well as an Oblivious Transfer (OT), a\nfundamental cryptographic primitive, are the QR cryptographic schemes selected\nto extend TPM 2.0. In particular, the Kyber algorithm for key encapsulation,\nthe Dilithium algorithm for digital signature, and a 3-round Random Oblivious\nTransfer (ROT) protocol, supporting protocols such as Multi-Party Computation\nand Private Set Intersection (PSI). The QR extended TPM 2.0 is implemented in\nARM and RISC-V embedded processors, its computational requirements are analysed\nand experimentally evaluated in comparison to the standard TPM. It is shown\nthat Kyber and Dilithium are faster at creating keys than RSA, due to the key\nsize and secure random sampling required in RSA, while they meet the same\nperformance level as ECC. For digital signatures, both in signature creation\nand verification, Dilithium is on par with RSA and ECC. The ROT protocol shows\ndecent performance and its support required small modifications to the TPM.\nThis paper also shows that it would be possible to backport the required code\nto already available TPMs to ensure that current TPMs remain secure against\nquantum adversaries.",
        "translated": "可信平台模块(TPM)作为安全系统中信任的根源，是执行加密原语的安全加密处理器。如果大规模量子计算成为现实，TPM 2.0标准中采用的加密原语将不再安全。因此，提供量子抵抗(QR)原语的 TPM 的设计是至关重要的，特别是在嵌入式系统的限制下。在本文中，我们研究 QR 原语和协议在标准 TPM 2.0中的部署。已经在 NIST QR 加密标准化过程中的加密算法以及基本加密原语“不知不觉传输(OT)”是选择用于扩展 TPM 2.0的 QR 加密方案。特别是用于密钥封装的 Kyber 算法、用于数字签名的 Dilithium 算法以及支持多方计算和私有集合交叉(PSI)等协议的3轮随机不知情传输(ROT)协议。在 ARM 和 RISC-V 嵌入式处理器上实现了 QR 扩展 TPM 2.0，并与标准 TPM 进行了计算需求分析和实验评估。结果表明，由于 RSA 需要密钥大小和安全随机采样，Kyber 和 Dilithium 在创建密钥方面比 RSA 更快，同时它们满足 ECC 的性能水平。对于数字签名，无论是在签名创建和验证方面，Dilithium 与 RSA 和 ECC 并驾齐驱。ROT 协议显示出良好的性能，其支持需要对 TPM 进行小的修改。本文件还表明，有可能将所需的代码支持到已有的 TPM，以确保目前的 TPM 在面对量子对手时仍然安全。"
    },
    {
        "title": "Quantum Privacy-preserving Two-party Circle Intersection Protocol Based\n  on Phase-encoded Query",
        "url": "http://arxiv.org/abs/2309.17293v1",
        "pub_date": "2023-09-29",
        "summary": "Privacy-preserving geometric intersection (PGI) is an important issue in\nSecure multiparty computation (SMC). The existing quantum PGI protocols are\nmainly based on grid coding, which requires a lot of computational complexity.\nThe phase-encoded query method which has been used in some Quantum SMC\nprotocols is suitable to solve the decision problem, but it needs to apply high\ndimensional Oracle operators. In this paper, we use the principle of\nphase-encoded query to solve an important PGI problem, namely\nprivacy-preserving two-party circle intersection. We study the implementation\nof Oracle operator in detail, and achieve polynomial computational complexity\nby decompsing it into quantum arithmetic operations. Performance analysis shows\nthat our protocol is correct and efficient, and can protect the privacy of all\nparticipants against internal and external attacks.",
        "translated": "保护隐私的几何交集(PGI)是安全多方计算(SMC)中的一个重要问题。现有的量子 PGI 协议主要基于网格编码，计算复杂度较高。一些量子 SMC 协议中使用的相位编码查询方法适合解决决策问题，但需要使用高维的 Oracle 算子。本文利用相位编码查询原理解决了一个重要的 PGI 问题，即保密的两方圆交集问题。详细研究了 Oracle 算子的实现，并将其分解为量子算术运算，实现了多项式计算复杂度。性能分析表明，我们的协议是正确和有效的，可以保护所有参与者的隐私免受内部和外部攻击。"
    },
    {
        "title": "Differentially Private Computation of Basic Reproduction Numbers in\n  Networked Epidemic Models",
        "url": "http://arxiv.org/abs/2309.17284v1",
        "pub_date": "2023-09-29",
        "summary": "The basic reproduction number of a networked epidemic model, denoted $R_0$,\ncan be computed from a network's topology to quantify epidemic spread. However,\ndisclosure of $R_0$ risks revealing sensitive information about the underlying\nnetwork, such as an individual's relationships within a social network.\nTherefore, we propose a framework to compute and release $R_0$ in a\ndifferentially private way. First, we provide a new result that shows how $R_0$\ncan be used to bound the level of penetration of an epidemic within a single\ncommunity as a motivation for the need of privacy, which may also be of\nindependent interest. We next develop a privacy mechanism to formally safeguard\nthe edge weights in the underlying network when computing $R_0$. Then we\nformalize tradeoffs between the level of privacy and the accuracy of values of\nthe privatized $R_0$. To show the utility of the private $R_0$ in practice, we\nuse it to bound this level of penetration under privacy, and concentration\nbounds on these analyses show they remain accurate with privacy implemented. We\napply our results to real travel data gathered during the spread of COVID-19,\nand we show that, under real-world conditions, we can compute $R_0$ in a\ndifferentially private way while incurring errors as low as $7.6\\%$ on average.",
        "translated": "网络流行病模型的基本传染数，表示为 $R _ 0 $，可以从网络的拓扑结构中计算出来，以量化流行病的传播。然而，$R _ 0 $的披露可能会暴露关于底层网络的敏感信息，比如个人在社交网络中的关系。因此，我们提出了一个以差异私有方式计算和发布 $R _ 0 $的框架。首先，我们提供了一个新的结果，表明 $R _ 0 $可以用来约束一个流行病在单一社区内的渗透水平，作为隐私需求的动机，这也可能是独立的利益。接下来，我们开发了一个隐私机制，以便在计算 $R _ 0 $时正式保护底层网络中的边权值。然后，我们将私有化 $R _ 0 $的隐私水平和价值准确性之间的权衡形式化。为了展示私有 $R _ 0 $在实际应用中的效用，我们使用它来约束隐私下的渗透水平，这些分析的集中界限表明它们在实现隐私时仍然是准确的。我们将我们的结果应用于在2019冠状病毒疾病传播期间收集的真实旅行数据，并且我们表明，在真实世界条件下，我们可以以一种不同的私人方式计算 $R _ 0 $，而产生的误差平均低至 $7.6% 。"
    },
    {
        "title": "Toward Robust Recommendation via Real-time Vicinal Defense",
        "url": "http://arxiv.org/abs/2309.17278v1",
        "pub_date": "2023-09-29",
        "summary": "Recommender systems have been shown to be vulnerable to poisoning attacks,\nwhere malicious data is injected into the dataset to cause the recommender\nsystem to provide biased recommendations. To defend against such attacks,\nvarious robust learning methods have been proposed. However, most methods are\nmodel-specific or attack-specific, making them lack generality, while other\nmethods, such as adversarial training, are oriented towards evasion attacks and\nthus have a weak defense strength in poisoning attacks.\n  In this paper, we propose a general method, Real-time Vicinal Defense (RVD),\nwhich leverages neighboring training data to fine-tune the model before making\na recommendation for each user. RVD works in the inference phase to ensure the\nrobustness of the specific sample in real-time, so there is no need to change\nthe model structure and training process, making it more practical. Extensive\nexperimental results demonstrate that RVD effectively mitigates targeted\npoisoning attacks across various models without sacrificing accuracy. Moreover,\nthe defensive effect can be further amplified when our method is combined with\nother strategies.",
        "translated": "推荐系统已被证明容易受到中毒攻击，恶意数据被注入数据集，导致推荐系统提供有偏见的推荐。为了抵御这种攻击，人们提出了各种鲁棒学习方法。然而，大多数方法是模型特定的或攻击特定的，缺乏一般性，而其他方法，如对抗性训练，面向规避攻击，因此在中毒攻击防御力量薄弱。在本文中，我们提出了一个通用的方法，即实时邻域防御(RVD) ，它利用相邻的训练数据来微调模型，然后为每个用户提出一个建议。RVD 工作在推理阶段，以保证特定样本的实时鲁棒性，因此不需要改变模型结构和训练过程，使其更加实用。大量的实验结果表明，RVD 在不牺牲精度的情况下，有效地减轻了各种模型中的靶向中毒攻击。此外，当我们的方法与其他策略相结合时，防御效应可以进一步放大。"
    },
    {
        "title": "Unaware, Unfunded and Uneducated: A Systematic Review of SME\n  Cybersecurity",
        "url": "http://arxiv.org/abs/2309.17186v1",
        "pub_date": "2023-09-29",
        "summary": "Small and Medium Enterprises (SMEs) are pivotal in the global economy,\naccounting for over 90% of businesses and 60% of employment worldwide. Despite\ntheir significance, SMEs have been disregarded from cybersecurity initiatives,\nrendering them ill-equipped to deal with the growing frequency, sophistication,\nand destructiveness of cyber-attacks. We systematically reviewed the\ncybersecurity literature on SMEs published between 2017 and 2023.\n  We focus on research discussing cyber threats, adopted controls, challenges,\nand constraints SMEs face in pursuing cybersecurity resilience.\n  Our search yielded 916 studies that we narrowed to 77 relevant papers. We\nidentified 44 unique themes and categorised them as novel findings or\nestablished knowledge. This distinction revealed that research on SMEs is\nshallow and has made little progress in understanding SMEs' roles, threats, and\nneeds. Studies often repeated early discoveries without replicating or offering\nnew insights.\n  The existing research indicates that the main challenges to attaining\ncybersecurity resilience of SMEs are a lack of awareness of the cybersecurity\nrisks, limited cybersecurity literacy and constrained financial resources.\nHowever, resource availability varied between developed and developing\ncountries. Our analysis indicated a relationship among these themes, suggesting\nthat limited literacy is the root cause of awareness and resource constraint\nissues.",
        "translated": "中小企业在全球经济中举足轻重，占全球90% 以上的企业和60% 的就业人数。尽管中小企业的重要性不言而喻，但它们一直被忽视在网络安全措施方面的重要性，致使它们无法应付日益频繁、复杂和具破坏性的网络攻击。我们系统地回顾了2017年至2023年间出版的关于中小企业的网络安全文献。我们专注于研究讨论网络威胁，采取的控制，挑战和限制中小企业在追求网络安全弹性所面临的问题。我们的研究产生了916项研究，我们缩小到77篇相关论文。我们确定了44个独特的主题，并将它们归类为新发现或已确立的知识。这种差异表明，对中小企业的研究是肤浅的，在了解中小企业的角色、威胁和需求方面进展甚微。研究往往重复早期的发现，而没有复制或提供新的见解。现有研究表明，中小企业在实现网络安全抗御能力方面面临的主要挑战是对网络安全风险缺乏认识、网络安全知识普及程度有限以及财政资源有限。然而，发达国家和发展中国家的资源供应情况各不相同。我们的分析表明了这些主题之间的关系，表明识字能力有限是认识和资源限制问题的根源。"
    },
    {
        "title": "Mostree : Malicious Secure Private Decision Tree Evaluation with\n  Sublinear Communication",
        "url": "http://arxiv.org/abs/2309.17124v1",
        "pub_date": "2023-09-29",
        "summary": "A private decision tree evaluation (PDTE) protocol allows a feature vector\nowner (FO) to classify its data using a tree model from a model owner (MO) and\nonly reveals an inference result to the FO. This paper proposes Mostree, a PDTE\nprotocol secure in the presence of malicious parties with sublinear\ncommunication. We design Mostree in the three-party honest-majority setting,\nwhere an (untrusted) computing party (CP) assists the FO and MO in the secure\ncomputation. We propose two low-communication oblivious selection (OS)\nprotocols by exploiting nice properties of three-party replicated secret\nsharing (RSS) and distributed point function. Mostree combines OS protocols\nwith a tree encoding method and three-party secure computation to achieve\nsublinear communication. We observe that most of the protocol components\nalready maintain privacy even in the presence of a malicious adversary, and\nwhat remains to achieve is correctness. To ensure correctness, we propose a set\nof lightweight consistency checks and seamlessly integrate them into Mostree.\nAs a result, Mostree achieves sublinear communication and malicious security\nsimultaneously. We implement Mostree and compare it with the state-of-the-art.\nExperimental results demonstrate that Mostree is efficient and comparable to\nsemi-honest PDTE schemes with sublinear communication. For instance, when\nevaluated on the MNIST dataset in a LAN setting, Mostree achieves an evaluation\nusing approximately 768 ms with communication of around 168 KB.",
        "translated": "私有决策树评估(PDTE)协议允许特征向量所有者(FO)使用来自模型所有者(MO)的树模型对其数据进行分类，并且只显示对 FO 的推断结果。提出了一种基于 Mostree 的 PDTE 协议，该协议可以在存在恶意方的情况下通过次线性通信实现安全。我们在三方诚实多数设置中设计 Mostree，其中一个(不可信的)计算方(CP)协助 FO 和 MO 进行安全计算。利用三方复制秘密共享(RSS)和分布式点函数的优良特性，提出了两种低通信的遗忘选择(OS)协议。Mostree 将 OS 协议与树编码方法和三方安全计算相结合，实现了次线性通信。我们观察到大多数协议组件即使在有恶意对手存在的情况下也已经保持了隐私，剩下要做的就是正确性。为了确保正确性，我们提出了一套轻量级的一致性检查，并将它们无缝地整合到 Mostree。因此，Mostree 实现了次线性通信和恶意安全同时进行。我们实现了 Mostree，并将其与最先进的技术进行了比较。实验结果表明，Mostree 算法与具有次线性通信的半诚实 PDTE 方案相比具有更高的效率和可比性。例如，当在局域网环境中对 MNIST 数据集进行评估时，Mostree 使用大约768毫秒来实现评估，通信大约为168 KB。"
    },
    {
        "title": "Leveraging Optimization for Adaptive Attacks on Image Watermarks",
        "url": "http://arxiv.org/abs/2309.16952v1",
        "pub_date": "2023-09-29",
        "summary": "Untrustworthy users can misuse image generators to synthesize high-quality\ndeepfakes and engage in online spam or disinformation campaigns. Watermarking\ndeters misuse by marking generated content with a hidden message, enabling its\ndetection using a secret watermarking key. A core security property of\nwatermarking is robustness, which states that an attacker can only evade\ndetection by substantially degrading image quality. Assessing robustness\nrequires designing an adaptive attack for the specific watermarking algorithm.\nA challenge when evaluating watermarking algorithms and their (adaptive)\nattacks is to determine whether an adaptive attack is optimal, i.e., it is the\nbest possible attack. We solve this problem by defining an objective function\nand then approach adaptive attacks as an optimization problem. The core idea of\nour adaptive attacks is to replicate secret watermarking keys locally by\ncreating surrogate keys that are differentiable and can be used to optimize the\nattack's parameters. We demonstrate for Stable Diffusion models that such an\nattacker can break all five surveyed watermarking methods at negligible\ndegradation in image quality. These findings emphasize the need for more\nrigorous robustness testing against adaptive, learnable attackers.",
        "translated": "不可靠的用户可能会滥用图像生成器来合成高质量的深度伪造图像，并参与在线垃圾邮件或虚假信息活动。水印通过使用隐藏消息标记生成的内容来防止误用，使用秘密水印密钥能够检测到这些内容。水印的一个核心安全特性是鲁棒性，即攻击者只能通过大幅度降低图像质量来逃避检测。评估鲁棒性需要针对特定的水印算法设计一种自适应攻击。在评估水印算法及其(自适应)攻击时，一个挑战是确定一个自适应攻击是否是最优的，也就是说，它是可能的最佳攻击。我们通过定义一个目标函数来解决这个问题，然后以自适应攻击为最佳化问题。我们的自适应攻击的核心思想是通过创建可微的代理密钥来在本地复制秘密水印密钥，这些代理密钥可以用来优化攻击的参数。我们证明了稳定扩散模型，这样的攻击者可以打破所有五个调查的水印方法在可以忽略的降低图像质量。这些发现强调了针对自适应的、可学习的攻击者进行更严格的鲁棒性测试的必要性。"
    },
    {
        "title": "Layered Security Guidance for Data Asset Management in Additive\n  Manufacturing",
        "url": "http://arxiv.org/abs/2309.16842v1",
        "pub_date": "2023-09-28",
        "summary": "Manufacturing industries are increasingly adopting additive manufacturing\n(AM) technologies to produce functional parts in critical systems. However, the\ninherent complexity of both AM designs and AM processes render them attractive\ntargets for cyber-attacks. Risk-based Information Technology (IT) and\nOperational Technology (OT) security guidance standards are useful resources\nfor AM security practitioners, but the guidelines they provide are insufficient\nwithout additional AM-specific revisions. Therefore, a structured layering\napproach is needed to efficiently integrate these revisions with preexisting IT\nand OT security guidance standards. To implement such an approach, this paper\nproposes leveraging the National Institute of Standards and Technology's\nCybersecurity Framework (CSF) to develop layered, risk-based guidance for\nfulfilling specific security outcomes. It begins with an in-depth literature\nreview that reveals the importance of AM data and asset management to\nrisk-based security. Next, this paper adopts the CSF asset identification and\nmanagement security outcomes as an example for providing AM-specific guidance\nand identifies the AM geometry and process definitions to aid manufacturers in\nmapping data flows and documenting processes. Finally, this paper uses the Open\nSecurity Controls Assessment Language to integrate the AM-specific guidance\ntogether with existing IT and OT security guidance in a rigorous and traceable\nmanner. This paper's contribution is to show how a risk-based layered approach\nenables the authoring, publishing, and management of AM-specific security\nguidance that is currently lacking. The authors believe implementation of the\nlayered approach would result in value-added, non-redundant security guidance\nfor AM that is consistent with the preexisting guidance.",
        "translated": "制造业正越来越多地采用三维打印技术在关键系统中生产功能性部件。然而，AM 设计和 AM 过程的内在复杂性使它们成为网络攻击的有吸引力的目标。基于风险的信息技术(IT)和操作技术(OT)安全指导标准对于 AM 安全从业者来说是有用的资源，但是如果没有特定于 AM 的修订，它们提供的指导方针是不够的。因此，需要一种结构化的分层方法来有效地将这些修订版与先前存在的 IT 和 OT 安全指导标准集成起来。为了实现这种方法，本文建议利用美国国家标准与技术研究所(National Institute of Standards and Technology)的网络安全框架(Cybersecurity Framework，CSF)来开发分层的、基于风险的指导，以实现具体的安全成果。本文首先对资产管理数据和资产管理对基于风险的安全性的重要性进行了深入的文献回顾。接下来，本文以 CSF 资产识别和管理安全成果为例，提供针对 AM 的指导，并识别 AM 几何和过程定义，以帮助制造商绘制数据流和记录过程。最后，本文使用开放式安全控制评估语言，以严格和可追踪的方式，将特定于 AM 的指导与现有的 IT 和 OT 安全指导集成在一起。本文的贡献在于展示了基于风险的分层方法如何支持特定于 AM 的安全指南的创作、发布和管理，而这些安全指南是目前所缺乏的。作者认为，分层方法的实现将为 AM 带来增值的、非冗余的安全指导，这与之前存在的指导是一致的。"
    },
    {
        "title": "FLEDGE: Ledger-based Federated Learning Resilient to Inference and\n  Backdoor Attacks",
        "url": "http://arxiv.org/abs/2310.02113v1",
        "pub_date": "2023-10-03",
        "summary": "Federated learning (FL) is a distributed learning process that uses a trusted\naggregation server to allow multiple parties (or clients) to collaboratively\ntrain a machine learning model without having them share their private data.\nRecent research, however, has demonstrated the effectiveness of inference and\npoisoning attacks on FL. Mitigating both attacks simultaneously is very\nchallenging. State-of-the-art solutions have proposed the use of poisoning\ndefenses with Secure Multi-Party Computation (SMPC) and/or Differential Privacy\n(DP). However, these techniques are not efficient and fail to address the\nmalicious intent behind the attacks, i.e., adversaries (curious servers and/or\ncompromised clients) seek to exploit a system for monetization purposes. To\novercome these limitations, we present a ledger-based FL framework known as\nFLEDGE that allows making parties accountable for their behavior and achieve\nreasonable efficiency for mitigating inference and poisoning attacks. Our\nsolution leverages crypto-currency to increase party accountability by\npenalizing malicious behavior and rewarding benign conduct. We conduct an\nextensive evaluation on four public datasets: Reddit, MNIST, Fashion-MNIST, and\nCIFAR-10. Our experimental results demonstrate that (1) FLEDGE provides strong\nprivacy guarantees for model updates without sacrificing model utility; (2)\nFLEDGE can successfully mitigate different poisoning attacks without degrading\nthe performance of the global model; and (3) FLEDGE offers unique reward\nmechanisms to promote benign behavior during model training and/or model\naggregation.",
        "translated": "联邦学习(Federated Learning，FL)是一种分布式学习过程，它使用一个可信的聚合服务器，允许多方(或客户)协作训练一个机器学习模型，而不需要他们共享他们的私有数据。然而，最近的研究已经证明了推理和中毒攻击对 FL 的有效性。同时减轻这两种攻击是非常具有挑战性的。最先进的解决方案已经提出使用安全多方计算(SMPC)和/或差分隐私(DP)来防止中毒。然而，这些技术并不高效，并且无法解决攻击背后的恶意意图，也就是说，对手(好奇的服务器和/或受损的客户端)试图利用系统来赚钱。为了克服这些限制，我们提出了一个基于分类账的 FL 框架，称为 FLEDGE，它允许让各方对自己的行为负责，并实现合理的效率，以减轻推理和中毒攻击。我们的解决方案利用加密货币，通过惩罚恶意行为和奖励良性行为来增加当事人的责任。我们对四个公共数据集进行了广泛的评估: Reddit，MNIST，Fashion-MNIST 和 CIFAR-10。我们的实验结果表明: (1) FLEDGE 在不牺牲模型实用性的前提下为模型更新提供了强有力的隐私保障; (2) FLEDGE 能够在不降低全局模型性能的情况下成功地减轻不同的中毒攻击; (3) FLEDGE 提供了独特的奖励机制，在模型训练和/或模型聚合过程中促进良性行为。"
    },
    {
        "title": "Gotta Catch 'em All: Aggregating CVSS Scores",
        "url": "http://arxiv.org/abs/2310.02062v1",
        "pub_date": "2023-10-03",
        "summary": "Security metrics are not standardized, but inter-national proposals such as\nthe Common Vulnerability ScoringSystem (CVSS) for quantifying the severity of\nknown vulnerabil-ities are widely used. Many CVSS aggregation mechanisms\nhavebeen proposed in the literature. Nevertheless, factors related tothe\ncontext of the System Under Test (SUT) are not taken intoaccount in the\naggregation process; vulnerabilities that in theoryaffect the SUT, but are not\nexploitable in reality. We propose aCVSS aggregation algorithm that integrates\ninformation aboutthe functionality disruption of the SUT, exploitation\ndifficulty,existence of exploits, and the context where the SUT operates.The\naggregation algorithm was applied to OpenPLC V3, showingthat it is capable of\nfiltering out vulnerabilities that cannot beexploited in the real conditions of\ndeployment of the particularsystem. Finally, because of the nature of the\nproposed algorithm,the result can be interpreted in the same way as a normal\nCVSS.",
        "translated": "安全性指标并没有标准化，但是国际上的提议，如通用漏洞评分系统(CVSS) ，用于量化已知漏洞的严重程度，得到了广泛的应用。文献中提出了许多 CVSS 聚集机制。然而，与被测系统(SUT)环境相关的因素在聚合过程中没有考虑到; 理论上影响 SUT 的脆弱性，但实际上不可利用。我们提出了一种 CVSS 聚合算法，它集成了 SUT 的功能中断、开发难度、开发存在性以及 SUT 运行的上下文等信息。将聚合算法应用于 OpenPLC V3，表明该算法能够过滤掉在部署特定系统的实际情况下无法利用的漏洞。最后，由于该算法的性质，结果可以解释为一个正常的 CVSS 相同的方式。"
    },
    {
        "title": "Security Weaknesses of Copilot Generated Code in GitHub",
        "url": "http://arxiv.org/abs/2310.02059v1",
        "pub_date": "2023-10-03",
        "summary": "Modern code generation tools use AI models, particularly Large Language\nModels (LLMs), to generate functional and complete code. While such tools are\nbecoming popular and widely available for developers, using these tools is\noften accompanied by security challenges. Therefore, it is important to assess\nthe quality of the generated code, especially in terms of its security.\nResearchers have recently explored various aspects of code generation tools,\nincluding security. However, many open questions about the security of the\ngenerated code require further investigation, especially the security issues of\nautomatically generated code in the wild. To this end, we conducted an\nempirical study by analyzing the security weaknesses in code snippets generated\nby GitHub Copilot that are found as part of publicly available projects hosted\non GitHub. The goal is to investigate the types of security issues and their\nscale in real-world scenarios (rather than crafted scenarios). To this end, we\nidentified 435 code snippets generated by Copilot from publicly available\nprojects. We then conducted extensive security analysis to identify Common\nWeakness Enumeration (CWE) instances in these code snippets. The results show\nthat (1) 35.8% of Copilot generated code snippets contain CWEs, and those\nissues are spread across multiple languages, (2) the security weaknesses are\ndiverse and related to 42 different CWEs, in which CWE-78: OS Command\nInjection, CWE-330: Use of Insufficiently Random Values, and CWE-703: Improper\nCheck or Handling of Exceptional Conditions occurred the most frequently, and\n(3) among the 42 CWEs identified, 11 of those belong to the currently\nrecognized 2022 CWE Top-25. Our findings confirm that developers should be\ncareful when adding code generated by Copilot (and similar AI code generation\ntools) and should also run appropriate security checks as they accept the\nsuggested code.",
        "translated": "现代代码生成工具使用人工智能模型，特别是大型语言模型(LLM)来生成功能完整的代码。虽然这些工具正在变得流行并且广泛地为开发人员所用，但是使用这些工具往往伴随着安全性方面的挑战。因此，评估生成的代码的质量非常重要，特别是在其安全性方面。研究人员最近探索了代码生成工具的各个方面，包括安全性。然而，有关生成代码的安全性的许多公开问题需要进一步研究，特别是自动生成代码的安全性问题。为此，我们进行了一项实证研究，分析了 GitHub Copilot 生成的代码片段的安全缺陷，这些代码片段是 GitHub 上公开可用项目的一部分。我们的目标是研究实际场景(而不是精心设计的场景)中的安全问题类型及其规模。为此，我们确定了 Copilot 从公开项目中生成的435个代码片段。然后，我们进行了广泛的安全性分析，以确定这些代码段中的公共弱点枚举(CWE)实例。结果显示: (1)35.8% 的 Copilot 生成的代码片段包含 CWEs，并且这些问题在多种语言之间传播; (2)安全弱点是多样的，与42个不同的 CWEs 有关，其中 CWE-78: OS 命令注入，CWE-330: 使用不充分的随机值，和 CWE-703: 异常条件的不当检查或处理发生最频繁，(3)在确定的42个 CWEs 中，其中11个属于目前公认的2022 CWE Top-25。我们的研究结果证实，开发人员在添加由 Copilot (以及类似的 AI 代码生成工具)生成的代码时应该小心谨慎，并且在接受建议的代码时应该运行适当的安全检查。"
    },
    {
        "title": "Steganalysis of AI Models LSB Attacks",
        "url": "http://arxiv.org/abs/2310.01969v1",
        "pub_date": "2023-10-03",
        "summary": "Artificial intelligence has made significant progress in the last decade,\nleading to a rise in the popularity of model sharing. The model zoo ecosystem,\na repository of pre-trained AI models, has advanced the AI open-source\ncommunity and opened new avenues for cyber risks. Malicious attackers can\nexploit shared models to launch cyber-attacks. This work focuses on the\nsteganalysis of injected malicious Least Significant Bit (LSB) steganography\ninto AI models, and it is the first work focusing on AI model attacks. In\nresponse to this threat, this paper presents a steganalysis method specifically\ntailored to detect and mitigate malicious LSB steganography attacks based on\nsupervised and unsupervised AI detection steganalysis methods. Our proposed\ntechnique aims to preserve the integrity of shared models, protect user trust,\nand maintain the momentum of open collaboration within the AI community. In\nthis work, we propose 3 steganalysis methods and open source our code. We found\nthat the success of the steganalysis depends on the LSB attack location. If the\nattacker decides to exploit the least significant bits in the LSB, the ability\nto detect the attacks is low. However, if the attack is in the most significant\nLSB bits, the attack can be detected with almost perfect accuracy.",
        "translated": "在过去的十年里，人工智能取得了重大的进步，导致了模型共享的流行。动物园模型生态系统是一个预先训练好的人工智能模型库，它推动了人工智能开源社区的发展，并为网络风险开辟了新的途径。恶意攻击者可以利用共享模型发动网络攻击。本文主要针对人工智能模型中注入的恶意最小有效位(LSB)隐写技术进行了研究，是针对人工智能模型攻击的首次研究。针对这种威胁，本文提出了一种基于监督和非监督 AI 检测隐写分析方法的专门用于检测和减轻恶意 LSB 隐写攻击的隐写分析方法。我们提出的技术旨在保持共享模型的完整性，保护用户信任，并保持人工智能社区内开放协作的势头。在这项工作中，我们提出了3个隐写分析方法和开放源码我们的代码。我们发现隐写分析的成功与否取决于 LSB 攻击的位置。如果攻击者决定利用 LSB 中最不重要的位，那么检测攻击的能力就很低。然而，如果攻击是在最重要的 LSB 位，攻击可以检测到几乎完美的准确性。"
    },
    {
        "title": "Beyond Labeling Oracles: What does it mean to steal ML models?",
        "url": "http://arxiv.org/abs/2310.01959v1",
        "pub_date": "2023-10-03",
        "summary": "Model extraction attacks are designed to steal trained models with only query\naccess, as is often provided through APIs that ML-as-a-Service providers offer.\nML models are expensive to train, in part because data is hard to obtain, and a\nprimary incentive for model extraction is to acquire a model while incurring\nless cost than training from scratch. Literature on model extraction commonly\nclaims or presumes that the attacker is able to save on both data acquisition\nand labeling costs. We show that the attacker often does not. This is because\ncurrent attacks implicitly rely on the adversary being able to sample from the\nvictim model's data distribution. We thoroughly evaluate factors influencing\nthe success of model extraction. We discover that prior knowledge of the\nattacker, i.e. access to in-distribution data, dominates other factors like the\nattack policy the adversary follows to choose which queries to make to the\nvictim model API. Thus, an adversary looking to develop an equally capable\nmodel with a fixed budget has little practical incentive to perform model\nextraction, since for the attack to work they need to collect in-distribution\ndata, saving only on the cost of labeling. With low labeling costs in the\ncurrent market, the usefulness of such attacks is questionable. Ultimately, we\ndemonstrate that the effect of prior knowledge needs to be explicitly decoupled\nfrom the attack policy. To this end, we propose a benchmark to evaluate attack\npolicy directly.",
        "translated": "模型提取攻击的目的是窃取只有查询访问的训练有素的模型，这通常是通过 ML-as-a-Service 提供商提供的 API 提供的。机器学习模型的训练成本很高，部分原因是数据很难获得，而模型提取的一个主要动机是获得一个模型，与从头开始训练相比，获得一个模型的成本更低。有关模型提取的文献通常声称或假定攻击者能够节省数据采集和标记成本。我们表明攻击者通常不会。这是因为当前的攻击隐式地依赖于对手能够从受害者模型的数据分布中采样。我们深入评估了影响模型提取成功的因素。我们发现，攻击者的先验知识(即对分布内数据的访问)支配着其他因素，如攻击者遵循的攻击策略，以选择对受害者模型 API 进行哪些查询。因此，一个对手寻求开发一个同样有能力的固定预算模型，几乎没有实际的动机来执行模型提取，因为为了攻击的工作，他们需要收集内部分发的数据，只节省了标签的成本。由于当前市场的贴标成本较低，这种攻击的有用性值得怀疑。最后，我们证明了先验知识的影响需要与攻击策略显式地解耦。为此，我们提出了一个直接评估攻击策略的基准。"
    },
    {
        "title": "Waveform Manipulation Against DNN-based Modulation Classification\n  Attacks",
        "url": "http://arxiv.org/abs/2310.01894v1",
        "pub_date": "2023-10-03",
        "summary": "In this paper we propose a method for defending against an eavesdropper that\nuses a Deep Neural Network (DNN) for learning the modulation of wireless\ncommunication signals. Our method is based on manipulating the emitted waveform\nwith the aid of a continuous time frequency-modulated (FM) obfuscating signal\nthat is mixed with the modulated data. The resulting waveform allows a\nlegitimate receiver (LRx) to demodulate the data but it increases the test\nerror of a pre-trained or adversarially-trained DNN classifier at the\neavesdropper. The scheme works for analog modulation and digital single carrier\nand multi carrier orthogonal frequency division multiplexing (OFDM) waveforms,\nwhile it can implemented in frame-based wireless protocols. The results\nindicate that careful selection of the parameters of the obfuscating waveform\ncan drop classification performance at the eavesdropper to less than 10% in\nAWGN and fading channels with no performance loss at the LRx.",
        "translated": "本文提出了一种利用深度神经网络(DNN)学习无线通信信号调制的防窃听方法。我们的方法是基于一个连续的时间频率调制(FM)模糊信号与调制数据混合的帮助下操纵发射波形。由此产生的波形允许合法接收机(LRx)解调数据，但它增加了测试误差的预先训练或对抗训练 DNN 分类器在窃听。该方案适用于模拟调制、数字单载波和多载波正交频分复用(OFDM)波形，同时可以在基于帧的无线协议中实现。结果表明，在 AWGN 和衰落信道中，仔细选择混淆波形参数可以使窃听者的分类性能降低到10% 以下，而 LRx 信道则没有性能损失。"
    },
    {
        "title": "Enhancing Workflow Security in Multi-Cloud Environments through\n  Monitoring and Adaptation upon Cloud Service and Network Security Violations",
        "url": "http://arxiv.org/abs/2310.01878v1",
        "pub_date": "2023-10-03",
        "summary": "Cloud computing has emerged as a crucial solution for handling data- and\ncompute-intensive workflows, offering scalability to address dynamic demands.\nHowever, ensuring the secure execution of workflows in the untrusted\nmulti-cloud environment poses significant challenges, given the sensitive\nnature of the involved data and tasks. The lack of comprehensive approaches for\ndetecting attacks during workflow execution, coupled with inadequate measures\nfor reacting to security and privacy breaches has been identified in the\nliterature. To close this gap, in this work, we propose an approach that\nfocuses on monitoring cloud services and networks to detect security violations\nduring workflow executions. Upon detection, our approach selects the optimal\nadaptation action to minimize the impact on the workflow. To mitigate the\nuncertain cost associated with such adaptations and their potential impact on\nother tasks in the workflow, we employ adaptive learning to determine the most\nsuitable adaptation action. Our approach is evaluated based on the performance\nof the detection procedure and the impact of the selected adaptations on the\nworkflows.",
        "translated": "云计算已经成为处理数据和计算密集型工作流的关键解决方案，为解决动态需求提供了可伸缩性。然而，鉴于所涉及的数据和任务的敏感性，确保在不可信的多云环境中安全地执行工作流构成了重大挑战。文献中指出，缺乏在工作流执行过程中检测攻击的综合方法，加上应对安全和隐私破坏的措施不足。为了弥补这一差距，在本文中，我们提出了一种方法，该方法侧重于监视云服务和网络，以便在工作流执行期间检测安全违规行为。在检测后，我们的方法选择最佳的适应行动，以最小化对工作流的影响。为了减少与这些适应有关的不确定成本及其对工作流程中其他任务的潜在影响，我们采用在线机机器学习来确定最合适的适应行动。我们的方法是基于检测过程的性能和选择的适应性对工作流的影响进行评估的。"
    },
    {
        "title": "Towards Stable Backdoor Purification through Feature Shift Tuning",
        "url": "http://arxiv.org/abs/2310.01875v1",
        "pub_date": "2023-10-03",
        "summary": "It has been widely observed that deep neural networks (DNN) are vulnerable to\nbackdoor attacks where attackers could manipulate the model behavior\nmaliciously by tampering with a small set of training samples. Although a line\nof defense methods is proposed to mitigate this threat, they either require\ncomplicated modifications to the training process or heavily rely on the\nspecific model architecture, which makes them hard to deploy into real-world\napplications. Therefore, in this paper, we instead start with fine-tuning, one\nof the most common and easy-to-deploy backdoor defenses, through comprehensive\nevaluations against diverse attack scenarios. Observations made through initial\nexperiments show that in contrast to the promising defensive results on high\npoisoning rates, vanilla tuning methods completely fail at low poisoning rate\nscenarios. Our analysis shows that with the low poisoning rate, the\nentanglement between backdoor and clean features undermines the effect of\ntuning-based defenses. Therefore, it is necessary to disentangle the backdoor\nand clean features in order to improve backdoor purification. To address this,\nwe introduce Feature Shift Tuning (FST), a method for tuning-based backdoor\npurification. Specifically, FST encourages feature shifts by actively deviating\nthe classifier weights from the originally compromised weights. Extensive\nexperiments demonstrate that our FST provides consistently stable performance\nunder different attack settings. Additionally, it is also convenient to deploy\nin real-world scenarios with significantly reduced computation costs. Our codes\nare available at\n\\url{https://github.com/AISafety-HKUST/stable_backdoor_purification}.",
        "translated": "深度神经网络(DNN)容易受到后门攻击，攻击者可以通过篡改一小组训练样本来恶意操纵模型行为。尽管提出了一系列的防御方法来减轻这种威胁，但它们要么需要对训练过程进行复杂的修改，要么严重依赖于特定的模型架构，这使得它们很难部署到实际应用中。因此，在本文中，我们将通过针对不同攻击场景的综合评估，从最常见且易于部署的后门防御之一的微调开始。通过最初的实验观察表明，与高中毒率的有希望的防御结果相反，香草调整方法在低中毒率的情况下完全失败。我们的分析表明，由于中毒率较低，后门和干净功能之间的纠缠会破坏基于调优的防御的效果。因此，有必要解开后门和清洁功能，以提高后门的净化。为了解决这个问题，我们介绍了 FeatureShift 优化(FST) ，一种基于优化的后门净化方法。具体来说，FST 通过主动偏离分类器权重来鼓励特征转移。大量的实验表明，我们的 FST 提供了一致的稳定性能在不同的攻击设置。此外，在实际场景中部署也非常方便，计算成本显著降低。我们的代码可以在 url { https://github.com/aisafety-hkust/stable_backdoor_purification }获得。"
    },
    {
        "title": "Multi-class Network Intrusion Detection with Class Imbalance via LSTM &amp;\n  SMOTE",
        "url": "http://arxiv.org/abs/2310.01850v1",
        "pub_date": "2023-10-03",
        "summary": "Monitoring network traffic to maintain the quality of service (QoS) and to\ndetect network intrusions in a timely and efficient manner is essential. As\nnetwork traffic is sequential, recurrent neural networks (RNNs) such as long\nshort-term memory (LSTM) are suitable for building network intrusion detection\nsystems. However, in the case of a few dataset examples of the rare attack\ntypes, even these networks perform poorly. This paper proposes to use\noversampling techniques along with appropriate loss functions to handle class\nimbalance for the detection of various types of network intrusions. Our deep\nlearning model employs LSTM with fully connected layers to perform multi-class\nclassification of network attacks. We enhance the representation of minority\nclasses: i) through the application of the Synthetic Minority Over-sampling\nTechnique (SMOTE), and ii) by employing categorical focal cross-entropy loss to\napply a focal factor to down-weight examples of the majority classes and focus\nmore on hard examples of the minority classes. Extensive experiments on KDD99\nand CICIDS2017 datasets show promising results in detecting network intrusions\n(with many rare attack types, e.g., Probe, R2L, DDoS, PortScan, etc.).",
        "translated": "监测网络流量以维持服务质量(QoS)并及时有效地检测网络入侵是必不可少的。由于网络流量是连续的，所以长时记忆(LSTM)等递归神经网络(RNN)适合于构建网络入侵检测系统。然而，在一些罕见攻击类型的数据集例子中，即使是这些网络也表现不佳。本文提出利用过采样技术和适当的损失函数来处理类不平衡，以检测各种类型的网络入侵。我们的深度学习模型使用 LSTM 和完全连通的层来执行网络攻击的多类分类。我们增强了少数群体的代表性: i)通过应用合成少数群体过度抽样技术(SMOTE) ，以及 ii)通过使用分类焦点交叉熵损失来应用焦点因素来降低多数群体的实例，并更多地关注少数群体的硬实例。在 KDD99和 CICIDS2017数据集上的广泛实验显示了在检测网络入侵(使用许多罕见的攻击类型，例如，探针、 R2L、 DDoS、 PortScan 等)方面有希望的结果。"
    },
    {
        "title": "AutoLoRa: A Parameter-Free Automated Robust Fine-Tuning Framework",
        "url": "http://arxiv.org/abs/2310.01818v1",
        "pub_date": "2023-10-03",
        "summary": "Robust Fine-Tuning (RFT) is a low-cost strategy to obtain adversarial\nrobustness in downstream applications, without requiring a lot of computational\nresources and collecting significant amounts of data. This paper uncovers an\nissue with the existing RFT, where optimizing both adversarial and natural\nobjectives through the feature extractor (FE) yields significantly divergent\ngradient directions. This divergence introduces instability in the optimization\nprocess, thereby hindering the attainment of adversarial robustness and\nrendering RFT highly sensitive to hyperparameters. To mitigate this issue, we\npropose a low-rank (LoRa) branch that disentangles RFT into two distinct\ncomponents: optimizing natural objectives via the LoRa branch and adversarial\nobjectives via the FE. Besides, we introduce heuristic strategies for\nautomating the scheduling of the learning rate and the scalars of loss terms.\nExtensive empirical evaluations demonstrate that our proposed automated RFT\ndisentangled via the LoRa branch (AutoLoRa) achieves new state-of-the-art\nresults across a range of downstream tasks. AutoLoRa holds significant\npractical utility, as it automatically converts a pre-trained FE into an\nadversarially robust model for downstream tasks without the need for searching\nhyperparameters.",
        "translated": "鲁棒微调(RFT)是一种低成本策略，可以在不需要大量计算资源和收集大量数据的情况下，在下游应用中获得对抗性鲁棒性。本文揭示了现有 RFT 的一个问题，即通过特征提取器(FE)优化敌对目标和自然目标会产生明显的梯度方向分歧。这种分歧在优化过程中引入了不稳定性，从而阻碍了对抗鲁棒性的实现，并使 RFT 对超参数高度敏感。为了缓解这一问题，我们建议设立一个低级别(LoRa)分支机构，将 RFT 分解为两个不同的组成部分: 通过 LoRa 分支机构优化自然目标和通过 FE 优化对抗目标。此外，我们还介绍了自动调度学习率和损失项标量的启发式策略。广泛的实证评估表明，我们提出的自动化 RFT 通过 LoRa 分公司(AutoloRa)实现了一系列下游任务的新的最先进的结果。AutoLoRa 具有重要的实用价值，因为它可以自动地将预先训练好的 FE 转换为下游任务的对抗鲁棒模型，而不需要搜索超参数。"
    },
    {
        "title": "No Forking Way: Detecting Cloning Attacks on Intel SGX Applications",
        "url": "http://arxiv.org/abs/2310.03002v1",
        "pub_date": "2023-10-04",
        "summary": "Forking attacks against TEEs like Intel SGX can be carried out either by\nrolling back the application to a previous state, or by cloning the application\nand by partitioning its inputs across the cloned instances. Current solutions\nto forking attacks require Trusted Third Parties (TTP) that are hard to find in\nreal-world deployments. In the absence of a TTP, many TEE applications rely on\nmonotonic counters to mitigate forking attacks based on rollbacks; however,\nthey have no protection mechanism against forking attack based on cloning. In\nthis paper, we analyze 72 SGX applications and show that approximately 20% of\nthose are vulnerable to forking attacks based on cloning - including those that\nrely on monotonic counters. To address this problem, we present CloneBuster,\nthe first practical clone-detection mechanism for Intel SGX that does not rely\non a TTP and, as such, can be used directly to protect existing applications.\nCloneBuster allows enclaves to (self-) detect whether another enclave with the\nsame binary is running on the same platform. To do so, CloneBuster relies on a\ncache-based covert channel for enclaves to signal their presence to (and detect\nthe presence of) clones on the same machine. We show that CloneBuster is robust\ndespite a malicious OS, only incurs a marginal impact on the application\nperformance, and adds approximately 800 LoC to the TCB. When used in\nconjunction with monotonic counters, CloneBuster allows applications to benefit\nfrom a comprehensive protection against forking attacks.",
        "translated": "针对 TEE (如 Intel SGX)的分叉攻击可以通过将应用程序回滚到以前的状态来实现，也可以通过克隆应用程序并在克隆的实例之间对其输入进行分区来实现。当前的分叉攻击解决方案需要可信的第三方(TTP) ，这在现实部署中很难找到。在没有 TTP 的情况下，许多 TEE 应用程序依赖于单调计数器来减轻基于回滚的分叉攻击; 但是，它们没有基于克隆的分叉攻击保护机制。在本文中，我们分析了72个 SGX 应用程序，结果表明大约20% 的应用程序容易受到基于克隆的分叉攻击——包括那些依赖单调计数器的应用程序。为了解决这个问题，我们介绍了 CloneBuster，这是 Intel SGX 第一个实用的克隆检测机制，它不依赖于 TTP，因此可以直接用于保护现有应用程序。CloneBuster 允许飞地(自我)检测具有相同二进制文件的另一飞地是否在同一平台上运行。为此，CloneBuster 依靠一个基于缓存的隐蔽通道来向同一台机器上的克隆发出存在信号(并检测存在)。我们表明，尽管存在恶意操作系统，CloneBuster 仍然是健壮的，对应用程序性能的影响很小，并且为 TCB 增加了大约800个 LoC。当与单调计数器一起使用时，CloneBuster 允许应用程序受益于全面的防止分叉攻击的保护。"
    },
    {
        "title": "Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models",
        "url": "http://arxiv.org/abs/2310.02949v1",
        "pub_date": "2023-10-04",
        "summary": "Warning: This paper contains examples of harmful language, and reader\ndiscretion is recommended. The increasing open release of powerful large\nlanguage models (LLMs) has facilitated the development of downstream\napplications by reducing the essential cost of data annotation and computation.\nTo ensure AI safety, extensive safety-alignment measures have been conducted to\narmor these models against malicious use (primarily hard prompt attack).\nHowever, beneath the seemingly resilient facade of the armor, there might lurk\na shadow. By simply tuning on 100 malicious examples with 1 GPU hour, these\nsafely aligned LLMs can be easily subverted to generate harmful content.\nFormally, we term a new attack as Shadow Alignment: utilizing a tiny amount of\ndata can elicit safely-aligned models to adapt to harmful tasks without\nsacrificing model helpfulness. Remarkably, the subverted models retain their\ncapability to respond appropriately to regular inquiries. Experiments across 8\nmodels released by 5 different organizations (LLaMa-2, Falcon, InternLM,\nBaiChuan2, Vicuna) demonstrate the effectiveness of shadow alignment attack.\nBesides, the single-turn English-only attack successfully transfers to\nmulti-turn dialogue and other languages. This study serves as a clarion call\nfor a collective effort to overhaul and fortify the safety of open-source LLMs\nagainst malicious attackers.",
        "translated": "警告: 本文包含有害语言的例子，建议读者慎用。强大的大型语言模型(LLM)不断增加的开放版本降低了数据注释和计算的基本成本，从而促进了下游应用程序的开发。为了确保人工智能的安全性，已经采取了广泛的安全校准措施来防范这些模型的恶意使用(主要是硬提示攻击)。然而，在盔甲看似坚固的外表下，可能潜藏着一个阴影。通过使用1个 GPU 小时调优100个恶意示例，这些安全对齐的 LLM 很容易被破坏，从而产生有害的内容。形式上，我们称一种新的攻击为“阴影对齐”: 利用极少量的数据可以引出安全对齐的模型来适应有害的任务，而不牺牲模型的帮助性。值得注意的是，被颠覆的模型保留了对常规询问作出适当反应的能力。通过对5个不同组织(LLaMa-2、 Falcon、 InternLM、 BaiChuan2、 Vicuna)发布的8个模型的实验，验证了阴影对准攻击的有效性。此外，单回合英语攻击成功地转换为多回合对话和其他语言。这项研究号召大家共同努力，彻底检查和加强开源 LLM 对恶意攻击者的安全性。"
    },
    {
        "title": "Circular external difference families, graceful labellings and cyclotomy",
        "url": "http://arxiv.org/abs/2310.02810v1",
        "pub_date": "2023-10-04",
        "summary": "(Strong) circular external difference families (which we denote as CEDFs and\nSCEDFs) can be used to construct nonmalleable threshold schemes. They are a\nvariation of (strong) external difference families, which have been extensively\nstudied in recent years. We provide a variety of constructions for CEDFs based\non graceful labellings ($\\alpha$-valuations) of lexicographic products $C_n\n\\boldsymbol{\\cdot} K_{\\ell}^c$, where $C_n$ denotes a cycle of length $n$. We\ndo not have any nontrivial examples of SCEDFs. However, we can construct close\napproximations (more specifically, certain types of circular algebraic\nmanipulation detection (AMD) codes) using the theory of cyclotomic numbers in\nfinite fields.",
        "translated": "(强)循环外部差分族(我们称之为 CEDFs 和 SCEDFs)可以用来构造不可延展的阈值方案。它们是近年来得到广泛研究的一类(强)外差族的变体。我们基于字典产品的优美标号($alpha $- 赋值)为 CEDF 提供了多种构造，其中 $C _ n $表示长度为 $n $的循环。我们没有任何 SCEDF 的重要示例。然而，我们可以构造闭近似(更具体地说，某些类型的圆代数操纵检测(AMD)码)使用分圆数的理论在有限领域。"
    },
    {
        "title": "Dissecting Smart Contract Languages: A Survey",
        "url": "http://arxiv.org/abs/2310.02799v1",
        "pub_date": "2023-10-04",
        "summary": "Blockchain is a distributed ledger technology that gained popularity for\nenabling the transformation of cryptocurrency among peers without mediation by\na centralized third-party authority. Smart contracts expand the applications of\nblockchain technology and have played a role in its widespread adoption. Smart\ncontracts are immutable digital programs that are deployed on blockchains to\ncodify agreements between parties. Existing smart contract implementations have\nfaced challenges, including security vulnerabilities, leading to significant\nlosses and concerns. This has stimulated a wave of attempts to improve Smart\nContract Languages (SCLs) to overcome implementation challenges and ensure code\nquality, producing many languages with diverse features. Scholars have made\nsome attempts to classify SCLs and clarify the process of selecting an SCL, but\nto the best of our knowledge, no comprehensive survey of existing SCLs has been\npublished. Our work surpasses earlier efforts by evaluating a significantly\nlarger set of SCLs, in greater depth, to ease the process of SCL selection for\nblockchain research and implementation. In this paper, we (1) propose a robust\nframework for comparing existing SCLs, (2) analyze and discuss 36 SCLs,\naddressing issues beyond those used to construct the comparison framework, and\n(3) define new parameters for future research and development of SCLs. The\nsurvey provides a guide for those who intend to select or use an SCL to\nimplement smart contracts, develop new SCLs, or add new extensions to the\nexisting SCLs.",
        "translated": "区块链(Blockchain)是一种分布式分类帐技术，它因支持在无需集中的第三方权威中介的情况下在对等点之间进行加密货币转换而受到欢迎。智能合同扩大了区块链技术的应用，并在其广泛采用中发挥了作用。智能合同是不可变的数字程序，部署在区块链上，以编纂各方之间的协议。现有的智能合同实施面临挑战，包括安全漏洞，导致重大损失和担忧。这引发了改进智能契约语言(Smart ContractLanguage，SCL)的浪潮，以克服实现挑战并确保代码质量，从而产生了许多具有不同特性的语言。学者们曾经尝试对 SCL 进行分类并阐明选择 SCL 的过程，但就我们所知，还没有对现有 SCL 进行全面的调查。我们的工作超越了早期的努力，通过评估更大的 SCL 集合，更深入地，以简化区块链研究和实施的 SCL 选择过程。本文(1)提出了一个比较现有 SCL 的稳健框架，(2)分析和讨论了36个 SCL，解决了构建比较框架以外的问题，(3)为今后 SCL 的研究和发展定义了新的参数。该调查为那些打算选择或使用 SCL 来实现智能契约、开发新 SCL 或向现有 SCL 添加新扩展的人提供了指南。"
    },
    {
        "title": "AGIR: Automating Cyber Threat Intelligence Reporting with Natural\n  Language Generation",
        "url": "http://arxiv.org/abs/2310.02655v1",
        "pub_date": "2023-10-04",
        "summary": "Cyber Threat Intelligence (CTI) reporting is pivotal in contemporary risk\nmanagement strategies. As the volume of CTI reports continues to surge, the\ndemand for automated tools to streamline report generation becomes increasingly\napparent. While Natural Language Processing techniques have shown potential in\nhandling text data, they often struggle to address the complexity of diverse\ndata sources and their intricate interrelationships. Moreover, established\nparadigms like STIX have emerged as de facto standards within the CTI\ncommunity, emphasizing the formal categorization of entities and relations to\nfacilitate consistent data sharing. In this paper, we introduce AGIR (Automatic\nGeneration of Intelligence Reports), a transformative Natural Language\nGeneration tool specifically designed to address the pressing challenges in the\nrealm of CTI reporting. AGIR's primary objective is to empower security\nanalysts by automating the labor-intensive task of generating comprehensive\nintelligence reports from formal representations of entity graphs. AGIR\nutilizes a two-stage pipeline by combining the advantages of template-based\napproaches and the capabilities of Large Language Models such as ChatGPT. We\nevaluate AGIR's report generation capabilities both quantitatively and\nqualitatively. The generated reports accurately convey information expressed\nthrough formal language, achieving a high recall value (0.99) without\nintroducing hallucination. Furthermore, we compare the fluency and utility of\nthe reports with state-of-the-art approaches, showing how AGIR achieves higher\nscores in terms of Syntactic Log-Odds Ratio (SLOR) and through questionnaires.\nBy using our tool, we estimate that the report writing time is reduced by more\nthan 40%, therefore streamlining the CTI production of any organization and\ncontributing to the automation of several CTI tasks.",
        "translated": "网络威胁情报(CTI)报告是当代风险管理策略的关键。随着气候技术倡议报告的数量继续激增，对简化报告生成的自动化工具的需求日益明显。虽然自然语言处理技术在处理文本数据方面显示出潜力，但它们往往难以解决不同数据源的复杂性及其错综复杂的相互关系。此外，像 STIX 这样的既定范例已经成为 CTI 社区的事实标准，强调实体和关系的正式分类，以促进一致的数据共享。在本文中，我们介绍 AGIR (自动生成智能报告) ，一个变革性的自然语言生成工具，专门设计用于解决在 CTI 报告领域的紧迫挑战。AGIR 的主要目标是通过将劳动密集型任务自动化，从实体图形的正式表示生成全面的情报报告，从而增强安全分析师的能力。AGIR 结合了基于模板的方法的优点和 ChatGPT 等大型语言模型的功能，利用了两阶段流水线。我们对 AGIR 的报告生成能力进行了定量和定性的评估。生成的报告准确地传达了通过正式语言表达的信息，在不引入幻觉的情况下达到了较高的回忆值(0.99)。此外，我们比较了报告的流利性和实用性与最先进的方法，显示 AGIR 如何实现更高的分数在句法对数比(SLOR)和通过问卷调查。通过使用我们的工具，我们估计报告编写时间减少了40% 以上，因此简化了任何组织的 CTI 生产，并有助于几个 CTI 任务的自动化。"
    },
    {
        "title": "RLTrace: Synthesizing High-Quality System Call Traces for OS Fuzz\n  Testing",
        "url": "http://arxiv.org/abs/2310.02609v1",
        "pub_date": "2023-10-04",
        "summary": "Securing operating system (OS) kernel is one central challenge in today's\ncyber security landscape. The cutting-edge testing technique of OS kernel is\nsoftware fuzz testing. By mutating the program inputs with random variations\nfor iterations, fuzz testing aims to trigger program crashes and hangs caused\nby potential bugs that can be abused by the inputs. To achieve high OS code\ncoverage, the de facto OS fuzzer typically composes system call traces as the\ninput seed to mutate and to interact with OS kernels. Hence, quality and\ndiversity of the employed system call traces become the prominent factor to\ndecide the effectiveness of OS fuzzing. However, these system call traces to\ndate are generated with hand-coded rules, or by analyzing system call logs of\nOS utility programs. Our observation shows that such system call traces can\nonly subsume common usage scenarios of OS system calls, and likely omit hidden\nbugs.\n  In this research, we propose a deep reinforcement learning-based solution,\ncalled RLTrace, to synthesize diverse and comprehensive system call traces as\nthe seed to fuzz OS kernels. During model training, the deep learning model\ninteracts with OS kernels and infers optimal system call traces w.r.t. our\nlearning goal -- maximizing kernel code coverage. Our evaluation shows that\nRLTrace outperforms other seed generators by producing more comprehensive\nsystem call traces, subsuming system call corner usage cases and subtle\ndependencies. By feeding the de facto OS fuzzer, SYZKALLER, with system call\ntraces synthesized by RLTrace, we show that SYZKALLER can achieve higher code\ncoverage for testing Linux kernels. Furthermore, RLTrace found one\nvulnerability in the Linux kernel (version 5.5-rc6), which is publicly unknown\nto the best of our knowledge by the time of writing.",
        "translated": "保护操作系统(OS)内核是当今网络安全领域的一个核心挑战。操作系统内核的前沿测试技术是软件模糊测试。通过为迭代改变程序输入的随机变化，模糊测试旨在触发程序崩溃和由可能被输入滥用的潜在错误引起的挂起。为了实现高的操作系统代码覆盖率，事实上的操作系统模糊器通常将系统调用跟踪作为输入种子来进行变异并与操作系统内核进行交互。因此，所使用的系统调用轨迹的质量和多样性成为决定操作系统模糊化有效性的重要因素。然而，到目前为止，这些系统调用跟踪是通过手工编码规则或通过分析 OS 实用程序的系统调用日志生成的。我们的观察表明，这样的系统调用跟踪只能包含 OS 系统调用的常见使用场景，并且可能忽略隐藏的 bug。在本研究中，我们提出了一个基于深度强化学习的解决方案，称为 RLTrace，以综合多样化和全面的系统调用跟踪作为模糊操作系统内核的种子。在模型训练过程中，深度学习模型与操作系统内核交互，并推断出最优的系统调用跟踪。我们的学习目标是——最大化内核代码覆盖率。我们的评估表明，RLTrace 通过生成更全面的系统调用跟踪、包含系统调用角使用情况和微妙的依赖关系，优于其他种子生成器。通过给 SYZKALLER 事实上的操作系统模糊器提供由 RlTrace 合成的系统调用跟踪，我们表明 SYZkALLER 可以实现更高的代码覆盖率来测试 Linux 内核。此外，RLTrace 在 Linux 内核(版本5.5-rc6)中发现了一个漏洞，到编写本文时，据我们所知，这个漏洞是公开的。"
    },
    {
        "title": "Practical, Private Assurance of the Value of Collaboration",
        "url": "http://arxiv.org/abs/2310.02563v1",
        "pub_date": "2023-10-04",
        "summary": "Two parties wish to collaborate on their datasets. However, before they\nreveal their datasets to each other, the parties want to have the guarantee\nthat the collaboration would be fruitful. We look at this problem from the\npoint of view of machine learning, where one party is promised an improvement\non its prediction model by incorporating data from the other party. The parties\nwould only wish to collaborate further if the updated model shows an\nimprovement in accuracy. Before this is ascertained, the two parties would not\nwant to disclose their models and datasets. In this work, we construct an\ninteractive protocol for this problem based on the fully homomorphic encryption\nscheme over the Torus (TFHE) and label differential privacy, where the\nunderlying machine learning model is a neural network. Label differential\nprivacy is used to ensure that computations are not done entirely in the\nencrypted domain, which is a significant bottleneck for neural network training\naccording to the current state-of-the-art FHE implementations. We prove the\nsecurity of our scheme in the universal composability framework assuming\nhonest-but-curious parties, but where one party may not have any expertise in\nlabelling its initial dataset. Experiments show that we can obtain the output,\ni.e., the accuracy of the updated model, with time many orders of magnitude\nfaster than a protocol using entirely FHE operations.",
        "translated": "双方希望在他们的数据集上进行协作。然而，在他们向对方透露他们的数据集之前，双方都希望能够保证合作能够取得丰硕的成果。我们从机器学习的角度来看待这个问题，其中一方承诺通过合并另一方的数据来改进其预测模型。只有在更新后的模型显示准确性有所提高的情况下，各方才愿意进一步合作。在确定这一点之前，双方都不希望透露他们的模型和数据集。在这项工作中，我们建立了一个交互式协议的基础上，这个问题的全同态加密方案在环形(tfHE)和标签差分隐私，其中基本的机器学习模型是一个神经网络。标签差分隐私是用来确保计算不完全在加密的领域，这是一个重要的瓶颈神经网络训练根据目前的国家的先进的 FHE 实施。我们在通用可组合性框架中证明了我们的方案的安全性，假设有诚实但好奇的一方，但其中一方可能没有任何标记其初始数据集的专业知识。实验表明，我们可以得到输出，即更新模型的准确性，与使用完全 FHE 操作的协议相比，时间数量级要快很多。"
    },
    {
        "title": "zkFL: Zero-Knowledge Proof-based Gradient Aggregation for Federated\n  Learning",
        "url": "http://arxiv.org/abs/2310.02554v1",
        "pub_date": "2023-10-04",
        "summary": "Federated Learning (FL) is a machine learning paradigm, which enables\nmultiple and decentralized clients to collaboratively train a model under the\norchestration of a central aggregator. Traditional FL solutions rely on the\ntrust assumption of the centralized aggregator, which forms cohorts of clients\nin a fair and honest manner. However, a malicious aggregator, in reality, could\nabandon and replace the client's training models, or launch Sybil attacks to\ninsert fake clients. Such malicious behaviors give the aggregator more power to\ncontrol clients in the FL setting and determine the final training results. In\nthis work, we introduce zkFL, which leverages zero-knowledge proofs (ZKPs) to\ntackle the issue of a malicious aggregator during the training model\naggregation process. To guarantee the correct aggregation results, the\naggregator needs to provide a proof per round. The proof can demonstrate to the\nclients that the aggregator executes the intended behavior faithfully. To\nfurther reduce the verification cost of clients, we employ a blockchain to\nhandle the proof in a zero-knowledge way, where miners (i.e., the nodes\nvalidating and maintaining the blockchain data) can verify the proof without\nknowing the clients' local and aggregated models. The theoretical analysis and\nempirical results show that zkFL can achieve better security and privacy than\ntraditional FL, without modifying the underlying FL network structure or\nheavily compromising the training speed.",
        "translated": "联邦学习(Federated Learning，FL)是一种机器学习范式，它允许多个分散的客户在中央聚合器的编排下协同训练一个模型。传统的 FL 解决方案依赖于集中聚合器的信任假设，它以公平和诚实的方式形成客户群。然而，实际上，恶意的聚合器可以放弃并替换客户端的训练模型，或者发动 Sybil 攻击来插入虚假的客户端。这样的恶意行为给聚合器更多的权力来控制 FL 设置中的客户端并决定最终的训练结果。在这项工作中，我们介绍了 zkFL，它利用零知识证明(ZKP)来解决训练模型聚合过程中的恶意聚合器问题。为了保证正确的聚合结果，聚合器需要每轮提供一个证明。证明可以向客户机证明聚合器忠实地执行预期的行为。为了进一步降低客户端的验证成本，我们使用区块链以零知识的方式处理证明，其中挖掘者(即，验证和维护区块链数据的节点)可以在不知道客户端的本地和聚合模型的情况下验证证明。理论分析和实验结果表明，zkFL 在不改变底层 FL 网络结构或严重影响训练速度的情况下，可以获得比传统 FL 更好的安全性和隐私性。"
    },
    {
        "title": "The Key to Deobfuscation is Pattern of Life, not Overcoming Encryption",
        "url": "http://arxiv.org/abs/2310.02536v1",
        "pub_date": "2023-10-04",
        "summary": "Preserving privacy is an undeniable benefit to users online. However, this\nbenefit (unfortunately) also extends to those who conduct cyber attacks and\nother types of malfeasance. In this work, we consider the scenario in which\nPrivacy Preserving Technologies (PPTs) have been used to obfuscate users who\nare communicating online with ill intentions. We present a novel methodology\nthat is effective at deobfuscating such sources by synthesizing measurements\nfrom key locations along protocol transaction paths. Our approach links online\npersonas with their origin IP addresses based on a Pattern of Life (PoL)\nanalysis, and is successful even when different PPTs are used. We show that,\nwhen monitoring in the correct places on the Internet, DNS over HTTPS (DoH) and\nDNS over TLS (DoT) can be deobfuscated with up to 100% accuracy, when they are\nthe only privacy-preserving technologies used. Our evaluation used multiple\nsimulated monitoring points and communications are sampled from an actual\nmultiyear-long social network message board to replay actual user behavior. Our\nevaluation compared plain old DNS, DoH, DoT, and VPN in order to quantify their\nrelative privacy-preserving abilities and provide recommendations for where\nideal monitoring vantage points would be in the Internet to achieve the best\nperformance. To illustrate the utility of our methodology, we created a\nproof-of-concept cybersecurity analyst dashboard (with backend processing\ninfrastructure) that uses a search engine interface to allow analysts to\ndeobfuscate sources based on observed screen names and by providing packet\ncaptures from subsets of vantage points.",
        "translated": "保护隐私对于在线用户来说是一个不可否认的好处。然而，这种好处(不幸的是)也延伸到那些谁进行网络攻击和其他类型的渎职。在这项工作中，我们考虑的情况下，隐私保护技术(PPT)已被用来混淆谁是在线交流用户的恶意。我们提出了一种新的方法，有效地解模糊这些来源的综合测量沿协议事务路径的关键位置。我们的方法基于一个生活模式(PoL)分析，将在线人物角色与其原始 IP 地址联系起来，即使使用不同的 PPT，这种方法也是成功的。我们表明，当在互联网上正确的地方进行监控时，基于 HTTPS (DoH)的 DNS 和基于 TLS (DoT)的 DNS 可以以高达100% 的准确性进行解模糊处理，而它们是唯一使用的保护隐私的技术。我们的评估使用了多个模拟监控点，并从一个实际的多年社交网络留言板中取样通信，以重现实际的用户行为。我们的评估比较了普通的旧 DNS，DoH，DoT 和 VPN，以量化它们相对的隐私保护能力，并为理想的监测有利位置在互联网上实现最佳性能提供建议。为了说明我们的方法的实用性，我们创建了一个概念验证的网络安全分析仪表板(带有后端处理基础设施) ，它使用一个搜索引擎界面，允许分析师根据观察到的屏幕名称来解释源，并通过提供来自有利位置子集的数据包捕获。"
    },
    {
        "title": "Identifying Vulnerability Patches by Comprehending Code Commits with\n  Comprehensive Change Contexts",
        "url": "http://arxiv.org/abs/2310.02530v1",
        "pub_date": "2023-10-04",
        "summary": "To help application developers apply vulnerability patches timely, security\nresearchers maintain vulnerability databases such as National Vulnerability\nDatabase (NVD). By directly monitoring NVD with the name of each used library,\napplication developers can be aware of vulnerabilities and their patches. Given\nthat the monitoring results of vulnerability patches are unreliable due to\npatch incompleteness of NVD, existing approaches employ deep-learning (DL)\nmodels to identify additional vulnerability patches by determining whether a\ncode commit fixes a vulnerability. However, these approaches suffer from low\naccuracy due to not considering code commits' comprehensive contexts such as\ncontrol/data-flow contexts or method-invocation contexts. To improve accuracy,\nwe design CompVPD, the first approach to identify vulnerability patches by\nfine-tuning a large language model (LLM) named StarCoder to comprehend code\ncommits with comprehensive contexts. Considering that including comprehensive\ncontexts needs to balance the context size and the training costs of LLM,\nCompVPD includes our two novel algorithms to generate comprehensive contexts\nwithin the given window size by removing irrelevant components (i.e., files,\nmethods, and statements) and adaptively expanding each context. We empirically\ncompare CompVPD with four state-of-the-art/practice (SOTA) approaches that\nidentify vulnerability patches. The results show that CompVPD improves the AUC\nscore by 11% and the F1 score by 30% when compared with the best scores of the\nSOTA approaches. Additionally, CompVPD provides high value to security practice\nby helping identify 20 vulnerability patches and 18 fixes of high-risk bugs\nfrom 2,500 recent code commits of five highly popular open-source projects.",
        "translated": "为了帮助应用程序开发人员及时应用漏洞补丁，安全研究人员维护漏洞数据库，如国家漏洞数据库(NVD)。通过使用每个使用过的库的名称直接监视 NVD，应用程序开发人员可以意识到漏洞及其补丁。由于 NVD 补丁的不完全性，使得漏洞补丁的监测结果不可靠，现有的方法采用深度学习(DL)模型，通过判断代码提交是否修复了漏洞来识别其他的漏洞补丁。然而，由于没有考虑代码提交的全面上下文(如控制/数据流上下文或方法调用上下文) ，这些方法的准确性较低。为了提高准确性，我们设计了 CompVPD，这是第一种通过对大型语言模型(LLM) StarCoder 进行微调来识别漏洞补丁的方法。考虑到包含全面的上下文需要平衡 LLM 的上下文大小和培训成本，CompVPD 包括我们的两个新算法，通过删除不相关的组件(即文件，方法和语句)并自适应地扩展每个上下文来在给定的窗口大小内生成全面的上下文。我们经验性地比较了 CompVPD 和四种识别漏洞补丁的最新/实践(SOTA)方法。结果表明，与 SOTA 方法的最佳评分相比，CompVPD 组 AUC 评分提高了11% ，F1评分提高了30% 。此外，CompVPD 通过帮助识别5个非常流行的开源项目最近提交的2,500个代码中的20个漏洞补丁和18个高风险 bug 修复程序，为安全实践提供了高价值。"
    },
    {
        "title": "An Uncertainty Principle for the Curvelet Transform, and the\n  Infeasibility of Quantum Algorithms for Finding Short Lattice Vectors",
        "url": "http://arxiv.org/abs/2310.03735v1",
        "pub_date": "2023-10-05",
        "summary": "The curvelet transform is a special type of wavelet transform, which is\nuseful for estimating the locations and orientations of waves propagating in\nEuclidean space. We prove an uncertainty principle that lower-bounds the\nvariance of these estimates, for radial wave functions in n dimensions.\n  As an application of this uncertainty principle, we show the infeasibility of\none approach to constructing quantum algorithms for solving lattice problems,\nsuch as the approximate shortest vector problem (approximate-SVP), and bounded\ndistance decoding (BDD). This gives insight into the computational\nintractability of approximate-SVP, which plays an important role in algorithms\nfor integer programming, and in post-quantum cryptosystems.\n  In this approach to solving lattice problems, one prepares quantum\nsuperpositions of Gaussian-like wave functions centered at lattice points. A\nkey step in this procedure requires finding the center of each Gaussian-like\nwave function, using the quantum curvelet transform. We show that, for any\nchoice of the Gaussian-like wave function, the error in this step will be above\nthe threshold required to solve BDD and approximate-SVP.",
        "translated": "曲波变换是一种特殊的小波变换，可用于估计波在欧氏空间中传播的位置和方向。证明了 n 维径向波函数估计方差的一个下界不确定性原理。作为这一不确定性原理的应用，我们证明了构造一种求解格点问题的量子算法的不可行性，例如近似最短向量问题(近似 SVP)和有界距离译码(BDD)。近似 SVP 在整数规划算法和后量子密码系统中扮演着重要角色。在这种解决晶格问题的方法中，我们准备了以晶格点为中心的类高斯波函数的量子叠加。这个过程中的一个关键步骤需要利用量子曲波变换找到每个类高斯波函数的中心。我们发现，对于任何类高斯波函数的选择，这一步的误差都将超过求解 BDD 和近似 SVP 所需的阈值。"
    },
    {
        "title": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users\n  Do Not Intend To!",
        "url": "http://arxiv.org/abs/2310.03693v1",
        "pub_date": "2023-10-05",
        "summary": "Optimizing large language models (LLMs) for downstream use cases often\ninvolves the customization of pre-trained LLMs through further fine-tuning.\nMeta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5\nTurbo on custom datasets also encourage this practice. But, what are the safety\ncosts associated with such custom fine-tuning? We note that while existing\nsafety alignment infrastructures can restrict harmful behaviors of LLMs at\ninference time, they do not cover safety risks when fine-tuning privileges are\nextended to end-users. Our red teaming studies find that the safety alignment\nof LLMs can be compromised by fine-tuning with only a few adversarially\ndesigned training examples. For instance, we jailbreak GPT-3.5 Turbo's safety\nguardrails by fine-tuning it on only 10 such examples at a cost of less than\n$0.20 via OpenAI's APIs, making the model responsive to nearly any harmful\ninstructions. Disconcertingly, our research also reveals that, even without\nmalicious intent, simply fine-tuning with benign and commonly used datasets can\nalso inadvertently degrade the safety alignment of LLMs, though to a lesser\nextent. These findings suggest that fine-tuning aligned LLMs introduces new\nsafety risks that current safety infrastructures fall short of addressing --\neven if a model's initial safety alignment is impeccable, it is not necessarily\nto be maintained after custom fine-tuning. We outline and critically analyze\npotential mitigations and advocate for further research efforts toward\nreinforcing safety protocols for the custom fine-tuning of aligned LLMs.",
        "translated": "为下游用例优化大型语言模型(LLM)通常需要通过进一步的微调对预先训练好的 LLM 进行定制。Meta 公开发布的美洲驼模型和 OpenAI 用于在定制数据集上微调 GPT-3.5 Turbo 的 API 也鼓励了这种做法。但是，与这种定制微调相关的安全成本是多少呢？我们注意到，虽然现有的安全校准基础设施可以在推断时限制 LLM 的有害行为，但是当微调特权扩展到最终用户时，它们并不涵盖安全风险。我们的红色团队研究发现，LLM 的安全校准可能会受到微调，只有几个对抗性设计的训练例子。例如，我们破解了 GPT-3.5 Turbo 的安全护栏，通过 OpenAI 的 API，我们只花了不到0.20美元就对10个这样的例子进行了微调，使得这个模型能够响应几乎所有有害的指令。令人不安的是，我们的研究还表明，即使没有恶意，简单的微调良性和常用的数据集也可能在不经意间降低 LLM 的安全校准，尽管程度较小。这些发现表明，微调一致的 LLM 引入了新的安全风险，而当前的安全基础设施没有解决这些风险——即使一个模型的初始安全一致性是无可挑剔的，也不一定要在定制的微调之后进行维护。我们概述并批判性地分析了潜在的缓解措施，并主张进一步研究加强定制微调定向 LLM 的安全协议。"
    },
    {
        "title": "Enhancing Exfiltration Path Analysis Using Reinforcement Learning",
        "url": "http://arxiv.org/abs/2310.03667v1",
        "pub_date": "2023-10-05",
        "summary": "Building on previous work using reinforcement learning (RL) focused on\nidentification of exfiltration paths, this work expands the methodology to\ninclude protocol and payload considerations. The former approach to\nexfiltration path discovery, where reward and state are associated specifically\nwith the determination of optimal paths, are presented with these additional\nrealistic characteristics to account for nuances in adversarial behavior. The\npaths generated are enhanced by including communication payload and protocol\ninto the Markov decision process (MDP) in order to more realistically emulate\nattributes of network based exfiltration events. The proposed method will help\nemulate complex adversarial considerations such as the size of a payload being\nexported over time or the protocol on which it occurs, as is the case where\nthreat actors steal data over long periods of time using system native ports or\nprotocols to avoid detection. As such, practitioners will be able to improve\nidentification of expected adversary behavior under various payload and\nprotocol assumptions more comprehensively.",
        "translated": "基于以前的工作，使用强化学习(RL)侧重于识别出口路径，这项工作扩展了方法学，以包括协议和有效载荷的考虑。前一种方法的出口路径发现，其中奖励和状态是与确定最佳路径具体相关的，提出了这些额外的现实特征，以解释在对抗行为的细微差别。生成的路径通过将通信有效载荷和协议纳入马可夫决策过程(mDP)得到增强，以便更真实地模拟基于网络的溢出事件的属性。拟议的方法将有助于模拟复杂的对抗性考虑因素，例如随着时间推移导出的有效载荷的大小或发生有效载荷的协议，例如威胁行为者使用系统本机端口或协议长时间窃取数据以避免被发现的情况。因此，从业人员将能够更全面地改进在各种有效载荷和协议假设下对预期敌对行为的识别。"
    },
    {
        "title": "Solving Degree Bounds For Iterated Polynomial Systems",
        "url": "http://arxiv.org/abs/2310.03637v1",
        "pub_date": "2023-10-05",
        "summary": "For Arithmetization-Oriented ciphers and hash functions Gr\\\"obner basis\nattacks are generally considered as the most competitive attack vector.\nUnfortunately, the complexity of Gr\\\"obner basis algorithms is only understood\nfor special cases, and it is needless to say that these cases do not apply to\nmost cryptographic polynomial systems. Therefore, cryptographers have to resort\nto experiments, extrapolations and hypotheses to assess the security of their\ndesigns. One established measure to quantify the complexity of linear\nalgebra-based Gr\\\"obner basis algorithms is the so-called solving degree.\nCaminata \\&amp; Gorla revealed that under a certain genericity condition on a\npolynomial system the solving degree is always upper bounded by the\nCastelnuovo-Mumford regularity and henceforth by the Macaulay bound, which only\ntakes the degrees and number of variables of the input polynomials into\naccount. In this paper we extend their framework to iterated polynomial\nsystems, the standard polynomial model for symmetric ciphers and hash\nfunctions. In particular, we prove solving degree bounds for various attacks on\nMiMC, Feistel-MiMC, Feistel-MiMC-Hash, Hades and GMiMC. Our bounds fall in line\nwith the hypothesized complexity of Gr\\\"obner basis attacks on these designs,\nand to the best of our knowledge this is the first time that a mathematical\nproof for these complexities is provided.\n  Moreover, by studying polynomials with degree falls we can prove lower bounds\non the Castelnuovo-Mumford regularity for attacks on MiMC, Feistel-MiMC and\nFeistel-MiMC-Hash provided that only a few solutions of the corresponding\niterated polynomial system originate from the base field. Hence,\nregularity-based solving degree estimations can never surpass a certain\nthreshold, a desirable property for cryptographic polynomial systems.",
        "translated": "对于面向算法的密码和散列函数来说，Gr“ obner 基攻击通常被认为是最具竞争力的攻击向量。遗憾的是，Gr“ obner 基算法的复杂性只能在特殊情况下理解，不用说，这些情况不适用于大多数密码多项式系统。因此，密码学家必须依靠实验、推断和假设来评估其设计的安全性。量化基于线性代数的 Gr“ obner 基算法复杂度的一个既定度量是所谓的求解度。Caminata & Gorla 揭示了在一定的广义条件下，多项式系统的解的度总是上界于 Castelnuovo-Mumford 正则性，从此上界于 Macaulay 界，即只考虑输入多项式的度和变量的个数。本文将其框架推广到迭代多项式系统、对称密码的标准多项式模型和哈希函数。特别地，我们证明了对 MiMC、 Feistel-MiMC、 Feistel-MiMC-Hash、 Hades 和 GMiMC 的各种攻击解的度界。我们的界限符合对这些设计的 Gr“ obner 基攻击的假设复杂性，据我们所知，这是第一次为这些复杂性提供数学证明。此外，通过研究具有度下降的多项式，我们可以证明攻击 miMC、 feistel-miMC 和 feistel-miMC-hash 的 Castelnuovo-Mumford 正则性的下界，只要相应的迭代多项式系统的少数解源于基域。因此，基于正则性的求解度估计永远不能超过一定的阈值，这是密码多项式系统的一个理想性质。"
    },
    {
        "title": "FASER: Binary Code Similarity Search through the use of Intermediate\n  Representations",
        "url": "http://arxiv.org/abs/2310.03605v1",
        "pub_date": "2023-10-05",
        "summary": "Being able to identify functions of interest in cross-architecture software\nis useful whether you are analysing for malware, securing the software supply\nchain or conducting vulnerability research. Cross-Architecture Binary Code\nSimilarity Search has been explored in numerous studies and has used a wide\nrange of different data sources to achieve its goals. The data sources\ntypically used draw on common structures derived from binaries such as function\ncontrol flow graphs or binary level call graphs, the output of the disassembly\nprocess or the outputs of a dynamic analysis approach. One data source which\nhas received less attention is binary intermediate representations. Binary\nIntermediate representations possess two interesting properties: they are cross\narchitecture by their very nature and encode the semantics of a function\nexplicitly to support downstream usage. Within this paper we propose Function\nas a String Encoded Representation (FASER) which combines long document\ntransformers with the use of intermediate representations to create a model\ncapable of cross architecture function search without the need for manual\nfeature engineering, pre-training or a dynamic analysis step. We compare our\napproach against a series of baseline approaches for two tasks; A general\nfunction search task and a targeted vulnerability search task. Our approach\ndemonstrates strong performance across both tasks, performing better than all\nbaseline approaches.",
        "translated": "无论你是在分析恶意软件、保护软件供应链还是进行漏洞研究，能够识别跨架构软件中感兴趣的功能都是有用的。交叉架构二进制代码最近邻搜索已经在许多研究中得到了探索，并且已经使用了广泛的不同数据源来实现其目标。通常使用的数据源利用从二进制文件派生的公共结构，如函数控制流图或二进制级调用图、反汇编过程的输出或动态分析方法的输出。一种受到较少关注的数据源是二进制中间表示。二进制中间表示具有两个有趣的特性: 它们本质上是跨架构的，并且显式地对函数的语义进行编码以支持下游的使用。在本文中，我们提出了功能作为一个字符串编码表示(FASER) ，它结合了长文档转换器和中间表示的使用，创建了一个能够跨体系结构的功能搜索模型，而不需要人工特征工程，预训练或动态分析步骤。我们比较了我们的方法与一系列的基线方法的两个任务: 一个一般的功能搜索任务和一个有针对性的漏洞搜索任务。我们的方法在两个任务中都表现出很强的性能，比所有的基线方法都要好。"
    },
    {
        "title": "Divide, Conquer and Verify: Improving Symbolic Execution Performance",
        "url": "http://arxiv.org/abs/2310.03598v1",
        "pub_date": "2023-10-05",
        "summary": "Symbolic Execution is a formal method that can be used to verify the behavior\nof computer programs and detect software vulnerabilities. Compared to other\ntesting methods such as fuzzing, Symbolic Execution has the advantage of\nproviding formal guarantees about the program. However, despite advances in\nperformance in recent years, Symbolic Execution is too slow to be applied to\nreal-world software. This is primarily caused by the \\emph{path explosion\nproblem} as well as by the computational complexity of SMT solving. In this\npaper, we present a divide-and-conquer approach for symbolic execution by\nexecuting individual slices and later combining the side effects. This way, the\noverall problem size is kept small, reducing the impact of computational\ncomplexity on large problems.",
        "translated": "符号执行是一种正式的方法，可用于验证计算机程序的行为和检测软件漏洞。与其他测试方法(如 fuzzing)相比，符号执行测试的优势在于为程序提供了正式的保证。然而，尽管近年来在性能方面取得了进步，但符号执行在应用于现实世界的软件方面还是太慢了。这主要是由于路径爆炸问题以及 SMT 求解的计算复杂性造成的。在本文中，我们提出了一种分而治之的方法，通过执行单独的符号执行片，然后结合副作用。通过这种方式，整个问题的规模保持较小，减少了计算复杂性对大型问题的影响。"
    },
    {
        "title": "CyMed: A Framework for Testing Cybersecurity of Connected Medical\n  Devices",
        "url": "http://arxiv.org/abs/2310.03583v1",
        "pub_date": "2023-10-05",
        "summary": "Connected Medical Devices (CMDs) have a large impact on patients as they\nallow them to lead a more normal life. Any malfunction could not only remove\nthe health benefits the CMDs provide, they could also cause further harm to the\npatient. Due to this, there are many safety regulations which must be adhered\nto prior to a CMD entering the market. However, while many detailed safety\nregulations exist, there are a fundamental lack of cybersecurity frameworks\napplicable to CMDs. While there are recent regulations which aim to enforce\ncybersecurity practices, they are vague and do not contain the concrete steps\nnecessary to implement cybersecurity. This paper aims to fill that gap by\ndescribing a framework, CyMed, to be used by vendors and ens-users, which\ncontains concrete measures to improve the resilience of CMDs against cyber\nattack. The CyMed framework is subsequently evaluated based on practical tests\nas well as expert interviews.",
        "translated": "联网医疗设备(CMD)对病人有很大的影响，因为它们使病人能够过上更加正常的生活。任何故障不仅会消除 CMD 提供的健康益处，还可能对患者造成进一步的伤害。因此，在 CMD 进入市场之前，必须遵守许多安全规定。然而，尽管存在许多详细的安全规定，但是基本上缺乏适用于 CMD 的网络安全框架。虽然最近有一些旨在实施网络安全做法的条例，但这些条例含糊不清，没有包含实施网络安全所需的具体步骤。本文旨在通过描述一个供供应商和使用者使用的框架 CyMed 来填补这一空白，其中包含提高 CMD 对网络攻击的弹性的具体措施。CyMed 框架随后将根据实际测试和专家访谈进行评估。"
    },
    {
        "title": "Digital Twin-Empowered Smart Attack Detection System for 6G Edge of\n  Things Networks",
        "url": "http://arxiv.org/abs/2310.03554v1",
        "pub_date": "2023-10-05",
        "summary": "As global Internet of Things (IoT) devices connectivity surges, a significant\nportion gravitates towards the Edge of Things (EoT) network. This shift prompts\nbusinesses to deploy infrastructure closer to end-users, enhancing\naccessibility. However, the growing EoT network expands the attack surface,\nnecessitating robust and proactive security measures. Traditional solutions\nfall short against dynamic EoT threats, highlighting the need for proactive and\nintelligent systems. We introduce a digital twin-empowered smart attack\ndetection system for 6G EoT networks. Leveraging digital twin and edge\ncomputing, it monitors and simulates physical assets in real time, enhancing\nsecurity. An online learning module in the proposed system optimizes the\nnetwork performance. Our system excels in proactive threat detection, ensuring\n6G EoT network security. The performance evaluations demonstrate its\neffectiveness, robustness, and adaptability using real datasets.",
        "translated": "随着全球物联网(IoT)设备连接的激增，很大一部分被物联网(EoT)所吸引。这种转变促使企业将基础设施部署在更接近最终用户的地方，从而提高可访问性。然而，不断增长的 EoT 网络扩大了攻击范围，需要强大和积极主动的安全措施。传统的解决方案在应对动态 EoT 威胁方面存在不足，突出了对主动和智能系统的需求。介绍了一种适用于6G EoT 网络的数字双授权智能攻击检测系统。它利用数字双胞胎和边缘计算，实时监测和模拟物理资产，提高安全性。该系统中的在线学习模块优化了网络性能。我们的系统擅长于主动的威胁检测，确保6G EoT 网络安全。性能评估表明，它的有效性，鲁棒性和适应性使用实际数据集。"
    },
    {
        "title": "Putting a Padlock on Lambda -- Integrating vTPMs into AWS Firecracker",
        "url": "http://arxiv.org/abs/2310.03522v1",
        "pub_date": "2023-10-05",
        "summary": "When software services use cloud providers to run their workloads, they place\nimplicit trust in the cloud provider, without an explicit trust relationship.\nOne way to achieve such explicit trust in a computer system is to use a\nhardware Trusted Platform Module (TPM), a coprocessor for trusted computing.\nHowever, in the case of managed platform-as-a-service (PaaS) offerings, there\nis currently no cloud provider that exposes TPM capabilities. In this paper, we\nimprove trust by integrating a virtual TPM device into the Firecracker\nhypervisor, originally developed by Amazon Web Services. In addition to this,\nmultiple performance tests along with an attack surface analysis are performed\nto evaluate the impact of the changes introduced. We discuss the results and\nconclude that the slight performance decrease and attack surface increase are\nacceptable trade-offs in order to enable trusted computing in PaaS offerings.",
        "translated": "当软件服务使用云提供者来运行它们的工作负载时，它们在云提供者中放置隐式信任，而没有显式的信任关系。在计算机系统中实现这种明确信任的一种方法是使用硬件可信平台模块(tPM) ，一种用于可信计算的协处理器。但是，对于托管平台即服务(PaaS)产品，目前还没有公开 TPM 功能的云提供商。在本文中，我们通过将虚拟 TPM 设备集成到最初由 Amazon Web Services 开发的 Fireacker 管理程序中来提高信任度。除此之外，还要执行多个性能测试以及攻击面分析，以评估引入的更改的影响。我们讨论了结果并得出结论，为了在 PaaS 产品中支持可信计算，性能的轻微下降和攻击面的增加是可以接受的权衡。"
    },
    {
        "title": "Supervising Smart Home Device Interactions: A Profile-Based Firewall\n  Approach",
        "url": "http://arxiv.org/abs/2310.03510v1",
        "pub_date": "2023-10-05",
        "summary": "Internet of Things devices can now be found everywhere, including in our\nhouseholds in the form of Smart Home networks. Despite their ubiquity, their\nsecurity is unsatisfactory, as demonstrated by recent attacks.\n  The IETF's MUD standard has as goal to simplify and automate the secure\ndeployment of end devices in networks. A MUD file contains a device specific\ndescription of allowed network activities (e.g., allowed IP ports or host\naddresses) and can be used to configure for example a firewall. A major\nweakness of MUD is that it is not expressive enough to describe traffic\npatterns representing device interactions, which often occur in modern Smart\nHome platforms.\n  In this article, we present a new language for describing such traffic\npatterns. The language allows writing device profiles that are more expressive\nthan MUD files and take into account the interdependencies of traffic\nconnections. We show how these profiles can be translated to efficient code for\na lightweight firewall leveraging NFTables to block non-conforming traffic. We\nevaluate our approach on traffic generated by various Smart Home devices, and\nshow that our system can accurately block unwanted traffic while inducing\nnegligible latency.",
        "translated": "物联网设备现在可以在任何地方找到，包括在我们的家庭形式的智能家居网络。尽管它们无处不在，但正如最近的攻击所显示的那样，它们的安全性并不令人满意。IETF 的 MUD 标准的目标是简化和自动化网络中终端设备的安全部署。MUD 文件包含允许的网络活动(例如，允许的 IP 端口或主机地址)的设备特定描述，可用于配置，例如防火墙。MUD 的一个主要缺点是它不足以描述代表设备交互的流量模式，这种模式经常出现在现代智能家居平台上。在本文中，我们提出了一种新的语言来描述这种流量模式。该语言允许编写比 MUD 文件更具表现力的设备配置文件，并考虑到通信连接的相互依赖性。我们展示了如何将这些概要文件转换为轻量级防火墙的高效代码，该防火墙利用 NFTables 来阻止不一致的流量。我们评估我们的方法所产生的流量由各种智能家居设备，并表明我们的系统可以准确地阻止不必要的流量，同时诱导微不足道的延迟。"
    },
    {
        "title": "Hermes: Unlocking Security Analysis of Cellular Network Protocols by\n  Synthesizing Finite State Machines from Natural Language Specifications",
        "url": "http://arxiv.org/abs/2310.04381v1",
        "pub_date": "2023-10-06",
        "summary": "In this paper, we present Hermes, an end-to-end framework to automatically\ngenerate formal representations from natural language cellular specifications.\nWe first develop a neural constituency parser, NEUTREX, to process\ntransition-relevant texts and extract transition components (i.e., states,\nconditions, and actions). We also design a domain-specific language to\ntranslate these transition components to logical formulas by leveraging\ndependency parse trees. Finally, we compile these logical formulas to generate\ntransitions and create the formal model as finite state machines. To\ndemonstrate the effectiveness of Hermes, we evaluate it on 4G NAS, 5G NAS, and\n5G RRC specifications and obtain an overall accuracy of 81-87%, which is a\nsubstantial improvement over the state-of-the-art. Our security analysis of the\nextracted models uncovers 3 new vulnerabilities and identifies 19 previous\nattacks in 4G and 5G specifications, and 7 deviations in commercial 4G\nbasebands.",
        "translated": "在本文中，我们提出了 Hermes，一个端到端的框架来自动生成自然语言细胞规范的形式化表示。我们首先开发了一个神经选区解析器 NEUTREX，用于处理与过渡相关的文本并提取过渡组件(即状态、条件和行为)。我们还设计了一个领域特定语言，利用依赖解析树将这些转换组件转换为逻辑公式。最后，我们编译这些逻辑公式来产生转换，并创建形式化模型作为有限状态机。为了证明 Hermes 的有效性，我们根据4G NAS，5G NAS 和5G RRC 规范对其进行评估，并获得81-87% 的总体准确率，这是对最先进技术的重大改进。我们对提取的模型进行的安全分析发现了3个新的漏洞，并确定了19个以前在4G 和5G 规范中的攻击，以及7个在商业4G 基带中的偏差。"
    },
    {
        "title": "Mapping the DeFi Crime Landscape: An Evidence-based Picture",
        "url": "http://arxiv.org/abs/2310.04356v1",
        "pub_date": "2023-10-06",
        "summary": "Over the past years, decentralized finance (DeFi) has been the target of\nnumerous profit-driven crimes. However, until now, the full prevalence and\ncumulative impact of these crimes have not been assessed. This study provides a\nfirst comprehensive assessment of profit-driven crimes targeting the DeFi\nsector. To achieve this, we collected data on 1155 crime events from 2017 to\n2022. Of these, 1050 were related to the DeFi industry and 105 to the\ncentralized finance (CeFi) industry. Focusing on the former, a taxonomy was\ndeveloped to clarify the similarities and differences among these crimes. All\nevents were mapped onto the DeFi stack to assess the impacted technical layers,\nand the financial damages were quantified to gauge their scale. The findings\nshow that the entire cryptoasset industry has suffered a minimum loss of\nUS$30B, with two thirds related to centralized finance (CeFi) and one third to\nDeFi. Focusing solely on the latter, the results highlight that during an\nattack, a DeFi actor (an entity developing a DeFi technology) can serve as a\ndirect target, as a perpetrator, or as an intermediary. The findings show that\nDeFi actors are the first victims of crimes targeting the DeFi industry: 52% of\ncrime events targeted them, primarily due to technical vulnerabilities at the\nprotocol layer, and these events accounted for 83% of all recorded financial\ndamages. On the other hand, in 40% of crime events, DeFi actors were themselves\nmalicious perpetrators, predominantly misusing contracts at the cryptoasset\nlayer (e.g., rug pull scams). However, these events accounted for only 17% of\nall financial damages. The study's findings offer a preliminary assessment of\nthe size and scope of crime events within the DeFi sector and highlight the\nvulnerable position of DeFi actors in the ecosystem.",
        "translated": "在过去的几年里，分散式金融(DeFi)已经成为众多利润驱动型犯罪的目标。然而，直到现在，这些犯罪的全面流行和累积影响还没有得到评估。这项研究首次全面评估了以 DeFi 部门为目标的利润驱动型犯罪。为此，我们收集了2017年至2022年间1155起犯罪事件的数据。其中，1050个与 DeFi 行业有关，105个与中央金融(CeFi)行业有关。针对前者，开发了一个分类学，以澄清这些犯罪之间的异同。所有事件都映射到 DeFi 堆栈上，以评估受影响的技术层，并量化经济损失，以衡量其规模。调查结果显示，整个加密资产行业至少遭受了300亿美元的损失，其中三分之二与集中融资(CeFi)有关，三分之一与 DeFi 有关。仅关注后者，研究结果强调，在攻击过程中，DeFi 参与者(开发 DeFi 技术的实体)可以作为直接目标，作为犯罪者，或者作为中间人。调查结果显示，DeFi 演员是针对 DeFi 行业的犯罪的第一受害者: 52% 的犯罪事件针对他们，主要是由于协议层的技术漏洞，这些事件占所有记录的经济损失的83% 。另一方面，在40% 的犯罪事件中，DeFi 参与者本身就是恶意的犯罪者，主要是在加密资产层滥用合同(例如，地毯拉骗局)。然而，这些事件只占所有经济损失的17% 。这项研究的结果提供了一个关于 DeFi 部门内犯罪事件的规模和范围的初步评估，并强调了 DeFi 参与者在生态系统中的脆弱地位。"
    },
    {
        "title": "Threat Trekker: An Approach to Cyber Threat Hunting",
        "url": "http://arxiv.org/abs/2310.04197v1",
        "pub_date": "2023-10-06",
        "summary": "Threat hunting is a proactive methodology for exploring, detecting and\nmitigating cyberattacks within complex environments. As opposed to conventional\ndetection systems, threat hunting strategies assume adversaries have\ninfiltrated the system; as a result they proactively search out any unusual\npatterns or activities which might indicate intrusion attempts.\n  Historically, this endeavour has been pursued using three investigation\nmethodologies: (1) Hypothesis-Driven Investigations; (2) Indicator of\nCompromise (IOC); and (3) High-level machine learning analysis-based\napproaches. Therefore, this paper introduces a novel machine learning paradigm\nknown as Threat Trekker. This proposal utilizes connectors to feed data\ndirectly into an event streaming channel for processing by the algorithm and\nprovide feedback back into its host network.\n  Conclusions drawn from these experiments clearly establish the efficacy of\nemploying machine learning for classifying more subtle attacks.",
        "translated": "威胁搜寻是一种在复杂环境中探索、检测和减轻网络攻击的主动方法。与传统的侦测系统不同，威胁搜寻策略假设对手已经渗透进系统; 因此，它们会主动搜寻任何可能显示入侵企图的不寻常模式或活动。从历史上看，这种努力一直使用三种调查方法: (1)假设驱动的调查; (2)妥协指标(IOC) ; 和(3)基于高级机器学习分析的方法。因此，本文介绍了一种新的机器学习范式，称为威胁旅行者。该方案利用连接器将数据直接送入事件流通道，由算法进行处理，并向其主机网络提供反馈。从这些实验中得出的结论清楚地证实了使用机器学习对更微妙的攻击进行分类的有效性。"
    },
    {
        "title": "Reviving Meltdown 3a",
        "url": "http://arxiv.org/abs/2310.04192v1",
        "pub_date": "2023-10-06",
        "summary": "Since the initial discovery of Meltdown and Spectre in 2017, different\nvariants of these attacks have been discovered. One often overlooked variant is\nMeltdown 3a, also known as Meltdown-CPL-REG. Even though Meltdown-CPL-REG was\ninitially discovered in 2018, the available information regarding the\nvulnerability is still sparse. In this paper, we analyze Meltdown-CPL-REG on 19\ndifferent CPUs from different vendors using an automated tool. We observe that\nthe impact is more diverse than documented and differs from CPU to CPU.\nSurprisingly, while the newest Intel CPUs do not seem affected by\nMeltdown-CPL-REG, the newest available AMD CPUs (Zen3+) are still affected by\nthe vulnerability. Furthermore, given our attack primitive CounterLeak, we show\nthat besides up-to-date patches, Meltdown-CPL-REG can still be exploited as we\nreenable performance-counter-based attacks on cryptographic algorithms, break\nKASLR, and mount Spectre attacks. Although Meltdown-CPL-REG is not as powerful\nas other transient-execution attacks, its attack surface should not be\nunderestimated.",
        "translated": "自从2017年第一次发现熔毁和幽灵，这些攻击的不同变种已经被发现。一个经常被忽视的变体是熔毁3a，也被称为熔毁 CPL-REG。尽管熔毁-CPL-REG 最初是在2018年发现的，但关于该漏洞的可用信息仍然很少。在本文中，我们使用一个自动化工具分析了来自不同供应商的19个不同 CPU 上的 Meldown-CPL-REG。我们观察到，与文档记录相比，这种影响更加多样化，并且不同 CPU 之间的影响也不同。令人惊讶的是，虽然最新的英特尔 CPU 似乎不受熔毁-CPL-REG 的影响，最新的可用 AMD CPU (Zen3 +)仍然受到漏洞的影响。此外，鉴于我们的攻击原始 CounterLeak，我们表明，除了最新的补丁，熔毁-CPL-REG 仍然可以利用，因为我们重新启用基于性能计数器的攻击加密算法，破解 KASLR，并安装 Spectre 攻击。虽然 Meldown-CPL-REG 不如其他瞬态执行攻击强大，但其攻击面也不容低估。"
    },
    {
        "title": "Indirect Meltdown: Building Novel Side-Channel Attacks from\n  Transient-Execution Attacks",
        "url": "http://arxiv.org/abs/2310.04183v1",
        "pub_date": "2023-10-06",
        "summary": "The transient-execution attack Meltdown leaks sensitive information by\ntransiently accessing inaccessible data during out-of-order execution. Although\nMeltdown is fixed in hardware for recent CPU generations, most\ncurrently-deployed CPUs have to rely on software mitigations, such as KPTI.\nStill, Meltdown is considered non-exploitable on current systems. In this\npaper, we show that adding another layer of indirection to Meltdown transforms\na transient-execution attack into a side-channel attack, leaking metadata\ninstead of data. We show that despite software mitigations, attackers can still\nleak metadata from other security domains by observing the success rate of\nMeltdown on non-secret data. With LeakIDT, we present the first cache-line\ngranular monitoring of kernel addresses. LeakIDT allows an attacker to obtain\ncycle-accurate timestamps for attacker-chosen interrupts. We use our attack to\nget accurate inter-keystroke timings and fingerprint visited websites. While we\npropose a low-overhead software mitigation to prevent the exploitation of\nLeakIDT, we emphasize that the side-channel aspect of transient-execution\nattacks should not be underestimated.",
        "translated": "短暂执行攻击熔毁泄漏的敏感信息通过短暂访问不可访问的数据在乱序执行。虽然在最近几代 CPU 的硬件中 Meldown 是固定的，但是大多数当前部署的 CPU 都必须依赖于软件缓解，例如 KPTI。不过，熔毁被认为是不可利用的目前的系统。在本文中，我们表明，添加另一层间接熔毁转换为一个临时执行攻击侧信道攻击，泄漏元数据而不是数据。我们表明，尽管软件缓解，攻击者仍然可以从其他安全领域泄漏元数据，通过观察熔毁的成功率非机密数据。使用 LeakIDT，我们提供了对内核地址的第一个缓存线粒度监视。LeakIDT 允许攻击者为攻击者选择的中断获取周期精确的时间戳。我们使用我们的攻击来获得精确的击键时间和指纹访问网站。虽然我们提出了一个低开销的软件缓解，以防止利用 LeakIDT，我们强调，边通道方面的瞬态执行攻击不应该被低估。"
    },
    {
        "title": "Dynamic Relation-Attentive Graph Neural Networks for Fraud Detection",
        "url": "http://arxiv.org/abs/2310.04171v1",
        "pub_date": "2023-10-06",
        "summary": "Fraud detection aims to discover fraudsters deceiving other users by, for\nexample, leaving fake reviews or making abnormal transactions. Graph-based\nfraud detection methods consider this task as a classification problem with two\nclasses: frauds or normal. We address this problem using Graph Neural Networks\n(GNNs) by proposing a dynamic relation-attentive aggregation mechanism. Based\non the observation that many real-world graphs include different types of\nrelations, we propose to learn a node representation per relation and aggregate\nthe node representations using a learnable attention function that assigns a\ndifferent attention coefficient to each relation. Furthermore, we combine the\nnode representations from different layers to consider both the local and\nglobal structures of a target node, which is beneficial to improving the\nperformance of fraud detection on graphs with heterophily. By employing dynamic\ngraph attention in all the aggregation processes, our method adaptively\ncomputes the attention coefficients for each node. Experimental results show\nthat our method, DRAG, outperforms state-of-the-art fraud detection methods on\nreal-world benchmark datasets.",
        "translated": "欺诈侦查旨在发现欺诈者欺骗其他用户，例如，留下虚假的评论或进行不正常的交易。基于图的欺诈检测方法把这个任务看作是一个分类问题，分为两类: 欺诈和普通欺诈。本文提出了一种动态关系注意聚集机制，并利用图神经网络(GNN)来解决这个问题。基于现实世界中许多图都包含不同类型的关系，我们提出学习每个关系的节点表示，并使用一个可学习的注意函数对每个关系赋予不同的注意系数来聚合节点表示。此外，结合不同层次的节点表示，考虑目标节点的局部和全局结构，有利于提高异质图的欺诈检测性能。该方法在聚合过程中引入动态图注意力，自适应地计算每个节点的注意力系数。实验结果表明，本文提出的 DRAG 方法在真实基准数据集上的欺诈检测性能优于现有的欺诈检测方法。"
    },
    {
        "title": "Minors solve the elliptic curve discrete logarithm problem",
        "url": "http://arxiv.org/abs/2310.04132v1",
        "pub_date": "2023-10-06",
        "summary": "The elliptic curve discrete logarithm problem is of fundamental importance in\npublic-key cryptography. It is in use for a long time. Moreover, it is an\ninteresting challenge in computational mathematics. Its solution is supposed to\nprovide interesting research directions.\n  In this paper, we explore ways to solve the elliptic curve discrete logarithm\nproblem. Our results are mostly computational. However, it seems, the methods\nthat we develop and directions that we pursue can provide a potent attack on\nthis problem. This work follows our earlier work, where we tried to solve this\nproblem by finding a zero minor in a matrix over the same finite field on which\nthe elliptic curve is defined. This paper is self-contained.",
        "translated": "椭圆曲线的离散对数问题在公开密钥加密中非常重要。它已经使用很长时间了。此外，这在计算数学方面也是一个有趣的挑战。它的解决方案应该提供有趣的研究方向。本文探讨解决椭圆曲线离散对数问题的方法。我们的结果主要是计算性的。然而，我们发展的方法和我们追求的方向似乎可以对这个问题提供一个有力的攻击。这项工作遵循我们早期的工作，在那里我们试图解决这个问题，通过找到一个零小数在一个矩阵在同一有限域上的椭圆曲线被定义。这张纸是独立的。"
    },
    {
        "title": "Kick Bad Guys Out! Zero-Knowledge-Proof-Based Anomaly Detection in\n  Federated Learning",
        "url": "http://arxiv.org/abs/2310.04055v1",
        "pub_date": "2023-10-06",
        "summary": "Federated learning (FL) systems are vulnerable to malicious clients that\nsubmit poisoned local models to achieve their adversarial goals, such as\npreventing the convergence of the global model or inducing the global model to\nmisclassify some data. Many existing defense mechanisms are impractical in\nreal-world FL systems, as they require prior knowledge of the number of\nmalicious clients or rely on re-weighting or modifying submissions. This is\nbecause adversaries typically do not announce their intentions before\nattacking, and re-weighting might change aggregation results even in the\nabsence of attacks. To address these challenges in real FL systems, this paper\nintroduces a cutting-edge anomaly detection approach with the following\nfeatures: i) Detecting the occurrence of attacks and performing defense\noperations only when attacks happen; ii) Upon the occurrence of an attack,\nfurther detecting the malicious client models and eliminating them without\nharming the benign ones; iii) Ensuring honest execution of defense mechanisms\nat the server by leveraging a zero-knowledge proof mechanism. We validate the\nsuperior performance of the proposed approach with extensive experiments.",
        "translated": "联邦学习(FL)系统容易受到恶意客户端的攻击，这些恶意客户端提交有毒的本地模型以实现其对抗性目标，如阻止全局模型的收敛或诱导全局模型对某些数据进行错误分类。许多现有的防御机制在现实的 FL 系统中是不切实际的，因为它们需要事先知道恶意客户端的数量，或者依赖于重新加权或修改提交。这是因为对手通常不会在攻击之前宣布他们的意图，即使没有攻击，重新加权也可能改变聚合结果。为了解决真实 FL 系统中的这些挑战，本文介绍了一种先进的异常检测方法，它具有以下特点: i)检测攻击的发生并只在攻击发生时执行防御操作; ii)在攻击发生时，进一步检测恶意客户端模型并消除它们而不损害良性客户端模型; iii)通过利用零知识证明机制确保服务器上防御机制的诚实执行。通过大量实验验证了该方法的优越性能。"
    },
    {
        "title": "DeMiST: Detection and Mitigation of Stealthy Analog Hardware Trojans",
        "url": "http://arxiv.org/abs/2310.03994v1",
        "pub_date": "2023-10-06",
        "summary": "The global semiconductor supply chain involves design and fabrication at\nvarious locations, which leads to multiple security vulnerabilities, e.g.,\nHardware Trojan (HT) insertion. Although most HTs target digital circuits, HTs\ncan be inserted in analog circuits. Therefore, several techniques have been\ndeveloped for HT insertions in analog circuits. Capacitance-based Analog\nHardware Trojan (AHT) is one of the stealthiest HT that can bypass most\nexisting HT detection techniques because it uses negligible charge accumulation\nin the capacitor to generate stealthy triggers. To address the charge sharing\nand accumulation issues, we propose a novel way to detect such\ncapacitance-based AHT in this paper. Secondly, we critically analyzed existing\nAHTs to highlight their respective limitations. We proposed a stealthier\ncapacitor-based AHT (fortified AHT) that can bypass our novel AHT detection\ntechnique by addressing these limitations. Finally, by critically analyzing the\nproposed fortified AHT and existing AHTs, we developed a robust two-phase\nframework (DeMiST) in which a synchronous system can mitigate the effects of\ncapacitance-based stealthy AHTs by turning off the triggering capability of\nAHT. In the first phase, we demonstrate how the synchronous system can avoid\nthe AHT during run-time by controlling the supply voltage of the intermediate\ncombinational circuits. In the second phase, we proposed a supply voltage duty\ncycle-based validation technique to detect capacitance-based AHTs. Furthermore,\nDeMiST amplified the switching activity for charge accumulation to such a\ndegree that it can be easily detectable using existing switching activity-based\nHT detection techniques.",
        "translated": "全球半导体供应链涉及不同地点的设计和制造，这会导致多重安全漏洞，例如硬件木马(HT)插入。虽然大多数高温超导电路的目标数字电路，高温超导电路可以插入模拟电路。因此，在模拟电路中的 HT 插入技术已经得到了发展。基于电容的模拟硬件木马(AHT)是最隐秘的 HT 之一，它可以绕过大多数现有的 HT 检测技术，因为它使用电容中可忽略的电荷积累来产生隐形触发器。为了解决电荷共享和积累的问题，本文提出了一种新的方法来检测这种基于电容的 AHT。其次，我们批判性地分析了现有的 AHT，以突出它们各自的局限性。我们提出了一种更隐蔽的基于电容器的 AHT (强化 AHT) ，可以绕过我们的新型 AHT 检测技术，解决这些局限性。最后，通过严格分析提出的增强型 AHT 和现有的 AHT，我们开发了一个鲁棒的两相框架(DeMiST) ，其中同步系统可以通过关闭 AHT 的触发能力来减轻基于电容的隐形 AHT 的影响。在第一阶段，我们演示了同步系统如何通过控制中间组合电路的电源电压来避免运行时的 AHT。在第二阶段，我们提出了一个基于电源电压占空比的验证技术来检测基于电容的 AHT。此外，DeMiST 放大了电荷积累的开关活性，以至于使用现有的基于开关活性的 HT 检测技术可以很容易地检测到它。"
    },
    {
        "title": "HDNA: A graph-based change detection in HTML pages(Deface Attack\n  Detection)",
        "url": "http://arxiv.org/abs/2310.03891v1",
        "pub_date": "2023-10-05",
        "summary": "In this paper, a new approach called HDNA (HTML DNA) is introduced for\nanalyzing and comparing Document Object Model (DOM) trees in order to detect\ndifferences in HTML pages. This method assigns an identifier to each HTML page\nbased on its structure, which proves to be particularly useful for detecting\nvariations caused by server-side updates, user interactions or potential\nsecurity risks. The process involves preprocessing the HTML content generating\na DOM tree and calculating the disparities between two or more trees. By\nassigning weights to the nodes valuable insights about their hierarchical\nimportance are obtained. The effectiveness of the HDNA approach has been\ndemonstrated in identifying changes in DOM trees even when dynamically\ngenerated content is involved. Not does this method benefit web developers,\ntesters, and security analysts by offering a deeper understanding of how web\npages evolve. It also helps ensure the functionality and performance of web\napplications. Additionally, it enables detection and response to\nvulnerabilities that may arise from modifications in DOM structures. As the web\necosystem continues to evolve HDNA proves to be a tool, for individuals engaged\nin web development, testing, or security analysis.",
        "translated": "本文提出了一种新的分析比较文档对象模型(DOM)树的方法 HDNA (HTML DNA) ，用于检测 HTML 页面中的差异。该方法根据每个 HTML 页面的结构为其分配一个标识符，这对于检测由服务器端更新、用户交互或潜在安全风险引起的变化特别有用。这个过程包括对生成 DOM 树的 HTML 内容进行预处理，并计算两个或多个树之间的差异。通过对节点赋权，可以获得节点层次重要性的有价值的见解。HDNA 方法在识别 DOM 树的变化方面的有效性已经得到证明，即使在涉及动态生成的内容时也是如此。这种方法并没有让网页开发人员、测试人员和安全分析人员更深入地了解网页是如何演变的。它还有助于确保 Web 应用程序的功能和性能。此外，它还支持检测和响应可能由 DOM 结构中的修改引起的漏洞。随着网络生态系统的不断发展，HDNA 被证明是一种工具，用于从事网络开发、测试或安全分析的个人。"
    },
    {
        "title": "DiCE -- A Data Encryption Proxy for the Cloud",
        "url": "http://arxiv.org/abs/2310.05710v1",
        "pub_date": "2023-10-09",
        "summary": "Outsourcing a relational database to the cloud offers several benefits,\nincluding scalability, availability, and cost-effectiveness. However, there are\nconcerns about the confidentiality and security of the outsourced data. A\ngeneral approach here would be to encrypt the data with a standardized\nencryption algorithm and then store the data only encrypted in the cloud. The\nproblem with this approach, however, is that with encryption, important\nproperties of the data such as sorting, format or comparability, which are\nessential for the functioning of database queries, are lost. One solution to\nthis problem is the use of (e.g. order-preserving) encryption algorithms, which\nalso preserve these properties in the encrypted data, thus enabling queries to\nencrypted data. These algorithms range from simple algorithms like Caesar\nencryption to secure algorithms like mOPE. In order to be able to use these\nalgorithms as easy as possible, ``DiCE'' a JDBC driver was developed, that\nparses SQL queries as a proxy and transparently encrypts and decrypts these\nqueries. This allows to execute many queries on an encrypted database in the\ncloud with (nearly) the performance as on unencrypted databases. The DiCE\ndriver can be used with any other JDBC driver and therefore supports a variety\nof databases. The driver can be configured to support different encryption\nalgorithms. To keep track of the operations, the ``Dice Information Client''\nhas been developed to track the encryption and decryption of the driver.\nAlthough the result of the performance analysis shows a certain overhead due to\nthe parsing and encryption of the SQL queries in the Dice driver, this overhead\nis significantly reduced by other influencing factors such as the network and\nparallel queries.",
        "translated": "将关系数据库外包到云计算有几个好处，包括可伸缩性、可用性和成本效益。然而，外包数据的机密性和安全性也存在问题。这里的一般方法是使用标准化的加密算法对数据进行加密，然后将仅加密的数据存储在云中。但是，这种方法的问题在于，加密会丢失数据的重要属性，如对数据库查询的功能至关重要的排序、格式或可比性。这个问题的一个解决方案是使用(例如保序)加密算法，这些算法也保留了加密数据中的这些属性，从而允许对加密数据进行查询。这些算法的范围从像凯撒加密这样的简单算法到像 mOPE 这样的安全算法。为了能够尽可能简单地使用这些算法，开发了一个 JDBC 驱动程序“ DiCE”，该驱动程序将 SQL 查询作为代理进行解析，并透明地对这些查询进行加密和解密。这允许在云中的加密数据库上执行许多查询，其性能(几乎)与在未加密数据库上的性能相当。DiCE 驱动程序可以与任何其他 JDBC 驱动程序一起使用，因此支持各种数据库。驱动程序可以配置为支持不同的加密算法。为了跟踪这些操作，开发了“ Dice Information Client”来跟踪驱动程序的加密和解密。虽然性能分析的结果显示由于 Dice 驱动程序中 SQL 查询的解析和加密带来了一定的开销，但是由于网络和并行查询等其他影响因素，这种开销显著降低。"
    },
    {
        "title": "A review of the security role of ISP mandated ONUs and ONTs in GPONs",
        "url": "http://arxiv.org/abs/2310.05687v1",
        "pub_date": "2023-10-09",
        "summary": "Home fiber connections are largely realized by using passive optical\nnetworks, in their most common form today relying on the GPON standard. Among\nother things, this standard specifies how the first node inside of customers'\nhomes, the so called ONU or ONT, has to behave, and which security features\nhave to be supported. Currently, customers in some European countries,\nincluding Germany, have freedom of choice between using terminal equipment\nprovided by the ISP or a self-selected open market device.We analyze the\nsecurity implications resulting from this freedom of choice and whether or not\nISP-mandated hardware would increase the security of the GPON. Our review\nreveals that there are no differences between an ISP-mandated ONU/ONT and a\nstandard conforming subscriber-selected ONU/ONT that would justify the security\nbased recommendation of an ISP-mandated ONU/ONT.",
        "translated": "家庭光纤连接在很大程度上是通过使用无源光网络来实现的，目前最常见的形式是依靠 GPON 标准。在其他方面，该标准规定了客户家中的第一个节点，即所谓的 ONU 或 ONT，必须如何工作，以及必须支持哪些安全特性。目前，包括德国在内的一些欧洲国家的客户可以自由选择使用 ISP 提供的终端设备还是自选的公开市场设备。我们分析了这种自由选择带来的安全影响，以及 ISP 强制的硬件是否会提高 GPON 的安全性。我们的审查表明，ISP 授权的 ONU/ONT 和符合标准的用户选择的 ONU/ONT 之间没有区别，这将证明 ISP 授权的 ONU/ONT 的基于安全的建议是合理的。"
    },
    {
        "title": "Mathematical problems and solutions of the Ninth International Olympiad\n  in Cryptography NSUCRYPTO",
        "url": "http://arxiv.org/abs/2310.05641v1",
        "pub_date": "2023-10-09",
        "summary": "Every year the International Olympiad in Cryptography Non-Stop University\nCRYPTO (NSUCRYPTO) offers mathematical problems for university and school\nstudents and, moreover, for professionals in the area of cryptography and\ncomputer science. The mail goal of NSUCRYPTO is to draw attention of students\nand young researchers to modern cryptography and raise awareness about open\nproblems in the field. We present problems of NSUCRYPTO'22 and their solutions.\nThere are 16 problems on the following topics: ciphers, cryptosystems,\nprotocols, e-money and cryptocurrencies, hash functions, matrices, quantum\ncomputing, S-boxes, etc. They vary from easy mathematical tasks that could be\nsolved by school students to open problems that deserve separate discussion and\nstudy. So, in this paper, we consider several open problems on three-pass\nprotocols, public and private keys pairs, modifications of discrete logarithm\nproblem, cryptographic permutations and quantum circuits.",
        "translated": "每年的国际密码学奥林匹克不间断大学 CRYPTO (NSUCRYPTO)为大学生和中学生提供数学问题，此外，还为密码学和计算机科学领域的专业人士提供数学问题。NSUCRYPTO 的邮件目标是吸引学生和年轻研究人员对现代密码学的关注，并提高对该领域开放问题的认识。我们提出了 NSUCRYPTO’22的问题及其解决方案。有16个问题涉及以下主题: 密码、密码系统、协议、电子货币和加密货币、哈希函数、矩阵、量子计算、 S 盒等。它们从学校学生可以解决的简单的数学任务，到值得单独讨论和研究的开放性问题，各不相同。因此，在本文中，我们考虑了三通协议、公钥和私钥对、离散对数问题的修改、密码排列和量子电路等几个开放性问题。"
    },
    {
        "title": "Decoding the Threat Landscape : ChatGPT, FraudGPT, and WormGPT in Social\n  Engineering Attacks",
        "url": "http://arxiv.org/abs/2310.05595v1",
        "pub_date": "2023-10-09",
        "summary": "In the ever-evolving realm of cybersecurity, the rise of generative AI models\nlike ChatGPT, FraudGPT, and WormGPT has introduced both innovative solutions\nand unprecedented challenges. This research delves into the multifaceted\napplications of generative AI in social engineering attacks, offering insights\ninto the evolving threat landscape using the blog mining technique. Generative\nAI models have revolutionized the field of cyberattacks, empowering malicious\nactors to craft convincing and personalized phishing lures, manipulate public\nopinion through deepfakes, and exploit human cognitive biases. These models,\nChatGPT, FraudGPT, and WormGPT, have augmented existing threats and ushered in\nnew dimensions of risk. From phishing campaigns that mimic trusted\norganizations to deepfake technology impersonating authoritative figures, we\nexplore how generative AI amplifies the arsenal of cybercriminals. Furthermore,\nwe shed light on the vulnerabilities that AI-driven social engineering\nexploits, including psychological manipulation, targeted phishing, and the\ncrisis of authenticity. To counter these threats, we outline a range of\nstrategies, including traditional security measures, AI-powered security\nsolutions, and collaborative approaches in cybersecurity. We emphasize the\nimportance of staying vigilant, fostering awareness, and strengthening\nregulations in the battle against AI-enhanced social engineering attacks. In an\nenvironment characterized by the rapid evolution of AI models and a lack of\ntraining data, defending against generative AI threats requires constant\nadaptation and the collective efforts of individuals, organizations, and\ngovernments. This research seeks to provide a comprehensive understanding of\nthe dynamic interplay between generative AI and social engineering attacks,\nequipping stakeholders with the knowledge to navigate this intricate\ncybersecurity landscape.",
        "translated": "在不断发展的网络安全领域，像 ChatGPT、 FraudGPT 和 WormGPT 这样的生成式人工智能模型的兴起，带来了创新的解决方案和前所未有的挑战。本研究深入探讨了生成式人工智能在社会工程攻击中的多方面应用，利用博客挖掘技术提供了对不断演变的威胁景观的深刻见解。生成式人工智能模型彻底改变了网络攻击领域，使恶意行为者能够制造令人信服和个性化的网络钓鱼诱饵，通过深度伪造操纵公众舆论，并利用人类的认知偏见。这些模型，ChatGPT、 FraudGPT 和 WormGPT，增加了现有的威胁，并开辟了新的风险维度。从模仿可信组织的网络钓鱼活动，到模仿权威人物的深度伪造技术，我们探索了生成性人工智能是如何放大网络罪犯的武器库的。此外，我们阐明了人工智能驱动的社会工程利用的弱点，包括心理操纵，有针对性的网络钓鱼，以及真实性危机。为了应对这些威胁，我们概述了一系列的战略，包括传统的安全措施，人工智能驱动的安全解决方案，以及网络安全方面的协作方法。我们强调在对抗人工智能强化的社会工程攻击中保持警惕、培养意识和加强监管的重要性。在一个拥有属性人工智能模型快速演变和缺乏训练数据的环境中，防御生成性人工智能威胁需要个人、组织和政府的不断适应和集体努力。本研究旨在全面了解生成性人工智能和社会工程攻击之间的动态相互作用，为利益相关者提供导航这一错综复杂的网络安全景观的知识。"
    },
    {
        "title": "RECESS Vaccine for Federated Learning: Proactive Defense Against Model\n  Poisoning Attacks",
        "url": "http://arxiv.org/abs/2310.05431v1",
        "pub_date": "2023-10-09",
        "summary": "Model poisoning attacks greatly jeopardize the application of federated\nlearning (FL). The effectiveness of existing defenses is susceptible to the\nlatest model poisoning attacks, leading to a decrease in prediction accuracy.\nBesides, these defenses are intractable to distinguish benign outliers from\nmalicious gradients, which further compromises the model generalization. In\nthis work, we propose a novel proactive defense named RECESS against model\npoisoning attacks. Different from the passive analysis in previous defenses,\nRECESS proactively queries each participating client with a delicately\nconstructed aggregation gradient, accompanied by the detection of malicious\nclients according to their responses with higher accuracy. Furthermore, RECESS\nuses a new trust scoring mechanism to robustly aggregate gradients. Unlike\nprevious methods that score each iteration, RECESS considers clients'\nperformance correlation across multiple iterations to estimate the trust score,\nsubstantially increasing fault tolerance. Finally, we extensively evaluate\nRECESS on typical model architectures and four datasets under various settings.\nWe also evaluated the defensive effectiveness against other types of poisoning\nattacks, the sensitivity of hyperparameters, and adaptive adversarial attacks.\nExperimental results show the superiority of RECESS in terms of reducing\naccuracy loss caused by the latest model poisoning attacks over five classic\nand two state-of-the-art defenses.",
        "translated": "模型中毒攻击严重影响了联邦学习的应用。现有防御系统的有效性容易受到最新模型中毒攻击的影响，导致预测精度下降。此外，这些防御难以区分良性异常值和恶意梯度，这进一步损害了模型泛化。在这项工作中，我们提出了一个新的主动防御称为 RECESS 对模型中毒攻击。不同于以往的被动防御分析，RECESS 主动查询每个参与的客户端与精心构造的聚合梯度，伴随着检测恶意客户端根据他们的反应更高的准确性。此外，RECESS 使用一种新的信任评分机制来稳健地聚合梯度。不像以前的方法，每个迭代评分，RECESS 考虑客户端的性能相关性跨多个迭代估计信任评分，大大增加容错。最后，我们对典型的模型架构和不同设置下的四个数据集上的 RECESS 进行了广泛的评估。我们还评估了针对其他类型的中毒攻击的防御效果、超参数的敏感性以及适应性对抗攻击。实验结果表明，RECESS 在减少最新模型中毒攻击所造成的精度损失方面优于五种经典和两种最先进的防御方法。"
    },
    {
        "title": "Federated Learning: A Cutting-Edge Survey of the Latest Advancements and\n  Applications",
        "url": "http://arxiv.org/abs/2310.05269v1",
        "pub_date": "2023-10-08",
        "summary": "In the realm of machine learning (ML) systems featuring client-host\nconnections, the enhancement of privacy security can be effectively achieved\nthrough federated learning (FL) as a secure distributed ML methodology. FL\neffectively integrates cloud infrastructure to transfer ML models onto edge\nservers using blockchain technology. Through this mechanism, it guarantees the\nstreamlined processing and data storage requirements of both centralized and\ndecentralized systems, with an emphasis on scalability, privacy considerations,\nand cost-effective communication. In current FL implementations, data owners\nlocally train their models, and subsequently upload the outcomes in the form of\nweights, gradients, and parameters to the cloud for overall model aggregation.\nThis innovation obviates the necessity of engaging Internet of Things (IoT)\nclients and participants to communicate raw and potentially confidential data\ndirectly with a cloud center. This not only reduces the costs associated with\ncommunication networks but also enhances the protection of private data. This\nsurvey conducts an analysis and comparison of recent FL applications, aiming to\nassess their efficiency, accuracy, and privacy protection. However, in light of\nthe complex and evolving nature of FL, it becomes evident that additional\nresearch is imperative to address lingering knowledge gaps and effectively\nconfront the forthcoming challenges in this field. In this study, we categorize\nrecent literature into the following clusters: privacy protection, resource\nallocation, case study analysis, and applications. Furthermore, at the end of\neach section, we tabulate the open areas and future directions presented in the\nreferenced literature, affording researchers and scholars an insightful view of\nthe evolution of the field.",
        "translated": "在以客户机-主机连接为特征的机器学习(ML)系统领域，通过联邦学习(FL)作为一种安全的分布式机器学习方法，可以有效地提高隐私安全性。FL 有效地整合了云基础设施，使用区块链技术将机器学习模型传输到边缘服务器上。通过这种机制，它保证了集中式和分散式系统的简化处理和数据存储需求，强调可扩展性、隐私考虑和成本效益通信。在当前的 FL 实现中，数据所有者在本地训练他们的模型，然后以权重、梯度和参数的形式将结果上传到云中，以便进行整体模型聚合。这一创新消除了物联网(IoT)客户和参与者直接与云中心交流原始和潜在机密数据的必要性。这不仅降低了与通信网络有关的成本，而且加强了对私人数据的保护。本调查对最近的 FL 应用程序进行分析和比较，旨在评估它们的效率、准确性和隐私保护。然而，鉴于外语的复杂性和不断演变的性质，显然需要进行更多的研究，以解决长期存在的知识差距，并有效应对这一领域即将出现的挑战。在这项研究中，我们将最近的文献分为以下几类: 隐私保护、资源分配、案例分析和应用。此外，在每个部分的结尾，我们列出了开放领域和未来的方向提出了参考文献，提供了研究人员和学者对该领域的演变深刻的看法。"
    },
    {
        "title": "Confidence-driven Sampling for Backdoor Attacks",
        "url": "http://arxiv.org/abs/2310.05263v1",
        "pub_date": "2023-10-08",
        "summary": "Backdoor attacks aim to surreptitiously insert malicious triggers into DNN\nmodels, granting unauthorized control during testing scenarios. Existing\nmethods lack robustness against defense strategies and predominantly focus on\nenhancing trigger stealthiness while randomly selecting poisoned samples. Our\nresearch highlights the overlooked drawbacks of random sampling, which make\nthat attack detectable and defensible. The core idea of this paper is to\nstrategically poison samples near the model's decision boundary and increase\ndefense difficulty. We introduce a straightforward yet highly effective\nsampling methodology that leverages confidence scores. Specifically, it selects\nsamples with lower confidence scores, significantly increasing the challenge\nfor defenders in identifying and countering these attacks. Importantly, our\nmethod operates independently of existing trigger designs, providing\nversatility and compatibility with various backdoor attack techniques. We\nsubstantiate the effectiveness of our approach through a comprehensive set of\nempirical experiments, demonstrating its potential to significantly enhance\nresilience against backdoor attacks in DNNs.",
        "translated": "后门攻击的目的是暗中将恶意触发器插入 DNN 模型，在测试场景中授予未经授权的控制权。现有的方法缺乏对防御策略的鲁棒性，主要集中在增强触发隐蔽性，同时随机选择中毒样本。我们的研究强调了随机抽样被忽视的缺点，这使得攻击可以检测和防御。这篇论文的核心思想是在模型的决策边界附近战略性地毒害样本，增加防御难度。我们引入了一个简单而高效的抽样方法，它利用了置信度得分。具体来说，它选择信心分数较低的样本，显著增加了防御者识别和反击这些攻击的挑战。重要的是，我们的方法独立于现有的触发器设计，提供通用性和兼容各种后门攻击技术。我们通过一系列全面的经验实验证实了我们的方法的有效性，证明了它在显著提高 DNN 对后门攻击的恢复力方面的潜力。"
    },
    {
        "title": "ROSTAM: A Passwordless Web Single Sign-on Solution Mitigating Server\n  Breaches and Integrating Credential Manager and Federated Identity Systems",
        "url": "http://arxiv.org/abs/2310.05222v1",
        "pub_date": "2023-10-08",
        "summary": "The challenge of achieving passwordless user authentication is real given the\nprevalence of web applications that keep asking passwords. Complicating this\nissue further, in an enterprise environment, a single sign-on (SSO) service is\noften maintained but not all applications can be integrated with it. We\nenvision a passwordless future which provides a frictionless and trustworthy\nonline experience for users by integrating credential management and federated\nidentity systems. In this regard, our implementation ROSTAM offers a dashboard\nthat presents all applications the user can access with a single click after a\npasswordless SSO. The security of web passwords on the credential manager is\nensured with a Master Key, rather than a Master Password, so that encrypted\npasswords can remain secure even if stolen from the server. We propose and\nimplement novel techniques for synchronization (pairing) and recovery of this\nMaster Key. We compare our solution to previous work using different evaluation\nframeworks, demonstrating that our hybrid solution combines the benefits of\ncredential management and federated identity systems.",
        "translated": "考虑到不断要求密码的 Web 应用程序的流行，实现无密码用户身份验证的挑战是真实存在的。使问题进一步复杂化的是，在企业环境中，通常会维护单点登录(SSO)服务，但并非所有应用程序都可以与其集成。我们设想一个无密码的未来，通过集成凭证管理和联邦身份系统，为用户提供一个无摩擦和可信赖的在线体验。在这方面，我们的实现 ROSTAM 提供了一个仪表板，它显示了用户在无密码 SSO 之后单击即可访问的所有应用程序。凭证管理器上的网络密码的安全性是通过一个主密钥(而不是主密码)来确保的，这样即使从服务器上盗取了加密的密码，也可以保持安全。我们提出并实现了这个主密钥的同步(配对)和恢复的新技术。我们将我们的解决方案与以前使用不同评估框架的工作进行比较，证明我们的混合解决方案结合了凭证管理和联邦身份系统的优点。"
    },
    {
        "title": "A Quantum Approach for Reducing Communications in Classical\n  Cryptographic Primitives",
        "url": "http://arxiv.org/abs/2310.05213v1",
        "pub_date": "2023-10-08",
        "summary": "How could quantum cryptography help us achieve what are not achievable in\nclassical cryptography? In this work we consider the following problem, which\nwe call succinct RSPV for classical functions (sRCF). Suppose $f$ is a function\ndescribed by a polynomial time classical Turing machine, which is public; the\nclient would like to sample a random $x$ as the function input and use a\nprotocol to send $f(x)$ to the server. What's more, (1) when the server is\nmalicious, what it knows in the passing space should be no more than $f(x)$;\n(2) the communication should be succinct (that is, independent to the running\ntime of evaluating $f$). Solving this problem in classical cryptography seems\nto require strong cryptographic primitives.\n  We show that, perhaps surprisingly, it's possible to solve this problem with\nquantum techniques under much weaker assumptions. By allowing for quantum\ncommunication and computations, we give a protocol for this problem assuming\nonly collapsing hash functions [Unr16]. Our work conveys an interesting message\nthat quantum cryptography could outperform classical cryptography in a new type\nof problems, that is, to reduce communications in meaningful primitives without\nusing heavy classical cryptographic primitives.",
        "translated": "量子密码学如何帮助我们实现传统密码学无法实现的目标？在这项工作中，我们考虑以下问题，我们称之为简洁的 RSPV 的经典函数(sRCF)。假设 $f $是一个由多项式时间的经典图灵机描述的函数，它是公共的; 客户机希望抽样一个随机的 $x $作为函数输入，并使用一个协议将 $f (x) $发送到服务器。更重要的是，(1)当服务器是恶意的，它在传递空间中知道的应该不超过 $f (x) $; (2)通信应该是简洁的(也就是说，与计算 $f $的运行时间无关)。在经典密码学中解决这个问题似乎需要强大的密码原语。我们展示了，也许令人惊讶的是，用量子技术在更弱的假设下解决这个问题是可能的。通过考虑量子通信和计算，我们给出了这个问题的一个协议，假设只有折叠散列函数[ Unr16]。我们的工作传达了一个有趣的信息，即量子密码学可以在一种新类型的问题中胜过传统的密码学，即减少有意义的原语中的通信，而不使用沉重的经典密码学原语。"
    },
    {
        "title": "Blockchain-Envisioned Disaster Relief Networks: Architecture,\n  Opportunities, and Open Issues",
        "url": "http://arxiv.org/abs/2310.05180v1",
        "pub_date": "2023-10-08",
        "summary": "Natural or man-made disasters pose significant challenges for delivering\ncritical relief to affected populations due to disruptions in critical\ninfrastructures and logistics networks. Unmanned aerial vehicles (UAVs)-aided\ndisaster relief networks (UDRNs) leverage UAVs to assist existing ground relief\nnetworks by swiftly assessing affected areas and timely delivering lifesaving\nsupplies. However, severe challenges in mutual coordination, trust, and\nsecurity hinder the deployment of UDRNs. This paper leverages the promising\nblockchain technology to address collaborative, trust-free, and traceable\ndisaster relief services. Specifically, we first present a general\nblockchain-oriented UDRN architecture incorporating space, air, and ground\nlayers. Subsequently, we propose to optimize the blockchain-based UDRN system,\nincluding (i) a series of smart contracts for transparent and automated relief\nassignment; (ii) a dynamic contract audit mechanism to prevent known/unknown\ncontract vulnerabilities; and (iii) a transaction forensics strategy with\non/off-chain cooperation for traceable relief services. Through a prototype\nimplementation, experimental results demonstrate the feasibility and\neffectiveness of our approach in terms of threat awareness latency and\nvulnerability detection rate. Lastly, we outline key open research issues\ncrucial to this emerging field.",
        "translated": "由于关键基础设施和后勤网络中断，自然或人为灾害对向受影响人口提供重大救济构成重大挑战。无人机辅助救灾网络(UDRNs)通过快速评估受灾地区和及时提供救生物资，利用无人机协助现有的地面救援网络。然而，相互协调、信任和安全方面的严峻挑战阻碍了 UDRN 的部署。本文利用有前途的区块链技术来解决协作、无信任和可追踪的救灾服务。具体来说，我们首先提出了一个通用的区块链导向 UDRN 体系结构，包括空间，空气和地面层。随后，我们建议优化基于区块链的 UDRN 系统，包括(i)一系列透明和自动化救济分配的智能合同; (ii)防止已知/未知合同漏洞的动态合同审计机制; 以及(iii)可追踪救济服务的上/下链合作的交易取证策略。通过一个原型实现，实验结果表明了该方法在威胁感知延迟和漏洞检测率方面的可行性和有效性。最后，我们概述了对这一新兴领域至关重要的关键开放研究问题。"
    },
    {
        "title": "Correlated Noise Provably Beats Independent Noise for Differentially\n  Private Learning",
        "url": "http://arxiv.org/abs/2310.06771v1",
        "pub_date": "2023-10-10",
        "summary": "Differentially private learning algorithms inject noise into the learning\nprocess. While the most common private learning algorithm, DP-SGD, adds\nindependent Gaussian noise in each iteration, recent work on matrix\nfactorization mechanisms has shown empirically that introducing correlations in\nthe noise can greatly improve their utility. We characterize the asymptotic\nlearning utility for any choice of the correlation function, giving precise\nanalytical bounds for linear regression and as the solution to a convex program\nfor general convex functions. We show, using these bounds, how correlated noise\nprovably improves upon vanilla DP-SGD as a function of problem parameters such\nas the effective dimension and condition number. Moreover, our analytical\nexpression for the near-optimal correlation function circumvents the cubic\ncomplexity of the semi-definite program used to optimize the noise correlation\nmatrix in previous work. We validate our theory with experiments on private\ndeep learning. Our work matches or outperforms prior work while being efficient\nboth in terms of compute and memory.",
        "translated": "差异私有学习算法在学习过程中注入了噪声。虽然最常见的私有学习算法 DP-sgD 在每次迭代中都会加入独立的高斯噪声，但是最近关于矩阵分解机制的工作已经经验性地表明，在噪声中引入相关性可以大大提高它们的效用。我们刻画了任意选择相关函数(量子场论)的渐近学习效用，给出了线性回归的精确解析界，并作为一般凸函数的凸程序的解。利用这些界限，我们展示了相关噪声如何作为问题参数(如有效维数和条件数)的函数可证明地改善了普通的 DP-SGD。此外，我们的近最优相关函数(量子场论)的解析表达式规避了先前工作中用来优化噪声相关矩阵的半定程序的立方复杂性。我们通过个人深度学习的实验验证了我们的理论。我们的工作与之前的工作相匹配或者优于之前的工作，同时在计算和内存方面都是高效的。"
    },
    {
        "title": "Comparing AI Algorithms for Optimizing Elliptic Curve Cryptography\n  Parameters in Third-Party E-Commerce Integrations: A Pre-Quantum Era Analysis",
        "url": "http://arxiv.org/abs/2310.06752v1",
        "pub_date": "2023-10-10",
        "summary": "This paper presents a comparative analysis between the Genetic Algorithm (GA)\nand Particle Swarm Optimization (PSO), two vital artificial intelligence\nalgorithms, focusing on optimizing Elliptic Curve Cryptography (ECC)\nparameters. These encompass the elliptic curve coefficients, prime number,\ngenerator point, group order, and cofactor. The study provides insights into\nwhich of the bio-inspired algorithms yields better optimization results for ECC\nconfigurations, examining performances under the same fitness function. This\nfunction incorporates methods to ensure robust ECC parameters, including\nassessing for singular or anomalous curves and applying Pollard's rho attack\nand Hasse's theorem for optimization precision. The optimized parameters\ngenerated by GA and PSO are tested in a simulated e-commerce environment,\ncontrasting with well-known curves like secp256k1 during the transmission of\norder messages using Elliptic Curve-Diffie Hellman (ECDH) and Hash-based\nMessage Authentication Code (HMAC). Focusing on traditional computing in the\npre-quantum era, this research highlights the efficacy of GA and PSO in ECC\noptimization, with implications for enhancing cybersecurity in third-party\ne-commerce integrations. We recommend the immediate consideration of these\nfindings before quantum computing's widespread adoption.",
        "translated": "本文对遗传算法(GA)和粒子群优化(PSO)这两种重要的人工智能算法进行了比较分析，重点是优化椭圆曲线密码学(ECC)参数。它们包括椭圆曲线系数、素数、生成点、群序和辅因子。这项研究提供了洞察哪些仿生算法产生更好的 ECC 配置优化结果，检查在相同的适应度函数下的性能。该函数结合了确保稳健 ECC 参数的方法，包括评估奇异或异常曲线和应用 Pollard’s rho 攻击和 Hasse 定理优化精度。遗传算法和粒子群优化算法生成的优化参数在模拟电子商务环境中进行了测试，并与椭圆曲线-迪菲赫尔曼(ECDH)和金钥杂凑讯息鑑别码(HMAC)在订单信息传输过程中的 secp256k1等著名曲线进行了对比。本研究以量子时代前的传统计算为研究对象，突出了遗传算法和粒子群算法在 ECC 优化中的有效性，对于提高第三方电子商务集成的网络安全性具有重要意义。我们建议在量子计算被广泛采用之前立即考虑这些发现。"
    },
    {
        "title": "A tiny public key scheme based on Niederreiter Cryptosystem",
        "url": "http://arxiv.org/abs/2310.06724v1",
        "pub_date": "2023-10-10",
        "summary": "Due to the weakness of public key cryptosystems encounter of quantum\ncomputers, the need to provide a solution was emerged. The McEliece\ncryptosystem and its security equivalent, the Niederreiter cryptosystem, which\nare based on Goppa codes, are one of the solutions, but they are not practical\ndue to their long key length. Several prior attempts to decrease the length of\nthe public key in code-based cryptosystems involved substituting the Goppa code\nfamily with other code families. However, these efforts ultimately proved to be\ninsecure. In 2016, the National Institute of Standards and Technology (NIST)\ncalled for proposals from around the world to standardize post-quantum\ncryptography (PQC) schemes to solve this issue. After receiving of various\nproposals in this field, the Classic McEliece cryptosystem, as well as the\nHamming Quasi-Cyclic (HQC) and Bit Flipping Key Encapsulation (BIKE), chosen as\ncode-based encryption category cryptosystems that successfully progressed to\nthe final stage. This article proposes a method for developing a code-based\npublic key cryptography scheme that is both simple and implementable. The\nproposed scheme has a much shorter public key length compared to the NIST\nfinalist cryptosystems. The key length for the primary parameters of the\nMcEliece cryptosystem (n=1024, k=524, t=50) ranges from 18 to 500 bits. The\nsecurity of this system is at least as strong as the security of the\nNiederreiter cryptosystem. The proposed structure is based on the Niederreiter\ncryptosystem which exhibits a set of highly advantageous properties that make\nit a suitable candidate for implementation in all extant systems.",
        "translated": "由于量子计算机遇到公钥密码体制的弱点，需要提供一种解决方案。基于 Goppa 码的 McEliece 密码体制及其安全等价体制 Niederreiter 密码体制是解决方案之一，但由于密钥长度过长而不实用。在基于代码的密码系统中，先前几次试图减少公钥长度的尝试涉及到用其他代码族替换 Goppa 代码族。然而，这些努力最终被证明是不安全的。2016年，美国国家标准与技术研究所(NIST)呼吁世界各国提出标准化后量子密码(PQC)方案来解决这一问题。在收到这一领域的各种建议后，经典 McEliece 密码体制以及 Hamming 准循环(HQC)和比特翻转密钥封装(BIKE)被选为基于代码的加密类别密码体制，并成功地发展到最后阶段。本文提出了一种开发基于代码的公钥密码体制的方法，这种方法既简单又可实现。与 NIST 终结密码系统相比，该方案具有更短的公钥长度。McEliece 密码体制主要参数(n = 1024，k = 524，t = 50)的密钥长度范围为18到500位。该系统的安全性至少与 Niederreiter 密码系统的安全性相当。该结构基于 Niederreiter 密码体制，该体制具有一系列非常有利的特性，使其成为所有现存系统中适合实现的候选体制。"
    },
    {
        "title": "Assessing the Impact of a Supervised Classification Filter on Flow-based\n  Hybrid Network Anomaly Detection",
        "url": "http://arxiv.org/abs/2310.06656v1",
        "pub_date": "2023-10-10",
        "summary": "Constant evolution and the emergence of new cyberattacks require the\ndevelopment of advanced techniques for defense. This paper aims to measure the\nimpact of a supervised filter (classifier) in network anomaly detection. We\nperform our experiments by employing a hybrid anomaly detection approach in\nnetwork flow data. For this purpose, we extended a state-of-the-art\nautoencoder-based anomaly detection method by prepending a binary classifier\nacting as a prefilter for the anomaly detector. The method was evaluated on the\npublicly available real-world dataset UGR'16. Our empirical results indicate\nthat the hybrid approach does offer a higher detection rate of known attacks\nthan a standalone anomaly detector while still retaining the ability to detect\nzero-day attacks. Employing a supervised binary prefilter has increased the AUC\nmetric by over 11%, detecting 30% more attacks while keeping the number of\nfalse positives approximately the same.",
        "translated": "不断的演变和新的网络攻击的出现要求发展先进的防御技术。本文旨在测量监督过滤器(分类器)在网络异常检测中的影响。我们在网络流量数据中采用混合异常检测方法进行实验。为此，我们扩展了一种最先进的基于自动编码器的异常检测检测方法，通过预置一个二进制分类器作为异常检测器的预滤波器。该方法在公开的现实世界数据集 UGR’16上进行了评估。我们的实验结果表明，与单独的异常检测器相比，混合方法在保留检测零日攻击能力的同时，确实提供了更高的已知攻击检测率。使用监督二进制预滤波器增加了超过11% 的 AUC 度量，检测30% 以上的攻击，同时保持假阳性的数量大致相同。"
    },
    {
        "title": "Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield\n  but Also a Catalyst for Model Inversion Attacks",
        "url": "http://arxiv.org/abs/2310.06549v1",
        "pub_date": "2023-10-10",
        "summary": "Label smoothing -- using softened labels instead of hard ones -- is a widely\nadopted regularization method for deep learning, showing diverse benefits such\nas enhanced generalization and calibration. Its implications for preserving\nmodel privacy, however, have remained unexplored. To fill this gap, we\ninvestigate the impact of label smoothing on model inversion attacks (MIAs),\nwhich aim to generate class-representative samples by exploiting the knowledge\nencoded in a classifier, thereby inferring sensitive information about its\ntraining data. Through extensive analyses, we uncover that traditional label\nsmoothing fosters MIAs, thereby increasing a model's privacy leakage. Even\nmore, we reveal that smoothing with negative factors counters this trend,\nimpeding the extraction of class-related information and leading to privacy\npreservation, beating state-of-the-art defenses. This establishes a practical\nand powerful novel way for enhancing model resilience against MIAs.",
        "translated": "标签平滑-使用软标签代替硬标签-是一种广泛采用的深度学习正则化方法，显示出多种好处，如增强了泛化和校准。然而，它对于保护模型隐私的影响还没有被探索。为了填补这一空白，我们研究了标签平滑对模型反演攻击(MIA)的影响，MIA 的目的是利用分类器中编码的知识生成类代表性样本，从而推断其训练数据的敏感信息。通过广泛的分析，我们发现传统的标签平滑助长了 MIA，从而增加了模型的隐私泄漏。更重要的是，我们揭示了消极因素的平滑反对这一趋势，阻碍了类相关信息的提取，导致隐私保护，击败了最先进的防御。这建立了一个实用和强大的新方法，以提高模型的抗干扰能力。"
    },
    {
        "title": "DASICS: Enhancing Memory Protection with Dynamic Compartmentalization",
        "url": "http://arxiv.org/abs/2310.06435v1",
        "pub_date": "2023-10-10",
        "summary": "In the existing software development ecosystem, security issues introduced by\nthird-party code cannot be overlooked. Among these security concerns, memory\naccess vulnerabilities stand out prominently, leading to risks such as the\ntheft or tampering of sensitive data. To address this issue, software-based\ndefense mechanisms have been established at the programming language, compiler,\nand operating system levels. However, as a trade-off, these mechanisms\nsignificantly reduce software execution efficiency. Hardware-software co-design\napproaches have sought to either construct entirely isolated trusted execution\nenvironments or attempt to partition security domains within the same address\nspace. While such approaches enhance efficiency compared to pure software\nmethods, they also encounter challenges related to granularity of protection,\nperformance overhead, and portability. In response to these challenges, we\npresent the DASICS (Dynamic in-Address-Space Isolation by Code Segments) secure\nprocessor design, which offers dynamic and flexible security protection across\nmultiple privilege levels, addressing data flow protection, control flow\nprotection, and secure system calls. We have implemented hardware FPGA\nprototypes and software QEMU simulator prototypes based on DASICS, along with\nnecessary modifications to system software for adaptability. We illustrate the\nprotective mechanisms and effectiveness of DASICS with two practical examples\nand provide potential real-world use cases where DASICS could be applied.",
        "translated": "在现有的软件开发生态系统中，不能忽视第三方代码引入的安全问题。在这些安全问题中，内存访问漏洞显得尤为突出，从而导致诸如窃取或篡改敏感数据等风险。为了解决这个问题，已经在编程语言、编译器和操作系统级别建立了基于软件的防御机制。然而，作为一种权衡，这些机制显著降低了软件执行效率。硬件-软件协同设计方法要么试图构建完全隔离的可信执行环境，要么试图在相同的地址空间内对安全域进行分区。虽然这些方法与纯软件方法相比提高了效率，但它们也遇到了与保护粒度、性能开销和可移植性相关的挑战。为了应对这些挑战，我们提出了 DASICS (动态地址内空间隔离代码段)安全处理器设计，它提供跨多个特权级别的动态和灵活的安全保护，解决数据流保护、控制流保护和安全系统调用。我们已经实现了硬件 FPGA 原型和基于 DASICS 的软件 QEMU 模拟器原型，并对系统软件进行了必要的适应性修改。我们用两个实际例子说明了 DASICS 的保护机制和有效性，并提供了可以应用 DASICS 的潜在实际用例。"
    },
    {
        "title": "Top of the Heap: Efficient Memory Error Protection for Many Heap Objects",
        "url": "http://arxiv.org/abs/2310.06397v1",
        "pub_date": "2023-10-10",
        "summary": "Exploits against heap memory errors continue to be a major concern. Although\nmany defenses have been proposed, heap data are not protected from attacks that\nexploit memory errors systematically. Research defenses focus on complete\ncoverage of heap objects, often giving up on comprehensive memory safety\nprotection and/or incurring high costs in performance overhead and memory\nusage. In this paper, we propose a solution for heap memory safety enforcement\nthat aims to provide comprehensive protection from memory errors efficiently by\nprotecting those heap objects whose accesses are provably safe from memory\nerrors. Specifically, we present the Uriah system that statically validates\nspatial and type memory safety for heap objects, isolating compliant objects on\na safe heap that enforces temporal type safety to prevent attacks on memory\nreuse. Using Uriah, 71.9% of heap allocation sites can be shown to produce\nobjects (73% of allocations are found safe) that satisfy spatial and type\nsafety, which are then isolated using Uriah's heap allocator from memory\naccesses via unsafe heap objects. Uriah only incurs 2.9% overhead and only uses\n9.3% more memory on SPEC CPU2006 (C/C++) benchmarks, showing that many heap\nobjects can be protected from all classes of memory errors efficiently.",
        "translated": "针对堆内存错误的利用仍然是一个主要问题。尽管已经提出了许多防御措施，但堆数据没有得到保护，无法避免系统地利用内存错误的攻击。研究防御关注于堆对象的完全覆盖，常常放弃全面的内存安全保护，并且/或者在性能开销和内存使用方面承担高成本。在本文中，我们提出了一个堆内存安全实施的解决方案，旨在通过保护那些访问被证明是安全的堆对象，有效地提供全面的内存错误保护。具体来说，我们展示了 Uriah 系统，该系统静态地验证堆对象的空间和类型内存安全性，将兼容对象隔离在安全堆上，强制实施时间类型安全，以防止对内存重用的攻击。使用 Uriah，可以显示71.9% 的堆分配站点生成满足空间和类型安全的对象(73% 的分配是安全的) ，然后使用 Uriah 的堆分配器将这些对象与通过不安全堆对象进行的内存访问隔离。在 SPEC CPU2006(C/C + +)基准测试中，Uriah 只产生了2.9% 的开销，并且只多使用了9.3% 的内存，这表明许多堆对象可以有效地避免所有类型的内存错误。"
    },
    {
        "title": "Jailbreak and Guard Aligned Language Models with Only Few In-Context\n  Demonstrations",
        "url": "http://arxiv.org/abs/2310.06387v1",
        "pub_date": "2023-10-10",
        "summary": "Large Language Models (LLMs) have shown remarkable success in various tasks,\nbut concerns about their safety and the potential for generating malicious\ncontent have emerged. In this paper, we explore the power of In-Context\nLearning (ICL) in manipulating the alignment ability of LLMs. We find that by\nproviding just few in-context demonstrations without fine-tuning, LLMs can be\nmanipulated to increase or decrease the probability of jailbreaking, i.e.\nanswering malicious prompts. Based on these observations, we propose In-Context\nAttack (ICA) and In-Context Defense (ICD) methods for jailbreaking and guarding\naligned language model purposes. ICA crafts malicious contexts to guide models\nin generating harmful outputs, while ICD enhances model robustness by\ndemonstrations of rejecting to answer harmful prompts. Our experiments show the\neffectiveness of ICA and ICD in increasing or reducing the success rate of\nadversarial jailbreaking attacks. Overall, we shed light on the potential of\nICL to influence LLM behavior and provide a new perspective for enhancing the\nsafety and alignment of LLMs.",
        "translated": "大型语言模型(LLM)已经在各种任务中取得了显著的成功，但是它们的安全性和生成恶意内容的可能性已经引起了人们的关注。在本文中，我们探讨了在上下文学习(ICL)在操纵 LLM 对齐能力方面的作用。我们发现，通过只提供少量的上下文演示而不进行微调，LLM 可以被操纵以增加或减少越狱的可能性，即回答恶意提示。在此基础上，我们提出了基于上下文攻击(ICA)和上下文防御(ICD)的方法来破解和保护对齐的语言模型。ICA 制造恶意环境来指导模型产生有害的输出，而 ICD 通过拒绝回答有害的提示来增强模型的稳健性。我们的实验显示了 ICA 和 ICD 在增加或减少对抗性越狱攻击的成功率方面的有效性。总的来说，我们阐明了 ICL 影响 LLM 行为的潜力，并为提高 LLM 的安全性和对齐性提供了一个新的视角。"
    },
    {
        "title": "Leveraging Diffusion-Based Image Variations for Robust Training on\n  Poisoned Data",
        "url": "http://arxiv.org/abs/2310.06372v1",
        "pub_date": "2023-10-10",
        "summary": "Backdoor attacks pose a serious security threat for training neural networks\nas they surreptitiously introduce hidden functionalities into a model. Such\nbackdoors remain silent during inference on clean inputs, evading detection due\nto inconspicuous behavior. However, once a specific trigger pattern appears in\nthe input data, the backdoor activates, causing the model to execute its\nconcealed function. Detecting such poisoned samples within vast datasets is\nvirtually impossible through manual inspection. To address this challenge, we\npropose a novel approach that enables model training on potentially poisoned\ndatasets by utilizing the power of recent diffusion models. Specifically, we\ncreate synthetic variations of all training samples, leveraging the inherent\nresilience of diffusion models to potential trigger patterns in the data. By\ncombining this generative approach with knowledge distillation, we produce\nstudent models that maintain their general performance on the task while\nexhibiting robust resistance to backdoor triggers.",
        "translated": "后门攻击对训练神经网络构成了严重的安全威胁，因为它们在模型中暗中引入了隐藏的功能。这种后门在推断干净输入时保持沉默，由于不明显的行为而躲避检测。然而，一旦一个特定的触发模式出现在输入数据中，后门激活，导致模型执行其隐藏的功能。通过人工检查，在庞大的数据集中检测这些中毒样本几乎是不可能的。为了应对这一挑战，我们提出了一种新的方法，通过利用最近的扩散模型的能力，使潜在中毒数据集的模型训练成为可能。具体来说，我们创建所有训练样本的合成变量，利用扩散模型对数据中潜在触发模式的固有弹性。通过将这种生成方法与知识提取相结合，我们产生了学生模型，保持了他们在任务上的一般表现，同时表现出对后门触发器的强大抵抗力。"
    },
    {
        "title": "Partition-based differentially private synthetic data generation",
        "url": "http://arxiv.org/abs/2310.06371v1",
        "pub_date": "2023-10-10",
        "summary": "Private synthetic data sharing is preferred as it keeps the distribution and\nnuances of original data compared to summary statistics. The state-of-the-art\nmethods adopt a select-measure-generate paradigm, but measuring large domain\nmarginals still results in much error and allocating privacy budget iteratively\nis still difficult. To address these issues, our method employs a\npartition-based approach that effectively reduces errors and improves the\nquality of synthetic data, even with a limited privacy budget. Results from our\nexperiments demonstrate the superiority of our method over existing approaches.\nThe synthetic data produced using our approach exhibits improved quality and\nutility, making it a preferable choice for private synthetic data sharing.",
        "translated": "与汇总统计相比，私有合成数据共享更受欢迎，因为它保持了原始数据的分布和细微差别。最新的方法采用选择测度生成范式，但是测量大域边缘仍然会导致很多误差，并且迭代分配隐私预算仍然是困难的。为了解决这些问题，我们的方法采用了一种基于分区的方法，这种方法有效地减少了错误并提高了合成数据的质量，即使在有限的保密预算下也是如此。我们的实验结果证明了我们的方法优于现有的方法。使用我们的方法生成的合成数据显示出改进的质量和实用性，使其成为私有合成数据共享的一个较好的选择。"
    },
    {
        "title": "DiPmark: A Stealthy, Efficient and Resilient Watermark for Large\n  Language Models",
        "url": "http://arxiv.org/abs/2310.07710v1",
        "pub_date": "2023-10-11",
        "summary": "Watermarking techniques offer a promising way to secure data via embedding\ncovert information into the data. A paramount challenge in the domain lies in\npreserving the distribution of original data during watermarking. Our research\nextends and refines existing watermarking framework, placing emphasis on the\nimportance of a distribution-preserving (DiP) watermark. Contrary to the\ncurrent strategies, our proposed DiPmark preserves the original token\ndistribution during watermarking (stealthy), is detectable without access to\nthe language model API or weights (efficient), and is robust to moderate\nchanges of tokens (resilient). This is achieved by incorporating a novel\nreweight strategy, combined with a hash function that assigns unique\n\\textit{i.i.d.} ciphers based on the context. The empirical benchmarks of our\napproach underscore its stealthiness, efficiency, and resilience, making it a\nrobust solution for watermarking tasks that demand impeccable quality\npreservation.",
        "translated": "水印技术通过在数据中嵌入隐蔽信息，为数据安全提供了一种有前途的方法。该领域最大的挑战在于在水印过程中保持原始数据的分布。我们的研究扩展和完善了现有的水印框架，强调了分布保持(DiP)水印的重要性。与目前的策略相反，我们提出的 DiPmark 在水印(隐形)期间保留了原始的令牌分布，在不访问语言模型 API 或权重(有效)的情况下可检测，并且对令牌的适度变化(弹性)具有鲁棒性。这是通过结合一种新的重新加权策略实现的，该策略结合了一个哈希函数，该函数根据上下文分配唯一的文本{ i.i.d. }密码。我们方法的经验基准突出了它的隐蔽性、效率和弹性，使其成为一个强大的水印任务解决方案，需要无可挑剔的质量保存。"
    },
    {
        "title": "Composite Backdoor Attacks Against Large Language Models",
        "url": "http://arxiv.org/abs/2310.07676v1",
        "pub_date": "2023-10-11",
        "summary": "Large language models (LLMs) have demonstrated superior performance compared\nto previous methods on various tasks, and often serve as the foundation models\nfor many researches and services. However, the untrustworthy third-party LLMs\nmay covertly introduce vulnerabilities for downstream tasks. In this paper, we\nexplore the vulnerability of LLMs through the lens of backdoor attacks.\nDifferent from existing backdoor attacks against LLMs, ours scatters multiple\ntrigger keys in different prompt components. Such a Composite Backdoor Attack\n(CBA) is shown to be stealthier than implanting the same multiple trigger keys\nin only a single component. CBA ensures that the backdoor is activated only\nwhen all trigger keys appear. Our experiments demonstrate that CBA is effective\nin both natural language processing (NLP) and multimodal tasks. For instance,\nwith $3\\%$ poisoning samples against the LLaMA-7B model on the Emotion dataset,\nour attack achieves a $100\\%$ Attack Success Rate (ASR) with a False Triggered\nRate (FTR) below $2.06\\%$ and negligible model accuracy degradation. The unique\ncharacteristics of our CBA can be tailored for various practical scenarios,\ne.g., targeting specific user groups. Our work highlights the necessity of\nincreased security research on the trustworthiness of foundation LLMs.",
        "translated": "大语言模型(LLM)在各种任务中的性能优于以往的方法，常常作为许多研究和服务的基础模型。但是，不可信的第三方 LLM 可能会秘密地为下游任务引入漏洞。本文从后门攻击的角度探讨 LLM 的脆弱性。与现有针对 LLM 的后门攻击不同，我们将多个触发键分散在不同的提示组件中。这样的复合后门攻击(CBA)被证明比只在单个组件中植入相同的多个触发键要隐蔽得多。CBA 确保后门只有在所有触发键出现时才被激活。我们的实验表明，CBA 在自然语言处理(NLP)和多模态任务中都是有效的。例如，在情感数据集中对 LLaMA-7B 模型进行3% 的中毒样本，我们的攻击获得了100% 的攻击成功率(ASR) ，错误触发率(FTR)低于2.06% ，模型精度降低可以忽略不计。我们的 CBA 的独特特征可以根据不同的实际情况进行调整，例如，针对特定的用户群。我们的工作强调了加强基金会 LLM 可信度安全性研究的必要性。"
    },
    {
        "title": "Prompt Backdoors in Visual Prompt Learning",
        "url": "http://arxiv.org/abs/2310.07632v1",
        "pub_date": "2023-10-11",
        "summary": "Fine-tuning large pre-trained computer vision models is infeasible for\nresource-limited users. Visual prompt learning (VPL) has thus emerged to\nprovide an efficient and flexible alternative to model fine-tuning through\nVisual Prompt as a Service (VPPTaaS). Specifically, the VPPTaaS provider\noptimizes a visual prompt given downstream data, and downstream users can use\nthis prompt together with the large pre-trained model for prediction. However,\nthis new learning paradigm may also pose security risks when the VPPTaaS\nprovider instead provides a malicious visual prompt. In this paper, we take the\nfirst step to explore such risks through the lens of backdoor attacks.\nSpecifically, we propose BadVisualPrompt, a simple yet effective backdoor\nattack against VPL. For example, poisoning $5\\%$ CIFAR10 training data leads to\nabove $99\\%$ attack success rates with only negligible model accuracy drop by\n$1.5\\%$. In particular, we identify and then address a new technical challenge\nrelated to interactions between the backdoor trigger and visual prompt, which\ndoes not exist in conventional, model-level backdoors. Moreover, we provide\nin-depth analyses of seven backdoor defenses from model, prompt, and input\nlevels. Overall, all these defenses are either ineffective or impractical to\nmitigate our BadVisualPrompt, implying the critical vulnerability of VPL.",
        "translated": "对于资源有限的用户来说，对预先训练好的大型计算机视觉模型进行微调是不可行的。因此，视觉提示学习(VPL)的出现为通过 Visual Prompt as a Service (VPPTaaS)进行模型微调提供了一种有效和灵活的替代方法。具体来说，VPPTaaS 提供商优化了给定下游数据的可视提示，下游用户可以将该提示与大型预先训练的模型一起使用以进行预测。然而，当 VPPTaaS 提供者提供恶意的视觉提示时，这种新的学习范例也可能带来安全风险。在本文中，我们首先从后门攻击的角度来探讨这些风险。具体来说，我们提出 BadVisualPrompt，一种针对 VPL 的简单而有效的后门攻击。例如，中毒 $5% $CIFAR10训练数据导致超过 $99% $攻击成功率，只有可忽略的模型精度下降 $1.5% $。特别是，我们识别并解决了与后门触发器和视觉提示之间的交互有关的一个新的技术挑战，这在传统的模型级后门中是不存在的。此外，我们还从模型、提示和输入层次深入分析了七种后门防御。总的来说，所有这些防御要么无效，要么不切实际，以减轻我们的 BadVisualPrompt，这意味着 VPL 的关键漏洞。"
    },
    {
        "title": "Cybersecurity as a Crosscutting Concept Across an Undergrad Computer\n  Science Curriculum: An Experience Report",
        "url": "http://arxiv.org/abs/2310.07625v1",
        "pub_date": "2023-10-11",
        "summary": "Although many Computer Science (CS) programs offer cybersecurity courses,\nthey are typically optional and placed at the periphery of the program. We\nadvocate to integrate cybersecurity as a crosscutting concept in CS curricula,\nwhich is also consistent with latest cybersecurity curricular guidelines, e.g.,\nCSEC2017. We describe our experience of implementing this crosscutting\nintervention across three undergraduate core CS courses at a leading technical\nuniversity in Europe between 2018 and 2023, collectively educating over 2200\nstudents. The security education was incorporated within CS courses using a\npartnership between the responsible course instructor and a security expert,\ni.e., the security expert (after consultation with course instructors)\ndeveloped and taught lectures covering multiple CSEC2017 knowledge areas. This\ncreated a complex dynamic between three stakeholders: the course instructor,\nthe security expert, and the students. We reflect on our intervention from the\nperspective of the three stakeholders -- we conducted a post-course survey to\ncollect student perceptions, and semi-supervised interviews with responsible\ncourse instructors and the security expert to gauge their experience. We found\nthat while the students were extremely enthusiastic about the security content\nand retained its impact several years later, the misaligned incentives for the\ninstructors and the security expert made it difficult to sustain this\nintervention without organizational support. By identifying limitations in our\nintervention, we suggest ideas for sustaining it.",
        "translated": "尽管许多计算机科学(CS)课程提供网络安全课程，但这些课程通常是可选的，并且处于该课程的外围。我们主张将网络安全作为一个横向概念纳入 CS 课程，这也与最新的网络安全课程指南(如 CSEC2017)相一致。我们描述了我们在2018年至2023年间在欧洲一所领先的技术大学的三门本科生 CS 核心课程中实施这种横向干预的经验，共培养了2200多名学生。安全教育被纳入 CS 课程，使用负责的课程教师和安全专家之间的伙伴关系，即安全专家(在与课程教师协商后)开发和讲授涵盖多个 CSEC2017知识领域的讲座。这在三个涉众之间创建了一个复杂的动态关系: 课程讲师、安全专家和学生。我们从三个利益相关者的角度反思我们的干预——我们进行了一个课后调查来收集学生的看法，并与负责任的课程教师和安全专家进行了半监督访谈来衡量他们的经验。我们发现，虽然学生们对安全内容极为热衷，并在几年后保持了其影响力，但对教师和安全专家的激励不一致，使得在没有组织支持的情况下难以维持这种干预。通过确定我们干预的局限性，我们提出了维持干预的想法。"
    },
    {
        "title": "In-Context Unlearning: Language Models as Few Shot Unlearners",
        "url": "http://arxiv.org/abs/2310.07579v2",
        "pub_date": "2023-10-11",
        "summary": "Machine unlearning, the study of efficiently removing the impact of specific\ntraining points on the trained model, has garnered increased attention of late,\ndriven by the need to comply with privacy regulations like the Right to be\nForgotten. Although unlearning is particularly relevant for LLMs in light of\nthe copyright issues they raise, achieving precise unlearning is\ncomputationally infeasible for very large models. To this end, recent work has\nproposed several algorithms which approximate the removal of training data\nwithout retraining the model. These algorithms crucially rely on access to the\nmodel parameters in order to update them, an assumption that may not hold in\npractice due to computational constraints or when the LLM is accessed via API.\nIn this work, we propose a new class of unlearning methods for LLMs we call\n''In-Context Unlearning'', providing inputs in context and without having to\nupdate model parameters. To unlearn a particular training instance, we provide\nthe instance alongside a flipped label and additional correctly labelled\ninstances which are prepended as inputs to the LLM at inference time. Our\nexperimental results demonstrate that these contexts effectively remove\nspecific information from the training set while maintaining performance levels\nthat are competitive with (or in some cases exceed) state-of-the-art unlearning\nmethods that require access to the LLM parameters.",
        "translated": "机器学习，即有效地消除特定训练点对训练模型的影响的研究，最近受到了越来越多的关注，这是由于需要遵守隐私法规，比如被遗忘的权利。虽然遗忘对于 LLM 来说特别重要，因为它们引起了版权问题，但是对于非常大的模型来说，实现精确的遗忘在计算上是不可行的。为此，最近的工作已经提出了几种算法，近似删除训练数据，而不再训练模型。这些算法主要依赖于对模型参数的访问来更新它们，由于计算约束或通过 API 访问 LLM，这种假设在实践中可能不成立。在这项工作中，我们提出了一类新的 LLM 的去学习方法，我们称之为“在上下文中去学习”，在上下文中提供输入，而不需要更新模型参数。为了忘记某个特定的训练实例，我们在一个翻转的标签旁边提供了该实例，另外还有一些正确标记的实例，这些实例在推断时被预先作为 LLM 的输入。我们的实验结果表明，这些上下文有效地从训练集中删除了特定的信息，同时保持了与需要访问 LLM 参数的最先进的去学习方法相竞争(或在某些情况下超过)的性能水平。"
    },
    {
        "title": "Code Polymorphism Meets Code Encryption: Confidentiality and\n  Side-Channel Protection of Software Components",
        "url": "http://arxiv.org/abs/2310.07327v1",
        "pub_date": "2023-10-11",
        "summary": "In this paper, we consider that, in practice, attack scenarios involving\nside-channel analysis combine two successive phases:an analysis phase,\ntargeting the extraction of information about the target and the identification\nof possible vulnerabilities;and an exploitation phase, applying attack\ntechniques on candidate vulnerabilities. We advocate that protections need to\ncoverthese two phases in order to be effective against real-life attacks. We\npresent PolEn, a toolchain and a processor architecturethat combine\ncountermeasures in order to provide an effective mitigation of side-channel\nattacks: as a countermeasure againstthe analysis phase, our approach considers\nthe use of code encryption; as a countermeasure against the exploitation\nphase,our approach considers the use of code polymorphism, because it relies on\nruntime code generation, and its combinationwith code encryption is\nparticularly challenging. Code encryption is supported by a processor extension\nsuch that machineinstructions are only decrypted inside the CPU, which\neffectively prevents reverse engineering or any extraction of usefulinformation\nfrom memory dumps. Code polymorphism is implemented by software means. It\nregularly changes the observablebehaviour of the program, making it\nunpredictable for an attacker, hence reducing the possibility to exploit\nside-channelleakages. We present a prototype implementation, based on the\nRISC-V Spike simulator and a modified LLVM toolchain. Inour experimental\nevaluation, we illustrate that PolEn effectively reduces side-channel leakages.\nFor the protected functionsevaluated, static memory use increases by a factor\nof 5 to 22, corresponding to the joint application of code encryption andcode\npolymorphism. The overhead, in terms of execution time, ranges between a factor\nof 1.8 and 4.6.",
        "translated": "在本文中，我们认为，在实践中，涉及边信道分析的攻击场景结合了两个连续的阶段: 一个分析阶段，针对目标信息的提取和可能的漏洞的识别; 和一个利用阶段，对候选漏洞应用攻击技术。我们主张，保护需要涵盖这两个阶段，以便有效地对付现实生活中的攻击。我们提出了 PolEn，一个工具链和一个处理器架构，结合对策，以提供一个有效的缓解副通道攻击: 作为一个对抗分析阶段的对策，我们的方法考虑使用代码加密; 作为一个对抗开发阶段的对策，我们的方法考虑使用代码多态性，因为它依赖于运行时代码生成，它与代码加密的结合是特别具有挑战性的。代码加密由一个处理器扩展支持，这样机器指令只能在 CPU 内部解密，有效地防止逆向工程或从内存转储中提取有用信息。代码多态性是通过软件手段实现的。它定期改变程序的可观察行为，使其对攻击者来说不可预测，从而减少了利用边通道泄漏的可能性。我们提出了一个原型实现，基于 RISC-V 穗模拟器和修改 LLVM 工具链。在我们的实验评估中，我们说明 PolEn 有效地减少了侧通道泄漏。对于被评估的受保护函数，静态内存使用增加了5到22倍，对应于代码加密和代码多态性的联合应用。就执行时间而言，开销在1.8到4.6之间。"
    },
    {
        "title": "Improved Membership Inference Attacks Against Language Classification\n  Models",
        "url": "http://arxiv.org/abs/2310.07219v1",
        "pub_date": "2023-10-11",
        "summary": "Artificial intelligence systems are prevalent in everyday life, with use\ncases in retail, manufacturing, health, and many other fields. With the rise in\nAI adoption, associated risks have been identified, including privacy risks to\nthe people whose data was used to train models. Assessing the privacy risks of\nmachine learning models is crucial to enabling knowledgeable decisions on\nwhether to use, deploy, or share a model. A common approach to privacy risk\nassessment is to run one or more known attacks against the model and measure\ntheir success rate. We present a novel framework for running membership\ninference attacks against classification models. Our framework takes advantage\nof the ensemble method, generating many specialized attack models for different\nsubsets of the data. We show that this approach achieves higher accuracy than\neither a single attack model or an attack model per class label, both on\nclassical and language classification tasks.",
        "translated": "人工智能系统在日常生活中非常普遍，在零售业、制造业、卫生和许多其他领域都有使用案例。随着人工智能应用的增加，相关的风险已经被确定，包括被用来训练模型的人的隐私风险。评估机器学习模型的隐私风险对于是否使用、部署或共享模型的知识型决策至关重要。隐私风险评估的一种常见方法是对模型运行一个或多个已知的攻击，并测量它们的成功率。我们提出了一个新的框架来运行对分类模型的成员推理攻击。我们的框架利用了集成方法，为不同的数据子集生成了许多专门的攻击模型。结果表明，无论是在经典分类还是语言分类任务中，该方法都比单一攻击模型或每个类标签的攻击模型具有更高的准确性。"
    },
    {
        "title": "My Brother Helps Me: Node Injection Based Adversarial Attack on Social\n  Bot Detection",
        "url": "http://arxiv.org/abs/2310.07159v1",
        "pub_date": "2023-10-11",
        "summary": "Social platforms such as Twitter are under siege from a multitude of\nfraudulent users. In response, social bot detection tasks have been developed\nto identify such fake users. Due to the structure of social networks, the\nmajority of methods are based on the graph neural network(GNN), which is\nsusceptible to attacks. In this study, we propose a node injection-based\nadversarial attack method designed to deceive bot detection models. Notably,\nneither the target bot nor the newly injected bot can be detected when a new\nbot is added around the target bot. This attack operates in a black-box\nfashion, implying that any information related to the victim model remains\nunknown. To our knowledge, this is the first study exploring the resilience of\nbot detection through graph node injection. Furthermore, we develop an\nattribute recovery module to revert the injected node embedding from the graph\nembedding space back to the original feature space, enabling the adversary to\nmanipulate node perturbation effectively. We conduct adversarial attacks on\nfour commonly used GNN structures for bot detection on two widely used\ndatasets: Cresci-2015 and TwiBot-22. The attack success rate is over 73\\% and\nthe rate of newly injected nodes being detected as bots is below 13\\% on these\ntwo datasets.",
        "translated": "Twitter 等社交平台正受到大量欺诈用户的围攻。作为回应，社交机器人检测任务已经开发出来，以识别这种虚假用户。由于社会网络的结构特点，大多数方法都是基于易受攻击的图神经网络(GNN)。在这项研究中，我们提出了一个基于节点注入的对抗性攻击方法来设计欺骗机器人检测模型。值得注意的是，当在目标机器人周围添加新的机器人时，既不能检测到目标机器人，也不能检测到新注入的机器人。这种攻击以黑匣子的方式进行，意味着任何与受害者模型相关的信息都是未知的。据我们所知，这是第一次研究通过图节点注入来探索机器人检测的弹性。此外，我们还开发了一个属性恢复模块，将注入的节点从图嵌入空间恢复到原来的特征空间，使得对手能够有效地操纵节点扰动。我们针对两个广泛使用的数据集: Cresci2015和 TwiBot-22，对四种常用的 GNN 结构进行对抗性攻击，用于机器人检测。在这两个数据集中，攻击成功率超过73% ，新注入的节点作为机器人被检测到的比率低于13% 。"
    },
    {
        "title": "No Privacy Left Outside: On the (In-)Security of TEE-Shielded DNN\n  Partition for On-Device ML",
        "url": "http://arxiv.org/abs/2310.07152v1",
        "pub_date": "2023-10-11",
        "summary": "On-device ML introduces new security challenges: DNN models become white-box\naccessible to device users. Based on white-box information, adversaries can\nconduct effective model stealing (MS) and membership inference attack (MIA).\nUsing Trusted Execution Environments (TEEs) to shield on-device DNN models aims\nto downgrade (easy) white-box attacks to (harder) black-box attacks. However,\none major shortcoming is the sharply increased latency (up to 50X). To\naccelerate TEE-shield DNN computation with GPUs, researchers proposed several\nmodel partition techniques. These solutions, referred to as TEE-Shielded DNN\nPartition (TSDP), partition a DNN model into two parts, offloading the\nprivacy-insensitive part to the GPU while shielding the privacy-sensitive part\nwithin the TEE. This paper benchmarks existing TSDP solutions using both MS and\nMIA across a variety of DNN models, datasets, and metrics. We show important\nfindings that existing TSDP solutions are vulnerable to privacy-stealing\nattacks and are not as safe as commonly believed. We also unveil the inherent\ndifficulty in deciding optimal DNN partition configurations (i.e., the highest\nsecurity with minimal utility cost) for present TSDP solutions. The experiments\nshow that such ``sweet spot'' configurations vary across datasets and models.\nBased on lessons harvested from the experiments, we present TEESlice, a novel\nTSDP method that defends against MS and MIA during DNN inference. TEESlice\nfollows a partition-before-training strategy, which allows for accurate\nseparation between privacy-related weights from public weights. TEESlice\ndelivers the same security protection as shielding the entire DNN model inside\nTEE (the ``upper-bound'' security guarantees) with over 10X less overhead (in\nboth experimental and real-world environments) than prior TSDP solutions and no\naccuracy loss.",
        "translated": "设备上机器学习引入了新的安全挑战: DNN 模型成为设备用户可访问的白盒。基于白盒信息，对手可以进行有效的模型窃取(MS)和成员推理攻击(MIA)。使用可信执行环境(TEE)来屏蔽设备上的 DNN 模型旨在将(容易的)白盒攻击降级为(难的)黑盒攻击。然而，一个主要的缺点是延迟急剧增加(高达50倍)。为了利用 GPU 加速 TEE 屏蔽 DNN 的计算，研究人员提出了几种模型划分技术。这些解决方案被称为 TEE 屏蔽 DNN 分区(TSDP) ，将 DNN 模型分为两部分，将隐私不敏感部分卸载到 GPU，同时屏蔽 TEE 内部的隐私敏感部分。本文使用 MS 和 MIA 跨各种 DNN 模型、数据集和度量标准对现有的 TSDP 解决方案进行了基准测试。我们展示了重要的发现，现有的 TSDP 解决方案容易受到隐私窃取攻击，并且不像通常认为的那样安全。我们还揭示了目前 TSDP 解决方案在确定最佳 DNN 分区配置(即，最高安全性和最低实用成本)方面的固有困难。实验表明，这种“最佳点”配置在不同的数据集和模型中有所不同。基于从实验中获得的教训，我们提出了 TEESlice，一种新的 TSDP 方法，防止 MS 和 MIA 在 DNN 推理过程中。TEESlice 遵循一种训练前分区策略，这种策略允许在与隐私相关的权重和公共权重之间进行精确的分离。TEESlice 提供了与屏蔽 TEE 内部整个 DNN 模型(“上限”安全保障)相同的安全保护，开销(在实验和现实环境中)比以前的 TSDP 解决方案少10倍以上，而且没有准确性损失。"
    },
    {
        "title": "ObliuSky: Oblivious User-Defined Skyline Query Processing in the Cloud",
        "url": "http://arxiv.org/abs/2310.07148v1",
        "pub_date": "2023-10-11",
        "summary": "The proliferation of cloud computing has greatly spurred the popularity of\noutsourced database storage and management, in which the cloud holding\noutsourced databases can process database queries on demand. Among others,\nskyline queries play an important role in the database field due to its\nprominent usefulness in multi-criteria decision support systems. To accommodate\nthe tailored needs of users, user-defined skyline query has recently emerged as\nan intriguing advanced type of skyline query, which allows users to define\ncustom preferences in their skyline queries (including the target attributes,\npreferred dominance relations, and range constraints on the target attributes).\nHowever, user-defined skyline query services, if deployed in the cloud, may\nraise critical privacy concerns as the outsourced databases and skyline queries\nmay contain proprietary/privacy-sensitive information, and the cloud might even\nsuffer from data breaches. In light of the above, this paper presents ObliuSky,\na new system framework enabling oblivious user-defined skyline query processing\nin the cloud. ObliuSky departs from the state-of-the-art prior work by not only\nproviding confidentiality protection for the content of the outsourced\ndatabase, the user-defined skyline query, and the query results, but also\nmaking the cloud oblivious to the data patterns (e.g., user-defined dominance\nrelations among database points and search access patterns) which may\nindirectly cause data leakages. We formally analyze the security guarantees and\nconduct extensive performance evaluations. The results show that while\nachieving much stronger security guarantees than the state-of-the-art prior\nwork, ObliuSky is superior in database and query encryption efficiency, with\npractically affordable query latency.",
        "translated": "云计算的普及极大地推动了外包数据库存储和管理的普及，在这种情况下，云存储外包数据库可以根据需要处理数据库查询。其中，轮廓查询由于在多准则决策支持系统中的突出应用，在数据库领域中发挥着重要作用。为了满足用户量身定制的需求，用户定义的天际线查询最近成为一种有趣的高级天际线查询类型，它允许用户在他们的天际线查询中定义自定义偏好(包括目标属性、首选优势关系和目标属性的范围约束)。然而，用户定义的天际线查询服务，如果部署在云中，可能会引起关键的隐私问题，因为外包数据库和天际线查询可能包含专有/隐私敏感信息，云甚至可能遭受数据泄露。鉴于上述情况，本文提出了一个新的系统框架 ObliuSky，它支持在云中进行用户自定义的天际线查询处理。ObliuSky 不仅为外包数据库的内容、用户定义的天际线查询和查询结果提供保密性保护，而且使云无视可能间接导致数据泄漏的数据模式(例如，数据库点之间的用户定义的支配关系和搜索访问模式) ，从而背离了先前的最先进的工作。我们正式分析安全保障，并进行广泛的绩效评估。实验结果表明，ObliuSky 在数据库和查询加密效率方面优于现有的技术，同时获得了比现有技术更强的安全保障，查询延迟几乎可以负担得起。"
    },
    {
        "title": "Differentially Private Non-convex Learning for Multi-layer Neural\n  Networks",
        "url": "http://arxiv.org/abs/2310.08425v1",
        "pub_date": "2023-10-12",
        "summary": "This paper focuses on the problem of Differentially Private Stochastic\nOptimization for (multi-layer) fully connected neural networks with a single\noutput node. In the first part, we examine cases with no hidden nodes,\nspecifically focusing on Generalized Linear Models (GLMs). We investigate the\nwell-specific model where the random noise possesses a zero mean, and the link\nfunction is both bounded and Lipschitz continuous. We propose several\nalgorithms and our analysis demonstrates the feasibility of achieving an excess\npopulation risk that remains invariant to the data dimension. We also delve\ninto the scenario involving the ReLU link function, and our findings mirror\nthose of the bounded link function. We conclude this section by contrasting\nwell-specified and misspecified models, using ReLU regression as a\nrepresentative example.\n  In the second part of the paper, we extend our ideas to two-layer neural\nnetworks with sigmoid or ReLU activation functions in the well-specified model.\nIn the third part, we study the theoretical guarantees of DP-SGD in Abadi et\nal. (2016) for fully connected multi-layer neural networks. By utilizing recent\nadvances in Neural Tangent Kernel theory, we provide the first excess\npopulation risk when both the sample size and the width of the network are\nsufficiently large. Additionally, we discuss the role of some parameters in\nDP-SGD regarding their utility, both theoretically and empirically.",
        "translated": "本文研究具有单输出节点的(多层)全连通神经网络的差分私有随机优化问题。在第一部分中，我们研究了没有隐藏节点的情况，特别关注广义线性模型(GLM)。我们研究了随机噪声具有零均值且链接函数是有界且 Lipschitz 连续的井特定模型。我们提出了几个算法，我们的分析证明了实现过剩的人口风险，保持不变的数据维的可行性。我们还深入研究了涉及 ReLU 链接函数的场景，我们的发现反映了有界链接函数的情况。我们以 ReLU 回归作为一个代表性的例子，通过对比精确指定和错误指定的模型来总结本节。在本文的第二部分中，我们将我们的想法推广到具有乙状结构或 ReLU 激活函数的两层神经网络。在第三部分，我们研究了 Abadi 等人(2016)的 DP-sgd 对于完全连通的多层神经网络的理论保证。利用神经正切核理论的最新进展，我们提供了当样本量和网络宽度都没有足够大时的第一个超额人口风险。此外，我们还讨论了 DP-SGD 中一些参数在理论和经验上的作用。"
    },
    {
        "title": "Identifying reducible $k$-tuples of vectors with subspace-proximity\n  sensitive hashing/filtering",
        "url": "http://arxiv.org/abs/2310.08416v1",
        "pub_date": "2023-10-12",
        "summary": "We introduce and analyse a family of hash and predicate functions that are\nmore likely to produce collisions for small reducible configurations of\nvectors. These may offer practical improvements to lattice sieving for short\nvectors. In particular, in one asymptotic regime the family exhibits\nsignificantly different convergent behaviour than existing hash functions and\npredicates.",
        "translated": "我们引入并分析了一系列散列和谓词函数，它们更有可能对向量的小型可约配置产生冲突。这些结果可以为短向量的格子筛分提供实用的改进。特别地，在一个渐近区域中，这个家族的收敛行为显著不同于现有的哈希函数和谓词。"
    },
    {
        "title": "2SFGL: A Simple And Robust Protocol For Graph-Based Fraud Detection",
        "url": "http://arxiv.org/abs/2310.08335v1",
        "pub_date": "2023-10-12",
        "summary": "Financial crime detection using graph learning improves financial safety and\nefficiency. However, criminals may commit financial crimes across different\ninstitutions to avoid detection, which increases the difficulty of detection\nfor financial institutions which use local data for graph learning. As most\nfinancial institutions are subject to strict regulations in regards to data\nprivacy protection, the training data is often isolated and conventional\nlearning technology cannot handle the problem. Federated learning (FL) allows\nmultiple institutions to train a model without revealing their datasets to each\nother, hence ensuring data privacy protection. In this paper, we proposes a\nnovel two-stage approach to federated graph learning (2SFGL): The first stage\nof 2SFGL involves the virtual fusion of multiparty graphs, and the second\ninvolves model training and inference on the virtual graph. We evaluate our\nframework on a conventional fraud detection task based on the\nFraudAmazonDataset and FraudYelpDataset. Experimental results show that\nintegrating and applying a GCN (Graph Convolutional Network) with our 2SFGL\nframework to the same task results in a 17.6\\%-30.2\\% increase in performance\non several typical metrics compared to the case only using FedAvg, while\nintegrating GraphSAGE with 2SFGL results in a 6\\%-16.2\\% increase in\nperformance compared to the case only using FedAvg. We conclude that our\nproposed framework is a robust and simple protocol which can be simply\nintegrated to pre-existing graph-based fraud detection methods.",
        "translated": "利用图形学习技术进行金融犯罪侦查，提高了金融安全性和效率。然而，为了避免被发现，犯罪分子可能跨越不同的机构实施金融犯罪，这增加了使用本地数据进行图形学习的金融机构被发现的难度。由于大多数金融机构在数据隐私保护方面受到严格的监管，培训数据往往是孤立的，传统的学习技术无法处理这个问题。联邦学习(FL)允许多个机构训练一个模型，而不必向彼此透露它们的数据集，从而确保数据隐私保护。本文提出了一种新的两阶段联邦图学习(2SFGL)方法: 2SFGL 的第一阶段涉及多方图的虚拟融合，第二阶段涉及对虚拟图的模型训练和推理。我们评估了基于 FraudAmazonDataset 和 FraudYelpDataset 的传统欺诈检测任务的框架。实验结果表明，与仅使用 FedAvg 的情况相比，将2SFGL 框架与 GCN (GraphConvolutionNetwork)集成并应用于相同的任务，在几个典型指标上的性能提高了17.6% -30.2% ，而将 GraphSAGE 与2SFGL 集成的性能提高了6% -16.2% 。我们的结论是，我们提出的框架是一个健壮和简单的协议，可以简单地集成到预先存在的基于图的欺诈检测方法。"
    },
    {
        "title": "Defending Our Privacy With Backdoors",
        "url": "http://arxiv.org/abs/2310.08320v1",
        "pub_date": "2023-10-12",
        "summary": "The proliferation of large AI models trained on uncurated, often sensitive\nweb-scraped data has raised significant privacy concerns. One of the concerns\nis that adversaries can extract information about the training data using\nprivacy attacks. Unfortunately, the task of removing specific information from\nthe models without sacrificing performance is not straightforward and has\nproven to be challenging. We propose a rather easy yet effective defense based\non backdoor attacks to remove private information such as names of individuals\nfrom models, and focus in this work on text encoders. Specifically, through\nstrategic insertion of backdoors, we align the embeddings of sensitive phrases\nwith those of neutral terms-\"a person\" instead of the person's name. Our\nempirical results demonstrate the effectiveness of our backdoor-based defense\non CLIP by assessing its performance using a specialized privacy attack for\nzero-shot classifiers. Our approach provides not only a new \"dual-use\"\nperspective on backdoor attacks, but also presents a promising avenue to\nenhance the privacy of individuals within models trained on uncurated\nweb-scraped data.",
        "translated": "大型人工智能模型的激增使得人们开始关注隐私问题。这些模型使用的是未经管理的、通常是敏感的网络数据。其中一个问题是，对手可以使用隐私攻击提取训练数据的信息。不幸的是，在不牺牲性能的情况下从模型中删除特定信息的任务并不简单，而且已被证明是具有挑战性的。我们提出了一个相当容易，但有效的防御基于后门攻击删除私人信息，如个人的名字从模型，并在这项工作的重点是文本编码器。具体来说，通过策略性的后门插入，我们将敏感词组的嵌入与中性词组的嵌入结合起来——“一个人”而不是这个人的名字。我们的实验结果通过使用针对零射击分类器的专用隐私攻击来评估 CLIP 的性能，从而证明了我们基于后门的防御策略的有效性。我们的方法不仅为后门攻击提供了一个新的“双重用途”的视角，而且还提供了一个有希望的途径来增强个人隐私的模型训练未经管理的网络刮取数据。"
    },
    {
        "title": "Harnessing the Power of LLM to Support Binary Taint Analysis",
        "url": "http://arxiv.org/abs/2310.08275v1",
        "pub_date": "2023-10-12",
        "summary": "This paper proposes LATTE, the first static binary taint analysis that is\npowered by a large language model (LLM). LATTE is superior to the state of the\nart (e.g., Emtaint, Arbiter, Karonte) in three aspects. First, LATTE is fully\nautomated while prior static binary taint analyzers need rely on human\nexpertise to manually customize taint propagation rules and vulnerability\ninspection rules. Second, LATTE is significantly effective in vulnerability\ndetection, demonstrated by our comprehensive evaluations. For example, LATTE\nhas found 37 new bugs in real-world firmware which the baselines failed to\nfind, and 7 of them have been assigned CVE numbers. Lastly, LATTE incurs\nremarkably low engineering cost, making it a cost-efficient and scalable\nsolution for security researchers and practitioners. We strongly believe that\nLATTE opens up a new direction to harness the recent advance in LLMs to improve\nvulnerability analysis for binary programs.",
        "translated": "本文首次提出了基于大语言模型(LLM)的静态二进制污染分析方法 LatTE。LATTE 在三个方面优于最先进的技术(例如，Emtaint，仲裁者，Karonte)。首先，LatTE 是完全自动化的，而以前的静态二进制污染分析器需要依赖人类的专业知识来手动定制污染传播规则和漏洞检查规则。其次，我们的综合评估表明，LatTE 在漏洞检测方面具有显著的效果。例如，LATTE 已经在现实世界的固件中发现了37个基线未能发现的新 bug，其中7个已经被分配了 CVE 编号。最后，LATTE 的工程成本非常低，使其成为安全研究人员和从业人员的一个具有成本效益和可扩展性的解决方案。我们坚信 LATTE 为利用 LLM 的最新进展改进二进制程序的漏洞分析开辟了新的方向。"
    },
    {
        "title": "Invisible Threats: Backdoor Attack in OCR Systems",
        "url": "http://arxiv.org/abs/2310.08259v1",
        "pub_date": "2023-10-12",
        "summary": "Optical Character Recognition (OCR) is a widely used tool to extract text\nfrom scanned documents. Today, the state-of-the-art is achieved by exploiting\ndeep neural networks. However, the cost of this performance is paid at the\nprice of system vulnerability. For instance, in backdoor attacks, attackers\ncompromise the training phase by inserting a backdoor in the victim's model\nthat will be activated at testing time by specific patterns while leaving the\noverall model performance intact. This work proposes a backdoor attack for OCR\nresulting in the injection of non-readable characters from malicious input\nimages. This simple but effective attack exposes the state-of-the-art OCR\nweakness, making the extracted text correct to human eyes but simultaneously\nunusable for the NLP application that uses OCR as a preprocessing step.\nExperimental results show that the attacked models successfully output\nnon-readable characters for around 90% of the poisoned instances without\nharming their performance for the remaining instances.",
        "translated": "光学字符识别(OCR)是一种广泛使用的从扫描文档中提取文本的工具。今天，最先进的技术是通过开发深层神经网络来实现的。然而，这种性能的代价是以系统的脆弱性为代价的。例如，在后门攻击中，攻击者通过在受害者的模型中插入一个后门来破坏训练阶段，该后门将在测试时被特定的模式激活，同时保持整个模型的性能完好无损。这项工作提出了一个后门攻击的 OCR 导致注入不可读的字符从恶意输入图像。这种简单但有效的攻击暴露了最先进的 OCR 的弱点，使得提取的文本对人眼来说是正确的，但同时不能用于使用 OCR 作为预处理步骤的 NLP 应用程序。实验结果表明，被攻击的模型能够成功地输出90% 左右的中毒实例的不可读字符，而不会影响剩余实例的性能。"
    },
    {
        "title": "Combining Decentralized IDentifiers with Proof of Membership to Enable\n  Trust in IoT Networks",
        "url": "http://arxiv.org/abs/2310.08163v1",
        "pub_date": "2023-10-12",
        "summary": "The Self-Sovereign Identity (SSI) is a decentralized paradigm enabling full\ncontrol over the data used to build and prove the identity. In Internet of\nThings networks with security requirements, the Self-Sovereign Identity can\nplay a key role and bring benefits with respect to centralized identity\nsolutions. The challenge is to make the SSI compatible with resource-constraint\nIoT networks. In line with this objective, the paper proposes and discusses an\nalternative (mutual) authentication process for IoT nodes under the same\nadministration domain. The main idea is to combine the Decentralized IDentifier\n(DID)-based verification of private key ownership with the verification of a\nproof that the DID belongs to an evolving trusted set. The solution is built\naround the proof of membership notion. The paper analyzes two membership\nsolutions, a novel solution designed by the Authors based on Merkle trees and a\nsecond one based on the adaptation of Boneh, Boyen and Shacham (BBS) group\nsignature scheme. The paper concludes with a performance estimation and a\ncomparative analysis.",
        "translated": "自我主权身份(SSI)是一种分散的范式，它能够完全控制用于建立和证明身份的数据。在具有安全需求的物联网中，自主身份在集中式身份解决方案中可以发挥关键作用并带来好处。挑战在于使 SSI 与资源受限的物联网相兼容。针对这一目标，本文提出并讨论了一种在同一管理域下的物联网节点可选择的(相互)认证流程。其主要思想是将基于分散标识符的私钥所有权验证与分散标识符属于发展中的可信集的证明验证相结合。解决方案是围绕成员概念的证明而建立的。本文分析了两种成员签名方案，一种是作者设计的基于 Merkle 树的新型成员签名方案，另一种是基于 Boneh，Boyen 和 Shacham (BBS)群签名方案的改进。本文最后给出了业绩评价和比较分析。"
    },
    {
        "title": "A Systematic Evaluation of Automated Tools for Side-Channel\n  Vulnerabilities Detection in Cryptographic Libraries",
        "url": "http://arxiv.org/abs/2310.08153v1",
        "pub_date": "2023-10-12",
        "summary": "To protect cryptographic implementations from side-channel vulnerabilities,\ndevelopers must adopt constant-time programming practices. As these can be\nerror-prone, many side-channel detection tools have been proposed. Despite\nthis, such vulnerabilities are still manually found in cryptographic libraries.\nWhile a recent paper by Jancar et al. shows that developers rarely perform\nside-channel detection, it is unclear if existing detection tools could have\nfound these vulnerabilities in the first place. To answer this question, we\nsurveyed the literature to build a classification of 34 side-channel detection\nframeworks. The classification we offer compares multiple criteria, including\nthe methods used, the scalability of the analysis or the threat model\nconsidered. We then built a unified common benchmark of representative\ncryptographic operations on a selection of 5 promising detection tools. This\nbenchmark allows us to better compare the capabilities of each tool, and the\nscalability of their analysis. Additionally, we offer a classification of\nrecently published side-channel vulnerabilities. We then test each of the\nselected tools on benchmarks reproducing a subset of these vulnerabilities as\nwell as the context in which they appear. We find that existing tools can\nstruggle to find vulnerabilities for a variety of reasons, mainly the lack of\nsupport for SIMD instructions, implicit flows, and internal secret generation.\nBased on our findings, we develop a set of recommendations for the research\ncommunity and cryptographic library developers, with the goal to improve the\neffectiveness of side-channel detection tools.",
        "translated": "为了保护加密实现不受侧信道漏洞的影响，开发人员必须采用常量时间编程实践。由于这些方法容易出错，因此提出了许多侧信道检测工具。尽管如此，在加密库中仍然可以手动发现这样的漏洞。虽然 Jancar 等人最近的一篇论文显示开发人员很少执行边通道检测，但是目前还不清楚现有的检测工具是否能够首先发现这些漏洞。为了回答这个问题，我们调查了文献，建立了34个边通道检测框架的分类。我们提供的分类比较了多个标准，包括所使用的方法、分析的可扩展性或所考虑的威胁模型。然后，我们选择了5种有前途的检测工具，建立了代表性密码操作的统一通用基准。这个基准测试允许我们更好地比较每个工具的功能，以及它们分析的可伸缩性。此外，我们还提供了最近发布的边通道漏洞分类。然后，我们在基准上测试每个选定的工具，这些基准重现了这些漏洞的一个子集以及它们出现的上下文。我们发现现有的工具由于各种原因很难找到漏洞，主要是缺乏对 SIMD 指令、隐式流和内部秘密生成的支持。基于我们的研究结果，我们为研究团体和密码库开发人员制定了一套建议，目的是提高边通道检测工具的有效性。"
    },
    {
        "title": "ZEST: Attention-based Zero-Shot Learning for Unseen IoT Device\n  Classification",
        "url": "http://arxiv.org/abs/2310.08036v1",
        "pub_date": "2023-10-12",
        "summary": "Recent research works have proposed machine learning models for classifying\nIoT devices connected to a network. However, there is still a practical\nchallenge of not having all devices (and hence their traffic) available during\nthe training of a model. This essentially means, during the operational phase,\nwe need to classify new devices not seen during the training phase. To address\nthis challenge, we propose ZEST -- a ZSL (zero-shot learning) framework based\non self-attention for classifying both seen and unseen devices. ZEST consists\nof i) a self-attention based network feature extractor, termed SANE, for\nextracting latent space representations of IoT traffic, ii) a generative model\nthat trains a decoder using latent features to generate pseudo data, and iii) a\nsupervised model that is trained on the generated pseudo data for classifying\ndevices. We carry out extensive experiments on real IoT traffic data; our\nexperiments demonstrate i) ZEST achieves significant improvement (in terms of\naccuracy) over the baselines; ii) ZEST is able to better extract meaningful\nrepresentations than LSTM which has been commonly used for modeling network\ntraffic.",
        "translated": "最近的研究工作已经提出了机器学习模型，用于对连接到网络的物联网设备进行分类。然而，在模型的训练过程中，仍然存在一个实际的挑战，那就是不能让所有的设备(以及它们的流量)都可用。这基本上意味着，在操作阶段，我们需要分类在培训阶段没有看到的新设备。为了解决这个问题，我们提出了 ZEST ——一个基于自我注意的 ZSL (zero-shot learning)框架，用于对可见和不可见设备进行分类。ZEST 包括 i)一个基于自我注意力的网络特征提取器(称为 SANE) ，用于提取物联网流量的潜在空间表征; ii)一个训练解码器使用潜在特征生成伪数据的生成模型; iii)一个训练生成伪数据用于分类设备的监督模型。我们对真实的物联网流量数据进行了广泛的实验，我们的实验表明: (1) ZEST 在准确性方面比基线数据有显著的提高; (2) ZEST 能够比 LSTM 更好地提取有意义的表示，LSTM 已经被广泛用于网络流量建模。"
    },
    {
        "title": "Why Train More? Effective and Efficient Membership Inference via\n  Memorization",
        "url": "http://arxiv.org/abs/2310.08015v1",
        "pub_date": "2023-10-12",
        "summary": "Membership Inference Attacks (MIAs) aim to identify specific data samples\nwithin the private training dataset of machine learning models, leading to\nserious privacy violations and other sophisticated threats. Many practical\nblack-box MIAs require query access to the data distribution (the same\ndistribution where the private data is drawn) to train shadow models. By doing\nso, the adversary obtains models trained \"with\" or \"without\" samples drawn from\nthe distribution, and analyzes the characteristics of the samples under\nconsideration. The adversary is often required to train more than hundreds of\nshadow models to extract the signals needed for MIAs; this becomes the\ncomputational overhead of MIAs. In this paper, we propose that by strategically\nchoosing the samples, MI adversaries can maximize their attack success while\nminimizing the number of shadow models. First, our motivational experiments\nsuggest memorization as the key property explaining disparate sample\nvulnerability to MIAs. We formalize this through a theoretical bound that\nconnects MI advantage with memorization. Second, we show sample complexity\nbounds that connect the number of shadow models needed for MIAs with\nmemorization. Lastly, we confirm our theoretical arguments with comprehensive\nexperiments; by utilizing samples with high memorization scores, the adversary\ncan (a) significantly improve its efficacy regardless of the MIA used, and (b)\nreduce the number of shadow models by nearly two orders of magnitude compared\nto state-of-the-art approaches.",
        "translated": "成员推理攻击(MIA)旨在识别机器学习模型私有训练数据集中的特定数据样本，从而导致严重的隐私侵犯和其他复杂的威胁。许多实际的黑盒 MIA 需要查询访问数据分布(在绘制私有数据的同一分布)来训练阴影模型。通过这样做，对手得到训练的模型“有”或“没有”样本从分布，并分析了特点的样本考虑。对手经常需要训练数百个阴影模型来提取 MIA 所需的信号，这就成为 MIA 的计算开销。在本文中，我们提出通过战略选择样本，MI 对手可以最大限度地提高他们的攻击成功率，同时最小化阴影模型的数量。首先，我们的动机实验表明，记忆是解释不同样本容易受到失忆影响的关键属性。我们通过一个理论界限将其形式化，该界限将多元智能的优势与记忆联系起来。其次，我们显示了样本复杂性界限，连接数量的阴影模型需要的 MIA 和记忆。最后，我们通过全面的实验来证实我们的理论论据，通过使用高记忆分数的样本，对手可以(a)显著提高其功效，而不管使用的是什么 MIA，(b)与最先进的方法相比，阴影模型的数量减少了近两个数量级。"
    },
    {
        "title": "User Inference Attacks on Large Language Models",
        "url": "http://arxiv.org/abs/2310.09266v1",
        "pub_date": "2023-10-13",
        "summary": "Fine-tuning is a common and effective method for tailoring large language\nmodels (LLMs) to specialized tasks and applications. In this paper, we study\nthe privacy implications of fine-tuning LLMs on user data. To this end, we\ndefine a realistic threat model, called user inference, wherein an attacker\ninfers whether or not a user's data was used for fine-tuning. We implement\nattacks for this threat model that require only a small set of samples from a\nuser (possibly different from the samples used for training) and black-box\naccess to the fine-tuned LLM. We find that LLMs are susceptible to user\ninference attacks across a variety of fine-tuning datasets, at times with near\nperfect attack success rates. Further, we investigate which properties make\nusers vulnerable to user inference, finding that outlier users (i.e. those with\ndata distributions sufficiently different from other users) and users who\ncontribute large quantities of data are most susceptible to attack. Finally, we\nexplore several heuristics for mitigating privacy attacks. We find that\ninterventions in the training algorithm, such as batch or per-example gradient\nclipping and early stopping fail to prevent user inference. However, limiting\nthe number of fine-tuning samples from a single user can reduce attack\neffectiveness, albeit at the cost of reducing the total amount of fine-tuning\ndata.",
        "translated": "微调是根据特定任务和应用程序裁剪大型语言模型(LLM)的一种常见而有效的方法。在本文中，我们研究了微调 LLM 对用户数据的隐私影响。为此，我们定义了一个实际的威胁模型，称为用户推断，其中攻击者推断用户的数据是否被用于微调。我们为这个威胁模型实现攻击，它只需要来自用户的一小组样本(可能与用于训练的样本不同)和对经过微调的 LLM 的黑盒访问。我们发现 LLM 容易受到各种微调数据集的用户推断攻击，有时攻击成功率接近完美。此外，我们还研究了哪些属性使用户容易受到用户推断的影响，发现异常用户(即那些数据分布与其他用户有很大不同的用户)和贡献大量数据的用户最容易受到攻击。最后，我们探讨了几种减轻隐私攻击的启发式方法。我们发现在训练算法中的干预，例如批量或每个例子的梯度剪裁和早期停止不能防止用户推理。然而，限制来自单个用户的微调示例的数量可能会降低攻击效率，尽管代价是减少微调数据的总量。"
    },
    {
        "title": "Tikuna: An Ethereum Blockchain Network Security Monitoring System",
        "url": "http://arxiv.org/abs/2310.09193v1",
        "pub_date": "2023-10-13",
        "summary": "Blockchain security is becoming increasingly relevant in today's cyberspace\nas it extends its influence in many industries. This paper focuses on\nprotecting the lowest level layer in the blockchain, particularly the P2P\nnetwork that allows the nodes to communicate and share information. The P2P\nnetwork layer may be vulnerable to several families of attacks, such as\nDistributed Denial of Service (DDoS), eclipse attacks, or Sybil attacks. This\nlayer is prone to threats inherited from traditional P2P networks, and it must\nbe analyzed and understood by collecting data and extracting insights from the\nnetwork behavior to reduce those risks. We introduce Tikuna, an open-source\ntool for monitoring and detecting potential attacks on the Ethereum blockchain\nP2P network, at an early stage. Tikuna employs an unsupervised Long Short-Term\nMemory (LSTM) method based on Recurrent Neural Network (RNN) to detect attacks\nand alert users. Empirical results indicate that the proposed approach\nsignificantly improves detection performance, with the ability to detect and\nclassify attacks, including eclipse attacks, Covert Flash attacks, and others\nthat target the Ethereum blockchain P2P network layer, with high accuracy. Our\nresearch findings demonstrate that Tikuna is a valuable security tool for\nassisting operators to efficiently monitor and safeguard the status of Ethereum\nvalidators and the wider P2P network",
        "translated": "随着区块链安全在许多行业的影响力不断扩大，它在当今网络空间正变得越来越重要。本文主要研究区块链最底层的保护问题，尤其是对等网络(P2P)中允许节点通信和共享信息的保护问题。P2P 网络层可能容易受到多种攻击，例如分布式分布式拒绝服务攻击(DDoS)、 Eclipse 攻击或 Sybil 攻击。这一层容易受到传统 P2P 网络遗传的威胁，必须通过收集数据和从网络行为中提取洞察力来分析和理解这些威胁，以减少这些风险。我们介绍 Tikuna，一个监视和检测对以太区块链 P2P 网络潜在攻击的开源工具，在早期阶段。Tikuna 使用一种基于递归神经网络(RNN)的无监督长短期记忆(lSTM)方法来检测攻击并提醒用户。实验结果表明，该方法能够高精度地检测和分类针对以太区块链 P2P 网络层的攻击，包括 Eclipse 攻击、隐蔽 Flash 攻击和其他攻击，从而显著提高了检测性能。我们的研究结果显示，Tikuna 是一个宝贵的安全工具，可协助营办商有效地监察和保障以太坊验证机制及更广泛的人对人网络的状况"
    },
    {
        "title": "DocCert: Nostrification, Document Verification and Authenticity\n  Blockchain Solution",
        "url": "http://arxiv.org/abs/2310.09136v1",
        "pub_date": "2023-10-13",
        "summary": "Many institutions and organizations require nostrification and verification\nof qualification as a prerequisite for hiring. The idea is to recognize the\nauthenticity of a copy or digital document issued by an institution in a\nforeign country and detect forgeries. Certificates, financial records, health\nrecords, official papers and others are often required to be attested from\nmultiple entities in distinct locations. However, in this digital era where\nmost applications happen online, and document copies are uploaded, the\ntraditional signature and seal methods are obsolete. In a matter of minutes and\nwith a simple photo editor, a certificate or document copy may be plagiarized\nor forged. Blockchain technology offers a decentralized approach to record and\nverify transactions without the need for huge infrastructure investment. In\nthis paper, we propose a blockchain based nostrification system, where awarding\ninstitutions generate a digital certificate, store in a public but permissioned\nblockchain, where students and other stakeholders may verify. We present a\nthorough discussion and formal evaluation of the proposed system.",
        "translated": "许多机构和组织要求将资格登记和核实作为聘用的先决条件。这个想法是为了确认由外国机构签发的副本或数字文件的真实性，并发现伪造文件。证书，财务记录，健康记录，官方文件和其他往往需要从不同地点的多个实体的证明。然而，在这个大多数应用程序发生在网上，文件副本被上传的数字时代，传统的签名和密封方法已经过时。在几分钟内，一个简单的照片编辑器，证书或文件副本可能被剽窃或伪造。区块链技术提供了一种分散的方法来记录和验证交易，而不需要大量的基础设施投资。在这篇论文中，我们提出了一个基于区块链的公示系统，授予机构生成一个数字证书，存储在一个公开但被许可的区块链中，在那里学生和其他利益相关者可以验证。我们提出了一个彻底的讨论和正式的评估提议的制度。"
    },
    {
        "title": "Split-and-Denoise: Protect large language model inference with local\n  differential privacy",
        "url": "http://arxiv.org/abs/2310.09130v1",
        "pub_date": "2023-10-13",
        "summary": "Large Language Models (LLMs) shows powerful capability in natural language\nunderstanding by capturing hidden semantics in vector space. This process\nenriches the value of the text embeddings for various downstream tasks, thereby\nfostering the Embedding-as-a-Service (EaaS) business model. However, the direct\ntransmission of text to servers poses a largely unaddressed risk of privacy\nleakage. To mitigate this issue, we introduce Split-N-Denoise (SnD), an\ninnovative framework that split the model to execute the token embedding layer\non the client side at minimal computational cost. This allows the client to\nintroduce noise prior to transmitting the embeddings to the server, and\nsubsequently receive and denoise the perturbed output embeddings for downstream\ntasks. Our approach is designed for the inference stage of LLMs and requires no\nmodifications to the model parameters. Extensive experiments demonstrate SnD's\neffectiveness in optimizing the privacy-utility tradeoff across various LLM\narchitectures and diverse downstream tasks. The results reveal a significant\nperformance improvement under the same privacy budget compared to the baseline,\noffering clients a privacy-preserving solution for local privacy protection.",
        "translated": "大语言模型(LLM)通过捕获向量空间中的隐语义，在自然语言理解方面表现出强大的能力。这个过程丰富了各种下游任务的文本嵌入的价值，从而形成了嵌入即服务(Embedding-as-a-Service，EaaS)业务模型。然而，将文本直接传输到服务器会带来隐私泄露的风险，这个问题在很大程度上没有得到解决。为了缓解这个问题，我们引入了 Split-N-Denoise (SnD) ，这是一个创新的框架，它将模型分离，以最小的计算开销在客户端执行令牌嵌入层。这允许客户端在将嵌入传输到服务器之前引入噪声，然后接收并消除下游任务的扰动输出嵌入。我们的方法是为 LLM 的推理阶段而设计的，不需要对模型参数进行修改。大量的实验证明了 SnD 在优化各种 LLM 架构和各种下游任务之间的隐私-效用权衡方面的有效性。结果表明，在相同的隐私预算下，性能显著提高，为客户提供了一个保护本地隐私的解决方案。"
    },
    {
        "title": "Privacy-Preserving Encrypted Low-Dose CT Denoising",
        "url": "http://arxiv.org/abs/2310.09101v1",
        "pub_date": "2023-10-13",
        "summary": "Deep learning (DL) has made significant advancements in tomographic imaging,\nparticularly in low-dose computed tomography (LDCT) denoising. A recent trend\ninvolves servers training powerful models with large amounts of self-collected\nprivate data and providing application programming interfaces (APIs) for users,\nsuch as Chat-GPT. To avoid model leakage, users are required to upload their\ndata to the server model, but this way raises public concerns about the\npotential risk of privacy disclosure, especially for medical data. Hence, to\nalleviate related concerns, in this paper, we propose to directly denoise LDCT\nin the encrypted domain to achieve privacy-preserving cloud services without\nexposing private data to the server. To this end, we employ homomorphic\nencryption to encrypt private LDCT data, which is then transferred to the\nserver model trained with plaintext LDCT for further denoising. However, since\ntraditional operations, such as convolution and linear transformation, in DL\nmethods cannot be directly used in the encrypted domain, we transform the\nfundamental mathematic operations in the plaintext domain into the operations\nin the encrypted domain. In addition, we present two interactive frameworks for\nlinear and nonlinear models in this paper, both of which can achieve lossless\noperating. In this way, the proposed methods can achieve two merits, the data\nprivacy is well protected and the server model is free from the risk of model\nleakage. Moreover, we provide theoretical proof to validate the lossless\nproperty of our framework. Finally, experiments were conducted to demonstrate\nthat the transferred contents are well protected and cannot be reconstructed.\nThe code will be released once the paper is accepted.",
        "translated": "深度学习(DL)在断层扫描成像方面取得了重大进展，特别是在低剂量 X射线计算机断层成像(LDCT)去噪方面。最近的一个趋势是，服务器用大量自收集的私有数据训练强大的模型，并为用户提供应用程序编程接口(API) ，比如 Chat-GPT。为了避免模型泄漏，用户被要求将他们的数据上传到服务器模型，但是这种方式引起了公众对隐私泄露的潜在风险的关注，特别是对于医疗数据。因此，为了减轻相关的担忧，本文提出在加密域中直接去噪 LDCT，以实现保护隐私的云服务，同时不向服务器暴露私有数据。为此，我们使用同态加密加密私有 LDCT 数据，然后将这些数据传输到使用纯文本 LDCT 训练的服务器模型，以便进一步去噪。然而，由于传统的数学运算，例如卷积和线性映射，不能直接在加密域中使用，因此我们将明文域中的基本数学运算转换为加密域中的运算。此外，本文还提出了线性和非线性模型的两个交互式框架，它们都能实现无损操作。通过这种方法，可以实现两个优点: 保护数据隐私和服务器模型不存在模型泄漏的风险。此外，我们还提供了理论证明，以验证我们的框架的无损性质。最后，通过实验证明，转移的内容得到了很好的保护，不能被重构。一旦论文被接受，代码将被公布。"
    },
    {
        "title": "Injective Rank Metric Trapdoor Functions with Homogeneous Errors",
        "url": "http://arxiv.org/abs/2310.08962v1",
        "pub_date": "2023-10-13",
        "summary": "In rank-metric cryptography, a vector from a finite dimensional linear space\nover a finite field is viewed as the linear space spanned by its entries. The\nrank decoding problem which is the analogue of the problem of decoding a random\nlinear code consists in recovering a basis of a random noise vector that was\nused to perturb a set of random linear equations sharing a secret solution.\nAssuming the intractability of this problem, we introduce a new construction of\ninjective one-way trapdoor functions. Our solution departs from the frequent\nway of building public key primitives from error-correcting codes where, to\nestablish the security, ad hoc assumptions about a hidden structure are made.\nOur method produces a hard-to-distinguish linear code together with low weight\nvectors which constitute the secret that helps recover the inputs.The key idea\nis to focus on trapdoor functions that take sufficiently enough input vectors\nsharing the same support. Applying then the error correcting algorithm designed\nfor Low Rank Parity Check (LRPC) codes, we obtain an inverting algorithm that\nrecovers the inputs with overwhelming probability.",
        "translated": "在秩度量密码学中，有限域上有限维线性空间的向量被看作是由其条目所跨越的线性空间。与随机线性码解码问题相似的秩解码问题是恢复一个随机噪声向量的基，该噪声向量用于扰动一组共享秘密解的随机线性方程组。假设这个问题是一个棘手的问题，我们引入了一个新的内射单向陷门函数的构造。我们的解决方案不同于频繁构建公钥原语的方法，而是采用纠错码的方法，在这种方法中，为了建立安全性，需要对隐藏结构进行特别的假设。我们的方法产生一个难以区分的线性代码连同低权重向量构成的秘密，有助于恢复输入。其关键思想是集中在陷门函数，采取足够的输入向量共享相同的支持。然后应用针对低阶奇偶校验(LRPC)码设计的纠错算法，得到了一种以压倒性概率恢复输入的反演算法。"
    },
    {
        "title": "Log Anomaly Detection on EuXFEL Nodes",
        "url": "http://arxiv.org/abs/2310.08951v1",
        "pub_date": "2023-10-13",
        "summary": "This article introduces a method to detect anomalies in the log data\ngenerated by control system nodes at the European XFEL accelerator. The primary\naim of this proposed method is to provide operators a comprehensive\nunderstanding of the availability, status, and problems specific to each node.\nThis information is vital for ensuring the smooth operation. The sequential\nnature of logs and the absence of a rich text corpus that is specific to our\nnodes poses significant limitations for traditional and learning-based\napproaches for anomaly detection. To overcome this limitation, we propose a\nmethod that uses word embedding and models individual nodes as a sequence of\nthese vectors that commonly co-occur, using a Hidden Markov Model (HMM). We\nscore individual log entries by computing a probability ratio between the\nprobability of the full log sequence including the new entry and the\nprobability of just the previous log entries, without the new entry. This ratio\nindicates how probable the sequence becomes when the new entry is added. The\nproposed approach can detect anomalies by scoring and ranking log entries from\nEuXFEL nodes where entries that receive high scores are potential anomalies\nthat do not fit the routine of the node. This method provides a warning system\nto alert operators about these irregular log events that may indicate issues.",
        "translated": "本文介绍了一种检测欧洲 XFEL 加速器控制系统节点产生的测井数据异常的方法。该方法的主要目的是为运营商提供一个全面的可用性，状态和问题的具体每个节点的理解。这些信息对于确保平稳运行至关重要。日志的连续性以及缺乏特定于我们节点的富文本语料库，对传统的基于学习的异常检测方法构成了严重的限制。为了克服这个限制，我们提出了一种使用单词嵌入的方法，并使用隐马尔可夫模型(hMM)将单个节点建模为这些通常共同出现的向量序列。我们通过计算包括新项的完整对数序列的概率与仅仅是以前的对数项的概率之间的概率比来对单个对数项进行评分，而不包括新项。这个比率表示当添加新条目时序列的可能性有多大。该方法通过对 EuXFEL 节点的日志条目进行评分和排序来检测异常，其中获得高分的条目是潜在的异常，不符合节点的常规。此方法提供了一个警告系统，用于向操作员报警这些可能指示问题的不规则日志事件。"
    },
    {
        "title": "Embarrassingly Simple Text Watermarks",
        "url": "http://arxiv.org/abs/2310.08920v1",
        "pub_date": "2023-10-13",
        "summary": "We propose Easymark, a family of embarrassingly simple yet effective\nwatermarks. Text watermarking is becoming increasingly important with the\nadvent of Large Language Models (LLM). LLMs can generate texts that cannot be\ndistinguished from human-written texts. This is a serious problem for the\ncredibility of the text. Easymark is a simple yet effective solution to this\nproblem. Easymark can inject a watermark without changing the meaning of the\ntext at all while a validator can detect if a text was generated from a system\nthat adopted Easymark or not with high credibility. Easymark is extremely easy\nto implement so that it only requires a few lines of code. Easymark does not\nrequire access to LLMs, so it can be implemented on the user-side when the LLM\nproviders do not offer watermarked LLMs. In spite of its simplicity, it\nachieves higher detection accuracy and BLEU scores than the state-of-the-art\ntext watermarking methods. We also prove the impossibility theorem of perfect\nwatermarking, which is valuable in its own right. This theorem shows that no\nmatter how sophisticated a watermark is, a malicious user could remove it from\nthe text, which motivate us to use a simple watermark such as Easymark. We\ncarry out experiments with LLM-generated texts and confirm that Easymark can be\ndetected reliably without any degradation of BLEU and perplexity, and\noutperform state-of-the-art watermarks in terms of both quality and\nreliability.",
        "translated": "我们提出 Easymark，一个令人尴尬的简单而有效的水印家族。随着大语言模型(LLM)的出现，文本水印变得越来越重要。LLM 可以生成与人写文本无法区分的文本。这对文本的可信度是一个严重的问题。Easymark 是这个问题的一个简单而有效的解决方案。Easymark 可以在不改变文本含义的情况下注入水印，而验证器可以检测文本是否来自采用 Easymark 的系统，是否具有高可信度。Easymark 非常容易实现，所以它只需要几行代码。Easymark 不需要访问 LLM，因此当 LLM 提供程序不提供水印 LLM 时，可以在用户端实现它。尽管它简单，但是它比最先进的文本水印方法获得了更高的检测精度和 BLEU 分数。我们还证明了完全水印的不可能性定理，这个定理本身是有价值的。该定理表明，无论水印是多么复杂，恶意用户可以从文本中删除它，这促使我们使用一个简单的水印，如 Easymark。我们使用 LLM 生成的文本进行实验，证实 Easymark 可以在不降低 BLEU 和困惑的情况下可靠地检测到，并且在质量和可靠性方面优于最先进的水印。"
    },
    {
        "title": "How to Rationally Select Your Delegatee in PoS",
        "url": "http://arxiv.org/abs/2310.08895v1",
        "pub_date": "2023-10-13",
        "summary": "This paper centers around a simple yet crucial question for everyday users:\nHow should one choose their delegated validators within proof-of-stake (PoS)\nprotocols, particularly in the context of Ethereum 2.0? This has been a\nlong-overlooked gap, as existing studies have primarily focused on\ninter-committee (validator set) behaviors and activities, while neglecting the\ndynamic formation of committees, especially for individual stakeholders seeking\nreliable validators. Our study bridges this gap by diving into the delegation\nprocess (normal users delegate their small-value tokens to delegatees who later\nact as validators) before entering an actual consensus phase.\n  We propose a Bayesian model to quantify normal users' trust in delegatees,\nwhich we further incorporate into a game-theoretical model to simulate users'\nreactions against a set of critical factors identified through extensive\nresearch (including 10+ staking service provider as well as 30+ PoS\nblockchains). Our results reveal that users tend to choose their delegatees and\nutilize their tokens by carefully weighing the delegation cost, the behaviors\nof other users, and the reputation of delegatees, ultimately reaching a Nash\nequilibrium. Unfortunately, the collective trend significantly increases the\nlikelihood of token concentration on a small number of delegatees.",
        "translated": "本文围绕着一个对于日常用户来说简单而关键的问题展开: 如何在风险证明(ProS)协议中选择他们委托的验证器，特别是在 Etherum 2.0的上下文中？这是一个长期被忽视的差距，因为现有的研究主要集中在委员会间(验证集)的行为和活动，而忽视了委员会的动态形成，特别是对于寻求可靠验证者的个人利益相关者。我们的研究在进入实际的共识阶段之前，通过一头扎进委托过程(普通用户将他们的小值令牌委托给委托人，然后委托人再充当验证者)来弥补这一差距。我们提出了一个贝叶斯模型来量化普通用户对委托人的信任，我们进一步将其纳入博弈理论模型，以模拟用户对通过广泛研究(包括10 + 加注服务提供商以及30 + PoS 区块链)确定的一组关键因素的反应。我们的研究结果显示，用户倾向于通过仔细权衡授权成本、其他用户的行为以及授权者的声誉来选择他们的代理人并使用他们的代号，最终达到一个纳什均衡点。不幸的是，这种集体趋势大大增加了象征性地集中在少数代表身上的可能性。"
    },
    {
        "title": "A one-query lower bound for unitary synthesis and breaking quantum\n  cryptography",
        "url": "http://arxiv.org/abs/2310.08870v1",
        "pub_date": "2023-10-13",
        "summary": "The Unitary Synthesis Problem (Aaronson-Kuperberg 2007) asks whether any\n$n$-qubit unitary $U$ can be implemented by an efficient quantum algorithm $A$\naugmented with an oracle that computes an arbitrary Boolean function $f$. In\nother words, can the task of implementing any unitary be efficiently reduced to\nthe task of implementing any Boolean function?\n  In this work, we prove a one-query lower bound for unitary synthesis. We show\nthat there exist unitaries $U$ such that no quantum polynomial-time oracle\nalgorithm $A^f$ can implement $U$, even approximately, if it only makes one\n(quantum) query to $f$. Our approach also has implications for quantum\ncryptography: we prove (relative to a random oracle) the existence of quantum\ncryptographic primitives that remain secure against all one-query adversaries\n$A^{f}$. Since such one-query algorithms can decide any language, solve any\nclassical search problem, and even prepare any quantum state, our result\nsuggests that implementing random unitaries and breaking quantum cryptography\nmay be harder than all of these tasks.\n  To prove this result, we formulate unitary synthesis as an efficient\nchallenger-adversary game, which enables proving lower bounds by analyzing the\nmaximum success probability of an adversary $A^f$. Our main technical insight\nis to identify a natural spectral relaxation of the one-query optimization\nproblem, which we bound using tools from random matrix theory.\n  We view our framework as a potential avenue to rule out polynomial-query\nunitary synthesis, and we state conjectures in this direction.",
        "translated": "酉合成问题(Aaronson-Kuperberg，2007)提出了一个问题: 是否可以通过一个高效的量子算法来实现任意 $n $- 量子比特的酉布尔函数。换言之，是否可以有效地将实施任何统一制度的任务减少到实施任何布尔函数的任务？本文证明了酉合成的一个查询下界。我们证明了存在单位元 $U $这样没有量子多项式时间预言算法 $A ^ f $可以实现 $U $，即使是近似的，如果它只对 $f $进行一个(量子)查询。我们的方法也对量子密码学有影响: 我们证明(相对于随机预言)量子密码原语的存在，这些原语对于所有一次查询的对手都是安全的。由于这种一次查询算法可以决定任何语言，解决任何经典的搜索问题，甚至准备任何量子态，我们的结果表明，实现随机酉和打破量子密码学可能比所有这些任务更难。为了证明这个结果，我们将酉合成描述为一个有效的挑战者-对手博弈，通过分析对手 $A ^ f $的最大成功概率来证明下界。我们的主要技术见解是识别一个查询最佳化问题的自然光谱松弛，我们使用随机矩阵理论的工具将其绑定。我们认为我们的框架是排除多项式查询酉合成的一个潜在途径，并且我们向这个方向陈述猜想。"
    },
    {
        "title": "Functional Invariants to Watermark Large Transformers",
        "url": "http://arxiv.org/abs/2310.11446v1",
        "pub_date": "2023-10-17",
        "summary": "The rapid growth of transformer-based models increases the concerns about\ntheir integrity and ownership insurance. Watermarking addresses this issue by\nembedding a unique identifier into the model, while preserving its performance.\nHowever, most existing approaches require to optimize the weights to imprint\nthe watermark signal, which is not suitable at scale due to the computational\ncost. This paper explores watermarks with virtually no computational cost,\napplicable to a non-blind white-box setting (assuming access to both the\noriginal and watermarked networks). They generate functionally equivalent\ncopies by leveraging the models' invariance, via operations like dimension\npermutations or scaling/unscaling. This enables to watermark models without any\nchange in their outputs and remains stealthy. Experiments demonstrate the\neffectiveness of the approach and its robustness against various model\ntransformations (fine-tuning, quantization, pruning), making it a practical\nsolution to protect the integrity of large models.",
        "translated": "基于变压器的模型的快速发展增加了对其完整性和所有权保险的关注。水印通过在模型中嵌入一个唯一标识符来解决这个问题，同时保持其性能。然而，现有的方法大多需要对水印信号的权值进行优化，由于计算量大，不适合在尺度上对水印信号进行压印。本文研究了一种适用于非盲白盒设置(假设同时访问原始网络和水印网络)的几乎不需要计算代价的水印算法。它们利用模型的不变性，通过维排列或缩放/取消缩放等操作，生成功能等同的副本。这使得水印模型没有任何改变其输出和保持隐形。实验证明了该方法的有效性及其对各种模型变换(微调、量化、剪枝)的鲁棒性，使其成为保护大型模型完整性的实用解决方案。"
    },
    {
        "title": "Trusted Provenance of Automated, Collaborative and Adaptive Data\n  Processing Pipelines",
        "url": "http://arxiv.org/abs/2310.11442v1",
        "pub_date": "2023-10-17",
        "summary": "To benefit from the abundance of data and the insights it brings data\nprocessing pipelines are being used in many areas of research and development\nin both industry and academia. One approach to automating data processing\npipelines is the workflow technology, as it also supports collaborative,\ntrial-and-error experimentation with the pipeline architecture in different\napplication domains. In addition to the necessary flexibility that such\npipelines need to possess, in collaborative settings cross-organisational\ninteractions are plagued by lack of trust. While capturing provenance\ninformation related to the pipeline execution and the processed data is a first\nstep towards enabling trusted collaborations, the current solutions do not\nallow for provenance of the change in the processing pipelines, where the\nsubject of change can be made on any aspect of the workflow implementing the\npipeline and on the data used while the pipeline is being executed. Therefore\nin this work we provide a solution architecture and a proof of concept\nimplementation of a service, called Provenance Holder, which enable provenance\nof collaborative, adaptive data processing pipelines in a trusted manner. We\nalso contribute a definition of a set of properties of such a service and\nidentify future research directions.",
        "translated": "为了从丰富的数据及其带来的见解中获益，数据处理管道正被用于工业界和学术界的许多研究和开发领域。自动化数据处理流水线的一种方法是工作流技术，因为它还支持在不同应用程序域中使用流水线体系结构进行协作、试错试验。除了这些管道需要具备的必要灵活性之外，在协作环境中，跨组织的交互还受到缺乏信任的困扰。虽然获取与管道执行和处理过的数据有关的来源信息是实现可信协作的第一步，但目前的解决方案不允许处理管道中的变更来源，在这种情况下，可以对执行管道的工作流程的任何方面以及在管道执行过程中使用的数据进行变更。因此，在这项工作中，我们提供了一个解决方案体系结构和一个名为 Provenance Holder 的服务的概念实现的证明，该服务以可信的方式支持协作的、自适应的数据处理管道的起源。我们还提供了此类服务的一组属性的定义，并确定了未来的研究方向。"
    },
    {
        "title": "Evaluating LLMs for Privilege-Escalation Scenarios",
        "url": "http://arxiv.org/abs/2310.11409v1",
        "pub_date": "2023-10-17",
        "summary": "Penetration testing, an essential component of cybersecurity, allows\norganizations to proactively identify and remediate vulnerabilities in their\nsystems, thus bolstering their defense mechanisms against potential\ncyberattacks. One recent advancement in the realm of penetration testing is the\nutilization of Language Models (LLMs). We explore the intersection of LLMs and\npenetration testing to gain insight into their capabilities and challenges in\nthe context of privilige escalation. We create an automated Linux\nprivilege-escalation benchmark utilizing local virtual machines. We introduce\nan LLM-guided privilege-escalation tool designed for evaluating different LLMs\nand prompt strategies against our benchmark. We analyze the impact of different\nprompt designs, the benefits of in-context learning, and the advantages of\noffering high-level guidance to LLMs. We discuss challenging areas for LLMs,\nincluding maintaining focus during testing, coping with errors, and finally\ncomparing them with both stochastic parrots as well as with human hackers.",
        "translated": "渗透测试是网络安全的一个重要组成部分，它使组织能够积极主动地识别和补救其系统中的漏洞，从而加强其防御机制，抵御潜在的网络攻击。渗透测试领域的一个最新进展是语言模型(LLM)的使用。我们探索 LLM 和渗透测试的交集，以深入了解它们在特权升级上下文中的能力和挑战。我们利用本地虚拟机创建了一个自动化的 Linux 特权升级基准测试。我们引入了一个 LLM 引导的权限升级工具，用于评估不同的 LLM 并根据我们的基准提示策略。我们分析了不同快速设计的影响，上下文学习的好处，以及为 LLM 提供高层次指导的优势。我们讨论 LLM 的挑战性领域，包括在测试期间保持焦点，处理错误，最后将它们与随机鹦鹉和人类黑客进行比较。"
    },
    {
        "title": "Last One Standing: A Comparative Analysis of Security and Privacy of\n  Soft Prompt Tuning, LoRA, and In-Context Learning",
        "url": "http://arxiv.org/abs/2310.11397v1",
        "pub_date": "2023-10-17",
        "summary": "Large Language Models (LLMs) are powerful tools for natural language\nprocessing, enabling novel applications and user experiences. However, to\nachieve optimal performance, LLMs often require adaptation with private data,\nwhich poses privacy and security challenges. Several techniques have been\nproposed to adapt LLMs with private data, such as Low-Rank Adaptation (LoRA),\nSoft Prompt Tuning (SPT), and In-Context Learning (ICL), but their comparative\nprivacy and security properties have not been systematically investigated. In\nthis work, we fill this gap by evaluating the robustness of LoRA, SPT, and ICL\nagainst three types of well-established attacks: membership inference, which\nexposes data leakage (privacy); backdoor, which injects malicious behavior\n(security); and model stealing, which can violate intellectual property\n(privacy and security). Our results show that there is no silver bullet for\nprivacy and security in LLM adaptation and each technique has different\nstrengths and weaknesses.",
        "translated": "大型语言模型(LLM)是自然语言处理的强大工具，可以实现新的应用程序和用户体验。然而，为了获得最佳性能，LLM 通常需要对私有数据进行适应，这带来了隐私和安全方面的挑战。已经提出了几种适应 LLM 与私有数据的技术，如低级适应(LoRA) ，软提示调整(SPT)和在上下文学习(ICL) ，但是它们的相对隐私性和安全性还没有得到系统的研究。在这项工作中，我们通过评估 LoRA、 SPT 和 ICL 对三种常见攻击的稳健性来填补这个空白: 会员推断，暴露数据泄露(隐私) ; 后门，注入恶意行为(安全) ; 模型窃取，可以侵犯知识产权(隐私和安全)。我们的研究结果表明，在 LLM 自适应中没有保护隐私和安全的灵丹妙药，每种技术都有不同的优缺点。"
    },
    {
        "title": "A Two-Layer Blockchain Sharding Protocol Leveraging Safety and Liveness\n  for Enhanced Performance",
        "url": "http://arxiv.org/abs/2310.11373v1",
        "pub_date": "2023-10-17",
        "summary": "Sharding is essential for improving blockchain scalability. Existing\nprotocols overlook diverse adversarial attacks, limiting transaction\nthroughput. This paper presents Reticulum, a groundbreaking sharding protocol\naddressing this issue, boosting blockchain scalability.\n  Reticulum employs a two-phase approach, adapting transaction throughput based\non runtime adversarial attacks. It comprises \"control\" and \"process\" shards in\ntwo layers. Process shards contain at least one trustworthy node, while control\nshards have a majority of trusted nodes. In the first phase, transactions are\nwritten to blocks and voted on by nodes in process shards. Unanimously accepted\nblocks are confirmed. In the second phase, blocks without unanimous acceptance\nare voted on by control shards. Blocks are accepted if the majority votes in\nfavor, eliminating first-phase opponents and silent voters. Reticulum uses\nunanimous voting in the first phase, involving fewer nodes, enabling more\nparallel process shards. Control shards finalize decisions and resolve\ndisputes.\n  Experiments confirm Reticulum's innovative design, providing high transaction\nthroughput and robustness against various network attacks, outperforming\nexisting sharding protocols for blockchain networks.",
        "translated": "分片对于提高区块链的可伸缩性至关重要。现有协议忽略了多种对手攻击，限制了事务吞吐量。本文提出了一种具有开创性的分片协议，可以提高区块链的可扩展性。网状结构采用两阶段方法，根据运行时对抗性攻击调整事务吞吐量。它包含两层的“控制”和“处理”碎片。进程切分至少包含一个可信节点，而控制切分则包含大多数可信节点。在第一阶段，事务被写入到块中，并由进程碎片中的节点进行投票。一致同意的区块被确认。在第二阶段，没有获得一致接受的块由控件碎片进行表决。如果大多数人投赞成票，阻挠就会被接受，从而排除了第一阶段的反对者和沉默的选民。网状结构在第一阶段使用一致投票，涉及更少的节点，从而实现更多的并行流程碎片。控制碎片最终确定决策并解决争议。实验证实了网状网的创新设计，提供高事务吞吐量和对各种网络攻击的健壮性，超过现有的区块链网络分片协议。"
    },
    {
        "title": "Detection of Malicious DNS-over-HTTPS Traffic: An Anomaly Detection\n  Approach using Autoencoders",
        "url": "http://arxiv.org/abs/2310.11325v1",
        "pub_date": "2023-10-17",
        "summary": "To maintain the privacy of users' web browsing history, popular browsers\nencrypt their DNS traffic using the DNS-over-HTTPS (DoH) protocol.\nUnfortunately, encrypting DNS packets prevents many existing intrusion\ndetection systems from using plaintext domain names to detect malicious\ntraffic. In this paper, we design an autoencoder that is capable of detecting\nmalicious DNS traffic by only observing the encrypted DoH traffic. Compared to\nprevious works, the proposed autoencoder looks for anomalies in DoH traffic,\nand thus can detect malicious traffic that has not been previously observed,\ni.e., zero-day attacks. We run extensive experiments to evaluate the\nperformance of our proposed autoencoder and compare it to that of other anomaly\ndetection algorithms, namely, local outlier factor, one-class support vector\nmachine, isolation forest, and variational autoencoders. We find that our\nproposed autoencoder achieves the highest detection performance, with a median\nF-1 score of 99\\% over several types of malicious traffic.",
        "translated": "为了保护用户网页浏览历史的隐私，流行的浏览器使用 DNS-over-HTTPS (DoH)协议加密他们的 DNS 流量。不幸的是，加密 DNS 数据包阻止了许多现有的入侵检测系统使用明文域名来检测恶意流量。在本文中，我们设计了一个自动编码器，能够检测恶意的 DNS 流量，只需要观察加密的 DoH 流量。与以往的工作相比，提出的自动编码器寻找异常的 DoH 流量，从而可以检测到以前没有观察到的恶意流量，即零日攻击。我们进行了大量的实验来评估我们提出的自动编码器的性能，并将其与其他异常检测算法进行比较，这些算法包括局部异常因子、单类支持向量机、隔离森林和变异自动编码器。我们发现，我们提出的自动编码器实现了最高的检测性能，中位数的 F-1评分为99% 的几种类型的恶意流量。"
    },
    {
        "title": "Locally Differentially Private Graph Embedding",
        "url": "http://arxiv.org/abs/2310.11060v1",
        "pub_date": "2023-10-17",
        "summary": "Graph embedding has been demonstrated to be a powerful tool for learning\nlatent representations for nodes in a graph. However, despite its superior\nperformance in various graph-based machine learning tasks, learning over graphs\ncan raise significant privacy concerns when graph data involves sensitive\ninformation. To address this, in this paper, we investigate the problem of\ndeveloping graph embedding algorithms that satisfy local differential privacy\n(LDP). We propose LDP-GE, a novel privacy-preserving graph embedding framework,\nto protect the privacy of node data. Specifically, we propose an LDP mechanism\nto obfuscate node data and adopt personalized PageRank as the proximity measure\nto learn node representations. Then, we theoretically analyze the privacy\nguarantees and utility of the LDP-GE framework. Extensive experiments conducted\nover several real-world graph datasets demonstrate that LDP-GE achieves\nfavorable privacy-utility trade-offs and significantly outperforms existing\napproaches in both node classification and link prediction tasks.",
        "translated": "图嵌入已被证明是学习图中节点潜在表示的有力工具。然而，尽管它在各种基于图形的机器学习任务中表现出色，但是当图形数据涉及敏感信息时，图形学习会引起重大的隐私问题。为了解决这个问题，在本文中，我们研究了开发满足局部差分隐私(lDP)的图嵌入算法的问题。为了保护节点数据的隐私，提出了一种新的隐私保护图嵌入框架 LDP-GE。具体来说，我们提出了一种 LDP 机制来混淆节点数据，并采用个性化的 PageRank 作为邻近度量来学习节点表示。然后，从理论上分析了 LDP-GE 框架的隐私保护和实用性。通过对几个真实世界图形数据集的大量实验表明，LDP-GE 在节点分类和链路预测任务方面取得了良好的隐私-效用权衡，并且明显优于现有的方法。"
    },
    {
        "title": "Investigating Threats Posed by SMS Origin Spoofing to IoT Devices",
        "url": "http://arxiv.org/abs/2310.11052v1",
        "pub_date": "2023-10-17",
        "summary": "The short message service (SMS) is a service for exchanging texts via mobile\nnetworks that has been developed not only as a means of text communication\nbetween subscribers but also as a means to remotely manage Internet of Things\n(IoT) devices. However, the originating number of an SMS can be spoofed. If IoT\ndevices authenticate administrators based on the originating number of an SMS,\nthe authentication is bypassed via SMS origin spoofing. Consequently, IoT\ndevices are at risk of accepting commands from attackers and performing\nunauthorized actions. Accordingly, in this study, the specifications of major\ncellular IoT gateways were evaluated by focusing on remote management via SMS,\nand the authentication bypass hypothesis was verified. The results showed that\n25 of the 32 targeted products supported SMS-based remote management, and 20\nimplemented authentication based on the originating number of the SMS.\nFurthermore, by spoofing the originating number of the SMS, one product was\ndemonstrated to be remotely exploitable through authentication bypassing. Thus,\nthis study revealed the threats posed by SMS origin spoofing to IoT devices and\nproved that SMS origin spoofing not only threatens text communication between\npeople but also puts machine communication at risk.",
        "translated": "短消息服务(SMS)是一种通过移动网络进行文本交换的服务，它不仅作为用户之间文本通信的一种手段，而且作为远程管理物联网(IoT)设备的一种手段。但是，可以欺骗 SMS 的原始号码。如果物联网设备根据 SMS 的原始号码对管理员进行身份验证，则会通过 SMS 原始欺骗绕过身份验证。因此，物联网设备有可能接受攻击者的命令并执行未经授权的操作。因此，在本研究中，主要的蜂窝物联网网关的规格评估的重点是远程管理通过短信，并验证了认证旁路假设。结果表明，32个目标产品中有25个支持基于 SMS 的远程管理，20个实现了基于 SMS 原始号的认证。此外，通过欺骗 SMS 的原始号码，证明了一种产品可以通过绕过身份验证进行远程利用。因此，本研究揭示了短信源欺骗对物联网设备造成的威胁，并证明了短信源欺骗不仅威胁到人与人之间的文本通信，而且使机器通信处于危险之中。"
    },
    {
        "title": "Optimal Private Discrete Distribution Estimation with One-bit\n  Communication",
        "url": "http://arxiv.org/abs/2310.11005v1",
        "pub_date": "2023-10-17",
        "summary": "We consider a private discrete distribution estimation problem with one-bit\ncommunication constraint. The privacy constraints are imposed with respect to\nthe local differential privacy and the maximal leakage. The estimation error is\nquantified by the worst-case mean squared error. We completely characterize the\nfirst-order asymptotics of this privacy-utility trade-off under the one-bit\ncommunication constraint for both types of privacy constraints by using ideas\nfrom local asymptotic normality and the resolution of a block design mechanism.\nThese results demonstrate the optimal dependence of the privacy-utility\ntrade-off under the one-bit communication constraint in terms of the parameters\nof the privacy constraint and the size of the alphabet of the discrete\ndistribution.",
        "translated": "我们考虑了一个带有单位通信约束的私有离散分布估计问题。私隐方面的限制包括局部差分隐私和最大泄漏量。估计误差由最坏情况下的均方差来量化。利用局部渐近正态性和块设计机制的分解思想，完全刻画了两类隐私约束在一位通信约束下的隐私-效用权衡的一阶渐近性。研究结果表明，在单位通信约束下，隐私约束参数和离散分布字母表的大小对隐私效用权衡的最优依赖性有显著影响。"
    },
    {
        "title": "Survey of Vulnerabilities in Large Language Models Revealed by\n  Adversarial Attacks",
        "url": "http://arxiv.org/abs/2310.10844v1",
        "pub_date": "2023-10-16",
        "summary": "Large Language Models (LLMs) are swiftly advancing in architecture and\ncapability, and as they integrate more deeply into complex systems, the urgency\nto scrutinize their security properties grows. This paper surveys research in\nthe emerging interdisciplinary field of adversarial attacks on LLMs, a subfield\nof trustworthy ML, combining the perspectives of Natural Language Processing\nand Security. Prior work has shown that even safety-aligned LLMs (via\ninstruction tuning and reinforcement learning through human feedback) can be\nsusceptible to adversarial attacks, which exploit weaknesses and mislead AI\nsystems, as evidenced by the prevalence of `jailbreak' attacks on models like\nChatGPT and Bard. In this survey, we first provide an overview of large\nlanguage models, describe their safety alignment, and categorize existing\nresearch based on various learning structures: textual-only attacks,\nmulti-modal attacks, and additional attack methods specifically targeting\ncomplex systems, such as federated learning or multi-agent systems. We also\noffer comprehensive remarks on works that focus on the fundamental sources of\nvulnerabilities and potential defenses. To make this field more accessible to\nnewcomers, we present a systematic review of existing works, a structured\ntypology of adversarial attack concepts, and additional resources, including\nslides for presentations on related topics at the 62nd Annual Meeting of the\nAssociation for Computational Linguistics (ACL'24).",
        "translated": "大型语言模型(LLM)在体系结构和功能方面正在迅速发展，随着它们更深入地集成到复杂系统中，审查其安全属性的紧迫性也随之增加。本文结合自然语言处理和安全的视角，对可信机器学习的一个子领域—— LLM 的对抗性攻击这一新兴的跨学科研究进行了综述。先前的研究表明，即使是安全性一致的 LLM (通过指令调整和人工反馈的强化学习)也可能受到对抗性攻击的影响，这些攻击利用了人工智能系统的弱点，误导了人工智能系统，例如对像 ChatGPT 和巴德这样的模型的“越狱”攻击的盛行就证明了这一点。在这个调查中，我们首先提供了大型语言模型的概述，描述了它们的安全校准，并将现有的基于各种学习结构的研究分类: 仅文本攻击，多模式攻击，以及专门针对复杂系统的其他攻击方法，如联邦学习或多智能体系统。我们还提供了全面的工作，侧重于漏洞的基本来源和潜在的防御系统的评论。为了让这个领域更容易为新手所接触，我们提供了一个现有工作的系统综述，一个对抗性攻击概念的结构化类型，以及额外的资源，包括在第62届计算机语言学协会年会(ACL’24)上相关主题的演示幻灯片。"
    },
    {
        "title": "A Cautionary Tale: On the Role of Reference Data in Empirical Privacy\n  Defenses",
        "url": "http://arxiv.org/abs/2310.12112v1",
        "pub_date": "2023-10-18",
        "summary": "Within the realm of privacy-preserving machine learning, empirical privacy\ndefenses have been proposed as a solution to achieve satisfactory levels of\ntraining data privacy without a significant drop in model utility. Most\nexisting defenses against membership inference attacks assume access to\nreference data, defined as an additional dataset coming from the same (or a\nsimilar) underlying distribution as training data. Despite the common use of\nreference data, previous works are notably reticent about defining and\nevaluating reference data privacy. As gains in model utility and/or training\ndata privacy may come at the expense of reference data privacy, it is essential\nthat all three aspects are duly considered. In this paper, we first examine the\navailability of reference data and its privacy treatment in previous works and\ndemonstrate its necessity for fairly comparing defenses. Second, we propose a\nbaseline defense that enables the utility-privacy tradeoff with respect to both\ntraining and reference data to be easily understood. Our method is formulated\nas an empirical risk minimization with a constraint on the generalization\nerror, which, in practice, can be evaluated as a weighted empirical risk\nminimization (WERM) over the training and reference datasets. Although we\nconceived of WERM as a simple baseline, our experiments show that,\nsurprisingly, it outperforms the most well-studied and current state-of-the-art\nempirical privacy defenses using reference data for nearly all relative privacy\nlevels of reference and training data. Our investigation also reveals that\nthese existing methods are unable to effectively trade off reference data\nprivacy for model utility and/or training data privacy. Overall, our work\nhighlights the need for a proper evaluation of the triad model utility /\ntraining data privacy / reference data privacy when comparing privacy defenses.",
        "translated": "在保护隐私的机器学习领域，经验隐私防御已被提出作为一种解决方案，以达到令人满意的训练数据隐私水平，而不会显著降低模型的实用性。大多数针对成员推断攻击的现有防御措施都假定访问参考数据，这些数据定义为来自与训练数据相同(或类似)底层分布的附加数据集。尽管参考数据的使用非常普遍，但是以前的研究对于定义和评估参考数据的隐私性都显得非常谨慎。由于在模型实用程序和/或培训数据保密性方面的收益可能以牺牲参考数据保密性为代价，因此必须适当考虑所有这三个方面。在本文中，我们首先考察了参考数据的可用性及其在以往工作中的隐私处理，并论证了公平比较辩护的必要性。其次，我们提出了一个基线防御，使得训练和参考数据方面的效用-隐私权衡更容易理解。我们的方法是一个经验风险最小化与约束的泛化误差，在实践中，可以评估为加权经验风险最小化(WERM)的培训和参考数据集。尽管我们认为 WERM 是一个简单的基线，但是我们的实验表明，令人惊讶的是，它比最先进的、研究最充分的、使用参考数据的、几乎所有相对隐私级别的参考数据和训练数据的隐私防御都要好。我们的调查还显示，这些现有的方法不能有效地交换参考数据隐私模型实用程序和/或培训数据隐私。总体而言，我们的工作强调了在比较隐私防御时，对三合会模式实用程序/培训数据隐私/参考数据隐私进行适当评估的必要性。"
    },
    {
        "title": "Envisioning the Future of Cyber Security in Post-Quantum Era: A Survey\n  on PQ Standardization, Applications, Challenges and Opportunities",
        "url": "http://arxiv.org/abs/2310.12037v1",
        "pub_date": "2023-10-18",
        "summary": "The rise of quantum computers exposes vulnerabilities in current public key\ncryptographic protocols, necessitating the development of secure post-quantum\n(PQ) schemes. Hence, we conduct a comprehensive study on various PQ approaches,\ncovering the constructional design, structural vulnerabilities, and offer\nsecurity assessments, implementation evaluations, and a particular focus on\nside-channel attacks. We analyze global standardization processes, evaluate\ntheir metrics in relation to real-world applications, and primarily focus on\nstandardized PQ schemes, selected additional signature competition candidates,\nand PQ-secure cutting-edge schemes beyond standardization. Finally, we present\nvisions and potential future directions for a seamless transition to the PQ\nera.",
        "translated": "量子计算机的兴起暴露了当前公钥密码协议的漏洞，需要发展安全的后量子(PQ)方案。因此，我们对各种 PQ 方法进行了全面的研究，包括结构设计、结构脆弱性，并提供安全评估、实施评估，以及侧向通道攻击。我们分析全球标准化过程，评估它们与现实世界应用相关的指标，并主要关注标准化 PQ 方案，选择额外的签名竞争候选方案，以及超越标准化的 PQ 安全尖端方案。最后，我们展示了无缝过渡到 PQ 时代的愿景和潜在的未来方向。"
    },
    {
        "title": "Quantifying Privacy Risks of Prompts in Visual Prompt Learning",
        "url": "http://arxiv.org/abs/2310.11970v1",
        "pub_date": "2023-10-18",
        "summary": "Large-scale pre-trained models are increasingly adapted to downstream tasks\nthrough a new paradigm called prompt learning. In contrast to fine-tuning,\nprompt learning does not update the pre-trained model's parameters. Instead, it\nonly learns an input perturbation, namely prompt, to be added to the downstream\ntask data for predictions. Given the fast development of prompt learning, a\nwell-generalized prompt inevitably becomes a valuable asset as significant\neffort and proprietary data are used to create it. This naturally raises the\nquestion of whether a prompt may leak the proprietary information of its\ntraining data. In this paper, we perform the first comprehensive privacy\nassessment of prompts learned by visual prompt learning through the lens of\nproperty inference and membership inference attacks. Our empirical evaluation\nshows that the prompts are vulnerable to both attacks. We also demonstrate that\nthe adversary can mount a successful property inference attack with limited\ncost. Moreover, we show that membership inference attacks against prompts can\nbe successful with relaxed adversarial assumptions. We further make some\ninitial investigations on the defenses and observe that our method can mitigate\nthe membership inference attacks with a decent utility-defense trade-off but\nfails to defend against property inference attacks. We hope our results can\nshed light on the privacy risks of the popular prompt learning paradigm. To\nfacilitate the research in this direction, we will share our code and models\nwith the community.",
        "translated": "大规模预先训练的模型通过一种称为迅速学习的新范式越来越适应下游任务。与微调相反，快速学习不更新预先训练的模型的参数。相反，它只学习一个输入扰动，即提示，以添加到下游任务数据进行预测。鉴于快速学习的快速发展，一个良好的广义提示不可避免地成为一个有价值的资产，因为重要的努力和专有数据被用来创建它。这自然会引起提示是否会泄露其培训数据的专有信息的问题。本文首次从属性推理和成员推理攻击的角度对视觉提示学习过程中学到的提示进行了全面的隐私评估。我们的经验评估表明，这些提示对于这两种攻击都是脆弱的。我们还演示了对手可以以有限的成本成功地进行属性推理攻击。此外，我们还展示了对提示的成员推理攻击可以通过放松的对抗性假设获得成功。我们进一步对防御进行了一些初步的调查，发现我们的方法可以减轻成员推理攻击与体面的效用-防御权衡，但未能抵御属性推理攻击。我们希望我们的研究结果可以揭示流行的及时学习范式的隐私风险。为了促进这方面的研究，我们将与社区分享我们的代码和模型。"
    },
    {
        "title": "Evolving Bitcoin Custody",
        "url": "http://arxiv.org/abs/2310.11911v1",
        "pub_date": "2023-10-18",
        "summary": "The broad topic of this thesis is the design and analysis of Bitcoin custody\nsystems. Both the technology and threat landscape are evolving constantly.\nTherefore, custody systems, defence strategies, and risk models should be\nadaptive too.\n  We introduce Bitcoin custody by describing the different types, design\nprinciples, phases and functions of custody systems. We review the technology\nstack of these systems and focus on the fundamentals; key-management and\nprivacy. We present a perspective we call the systems view. It is an attempt to\ncapture the full complexity of a custody system, including technology, people,\nand processes. We review existing custody systems and standards.\n  We explore Bitcoin covenants. This is a mechanism to enforce constraints on\ntransaction sequences. Although previous work has proposed how to construct and\napply Bitcoin covenants, these require modifying the consensus rules of\nBitcoin, a notoriously difficult task. We introduce the first detailed\nexposition and security analysis of a deleted-key covenant protocol, which is\ncompatible with current consensus rules. We demonstrate a range of security\nmodels for deleted-key covenants which seem practical, in particular, when\napplied in autonomous (user-controlled) custody systems. We conclude with a\ncomparative analysis with previous proposals.\n  Covenants are often proclaimed to be an important primitive for custody\nsystems, but no complete design has been proposed to validate that claim. To\naddress this, we propose an autonomous custody system called Ajolote which uses\ndeleted-key covenants to enforce a vault sequence. We evaluate Ajolote with; a\nmodel of its state dynamics, a privacy analysis, and a risk model. We propose a\nthreat model for custody systems which captures a realistic attacker for a\nsystem with offline devices and user-verification. We perform ceremony analysis\nto construct the risk model.",
        "translated": "本文的主题是比特币托管系统的设计与分析。技术和威胁都在不断演变。因此，监护制度、辩护策略和风险模型也应具有适应性。通过对比特币托管系统的不同类型、设计原则、阶段和功能的描述，对比特币托管进行了介绍。我们回顾了这些系统的技术堆栈，重点讨论了基本原理: 密钥管理和隐私。我们提出了一个视角，我们称之为系统视图。它试图捕捉监护系统的全部复杂性，包括技术、人员和过程。我们审查现有的监护制度和标准。我们研究比特币契约。这是一种对事务序列强制约束的机制。尽管以前的工作已经提出了如何构建和应用比特币协议，但这些协议需要修改比特币的共识规则，这是一项出了名的困难任务。我们首先详细介绍了一个删除密钥的契约协议，并对其进行了安全性分析，该协议符合当前的协商一致规则。我们展示了一系列的安全模型删除密钥契约似乎是实用的，特别是，当应用在自治(用户控制)保管系统。最后，我们对以前的建议进行了比较分析。人们常常宣称，契约是监护制度的一个重要基本要素，但没有提出任何完整的设计来验证这一主张。为了解决这个问题，我们提出了一个名为 Ajolote 的自治保管系统，它使用删除密钥契约来强制保险库序列。我们对 Ajolote 进行评估，包括状态动态模型、隐私分析和风险模型。我们提出了一个监管系统的威胁模型，它捕获了一个具有离线设备和用户验证的系统的现实攻击者。我们通过仪式分析来构建风险模型。"
    },
    {
        "title": "Malicious Agent Detection for Robust Multi-Agent Collaborative\n  Perception",
        "url": "http://arxiv.org/abs/2310.11901v1",
        "pub_date": "2023-10-18",
        "summary": "Recently, multi-agent collaborative (MAC) perception has been proposed and\noutperformed the traditional single-agent perception in many applications, such\nas autonomous driving. However, MAC perception is more vulnerable to\nadversarial attacks than single-agent perception due to the information\nexchange. The attacker can easily degrade the performance of a victim agent by\nsending harmful information from a malicious agent nearby. In this paper, we\nextend adversarial attacks to an important perception task -- MAC object\ndetection, where generic defenses such as adversarial training are no longer\neffective against these attacks. More importantly, we propose Malicious Agent\nDetection (MADE), a reactive defense specific to MAC perception that can be\ndeployed by each agent to accurately detect and then remove any potential\nmalicious agent in its local collaboration network. In particular, MADE\ninspects each agent in the network independently using a semi-supervised\nanomaly detector based on a double-hypothesis test with the Benjamini-Hochberg\nprocedure to control the false positive rate of the inference. For the two\nhypothesis tests, we propose a match loss statistic and a collaborative\nreconstruction loss statistic, respectively, both based on the consistency\nbetween the agent to be inspected and the ego agent where our detector is\ndeployed. We conduct comprehensive evaluations on a benchmark 3D dataset\nV2X-sim and a real-road dataset DAIR-V2X and show that with the protection of\nMADE, the drops in the average precision compared with the best-case \"oracle\"\ndefender against our attack are merely 1.28% and 0.34%, respectively, much\nlower than 8.92% and 10.00% for adversarial training, respectively.",
        "translated": "近年来，多智能体协同感知技术在自主驾驶等许多应用领域得到了广泛的应用，其性能优于传统的单智能体感知技术。然而，由于信息交换的原因，MAC 感知比单一代理感知更容易受到敌对攻击的影响。攻击者可以很容易地通过从附近的恶意代理发送有害信息来降低受害代理的性能。在本文中，我们将对抗性攻击扩展到一个重要的感知任务—— MAC 目标检测，在这个任务中，对抗性训练等一般防御手段不再有效。更重要的是，我们提出了恶意代理检测(MADE) ，一种特定于 MAC 感知的反应性防御，每个代理可以部署，以准确地检测和消除其本地协作网络中任何潜在的恶意代理。特别是，MADE 使用基于 Benjamini-Hochberg 程序的双假设检验的半监督异常检测器独立检查网络中的每个代理，以控制推断的假阳性率。对于这两个假设检验，我们分别提出了一个匹配损失统计量和一个协同重构损失统计量，这两个统计量都是基于被检测代理和我们的检测器所部署的自我代理之间的一致性。我们对一个基准的三维数据集 V2X-sim 和一个实际道路数据集 DAIR-V2X 进行了全面的评估，结果表明，在 MADE 的保护下，与最佳情况下的“甲骨文”防御者相比，针对我们的攻击的平均精度下降仅分别为1.28% 和0.34% ，远远低于对抗性训练的8.92% 和10.00% 。"
    },
    {
        "title": "Revisiting Transferable Adversarial Image Examples: Attack\n  Categorization, Evaluation Guidelines, and New Insights",
        "url": "http://arxiv.org/abs/2310.11850v1",
        "pub_date": "2023-10-18",
        "summary": "Transferable adversarial examples raise critical security concerns in\nreal-world, black-box attack scenarios. However, in this work, we identify two\nmain problems in common evaluation practices: (1) For attack transferability,\nlack of systematic, one-to-one attack comparison and fair hyperparameter\nsettings. (2) For attack stealthiness, simply no comparisons. To address these\nproblems, we establish new evaluation guidelines by (1) proposing a novel\nattack categorization strategy and conducting systematic and fair\nintra-category analyses on transferability, and (2) considering diverse\nimperceptibility metrics and finer-grained stealthiness characteristics from\nthe perspective of attack traceback. To this end, we provide the first\nlarge-scale evaluation of transferable adversarial examples on ImageNet,\ninvolving 23 representative attacks against 9 representative defenses. Our\nevaluation leads to a number of new insights, including consensus-challenging\nones: (1) Under a fair attack hyperparameter setting, one early attack method,\nDI, actually outperforms all the follow-up methods. (2) A state-of-the-art\ndefense, DiffPure, actually gives a false sense of (white-box) security since\nit is indeed largely bypassed by our (black-box) transferable attacks. (3) Even\nwhen all attacks are bounded by the same $L_p$ norm, they lead to dramatically\ndifferent stealthiness performance, which negatively correlates with their\ntransferability performance. Overall, our work demonstrates that existing\nproblematic evaluations have indeed caused misleading conclusions and missing\npoints, and as a result, hindered the assessment of the actual progress in this\nfield.",
        "translated": "在现实世界的黑盒攻击场景中，可转移的敌对示例引起了关键的安全问题。然而，在这项工作中，我们发现两个主要问题，在常见的评估实践: (1)攻击的可转移性，缺乏系统的，一对一的攻击比较和公平的超参数设置。(2)对于攻击隐蔽性，只是没有比较。为了解决这些问题，我们建立了新的评估指南: (1)提出了一种新的攻击分类策略，并对可转移性进行了系统和公平的范畴内分析; (2)从攻击回溯的角度考虑了不同的不可感知性指标和细粒度的隐蔽性特征。为此，我们在 ImageNet 上提供了第一个大规模的可转移对手实例评估，涉及23个代表性攻击9个代表性防御。我们的评估带来了一些新的见解，其中包括挑战共识的见解: (1)在公平的攻击超参数设置下，一种早期攻击方法 DI 实际上优于所有后续方法。(2)一种最先进的防御技术——“区分纯粹”，实际上给人一种(白盒)安全的错觉，因为它确实在很大程度上被我们的(黑盒)可转移攻击所绕过。(3)即使所有的攻击都以相同的 $L _ p $规范为界，它们的隐蔽性能也会有显著的差异，这与它们的可转移性能呈负相关。总的来说，我们的工作表明，现有的有问题的评价确实造成了误导性的结论和遗漏的要点，因此阻碍了对这一领域实际进展的评估。"
    },
    {
        "title": "On the Classification of Weierstrass Elliptic Curves over $\\mathbb{Z}_n$",
        "url": "http://arxiv.org/abs/2310.11768v1",
        "pub_date": "2023-10-18",
        "summary": "The development of secure cryptographic protocols and the subsequent attack\nmechanisms have been placed in the literature with the utmost curiosity.\n  While sophisticated quantum attacks bring a concern to the classical\ncryptographic protocols present in the applications used in everyday life, the\nnecessity of developing post-quantum protocols is felt primarily.\n  In post-quantum cryptography, elliptic curve-base protocols are exciting to\nthe researchers.\n  While the comprehensive study of elliptic curves over finite fields is well\nknown, the extended study over finite rings is still missing.\n  In this work, we generalize the study of Weierstrass elliptic curves over\nfinite ring $\\mathbb{Z}_n$ through classification.\n  Several expressions to compute critical factors in studying elliptic curves\nare conferred.\n  An all-around computational classification on the Weierstrass elliptic curves\nover $\\mathbb{Z}_n$ for rigorous understanding is also attached to this work.",
        "translated": "安全密码协议的发展和随后的攻击机制在文献中已经被放在最大的好奇心。虽然复杂的量子攻击引起了人们对日常生活中使用的经典加密协议的关注，但人们首先感到有必要开发后量子协议。在后量子密码学中，椭圆曲线基协议是一个令人兴奋的研究课题。虽然有限域上椭圆曲线的综合研究是众所周知的，但对有限环上椭圆曲线的扩展研究仍然缺乏。本文通过对有限环上 Weierstrass 椭圆曲线的分类，推广了对有限环上 Weierstrass 椭圆曲线的研究。给出了计算椭圆曲线临界因子的几种表达式。本文还对 $mathbb { Z } _ n $上 Weierstrass 椭圆曲线进行了全面的计算分类。"
    },
    {
        "title": "PhishReplicant: A Language Model-based Approach to Detect Generated\n  Squatting Domain Names",
        "url": "http://arxiv.org/abs/2310.11763v1",
        "pub_date": "2023-10-18",
        "summary": "Domain squatting is a technique used by attackers to create domain names for\nphishing sites. In recent phishing attempts, we have observed many domain names\nthat use multiple techniques to evade existing methods for domain squatting.\nThese domain names, which we call generated squatting domains (GSDs), are quite\ndifferent in appearance from legitimate domain names and do not contain brand\nnames, making them difficult to associate with phishing. In this paper, we\npropose a system called PhishReplicant that detects GSDs by focusing on the\nlinguistic similarity of domain names. We analyzed newly registered and\nobserved domain names extracted from certificate transparency logs, passive\nDNS, and DNS zone files. We detected 3,498 domain names acquired by attackers\nin a four-week experiment, of which 2,821 were used for phishing sites within a\nmonth of detection. We also confirmed that our proposed system outperformed\nexisting systems in both detection accuracy and number of domain names\ndetected. As an in-depth analysis, we examined 205k GSDs collected over 150\ndays and found that phishing using GSDs was distributed globally. However,\nattackers intensively targeted brands in specific regions and industries. By\nanalyzing GSDs in real time, we can block phishing sites before or immediately\nafter they appear.",
        "translated": "域名抢注是攻击者用来为钓鱼网站创建域名的一种技术。在最近的钓鱼攻击中，我们发现许多域名使用多种技术来规避现有的域名抢注方法。这些域名，我们称之为生成域名(GSD) ，与合法域名在外观上有很大不同，并且不包含品牌名称，这使得它们很难与网络钓鱼相关联。在本文中，我们提出了一个名为 PhishReplicant 的系统，该系统通过关注域名的语言相似性来检测 GSD。我们分析了从证书透明日志、被动 DNS 和 DNS 区域文件中提取的新注册和观察到的域名。在为期四周的实验中，我们检测到攻击者获得的3,498个域名，其中2,821个在检测到后的一个月内被用于钓鱼网站。我们还证实，我们提议的系统在检测准确性和检测到的域名数量方面都优于现有系统。作为一个深入的分析，我们检查了在150天内收集的205k GSD，发现使用 GSD 的网络钓鱼分布在全球各地。然而，攻击者密集地针对特定地区和行业的品牌。通过实时分析 GSD，我们可以在钓鱼网站出现之前或之后立即阻止它们。"
    },
    {
        "title": "Federated Heterogeneous Graph Neural Network for Privacy-preserving\n  Recommendation",
        "url": "http://arxiv.org/abs/2310.11730v1",
        "pub_date": "2023-10-18",
        "summary": "Heterogeneous information network (HIN), which contains rich semantics\ndepicted by meta-paths, has become a powerful tool to alleviate data sparsity\nin recommender systems. Existing HIN-based recommendations hold the data\ncentralized storage assumption and conduct centralized model training. However,\nthe real-world data is often stored in a distributed manner for privacy\nconcerns, resulting in the failure of centralized HIN-based recommendations. In\nthis paper, we suggest the HIN is partitioned into private HINs stored in the\nclient side and shared HINs in the server. Following this setting, we propose a\nfederated heterogeneous graph neural network (FedHGNN) based framework, which\ncan collaboratively train a recommendation model on distributed HINs without\nleaking user privacy. Specifically, we first formalize the privacy definition\nin the light of differential privacy for HIN-based federated recommendation,\nwhich aims to protect user-item interactions of private HIN as well as user's\nhigh-order patterns from shared HINs. To recover the broken meta-path based\nsemantics caused by distributed data storage and satisfy the proposed privacy,\nwe elaborately design a semantic-preserving user interactions publishing\nmethod, which locally perturbs user's high-order patterns as well as related\nuser-item interactions for publishing. After that, we propose a HGNN model for\nrecommendation, which conducts node- and semantic-level aggregations to capture\nrecovered semantics. Extensive experiments on three datasets demonstrate our\nmodel outperforms existing methods by a large margin (up to 34% in HR@10 and\n42% in NDCG@10) under an acceptable privacy budget.",
        "translated": "异构信息网络(HIN)包含丰富的元路径描述语义，已成为缓解推荐系统数据稀疏性的有力工具。现有的基于 HIN 的建议持有数据集中存储的假设，并进行集中模型训练。然而，现实世界中的数据通常以分布式的方式存储，以解决隐私问题，这导致了基于 HIN 的集中式建议的失败。在本文中，我们建议将 HIN 划分为存储在客户端的私有 HIN 和服务器端的共享 HIN。在此基础上，提出了一种基于联邦异构图神经网络(FedHGNN)的框架，该框架可以在不泄露用户隐私的情况下协同训练分布式 HIN 上的推荐模型。具体来说，我们首先根据基于 HIN 的联邦推荐的差分隐私来正式定义隐私，该推荐旨在保护私有 HIN 的用户项交互以及共享 HIN 的用户高阶模式。为了恢复分布式数据存储造成的基于元路径的语义断裂，满足提出的隐私保护要求，我们设计了一种语义保护的用户交互发布方法，该方法局部扰动用户的高阶模式以及相关的用户-项交互发布。在此基础上，提出了一种 HGNN 推荐模型，该模型通过节点级和语义级的聚合来捕获恢复的语义。在三个数据集上的大量实验表明，在可接受的隐私预算下，我们的模型比现有方法有很大优势(HR@10的优势达到34% ，NDCG@10的优势达到42%)。"
    },
    {
        "title": "MalDICT: Benchmark Datasets on Malware Behaviors, Platforms,\n  Exploitation, and Packers",
        "url": "http://arxiv.org/abs/2310.11706v1",
        "pub_date": "2023-10-18",
        "summary": "Existing research on malware classification focuses almost exclusively on two\ntasks: distinguishing between malicious and benign files and classifying\nmalware by family. However, malware can be categorized according to many other\ntypes of attributes, and the ability to identify these attributes in\nnewly-emerging malware using machine learning could provide significant value\nto analysts. In particular, we have identified four tasks which are\nunder-represented in prior work: classification by behaviors that malware\nexhibit, platforms that malware run on, vulnerabilities that malware exploit,\nand packers that malware are packed with. To obtain labels for training and\nevaluating ML classifiers on these tasks, we created an antivirus (AV) tagging\ntool called ClarAVy. ClarAVy's sophisticated AV label parser distinguishes\nitself from prior AV-based taggers, with the ability to accurately parse 882\ndifferent AV label formats used by 90 different AV products. We are releasing\nbenchmark datasets for each of these four classification tasks, tagged using\nClarAVy and comprising nearly 5.5 million malicious files in total. Our malware\nbehavior dataset includes 75 distinct tags - nearly 7x more than the only prior\nbenchmark dataset with behavioral tags. To our knowledge, we are the first to\nrelease datasets with malware platform and packer tags.",
        "translated": "现有的恶意软件分类研究主要集中在两个方面: 区分恶意文件和良性文件;。然而，恶意软件可以根据许多其他类型的属性进行分类，利用机器学习识别新出现的恶意软件中的这些属性的能力可以为分析人员提供重要的价值。特别是，我们已经确定了在以前的工作中没有充分表现出来的四个任务: 按照恶意软件表现出的行为进行分类，恶意软件运行的平台，恶意软件利用的漏洞，以及恶意软件包装器。为了在这些任务中获得训练和评估机器学习分类器的标签，我们创建了一个名为 ClarAVy 的反病毒(AV)标签工具。ClarAVy 的先进的 AV 标签解析器区别于以前的基于 AV 的标签器，能够准确解析90种不同 AV 产品使用的882种不同 AV 标签格式。我们正在发布这四个分类任务中每一个的基准数据集，使用 ClarAVy 标记，总共包含近550万个恶意文件。我们的恶意软件行为数据集包括75个不同的标签-比之前唯一的带有行为标签的基准数据集多了近7倍。据我们所知，我们是第一个释放恶意软件平台和封装器标签的数据集。"
    },
    {
        "title": "Digital Twin-Enabled Intelligent DDoS Detection Mechanism for Autonomous\n  Core Networks",
        "url": "http://arxiv.org/abs/2310.12924v1",
        "pub_date": "2023-10-19",
        "summary": "Existing distributed denial of service attack (DDoS) solutions cannot handle\nhighly aggregated data rates; thus, they are unsuitable for Internet service\nprovider (ISP) core networks. This article proposes a digital twin-enabled\nintelligent DDoS detection mechanism using an online learning method for\nautonomous systems. Our contributions are three-fold: we first design a DDoS\ndetection architecture based on the digital twin for ISP core networks. We\nimplemented a Yet Another Next Generation (YANG) model and an automated feature\nselection (AutoFS) module to handle core network data. We used an online\nlearning approach to update the model instantly and efficiently, improve the\nlearning model quickly, and ensure accurate predictions. Finally, we reveal\nthat our proposed solution successfully detects DDoS attacks and updates the\nfeature selection method and learning model with a true classification rate of\nninety-seven percent. Our proposed solution can estimate the attack within\napproximately fifteen minutes after the DDoS attack starts.",
        "translated": "现有的分布式分布式拒绝服务攻击攻击(dDoS)解决方案无法处理高度聚合的数据速率，因此不适合互联网服务供应商(ISP)的核心网络。本文提出了一种基于自治系统在线学习方法的数字双机智能 DDoS 检测机制。我们的贡献有三个方面: 我们首先为 ISP 核心网络设计了一个基于数字孪生子的 DDoS 检测体系结构。我们实现了一个新一代(YANG)模型和一个自动特征选择(AutoFS)模块来处理核心网络数据。我们使用了一种在线学习方法来即时有效地更新模型，快速改进学习模型，并确保准确的预测。最后，我们发现我们提出的解决方案成功地检测到 DDoS 攻击，并且以97% 的真实分类率更新了特征选择方法和学习模型。我们提出的解决方案可以在 DDoS 攻击开始后大约15分钟内估计攻击。"
    },
    {
        "title": "Network-Aware AutoML Framework for Software-Defined Sensor Networks",
        "url": "http://arxiv.org/abs/2310.12914v1",
        "pub_date": "2023-10-19",
        "summary": "As the current detection solutions of distributed denial of service attacks\n(DDoS) need additional infrastructures to handle high aggregate data rates,\nthey are not suitable for sensor networks or the Internet of Things. Besides,\nthe security architecture of software-defined sensor networks needs to pay\nattention to the vulnerabilities of both software-defined networks and sensor\nnetworks. In this paper, we propose a network-aware automated machine learning\n(AutoML) framework which detects DDoS attacks in software-defined sensor\nnetworks. Our framework selects an ideal machine learning algorithm to detect\nDDoS attacks in network-constrained environments, using metrics such as\nvariable traffic load, heterogeneous traffic rate, and detection time while\npreventing over-fitting. Our contributions are two-fold: (i) we first\ninvestigate the trade-off between the efficiency of ML algorithms and\nnetwork/traffic state in the scope of DDoS detection. (ii) we design and\nimplement a software architecture containing open-source network tools, with\nthe deployment of multiple ML algorithms. Lastly, we show that under the denial\nof service attacks, our framework ensures the traffic packets are still\ndelivered within the network with additional delays.",
        "translated": "由于目前分布式分布式拒绝服务攻击攻击(DDoS)的检测解决方案需要额外的基础设施来处理高汇总数据率，因此它们不适用于传感器网络或物联网。此外，软件定义传感器网络的安全体系结构需要同时考虑软件定义网络和传感器网络的脆弱性。本文提出了一种基于网络感知的自动机器学习(AutoML)框架，用于检测软件定义的传感器网络中的 DDoS 攻击。我们的框架选择了一种理想的机器学习算法来检测网络约束环境中的 DDoS 攻击，在防止过度拟合的同时，使用可变流量负载、异构流量率和检测时间等指标。我们的贡献有两方面: (i)我们首先研究了在 DDoS 检测范围内 ML 算法的效率和网络/流量状态之间的权衡。(ii)我们设计并实现了一个包含开源网络工具的软件架构，并部署了多个 ML 算法。最后，我们展示了在分布式拒绝服务攻击攻击下，我们的框架确保流量数据包仍然在网络中传递，并带有额外的延迟。"
    },
    {
        "title": "Blind quantum machine learning with quantum bipartite correlator",
        "url": "http://arxiv.org/abs/2310.12893v1",
        "pub_date": "2023-10-19",
        "summary": "Distributed quantum computing is a promising computational paradigm for\nperforming computations that are beyond the reach of individual quantum\ndevices. Privacy in distributed quantum computing is critical for maintaining\nconfidentiality and protecting the data in the presence of untrusted computing\nnodes. In this work, we introduce novel blind quantum machine learning\nprotocols based on the quantum bipartite correlator algorithm. Our protocols\nhave reduced communication overhead while preserving the privacy of data from\nuntrusted parties. We introduce robust algorithm-specific privacy-preserving\nmechanisms with low computational overhead that do not require complex\ncryptographic techniques. We then validate the effectiveness of the proposed\nprotocols through complexity and privacy analysis. Our findings pave the way\nfor advancements in distributed quantum computing, opening up new possibilities\nfor privacy-aware machine learning applications in the era of quantum\ntechnologies.",
        "translated": "分布式量子计算是一种很有前途的计算范式，用于执行超出单个量子设备范围的计算。分布式量子计算中的隐私对于在不可信计算节点存在的情况下维护机密性和保护数据至关重要。本文介绍了一种新的基于量子二部相关器算法的盲量子机器学习协议。我们的协议降低了通信开销，同时保护了来自不可信方的数据隐私。我们引入了鲁棒的特定于算法的隐私保护机制，具有较低的计算开销，不需要复杂的密码技术。然后通过复杂度分析和隐私分析验证了该协议的有效性。我们的发现为分布式量子计算的进步铺平了道路，为量子技术时代的隐私意识机器学习应用开辟了新的可能性。"
    },
    {
        "title": "TwinPot: Digital Twin-assisted Honeypot for Cyber-Secure Smart Seaports",
        "url": "http://arxiv.org/abs/2310.12880v1",
        "pub_date": "2023-10-19",
        "summary": "The idea of next-generation ports has become more apparent in the last ten\nyears in response to the challenge posed by the rising demand for efficiency\nand the ever-increasing volume of goods. In this new era of intelligent\ninfrastructure and facilities, it is evident that cyber-security has recently\nreceived the most significant attention from the seaport and maritime\nauthorities, and it is a primary concern on the agenda of most ports.\nTraditional security solutions can be applied to safeguard IoT and\nCyber-Physical Systems (CPS) from harmful entities. Nevertheless, security\nresearchers can only watch, examine, and learn about the behaviors of attackers\nif these solutions operate more transparently. Herein, honeypots are potential\nsolutions since they offer valuable information about the attackers. It can be\nvirtual or physical. Virtual honeypots must be more realistic to entice\nattackers, necessitating better high-fidelity. To this end, Digital Twin (DT)\ntechnology can be employed to increase the complexity and simulation fidelity\nof the honeypots. Seaports can be attacked from both their existing devices and\nexternal devices at the same time. Existing mechanisms are insufficient to\ndetect external attacks; therefore, the current systems cannot handle attacks\nat the desired level. DT and honeypot technologies can be used together to\ntackle them. Consequently, we suggest a DT-assisted honeypot, called TwinPot,\nfor external attacks in smart seaports. Moreover, we propose an intelligent\nattack detection mechanism to handle different attack types using DT for\ninternal attacks. Finally, we build an extensive smart seaport dataset for\ninternal and external attacks using the MANSIM tool and two existing datasets\nto test the performance of our system. We show that under simultaneous internal\nand external attacks on the system, our solution successfully detects internal\nand external attacks.",
        "translated": "下一代港口的想法在过去十年中变得更加明显，以应对对效率的需求不断增加和货物数量不断增加所带来的挑战。在这个智能基础设施和设施的新时代，网络安全显然最近得到了海港和海事当局最重要的关注，它是大多数港口议程上的一个主要关切。传统的安全解决方案可以用来保护物联网和网络物理系统(CPS)免受有害实体的攻击。然而，安全研究人员只能观察、检查和了解攻击者的行为，如果这些解决方案的操作更加透明的话。在这里，蜜罐是潜在的解决方案，因为它们提供了有关攻击者的有价值的信息。它可以是虚拟的，也可以是物理的。为了吸引攻击者，虚拟蜜罐必须更加现实，这就需要更高的保真度。为此，可以采用数字双机(DT)技术来提高蜜罐的复杂度和仿真逼真度。海港可以同时受到现有设备和外部设备的攻击。现有机制不足以检测外部攻击; 因此，当前系统无法在所需的级别上处理攻击。DT 和蜜罐技术可以一起用来解决这些问题。因此，我们建议使用 DT 辅助的蜜罐，称为 TwinPot，用于智能海港的外部攻击。此外，我们提出了一个智能攻击检测机制，以处理不同的攻击类型使用 DT 的内部攻击。最后，我们使用 MANSIM 工具和两个已有的数据集建立了一个广泛的智能海港数据集，用于内部和外部攻击，以测试我们的系统的性能。我们表明，在系统同时受到内部和外部攻击的情况下，我们的解决方案成功地检测到了内部和外部攻击。"
    },
    {
        "title": "Privately Answering Queries on Skewed Data via Per Record Differential\n  Privacy",
        "url": "http://arxiv.org/abs/2310.12827v1",
        "pub_date": "2023-10-19",
        "summary": "We consider the problem of the private release of statistics (like aggregate\npayrolls) where it is critical to preserve the contribution made by a small\nnumber of outlying large entities. We propose a privacy formalism, per-record\nzero concentrated differential privacy (PzCDP), where the privacy loss\nassociated with each record is a public function of that record's value. Unlike\nother formalisms which provide different privacy losses to different records,\nPzCDP's privacy loss depends explicitly on the confidential data. We define our\nformalism, derive its properties, and propose mechanisms which satisfy PzCDP\nthat are uniquely suited to publishing skewed or heavy-tailed statistics, where\na small number of records contribute substantially to query answers. This\ntargeted relaxation helps overcome the difficulties of applying standard DP to\nthese data products.",
        "translated": "我们考虑私下发布统计数据(如总工资)的问题，在这个问题上，保留少数外围大型实体的贡献至关重要。我们提出了一种隐私形式，即每个记录的零集中差分隐私(pzcDP) ，其中每个记录的隐私损失是该记录价值的公共函数。与其他形式主义不同的是，PzCDP 的隐私损失明确依赖于机密数据。我们定义了我们的形式主义，推导出它的性质，并提出了满足 PzCDP 的机制，这些机制独特地适合于发布倾斜或重尾统计数据，其中少量记录对查询答案有很大贡献。这种有针对性的放松有助于克服将标准 DP 应用于这些数据产品的困难。"
    },
    {
        "title": "Prompt Injection Attacks and Defenses in LLM-Integrated Applications",
        "url": "http://arxiv.org/abs/2310.12815v1",
        "pub_date": "2023-10-19",
        "summary": "Large Language Models (LLMs) are increasingly deployed as the backend for a\nvariety of real-world applications called LLM-Integrated Applications. Multiple\nrecent works showed that LLM-Integrated Applications are vulnerable to prompt\ninjection attacks, in which an attacker injects malicious instruction/data into\nthe input of those applications such that they produce results as the attacker\ndesires. However, existing works are limited to case studies. As a result, the\nliterature lacks a systematic understanding of prompt injection attacks and\ntheir defenses. We aim to bridge the gap in this work. In particular, we\npropose a general framework to formalize prompt injection attacks. Existing\nattacks, which are discussed in research papers and blog posts, are special\ncases in our framework. Our framework enables us to design a new attack by\ncombining existing attacks. Moreover, we also propose a framework to\nsystematize defenses against prompt injection attacks. Using our frameworks, we\nconduct a systematic evaluation on prompt injection attacks and their defenses\nwith 10 LLMs and 7 tasks. We hope our frameworks can inspire future research in\nthis field. Our code is available at\nhttps://github.com/liu00222/Open-Prompt-Injection.",
        "translated": "大型语言模型(LLM)越来越多地被部署为各种实际应用程序的后端，称为 LLM 集成应用程序。最近的多项研究表明 LLM 集成应用程序很容易受到提示注入攻击，攻击者在这些应用程序的输入中注入恶意的指令/数据，从而按照攻击者的意愿产生结果。然而，现有的研究仅限于个案研究。因此，文献缺乏对及时注射攻击及其防御的系统理解。我们的目标是弥合这项工作中的差距。特别地，我们提出了一个通用的框架来形式化及时注入攻击。研究论文和博客文章中讨论的现有攻击是我们框架中的特例。我们的框架使我们能够通过组合现有的攻击来设计新的攻击。此外，我们还提出了一个框架，以系统化的防御即时注射攻击。使用我们的框架，我们对10个 LLM 和7个任务的快速注入攻击及其防御进行了系统的评估。我们希望我们的框架能够激励这一领域未来的研究。我们的代码可以在 https://github.com/liu00222/open-prompt-injection 找到。"
    },
    {
        "title": "Tight Short-Lived Signatures",
        "url": "http://arxiv.org/abs/2310.12723v1",
        "pub_date": "2023-10-19",
        "summary": "A Time-lock puzzle (TLP) sends information into the future: a predetermined\nnumber of sequential computations must occur (i.e., a predetermined amount of\ntime must pass) to retrieve the information, regardless of parallelization.\nBuoyed by the excitement around secure decentralized applications and\ncryptocurrencies, the last decade has witnessed numerous constructions of TLP\nvariants and related applications (e.g., cost-efficient blockchain designs,\nrandomness beacons, e-voting, etc.).\n  In this poster, we first extend the notion of TLP by formally defining the\n\"time-lock public key encryption\" (TLPKE) scheme. Next, we introduce and\nconstruct a \"tight short-lived signatures\" scheme using our TLPKE. Furthermore,\nto test the validity of our proposed schemes, we do a proof-of-concept\nimplementation and run detailed simulations.",
        "translated": "时间锁定(Time-lock)难题(TLP)将信息发送到未来: 为了检索信息，必须进行预定数量的连续计算(即，必须传递预定数量的时间) ，而不管并行性如何。在安全分散应用程序和加密货币令人兴奋的鼓舞下，过去十年见证了许多 TLP 变体和相关应用程序的构建(例如，低成本的区块链设计、随机信标、电子投票等)。在这张海报中，我们首先通过正式定义“时间锁公钥加密”(TLPKE)方案来扩展 TLP 的概念。接下来，我们使用我们的 TLPKE 引入并构造一个“紧凑的短寿命签名”方案。此外，为了检验我们提出的方案的有效性，我们进行了概念验证实现并进行了详细的仿真。"
    },
    {
        "title": "Trenchcoat: Human-Computable Hashing Algorithms for Password Generation",
        "url": "http://arxiv.org/abs/2310.12706v1",
        "pub_date": "2023-10-19",
        "summary": "The average user has between 90-130 online accounts, and around $3 \\times\n10^{11}$ passwords are in use this year. Most people are terrible at\nremembering \"random\" passwords, so they reuse or create similar passwords using\na combination of predictable words, numbers, and symbols. Previous\npassword-generation or management protocols have imposed so large a cognitive\nload that users have abandoned them in favor of insecure yet simpler methods\n(e.g., writing them down or reusing minor variants).\n  We describe a range of candidate human-computable \"hash\" functions suitable\nfor use as password generators - as long as the human (with minimal education\nassumptions) keeps a single, easily-memorizable \"master\" secret - and rate them\nby various metrics, including effective security.\n  These functions hash master-secrets with user accounts to produce sub-secrets\nthat can be used as passwords; $F_R($s$, w) \\longrightarrow y$, takes a website\n$w$, produces a password $y$, parameterized by master secret $s$, which may or\nmay not be a string.\n  We exploit the unique configuration $R$ of each user's associative and\nimplicit memory (detailed in section 2) to ensure that sources of randomness\nunique to each user are present in each master-secret $F_R$. An adversary\ncannot compute or verify $F_R$ efficiently since $R$ is unique to each\nindividual; in that sense, our hash function is similar to a physically\nunclonable function. For the algorithms we propose, the user need only complete\nprimitive operations such as addition, spatial navigation or searching.\nCritically, most of our methods are also accessible to neurodiverse, or\ncognitively or physically differently-abled persons.\n  We present results from a survey (n=134 individuals) investigating real-world\nusage of these methods and how people currently come up with their passwords,\nwe also survey 400 websites to collate current password advice.",
        "translated": "平均每个用户拥有90-130个在线账户，今年使用的密码大约是10 ^ {11} $的3倍。大多数人都很难记住“随机”密码，所以他们重复使用或创建类似的密码，使用可预测的单词、数字和符号的组合。以前的密码生成或管理协议给用户带来了巨大的认知负荷，以至于用户放弃了它们，转而使用不安全但更简单的方法(例如，将它们写下来或重用次要的变体)。我们描述了一系列适合用作密码生成器的候选人类可计算的“哈希”函数——只要人类(以最小的教育假设)保持一个单一的、容易记忆的“主”秘密——并根据各种指标对它们进行评级，包括有效的安全性。这些函数使用用户帐户散列主机密，生成可用作密码的子机密; $F _ R ($s $，w) longgrightarrow y $，接受一个网站 $w $，生成一个密码 $y $，由主机密 $s $参数化，它可能是也可能不是一个字符串。我们利用每个用户的关联和隐式内存的唯一配置 $R $(详见第2节) ，以确保每个用户独有的随机性源出现在每个主机密 $F _ R $中。对手无法有效地计算或验证 $F _ R $，因为 $R $对于每个人都是唯一的; 从这个意义上说，我们的 hash 函数类似于物理上不可克隆的函数。对于我们提出的算法，用户只需要完成基本的操作，如加法、空间导航或搜索。至关重要的是，我们的大多数方法对于神经多样性、认知或身体不同能力的人来说也是可行的。我们展示了一项调查的结果(n = 134个人) ，调查这些方法在现实世界中的使用情况，以及人们目前是如何想出他们的密码的，我们还调查了400个网站，以整理当前的密码建议。"
    },
    {
        "title": "RANDGENER: Distributed Randomness Beacon from Verifiable Delay Function",
        "url": "http://arxiv.org/abs/2310.12693v1",
        "pub_date": "2023-10-19",
        "summary": "Buoyed by the excitement around secure decentralized applications, the last\nfew decades have seen numerous constructions of distributed randomness beacons\n(DRB) along with use cases; however, a secure DRB (in many variations) remains\nan open problem. We further note that it is natural to want some kind of reward\nfor participants who spend time and energy evaluating the randomness beacon\nvalue -- this is already common in distributed protocols.\n  In this work, we present RandGener, a novel $n$-party commit-reveal-recover\n(or collaborative) DRB protocol with a novel reward and penalty mechanism along\nwith a set of realistic guarantees. We design our protocol using trapdoor\nwatermarkable verifiable delay functions in the RSA group setting (without\nrequiring a trusted dealer or distributed key generation).",
        "translated": "受到安全分散应用程序的鼓舞，在过去的几十年中已经看到了许多分布式随机信标(DRB)的构造和用例; 然而，安全 DRB (在许多变体中)仍然是一个开放的问题。我们进一步指出，对于花费时间和精力评估随机性信标值的参与者来说，希望获得某种奖励是很自然的事情——这在分布式协议中已经很常见了。在这项工作中，我们介绍了 RandGener，一个新颖的 $n 方提交-揭示-恢复(或协作) DRB 协议，它有一个新颖的奖励和惩罚机制，以及一组现实的保证。我们在 RSA 组设置中使用活门水印可验证延迟函数来设计我们的协议(不需要可信的经销商或分布式密钥生成)。"
    },
    {
        "title": "knowCC: Knowledge, awareness of computer &amp; cyber ethics between\n  CS/non-CS university students",
        "url": "http://arxiv.org/abs/2310.12684v1",
        "pub_date": "2023-10-19",
        "summary": "Technology has advanced dramatically in the previous several years. There are\nalso cyber assaults. Cyberattacks pose a possible danger to information\nsecurity and the general public. Since data practice and internet consumption\nrates continue to upswing, cyber awareness has become progressively important.\nFurthermore, as businesses pace their digital transformation with mobile\ndevices, cloud services, communal media, and Internet of Things services,\ncybersecurity has appeared as a critical issue in corporate risk management.\nThis research focuses on the relations between cybersecurity awareness, cyber\nknowledge, computer ethics, cyber ethics, and cyber behavior, as well as\nprotective tools, across university students in general. The findings express\nthat while internet users are alert of cyber threats, they only take the most\nelementary and easy-to-implement precautions. Several knowledge and awareness\nhave been proposed to knob the issue of cyber security. It also grants the\nprinciples of cybersecurity in terms of its structure, workforces, and evidence\npertaining to the shield of personal information in the cyber world. The first\nstep is for people to educate themselves about the negative aspects of the\ninternet and to learn more about cyber threats so that they can notice when an\nattack is taking place. To validate the efficiency of the suggested analysis\nbetween CS and non-CS university students, case study along with several\ncomparisons are provided.",
        "translated": "在过去的几年里，技术有了显著的进步。还有网络攻击。网络攻击可能对信息安全和公众构成威胁。由于数据实践和互联网消费率持续上升，网络意识已变得越来越重要。此外，随着企业通过移动设备、云服务、公共媒体和物联网服务加快数字化转型的步伐，网络安全已成为企业风险管理中的一个关键问题。本研究主要探讨大学生网络安全意识、网络知识、计算机伦理、网络伦理、网络行为以及网络防护工具之间的关系。研究结果显示，虽然互联网用户对网络威胁有所警觉，但他们只采取了最基本和最容易实施的预防措施。一些知识和意识已经被提出来解决网络安全问题。它还授予网络安全的原则方面的结构，工作人员，和证据有关的盾牌个人信息在网络世界。第一步是让人们了解互联网的负面影响，并学习更多有关网络威胁的知识，以便他们能够注意到攻击何时发生。为了验证所建议的分析在 CS 大学生和非 CS 大学生之间的有效性，提供了案例研究和几个比较。"
    },
    {
        "title": "Zero-Knowledge Proofs for Questionnaire Result Verification in Smart\n  Contracts",
        "url": "http://arxiv.org/abs/2310.13618v1",
        "pub_date": "2023-10-20",
        "summary": "We present an implementation of a Web3 platform that leverages the Groth16\nZero-Knowledge Proof schema to verify the validity of questionnaire results\nwithin Smart Contracts. Our approach ensures that the answer key of the\nquestionnaire remains undisclosed throughout the verification process, while\nensuring that the evaluation is done fairly. To accomplish this, users respond\nto a series of questions, and their answers are encoded and securely\ntransmitted to a hidden backend. The backend then performs an evaluation of the\nuser's answers, generating the overall result of the questionnaire.\nAdditionally, it generates a Zero-Knowledge Proof, attesting that the answers\nwere appropriately evaluated against a valid set of constraints. Next, the user\nsubmits their result along with the proof to a Smart Contract, which verifies\ntheir validity and issues a non-fungible token (NFT) as an attestation of the\nuser's test result. In this research, we implemented the Zero-Knowledge\nfunctionality using Circom 2 and deployed the Smart Contract using Solidity,\nthereby showcasing a practical and secure solution for questionnaire validity\nverification in the context of Smart Contracts.",
        "translated": "我们展示了一个 Web3平台的实现，它利用 Groth16 Zero-Knowledge Proof 模式来验证智能合同中问卷调查结果的有效性。我们的方法确保在整个核查过程中不披露问卷的答案，同时确保公平地进行评估。为了实现这一点，用户回答一系列问题，他们的答案被编码并安全地传输到一个隐藏的后端。然后，后端对用户的答案进行评估，生成问卷的总体结果。此外，它还生成了一个“零知识证明”(Zero-Knowledge Proof) ，证明根据一组有效的约束对答案进行了适当的评估。接下来，用户将结果与证明一起提交给智能合同，智能合同验证它们的有效性，并发出一个不可替换令牌(NFT)作为用户测试结果的证明。在本研究中，我们使用 Circom 2实现了零知识功能，并使用 Solidy 部署了智能合同，从而展示了一个实用而安全的智能合同环境下问卷有效性验证的解决方案。"
    },
    {
        "title": "Watch Nearby! Privacy Analysis of the People Nearby Service of Telegram",
        "url": "http://arxiv.org/abs/2310.13528v1",
        "pub_date": "2023-10-20",
        "summary": "People Nearby is a service offered by Telegram that allows a user to discover\nother Telegram users, based only on geographical proximity. Nearby users are\nreported with a rough estimate of their distance from the position of the\nreference user, allowing Telegram to claim location privacy In this paper, we\nsystematically analyze the location privacy provided by Telegram to users of\nthe People Nearby service. Through an extensive measurement campaign run by\nspoofing the user's location all over the world, we reverse-engineer the\nalgorithm adopted by People Nearby to compute distances between users. Although\nthe service protects against precise user localization, we demonstrate that\nlocation privacy is always lower than the one declared by Telegram of 500\nmeters. Specifically, we discover that location privacy is a function of the\ngeographical position of the user. Indeed, the radius of the location privacy\narea (localization error) spans between 400 meters (close to the equator) and\n128 meters (close to the poles), with a difference of up to 75% (worst case)\ncompared to what Telegram declares. After our responsible disclosure, Telegram\nupdated the FAQ associated with the service. Finally, we provide some solutions\nand countermeasures that Telegram can implement to improve location privacy. In\ngeneral, the reported findings highlight the significant privacy risks\nassociated with using People Nearby service.",
        "translated": "People Nearby 是 Telegram 提供的一项服务，它允许用户仅根据地理上的接近程度发现 Telegram 的其他用户。本文系统地分析了 Telegram 为周围用户提供的位置隐私，并通过粗略估计用户与参考用户位置的距离，实现了 Telegram 对用户位置隐私的主张。通过在全球范围内通过欺骗用户的位置进行广泛的测量活动，我们逆向工程了 People Nearby 所采用的算法来计算用户之间的距离。尽管该服务可以防止精确的用户定位，但是我们证明位置隐私总是低于 Telegram 所声明的500米。具体来说，我们发现位置隐私是用户地理位置的函数。实际上，位置隐私区域(定位错误)的半径介于400米(接近赤道)和128米(接近两极)之间，与 Telegram 公布的数据相比，差异高达75% (最糟糕的情况)。在我们负责任的披露之后，Telegram 更新了与该服务相关的常见问题解答。最后，提出了 Telegram 为改善位置隐私可以实施的一些解决方案和对策。一般来说，报告的结果强调了与使用“人在附近”服务相关的重大隐私风险。"
    },
    {
        "title": "FLTracer: Accurate Poisoning Attack Provenance in Federated Learning",
        "url": "http://arxiv.org/abs/2310.13424v1",
        "pub_date": "2023-10-20",
        "summary": "Federated Learning (FL) is a promising distributed learning approach that\nenables multiple clients to collaboratively train a shared global model.\nHowever, recent studies show that FL is vulnerable to various poisoning\nattacks, which can degrade the performance of global models or introduce\nbackdoors into them. In this paper, we first conduct a comprehensive study on\nprior FL attacks and detection methods. The results show that all existing\ndetection methods are only effective against limited and specific attacks. Most\ndetection methods suffer from high false positives, which lead to significant\nperformance degradation, especially in not independent and identically\ndistributed (non-IID) settings. To address these issues, we propose FLTracer,\nthe first FL attack provenance framework to accurately detect various attacks\nand trace the attack time, objective, type, and poisoned location of updates.\nDifferent from existing methodologies that rely solely on cross-client anomaly\ndetection, we propose a Kalman filter-based cross-round detection to identify\nadversaries by seeking the behavior changes before and after the attack. Thus,\nthis makes it resilient to data heterogeneity and is effective even in non-IID\nsettings. To further improve the accuracy of our detection method, we employ\nfour novel features and capture their anomalies with the joint decisions.\nExtensive evaluations show that FLTracer achieves an average true positive rate\nof over $96.88\\%$ at an average false positive rate of less than $2.67\\%$,\nsignificantly outperforming SOTA detection methods. \\footnote{Code is available\nat \\url{https://github.com/Eyr3/FLTracer}.}",
        "translated": "联邦学习(FL)是一种很有前途的分布式学习方法，它使多个客户能够协同训练一个共享的全局模型。然而，最近的研究表明，FL 容易受到各种中毒攻击，这可能会降低全局模型的性能或引入后门进入他们。本文首先对先前的 FL 攻击和检测方法进行了全面的研究。结果表明，所有现有的检测方法只对有限的和特定的攻击有效。大多数检测方法都存在很高的假阳性，这会导致显著的性能下降，特别是在不独立和同一分布(非 IID)设置中。为了解决这些问题，我们提出了 FLTracer，第一个 FL 攻击起源框架来准确检测各种攻击，并跟踪攻击时间、目标、类型和中毒位置的更新。与现有的仅仅依赖于跨客户端异常检测的方法不同，我们提出了一种基于卡尔曼滤波的跨轮检测方法，通过寻找攻击前后的行为变化来识别对手。因此，这使得它对数据异构性具有弹性，并且即使在非 IID 设置中也是有效的。为了进一步提高我们的检测方法的准确性，我们采用了四个新的特征，并捕捉他们的异常与联合决策。广泛的评估表明，FLTracer 实现了平均真阳性率超过96.88% $，平均假阳性率低于2.67% $，明显优于 SOTA 检测方法。脚注{代码可在网址{ https://github.com/eyr3/fltracer }下载。}"
    },
    {
        "title": "On the Effect of Clock Frequency on Voltage and Electromagnetic Fault\n  Injection",
        "url": "http://arxiv.org/abs/2310.13389v1",
        "pub_date": "2023-10-20",
        "summary": "We investigate the influence of clock frequency on the success rate of a\nfault injection attack. In particular, we examine the success rate of voltage\nand electromagnetic fault attacks for varying clock frequencies. Using three\ndifferent tests that cover different components of a System-on-Chip, we perform\nfault injection while its CPU operates at different clock frequencies. Our\nresults show that the attack's success rate increases with an increase in clock\nfrequency for both voltage and EM fault injection attacks. As the technology\nadvances push the clock frequency further, these results can help assess the\nimpact of fault injection attacks more accurately and develop appropriate\ncountermeasures to address them.",
        "translated": "我们研究了时钟频率对故障注入攻击成功率的影响。特别是，我们研究了不同时钟频率下电压和电磁故障攻击的成功率。使用三种不同的测试，覆盖了片上系统的不同组件，我们执行故障注入，而其 CPU 运行在不同的时钟频率。我们的结果表明，对于电压和电磁故障注入攻击，随着时钟频率的增加，攻击的成功率增加。随着技术的进步进一步推动时钟频率，这些结果可以帮助更准确地评估故障注入攻击的影响，并制定适当的对策来解决这些问题。"
    },
    {
        "title": "An LLM can Fool Itself: A Prompt-Based Adversarial Attack",
        "url": "http://arxiv.org/abs/2310.13345v1",
        "pub_date": "2023-10-20",
        "summary": "The wide-ranging applications of large language models (LLMs), especially in\nsafety-critical domains, necessitate the proper evaluation of the LLM's\nadversarial robustness. This paper proposes an efficient tool to audit the\nLLM's adversarial robustness via a prompt-based adversarial attack\n(PromptAttack). PromptAttack converts adversarial textual attacks into an\nattack prompt that can cause the victim LLM to output the adversarial sample to\nfool itself. The attack prompt is composed of three important components: (1)\noriginal input (OI) including the original sample and its ground-truth label,\n(2) attack objective (AO) illustrating a task description of generating a new\nsample that can fool itself without changing the semantic meaning, and (3)\nattack guidance (AG) containing the perturbation instructions to guide the LLM\non how to complete the task by perturbing the original sample at character,\nword, and sentence levels, respectively. Besides, we use a fidelity filter to\nensure that PromptAttack maintains the original semantic meanings of the\nadversarial examples. Further, we enhance the attack power of PromptAttack by\nensembling adversarial examples at different perturbation levels. Comprehensive\nempirical results using Llama2 and GPT-3.5 validate that PromptAttack\nconsistently yields a much higher attack success rate compared to AdvGLUE and\nAdvGLUE++. Interesting findings include that a simple emoji can easily mislead\nGPT-3.5 to make wrong predictions.",
        "translated": "大语言模型(LLM)的广泛应用，特别是在安全关键领域，需要对 LLM 的对抗性鲁棒性进行适当的评估。本文提出了一种有效的工具来审计 LLM 的对抗性健壮性通过提示为基础的对抗性攻击(提示攻击)。提示攻击将对手的文本攻击转换为攻击提示，可以导致受害者 LLM 输出对手的样本来欺骗自己。攻击提示由三个重要部分组成: (1)原始输入(OI) ，包括原始样本及其地面真实标签; (2)攻击目标(AO) ，说明在不改变语义含义的情况下生成新样本的任务描述; (3)包含扰动指令的攻击指导(AG) ，指导 LLM 通过分别在字符、词和句子层面扰动原始样本来完成任务。此外，我们使用保真过滤器来确保 PromptAttack 保持对手示例的原始语义含义。进一步，我们通过在不同的扰动级别上集成对手示例来增强 PromptAttack 的攻击能力。使用 Llama2和 GPT-3.5的综合实验结果证实，与 AdvGLUE 和 AdvGLUE + + 相比，Prompthack 始终能够产生更高的攻击成功率。有趣的发现包括一个简单的表情符号很容易误导 GPT-3.5做出错误的预测。"
    },
    {
        "title": "Anomaly Detection of Command Shell Sessions based on DistilBERT:\n  Unsupervised and Supervised Approaches",
        "url": "http://arxiv.org/abs/2310.13247v1",
        "pub_date": "2023-10-20",
        "summary": "Anomaly detection in command shell sessions is a critical aspect of computer\nsecurity. Recent advances in deep learning and natural language processing,\nparticularly transformer-based models, have shown great promise for addressing\ncomplex security challenges. In this paper, we implement a comprehensive\napproach to detect anomalies in Unix shell sessions using a pretrained\nDistilBERT model, leveraging both unsupervised and supervised learning\ntechniques to identify anomalous activity while minimizing data labeling. The\nunsupervised method captures the underlying structure and syntax of Unix shell\ncommands, enabling the detection of session deviations from normal behavior.\nExperiments on a large-scale enterprise dataset collected from production\nsystems demonstrate the effectiveness of our approach in detecting anomalous\nbehavior in Unix shell sessions. This work highlights the potential of\nleveraging recent advances in transformers to address important computer\nsecurity challenges.",
        "translated": "命令 shell 会话中的异常检测是计算机安全的一个关键方面。在深度学习和自然语言处理方面的最新进展，特别是基于转换器的模型，已经显示出解决复杂的安全挑战的巨大前景。在本文中，我们实现了一个全面的方法来检测 Unix shell 会话中的异常使用预先训练的 DistilBERT 模型，利用无监督和监督式学习技术来识别异常活动，同时最小化数据标签。无监督方法捕获 Unix shell 命令的底层结构和语法，从而能够检测会话偏离正常行为。在从生产系统收集的大型企业数据集上进行的实验证明了我们的方法在检测 Unix shell 会话中的异常行为方面的有效性。这项工作突出了利用变压器的最新进展来应对重要的计算机安全挑战的潜力。"
    },
    {
        "title": "Adaptive Experimental Design for Intrusion Data Collection",
        "url": "http://arxiv.org/abs/2310.13224v1",
        "pub_date": "2023-10-20",
        "summary": "Intrusion research frequently collects data on attack techniques currently\nemployed and their potential symptoms. This includes deploying honeypots,\nlogging events from existing devices, employing a red team for a sample attack\ncampaign, or simulating system activity. However, these observational studies\ndo not clearly discern the cause-and-effect relationships between the design of\nthe environment and the data recorded. Neglecting such relationships increases\nthe chance of drawing biased conclusions due to unconsidered factors, such as\nspurious correlations between features and errors in measurement or\nclassification. In this paper, we present the theory and empirical data on\nmethods that aim to discover such causal relationships efficiently. Our\nadaptive design (AD) is inspired by the clinical trial community: a variant of\na randomized control trial (RCT) to measure how a particular ``treatment''\naffects a population. To contrast our method with observational studies and\nRCT, we run the first controlled and adaptive honeypot deployment study,\nidentifying the causal relationship between an ssh vulnerability and the rate\nof server exploitation. We demonstrate that our AD method decreases the total\ntime needed to run the deployment by at least 33%, while still confidently\nstating the impact of our change in the environment. Compared to an analogous\nhoneypot study with a control group, our AD requests 17% fewer honeypots while\ncollecting 19% more attack recordings than an analogous honeypot study with a\ncontrol group.",
        "translated": "入侵研究经常收集关于目前使用的攻击技术及其潜在症状的数据。这包括部署蜜罐、从现有设备记录事件、雇用红队进行示例攻击活动或模拟系统活动。然而，这些观察性研究并没有清楚地辨别环境设计和所记录的数据之间的因果关系。忽略这种关系会增加由于未考虑因素而得出有偏见的结论的机会，例如特征之间的虚假相关性以及测量或分类中的误差。在本文中，我们提出的理论和实证数据的方法，旨在发现这种因果关系的有效性。我们的适应性设计(AD)是受到临床试验社区的启发: 一个随机对照试验(RCT)的变体，以衡量一个特定的“治疗”如何影响一个群体。为了将我们的方法与观察性研究和 RCT 进行对比，我们运行了第一个受控和自适应蜜罐部署研究，确定了 ssh 漏洞和服务器利用率之间的因果关系。我们展示了我们的 AD 方法将运行部署所需的总时间减少了至少33% ，同时仍然自信地陈述了我们的环境变更的影响。与对照组的类似蜜罐研究相比，我们的 AD 要求的蜜罐数量减少了17% ，而收集的攻击记录却比对照组的类似蜜罐研究多了19% 。"
    },
    {
        "title": "Privacy Preserving Decision Tree Training and Prediction via Fully\n  Homomorphic Encryption with No Decryption",
        "url": "http://arxiv.org/abs/2310.13140v1",
        "pub_date": "2023-10-19",
        "summary": "With data-outsourcing becoming commonplace, there grows a need for secure\noutsourcing of data and machine learning models. Namely, data and model owners\n(client) often have a need for their information to remain private and secure\nagainst the potentially untrusted computing resource (server) to whom they want\nto outsource said data and models to. Various approaches to privacy-preserving\nmachine learning (PPML) have been devised with different techniques and\nsolutions introduced in the past. These solutions often involved one of two\ncompromises: (1) client-server interactions to allow intermediary rounds of\ndecryption and re-encryption of data or (2) complex architectures for\nmulti-party computation. This paper devises a paradigm using Fully Homomorphic\nEncryption (FHE) that minimizes architectural complexity and removes\nclient-side involvement during the training and prediction lifecycle of machine\nlearning models. In addition, the paradigm proposed in this work achieves both\nmodel security as well as data security. To remove client-side involvement, the\ndevised paradigm proposes a no decryption approach that allows the server to\nhandle PPML in its entirety without rounds of decryption and re-encryption. To\nthe best of our knowledge, this paradigm is the first to achieve\nprivacy-preserving decision tree training with no decryption while maintaining\na simple client-server architecture.",
        "translated": "随着数据外包的普及，对数据和机器学习模型的安全外包的需求日益增长。也就是说，数据和模型所有者(客户端)通常需要他们的信息保持私有，并且针对潜在的不可信计算资源(服务器)保证安全，他们希望将所述数据和模型外包给这些资源(服务器)。保护隐私的机器学习(PPML)的各种方法已被设计与不同的技术和解决方案在过去的介绍。这些解决方案通常涉及两种折衷方案中的一种: (1)客户机-服务器交互，允许中间轮的数据解密和重新加密; 或者(2)多方计算的复杂体系结构。本文设计了一个使用完全同态加密(fHE)的范例，最大限度地降低了架构的复杂性，并消除了客户端在机器学习模型的训练和预测生命周期中的参与。此外，本文提出的范式实现了模型安全性和数据安全性。为了消除客户端参与，设计的范例提出了一种无解密方法，允许服务器完全处理 PPML，而不需要进行多轮解密和重新加密。据我们所知，这个范例是第一个在不解密的情况下实现保护隐私的决策树训练，同时维护一个简单的客户机-服务器架构的范例。"
    },
    {
        "title": "Mean Estimation Under Heterogeneous Privacy Demands",
        "url": "http://arxiv.org/abs/2310.13137v1",
        "pub_date": "2023-10-19",
        "summary": "Differential Privacy (DP) is a well-established framework to quantify privacy\nloss incurred by any algorithm. Traditional formulations impose a uniform\nprivacy requirement for all users, which is often inconsistent with real-world\nscenarios in which users dictate their privacy preferences individually. This\nwork considers the problem of mean estimation, where each user can impose their\nown distinct privacy level. The algorithm we propose is shown to be minimax\noptimal and has a near-linear run-time. Our results elicit an interesting\nsaturation phenomenon that occurs. Namely, the privacy requirements of the most\nstringent users dictate the overall error rates. As a consequence, users with\nless but differing privacy requirements are all given more privacy than they\nrequire, in equal amounts. In other words, these privacy-indifferent users are\ngiven a nontrivial degree of privacy for free, without any sacrifice in the\nperformance of the estimator.",
        "translated": "数据差分隐私(DP)是一个行之有效的框架，用于量化任何算法引起的隐私损失。传统的公式对所有用户强加了一个统一的隐私要求，这往往与现实世界中用户单独规定其隐私偏好的情况不一致。这项工作考虑了均值估计的问题，其中每个用户可以施加自己不同的隐私级别。我们提出的算法被证明是极大极小最优的，并具有近线性的运行时间。我们的结果引出了一个有趣的饱和现象。也就是说，最严格的用户的隐私要求决定了整体的错误率。因此，隐私要求较少但不同的用户都获得了比他们所需要的更多的隐私，而且数量相等。换句话说，这些对隐私漠不关心的用户可以免费获得一定程度的隐私，而不需要为估计器的性能做出任何牺牲。"
    },
    {
        "title": "Critical Path Prioritization Dashboard for Alert-driven Attack Graphs",
        "url": "http://arxiv.org/abs/2310.13079v1",
        "pub_date": "2023-10-19",
        "summary": "Although intrusion alerts can provide threat intelligence regarding attacker\nstrategies, extracting such intelligence via existing tools is expensive and\ntime-consuming. Earlier work has proposed SAGE, which generates attack graphs\nfrom intrusion alerts using unsupervised sequential machine learning. This\npaper proposes a querying and prioritization-enabled visual analytics dashboard\nfor SAGE. The dashboard has three main components: (i) a Graph Explorer that\npresents a global view of all attacker strategies, (ii) a Timeline Viewer that\ncorrelates attacker actions chronologically, and (iii) a Recommender Matrix\nthat highlights prevalent critical alerts via a MITRE ATT&amp;CK-inspired attack\nstage matrix. We describe the utility of the proposed dashboard using intrusion\nalerts collected from a distributed multi-stage team-based attack scenario. We\nevaluate the utility of the dashboard through a user study. Based on the\nresponses of a small set of security practitioners, we find that the dashboard\nis useful in depicting attacker strategies and attack progression, but can be\nimproved in terms of usability.",
        "translated": "虽然入侵警报可以提供关于攻击者策略的威胁情报，但是通过现有工具提取这种情报是昂贵和耗时的。早期的工作已经提出了 SAGE，它使用无监督的顺序机器学习从入侵警报生成攻击图。本文提出了一个基于查询和优先级的 SAGE 可视化分析仪表板。仪表板有三个主要组成部分: (i)一个图形浏览器，显示所有攻击者策略的全局视图，(ii)一个时间轴浏览器，按时间顺序关联攻击者的行动，和(iii)一个推荐矩阵，通过 MITRE ATT 和 CK 启发的攻击阶段矩阵突出流行的关键警报。我们使用从分布式多阶段基于团队的攻击场景中收集的入侵警报来描述所提议的仪表板的效用。我们通过用户研究来评估仪表板的实用性。基于一小部分安全实践者的反应，我们发现仪表板在描述攻击者策略和攻击进展方面很有用，但在可用性方面可以得到改进。"
    },
    {
        "title": "AutoDAN: Automatic and Interpretable Adversarial Attacks on Large\n  Language Models",
        "url": "http://arxiv.org/abs/2310.15140v1",
        "pub_date": "2023-10-23",
        "summary": "Safety alignment of Large Language Models (LLMs) can be compromised with\nmanual jailbreak attacks and (automatic) adversarial attacks. Recent work\nsuggests that patching LLMs against these attacks is possible: manual jailbreak\nattacks are human-readable but often limited and public, making them easy to\nblock; adversarial attacks generate gibberish prompts that can be detected\nusing perplexity-based filters. In this paper, we show that these solutions may\nbe too optimistic. We propose an interpretable adversarial attack,\n\\texttt{AutoDAN}, that combines the strengths of both types of attacks. It\nautomatically generates attack prompts that bypass perplexity-based filters\nwhile maintaining a high attack success rate like manual jailbreak attacks.\nThese prompts are interpretable and diverse, exhibiting strategies commonly\nused in manual jailbreak attacks, and transfer better than their non-readable\ncounterparts when using limited training data or a single proxy model. We also\ncustomize \\texttt{AutoDAN}'s objective to leak system prompts, another\njailbreak application not addressed in the adversarial attack literature. %,\ndemonstrating the versatility of the approach. We can also customize the\nobjective of \\texttt{AutoDAN} to leak system prompts, beyond the ability to\nelicit harmful content from the model, demonstrating the versatility of the\napproach. Our work provides a new way to red-team LLMs and to understand the\nmechanism of jailbreak attacks.",
        "translated": "大型语言模型(LLM)的安全对齐可能会受到手动越狱攻击和(自动)对抗性攻击的影响。最近的研究表明，针对这些攻击修补 LLM 是可能的: 手动越狱攻击是人类可读的，但往往是有限的和公开的，使他们很容易阻止; 对抗性攻击产生胡言乱语的提示，可以检测到使用困惑为基础的过滤器。在本文中，我们表明，这些解决方案可能过于乐观。我们提出了一种可解释的对抗性攻击，texttt { AutoDAN } ，它结合了两种类型攻击的优势。它自动生成攻击提示，绕过基于困惑的过滤器，同时保持较高的攻击成功率，如手动越狱攻击。这些提示是可解释的和多样化的，展示了人工越狱攻击中常用的策略，并且在使用有限的训练数据或单一代理模型时比不可读的对应方传输得更好。我们还自定义 textt { AutoDAN }的目标以泄漏系统提示，这是另一个在对手攻击文献中没有提到的越狱应用程序。% ，展示了该方法的多功能性。我们还可以将 texttt { AutoDAN }的目标定制为泄漏系统提示符，超出了从模型中引出有害内容的能力，从而展示了该方法的通用性。我们的工作为红队 LLM 提供了一种新的方法，并理解了越狱攻击的机制。"
    },
    {
        "title": "Dihedral Quantum Codes",
        "url": "http://arxiv.org/abs/2310.15092v1",
        "pub_date": "2023-10-23",
        "summary": "We study dihedral quantum codes of short block length, a large class of\nquantum CSS codes obtained by the lifted product construction. We present code\nconstruction and give a formula for the code dimension, depending on the two\nclassical codes on which the CSS code is based on. We also give a lower bound\non the code distance. Finally we construct an example of short dihedral quantum\ncodes, improving parameters of previously known quantum codes.",
        "translated": "我们研究了短块长度的二面体量子码，这是由提升积构造得到的一大类量子 CSS 码。我们提出了代码结构，并给出了代码维数的公式，这取决于 CSS 代码所基于的两个经典代码。我们还给出了代码距离的一个下界。最后，我们构造了一个短二面体量子码的例子，改进了已知量子码的参数。"
    },
    {
        "title": "On the Detection of Image-Scaling Attacks in Machine Learning",
        "url": "http://arxiv.org/abs/2310.15085v1",
        "pub_date": "2023-10-23",
        "summary": "Image scaling is an integral part of machine learning and computer vision\nsystems. Unfortunately, this preprocessing step is vulnerable to so-called\nimage-scaling attacks where an attacker makes unnoticeable changes to an image\nso that it becomes a new image after scaling. This opens up new ways for\nattackers to control the prediction or to improve poisoning and backdoor\nattacks. While effective techniques exist to prevent scaling attacks, their\ndetection has not been rigorously studied yet. Consequently, it is currently\nnot possible to reliably spot these attacks in practice.\n  This paper presents the first in-depth systematization and analysis of\ndetection methods for image-scaling attacks. We identify two general detection\nparadigms and derive novel methods from them that are simple in design yet\nsignificantly outperform previous work. We demonstrate the efficacy of these\nmethods in a comprehensive evaluation with all major learning platforms and\nscaling algorithms. First, we show that image-scaling attacks modifying the\nentire scaled image can be reliably detected even under an adaptive adversary.\nSecond, we find that our methods provide strong detection performance even if\nonly minor parts of the image are manipulated. As a result, we can introduce a\nnovel protection layer against image-scaling attacks.",
        "translated": "图像缩放是机器学习和计算机视觉系统的重要组成部分。不幸的是，这个预处理步骤很容易受到所谓的图像缩放攻击，攻击者对图像进行不易察觉的更改，以便在缩放后变成新的图像。这为攻击者控制预测或改进中毒和后门攻击开辟了新的途径。尽管存在防止缩放攻击的有效技术，但是它们的检测还没有得到严格的研究。因此，目前不可能在实践中可靠地发现这些攻击。本文首先对图像缩放攻击的检测方法进行了深入的系统化和分析。我们确定了两个一般的检测范例，并从中推导出新的方法，这些方法在设计上很简单，但明显优于以前的工作。我们证明了这些方法在所有主要学习平台和缩放算法的综合评估中的有效性。首先，我们证明了即使在自适应攻击下，图像缩放攻击也能可靠地检测到修改整幅缩放图像的攻击。其次，我们发现我们的方法提供了强大的检测性能，即使只有小部分的图像被操纵。因此，我们可以引入一个新的保护层来抵抗图像缩放攻击。"
    },
    {
        "title": "Did the Neurons Read your Book? Document-level Membership Inference for\n  Large Language Models",
        "url": "http://arxiv.org/abs/2310.15007v1",
        "pub_date": "2023-10-23",
        "summary": "With large language models (LLMs) poised to become embedded in our daily\nlives, questions are starting to be raised about the dataset(s) they learned\nfrom. These questions range from potential bias or misinformation LLMs could\nretain from their training data to questions of copyright and fair use of\nhuman-generated text. However, while these questions emerge, developers of the\nrecent state-of-the-art LLMs become increasingly reluctant to disclose details\non their training corpus. We here introduce the task of document-level\nmembership inference for real-world LLMs, i.e. inferring whether the LLM has\nseen a given document during training or not. First, we propose a procedure for\nthe development and evaluation of document-level membership inference for LLMs\nby leveraging commonly used data sources for training and the model release\ndate. We then propose a practical, black-box method to predict document-level\nmembership and instantiate it on OpenLLaMA-7B with both books and academic\npapers. We show our methodology to perform very well, reaching an impressive\nAUC of 0.856 for books and 0.678 for papers. We then show our approach to\noutperform the sentence-level membership inference attacks used in the privacy\nliterature for the document-level membership task. We finally evaluate whether\nsmaller models might be less sensitive to document-level inference and show\nOpenLLaMA-3B to be approximately as sensitive as OpenLLaMA-7B to our approach.\nTaken together, our results show that accurate document-level membership can be\ninferred for LLMs, increasing the transparency of technology poised to change\nour lives.",
        "translated": "随着大型语言模型(LLM)准备嵌入到我们的日常生活中，人们开始对他们所学习的数据集提出问题。这些问题的范围从潜在的偏见或错误信息 LLM 可以保留从他们的培训数据的版权和合理使用人类生成的文本的问题。然而，当这些问题出现的时候，最近最先进的 LLM 的开发者变得越来越不愿意透露他们培训语料库的细节。我们在这里介绍了真实世界 LLM 的文档级成员推断任务，即推断 LLM 在培训期间是否看到了给定的文档。首先，我们通过利用常用的数据源进行培训和模型发布日期，提出了一个开发和评估 LLM 的文档级成员推断的过程。然后，我们提出了一种实用的黑盒方法来预测文档级别的成员资格，并在 OpenLLaMA-7B 上用书籍和学术论文进行了实例化。我们的方法表现得非常好，图书的 AUC 达到了令人印象深刻的0.856，论文的 AUC 达到了0.678。然后，我们展示了我们的方法，用于执行文档级成员资格任务的隐私文献中使用的句子级成员资格推断攻击。最后，我们评估较小的模型是否可能对文档级别的推断不太敏感，并显示 OpenLLaMA-3B 对我们的方法的敏感度与 OpenLLaMA-7B 大致相当。综上所述，我们的研究结果表明，可以推断出 LLM 的准确的文档级成员资格，从而提高了技术的透明度，准备改变我们的生活。"
    },
    {
        "title": "Location Estimation and Recovery using 5G Positioning: Thwarting GNSS\n  Spoofing Attacks",
        "url": "http://arxiv.org/abs/2310.14885v1",
        "pub_date": "2023-10-23",
        "summary": "The availability of cheap GNSS spoofers can prevent safe navigation and\ntracking of road users. It can lead to loss of assets, inaccurate fare\nestimation, enforcing the wrong speed limit, miscalculated toll tax, passengers\nreaching an incorrect location, etc. The techniques designed to prevent and\ndetect spoofing by using cryptographic solutions or receivers capable of\ndifferentiating legitimate and attack signals are insufficient in detecting\nGNSS spoofing of road users. Recent studies, testbeds, and 3GPP standards are\nexploring the possibility of hybrid positioning, where GNSS data will be\ncombined with the 5G-NR positioning to increase the security and accuracy of\npositioning. We design the Location Estimation and Recovery(LER) systems to\nestimate the correct absolute position using the combination of GNSS and 5G\npositioning with other road users, where a subset of road users can be\nmalicious and collude to prevent spoofing detection. Our Location Verification\nProtocol extends the understanding of Message Time of Arrival Codes (MTAC) to\nprevent attacks against malicious provers. The novel Recovery and Meta Protocol\nuses road users' dynamic and unpredictable nature to detect GNSS spoofing. This\nprotocol provides fast detection of GNSS spoofing with a very low rate of false\npositives and can be customized to a large family of settings. Even in a\n(highly unrealistic) worst-case scenario where each user is malicious with a\nprobability of as large as 0.3, our protocol detects GNSS spoofing with high\nprobability after communication and ranging with at most 20 road users, with a\nfalse positive rate close to 0. SUMO simulations for road traffic show that we\ncan detect GNSS spoofing in 2.6 minutes since its start under moderate traffic\nconditions.",
        "translated": "提供廉价的全球导航卫星系统欺骗器可能会妨碍道路使用者的安全导航和跟踪。它可能导致资产损失，票价估算不准确，执行错误的速度限制，计算错误的通行费税，乘客到达错误的地点等。旨在通过使用能够区分合法信号和攻击信号的加密解决方案或接收器来防止和检测欺骗的技术不足以检测道路使用者的全球导航卫星系统欺骗。最近的研究、试验台和3GPP 标准正在探索混合定位的可能性，其中 GNSS 数据将与5G-NR 定位相结合，以提高定位的安全性和准确性。我们设计了位置估计和恢复(LER)系统，使用 GNSS 和5G 定位与其他道路用户的组合来估计正确的绝对位置，其中一部分道路用户可能是恶意的，并共谋以防止欺骗检测。我们的位置验证协议扩展了对到达码消息时间(MTAC)的理解，以防止针对恶意证明程序的攻击。新的恢复和元协议利用道路使用者的动态和不可预测的性质来检测 GNSS 欺骗。该协议提供了全球导航卫星系统欺骗的快速检测和非常低的假阳性率，并可以定制到一个大家庭的设置。即使在一个(非常不现实的)绝境求生手册中，每个用户的恶意概率高达0.3，我们的协议在与最多20个道路用户通信后检测到 GNSS 欺骗的概率很高，误报率接近0。SUMO 对道路交通的模拟显示，自 GNSS 在中等交通条件下启动以来，我们可以在2.6分钟内检测到欺骗。"
    },
    {
        "title": "Zero-knowledge Proof Meets Machine Learning in Verifiability: A Survey",
        "url": "http://arxiv.org/abs/2310.14848v1",
        "pub_date": "2023-10-23",
        "summary": "With the rapid advancement of artificial intelligence technology, the usage\nof machine learning models is gradually becoming part of our daily lives.\nHigh-quality models rely not only on efficient optimization algorithms but also\non the training and learning processes built upon vast amounts of data and\ncomputational power. However, in practice, due to various challenges such as\nlimited computational resources and data privacy concerns, users in need of\nmodels often cannot train machine learning models locally. This has led them to\nexplore alternative approaches such as outsourced learning and federated\nlearning. While these methods address the feasibility of model training\neffectively, they introduce concerns about the trustworthiness of the training\nprocess since computations are not performed locally. Similarly, there are\ntrustworthiness issues associated with outsourced model inference. These two\nproblems can be summarized as the trustworthiness problem of model\ncomputations: How can one verify that the results computed by other\nparticipants are derived according to the specified algorithm, model, and input\ndata? To address this challenge, verifiable machine learning (VML) has emerged.\nThis paper presents a comprehensive survey of zero-knowledge proof-based\nverifiable machine learning (ZKP-VML) technology. We first analyze the\npotential verifiability issues that may exist in different machine learning\nscenarios. Subsequently, we provide a formal definition of ZKP-VML. We then\nconduct a detailed analysis and classification of existing works based on their\ntechnical approaches. Finally, we discuss the key challenges and future\ndirections in the field of ZKP-based VML.",
        "translated": "随着人工智能技术的飞速发展，机器学习模型的应用逐渐成为人们日常生活的一部分。高质量的模型不仅依赖于有效的优化算法，而且依赖于建立在大量数据和计算能力基础上的训练和学习过程。然而，在实践中，由于计算资源有限和数据隐私问题等各种挑战，需要模型的用户往往无法在本地培训机器学习模型。这促使他们探索其他方法，如外包学习和联合学习。虽然这些方法有效地解决了模型训练的可行性，但是由于计算不在本地进行，因此它们引入了对训练过程可信度的关注。类似地，还有与外包模型推理相关的可信度问题。这两个问题可以概括为模型计算的可信性问题: 如何验证其他参与者计算的结果是根据指定的算法、模型和输入数据得出的？为了应对这一挑战，可验证机器学习(VML)应运而生。本文综述了基于零知识证明的可验证机器学习(ZKP-VML)技术。我们首先分析在不同的机器学习场景中可能存在的潜在可验证性问题。随后，我们给出了 ZKP-VML 的形式化定义。然后，我们进行了详细的分析和分类，现有的工作基于他们的技术方法。最后，我们讨论了基于 ZKP 的 VML 所面临的主要挑战和未来的发展方向。"
    },
    {
        "title": "Mysticeti: Low-Latency DAG Consensus with Fast Commit Path",
        "url": "http://arxiv.org/abs/2310.14821v1",
        "pub_date": "2023-10-23",
        "summary": "We introduce Mysticeti-C a byzantine consensus protocol with low-latency and\nhigh resource efficiency. It leverages a DAG based on Threshold Clocks and\nincorporates innovations in pipelining and multiple leaders to reduce latency\nin the steady state and under crash failures. Mysticeti-FPC incorporates a fast\ncommit path that has even lower latency. We prove the safety and liveness of\nthe protocols in a byzantine context. We evaluate Mysticeti and compare it with\nstate-of-the-art consensus and fast path protocols to demonstrate its low\nlatency and resource efficiency, as well as more graceful degradation under\ncrash failures. Mysticeti is the first byzantine protocol to achieve WAN\nlatency of 0.5s for consensus commit, at a throughput of over 50k TPS that\nmatches the state-of-the-art.",
        "translated": "我们引入了一个具有低延迟和高资源效率的拜占庭协商一致协议 Mysticeti-C。它利用了一个基于阈值时钟的 DAG，并结合了流水线和多个领导的创新，以减少稳定状态和崩溃故障下的延迟。Mysticeti-FPC 采用了一个更低延迟的快速提交路径。我们证明了协议在拜占庭环境下的安全性和活性。我们评估 Mysticeti 并将其与最先进的共识和快速路径协议进行比较，以证明其低延迟和资源效率，以及在崩溃失败时更优雅的降级。Mysticeti 是第一个在协商一致提交时达到0.5秒的广域网延迟的拜占庭协议，其吞吐量超过50k TPS，与最先进的协议相匹配。"
    },
    {
        "title": "B^2SFL: A Bi-level Blockchained Architecture for Secure Federated\n  Learning-based Traffic Prediction",
        "url": "http://arxiv.org/abs/2310.14669v1",
        "pub_date": "2023-10-23",
        "summary": "Federated Learning (FL) is a privacy-preserving machine learning (ML)\ntechnology that enables collaborative training and learning of a global ML\nmodel based on aggregating distributed local model updates. However, security\nand privacy guarantees could be compromised due to malicious participants and\nthe centralized FL server. This article proposed a bi-level blockchained\narchitecture for secure federated learning-based traffic prediction. The bottom\nand top layer blockchain store the local model and global aggregated parameters\naccordingly, and the distributed homomorphic-encrypted federated averaging\n(DHFA) scheme addresses the secure computation problems. We propose the partial\nprivate key distribution protocol and a partially homomorphic\nencryption/decryption scheme to achieve the distributed privacy-preserving\nfederated averaging model. We conduct extensive experiments to measure the\nrunning time of DHFA operations, quantify the read and write performance of the\nblockchain network, and elucidate the impacts of varying regional group sizes\nand model complexities on the resulting prediction accuracy for the online\ntraffic flow prediction task. The results indicate that the proposed system can\nfacilitate secure and decentralized federated learning for real-world traffic\nprediction tasks.",
        "translated": "联邦学习(FL)是一种保护隐私的机器学习(ML)技术，它能够通过聚合分布式本地模型更新，对全局 ML 模型进行协同训练和学习。但是，由于恶意参与者和集中的 FL 服务器，安全性和隐私保护可能会受到损害。提出了一种基于安全联邦学习的双层区块链流量预测体系结构。底层和顶层区块链相应地存储局部模型和全局聚合参数，分布式同态加密联邦平均(DHFA)方案解决了安全计算问题。我们提出了部分私钥分发协议和部分同态加密/解密方案，以实现分布式保密联邦平均模型。我们进行了广泛的实验来测量 DHFA 操作的运行时间，量化区块链网络的读写性能，并阐明了不同区域组大小和模型复杂度对在线流量预测任务的预测精度的影响。仿真结果表明，该系统可以有效地提高实际交通预测任务的安全性和分散性。"
    },
    {
        "title": "Understanding Read Disturbance in High Bandwidth Memory: An Experimental\n  Analysis of Real HBM2 DRAM Chips",
        "url": "http://arxiv.org/abs/2310.14665v1",
        "pub_date": "2023-10-23",
        "summary": "DRAM read disturbance is a significant and worsening safety, security, and\nreliability issue of modern DRAM chips that can be exploited to break memory\nisolation. Two prominent examples of read-disturb phenomena are RowHammer and\nRowPress. However, no prior work extensively studies read-disturb phenomena in\nmodern high-bandwidth memory (HBM) chips. In this work, we experimentally\ndemonstrate the effects of read disturbance and uncover the inner workings of\nundocumented in-DRAM read disturbance mitigation mechanisms in HBM.\n  Our characterization of six real HBM2 DRAM chips shows that (1) the number of\nread disturbance bitflips and the number of row activations needed to induce\nthe first read disturbance bitflip significantly varies between different HBM2\nchips and different 3D-stacked channels, pseudo channels, banks, and rows\ninside an HBM2 chip. (2) The DRAM rows at the end and in the middle of a DRAM\nbank exhibit significantly fewer read disturbance bitflips than the rest of the\nrows. (3) It takes fewer additional activations to induce more read disturbance\nbitflips in a DRAM row if the row exhibits the first bitflip already at a\nrelatively high activation count. (4) HBM2 chips exhibit read disturbance\nbitflips with only two row activations when rows are kept active for an\nextremely long time.\n  We show that a modern HBM2 DRAM chip implements undocumented read disturbance\ndefenses that can track potential aggressor rows based on how many times they\nare activated, and refresh their victim rows with every 17 periodic refresh\noperations. We draw key takeaways from our observations and discuss their\nimplications for future read disturbance attacks and defenses. We explain how\nour findings could be leveraged to develop both i) more powerful read\ndisturbance attacks and ii) more efficient read disturbance defense mechanisms.",
        "translated": "DRAM 读取干扰是现代 DRAM 芯片安全性、安全性和可靠性的一个重要且日益严重的问题，可用于打破存储器隔离。阅读干扰现象的两个突出例子是 RowHammer 和 RowPress。然而，目前还没有研究现代高带宽存储器芯片中的读干扰现象的工作。在这项工作中，我们实验证明了读干扰的影响，并揭示了内部工作的未记录的 DRAM 读干扰缓解机制在 HBM。我们对六个实际的 hBM2 dRAM 芯片的角色塑造表明: (1)在不同的 hBM2芯片和不同的3D 堆叠通道、伪通道、银行和行之间，引起第一个读干扰位翻转所需的读干扰位翻转的数量和行激活的数量显著不同。(2)位于 DRAM 组末端和中间的 DRAM 行的读扰动位翻转明显少于其他行。(3)如果 DRAM 行已经以相对较高的激活计数显示了第一个位翻转，则只需较少的额外激活就可以引起更多的读扰动位翻转。(4)当行保持极长时间的活动时，HBM2芯片只有两行激活，表现出读干扰位翻转。我们展示了一个现代的 HBM2 DRAM 芯片实现了未记录的读干扰防御，它可以根据潜在的攻击行被激活的次数来跟踪它们，并且每17次周期性的刷新操作就刷新它们的受害行。我们从我们的观察中得出关键的结论，并讨论它们对未来阅读干扰攻击和防御的影响。我们解释了如何利用我们的发现来开发 i)更强大的读干扰攻击和 ii)更有效的读干扰防御机制。"
    },
    {
        "title": "CryptoVerif: a Computationally-Sound Security Protocol Verifier (Initial\n  Version with Communications on Channels)",
        "url": "http://arxiv.org/abs/2310.14658v1",
        "pub_date": "2023-10-23",
        "summary": "This document presents the security protocol verifier CryptoVerif.CryptoVerif\ndoes not rely on the symbolic, Dolev-Yao model, but on the computational model.\nIt can verify secrecy, correspondence (which include authentication), and\nindistinguishability properties. It produces proofs presented as sequences of\ngames, like those manually written by cryptographers; these games are\nformalized in aprobabilistic process calculus. CryptoVerif provides a generic\nmethod for specifying security properties of the cryptographic primitives.It\nproduces proofs valid for any number of sessions of the protocol, and provides\nan upper bound on the probability of success of an attack against the protocol\nas a function of the probability of breaking each primitive and of the number\nof sessions. It can work automatically, or the user can guide it with manual\nproof indications.",
        "translated": "本文档介绍了安全协议验证器 CryptoVerif。CryptoVerif 并不依赖于象征性的多列夫-姚模型，而是依赖于计算模型。它可以验证保密性、通信(包括身份验证)和不可区分性属性。它以游戏序列的形式提供证明，就像那些由密码学家手工编写的游戏，这些游戏以概率进程代数进行形式化。CryptoVerif 提供一个指定加密原语的安全属性的通用方法。它产生对协议的任意数量会话有效的证明，并提供一个针对协议的攻击成功概率的上限，作为破坏每个原语的概率和会话数量的函数。它可以自动工作，或用户可以指导它与手动证明指示。"
    },
    {
        "title": "An Efficient Method for Realizing Contractions of Access Structures in\n  Cloud Storage",
        "url": "http://arxiv.org/abs/2310.15972v1",
        "pub_date": "2023-10-24",
        "summary": "In single-cloud storage, ciphertext-policy attribute-based encryption\n(CP-ABE) allows one to encrypt any data under an access structure to a cloud\nserver, specifying what attributes are required to decrypt. In multi-cloud\nstorage, a secret sharing scheme (SSS) allows one to split any data into\nmultiple shares, one to a single server, and specify which subset of the\nservers are able to recover the data. It is an interesting problem to remove\nsome attributes/servers but still enable the remaining attributes/servers in\nevery authorized set to recover the data. The problem is related to the\ncontraction problem of access structures for SSSs. In this paper, we propose a\nmethod that can efficiently transform a given SSS for an access structure to\nSSSs for contractions of the access structure. We show its applications in\nsolving the attribute removal problem in the CP-ABE based single-cloud storage\nand the data relocating problem in multi-cloud storage. Our method results in\nsolutions that require either less server storage or even no additional server\nstorage.",
        "translated": "在单云存储中，基于密文策略属性的加密(CP-ABE)允许对云服务器访问结构下的任何数据进行加密，指定解密所需的属性。在多云存储中，一个秘密共享方案(SSS)允许将任何数据分割成多个共享，一个共享到单个服务器，并指定服务器的哪个子集能够恢复数据。移除一些属性/服务器，但仍然允许每个授权集中的其余属性/服务器恢复数据，这是一个有趣的问题。这个问题涉及到接入结构的收缩问题。在本文中，我们提出了一种方法，可以有效地转换给定的接入结构的 SSS 到接入结构压缩的 SSS。我们展示了它在解决基于 CP-ABE 的单云存储中的属性移除问题和多云存储中的数据迁移问题中的应用。我们的方法产生的解决方案要么需要更少的服务器存储，要么甚至不需要额外的服务器存储。"
    },
    {
        "title": "Redactable Signature Schemes and Zero-knowledge Proofs: A comparative\n  examination for applications in Decentralized Digital Identity Systems",
        "url": "http://arxiv.org/abs/2310.15934v1",
        "pub_date": "2023-10-24",
        "summary": "Redactable Signature Schemes and Zero-Knowledge Proofs are two radically\ndifferent approaches to enable privacy. This paper analyses their merits and\ndrawbacks when applied to decentralized identity system. Redactable Signatures,\nthough competitively quick and compact, are not as expressive as zero-knowledge\nproofs and do not provide the same level of privacy. On the other hand,\nzero-knowledge proofs can be much faster but some protocols require a trusted\nset-up. We conclude that given the benefits and drawbacks, redactable\nsignatures are more appropriate at an earlier stage and zero-knowledge proofs\nare more appropriate at a later stage for decentralized identity systems",
        "translated": "可编辑签名方案和零知识证明是两种完全不同的保护隐私的方法。分析了它们在分散式身份系统中应用的优缺点。可编辑签名，虽然竞争快速和紧凑，不像零知识证明表达，并没有提供同等水平的隐私。另一方面，零知识证明可以快得多，但有些协议需要可信的设置。我们的结论是，考虑到这些优缺点，可编辑签名在早期阶段更合适，零知识证明在后期阶段更适合于分散式身份系统"
    },
    {
        "title": "Exploring the Risks and Challenges of National Electronic Identity\n  (NeID) System",
        "url": "http://arxiv.org/abs/2310.15813v1",
        "pub_date": "2023-10-24",
        "summary": "Many countries have embraced national electronic identification (NeID)\nsystems, recognising their potential to foster a fair, transparent, and\nwell-governed society by ensuring the secure verification of citizens'\nidentities. The inclusive nature of NeID empowers people to exercise their\nrights while holding them accountable for fulfilling their obligations.\nNevertheless, the development and implementation of these complex\nidentity-verification systems have raised concerns regarding security, privacy,\nand exclusion. In this study, we discuss the different categories of NeID risk\nand explore the successful deployment of these systems, while examining how the\nspecific risks and other challenges posed by this technology are addressed.\nBased on the review of the different NeID systems and the efforts made to\nmitigate the unique risks and challenges presented within each deployment, we\nhighlighted the best practices for mitigating risk, including implementing\nstrong security measures, conducting regular risk assessments, and involving\nstakeholders in the design and implementation of the system.",
        "translated": "许多国家已经接受了国家电子身份识别系统(NeID) ，认识到它们通过确保公民身份的安全验证，有可能促进建立一个公平、透明和治理良好的社会。NeID 的包容性使人们能够行使自己的权利，同时要求他们对履行自己的义务负责。然而，这些复杂的身份验证系统的开发和实现引起了人们对安全、隐私和排斥的关注。在这项研究中，我们讨论了不同类别的 NeID 风险，并探讨了这些系统的成功部署，同时研究了如何处理这种技术带来的具体风险和其他挑战。根据对不同 NeID 系统的审查以及为减轻每次部署中出现的独特风险和挑战所做的努力，我们强调了减轻风险的最佳做法，包括实施强有力的安全措施，进行定期风险评估，并让利益攸关方参与系统的设计和实施。"
    },
    {
        "title": "An Impact and Risk Assessment Framework for National Electronic Identity\n  (eID) Systems",
        "url": "http://arxiv.org/abs/2310.15784v1",
        "pub_date": "2023-10-24",
        "summary": "Electronic identification (eID) systems allow citizens to assert and\nauthenticate their identities for various purposes, such as accessing\ngovernment services or conducting financial transactions. These systems improve\nuser access to rights, services, and the formal economy. As eID systems become\nan essential facet of national development, any failure, compromise, or misuse\ncan be costly and damaging to the government, users, and society. Therefore, an\neffective risk assessment is vital for identifying emerging risks to the system\nand assessing their impact. However, developing a comprehensive risk assessment\nfor these systems must extend far beyond focusing on technical security and\nprivacy impacts and must be conducted with a contextual understanding of\nstakeholders and the communities these systems serve. In this study, we posit\nthat current risk assessments do not address risk factors for all key\nstakeholders and explore how potential compromise could impact them each in\nturn. In the examination of the broader impact of risks and the potentially\nsignificant consequences for stakeholders, we propose a framework that\nconsiders a wide range of factors, including the social, economic, and\npolitical contexts in which these systems were implemented. This provides a\nholistic platform for a better assessment of risk to the eID system.",
        "translated": "电子身份证(eID)系统允许公民为各种目的断言和验证自己的身份，例如访问政府服务或进行金融交易。这些系统改善了用户对权利、服务和正规经济的访问。随着电子身份识别系统成为国家发展的一个重要方面，任何失败、妥协或误用都可能造成高昂的代价，并对政府、用户和社会造成损害。因此，有效的风险评估对于识别系统新出现的风险和评估其影响至关重要。然而，对这些系统进行全面风险评估的范围必须远远超出侧重于技术安全和隐私影响的范围，而且必须在进行评估时考虑到利益攸关方和这些系统所服务的社区的背景情况。在这项研究中，我们假设目前的风险评估并不涉及所有关键利益相关者的风险因素，并探讨潜在的妥协如何依次影响他们。在审查更广泛的影响风险和潜在的重大后果的利益相关者，我们提出了一个框架，考虑广泛的因素，包括社会，经济和政治背景下，这些系统的实施。这为更好地评估电子身份识别系统的风险提供了一个整体平台。"
    },
    {
        "title": "Light up that Droid! On the Effectiveness of Static Analysis Features\n  against App Obfuscation for Android Malware Detection",
        "url": "http://arxiv.org/abs/2310.15645v1",
        "pub_date": "2023-10-24",
        "summary": "Malware authors have seen obfuscation as the mean to bypass malware detectors\nbased on static analysis features. For Android, several studies have confirmed\nthat many anti-malware products are easily evaded with simple program\ntransformations. As opposed to these works, ML detection proposals for Android\nleveraging static analysis features have also been proposed as\nobfuscation-resilient. Therefore, it needs to be determined to what extent the\nuse of a specific obfuscation strategy or tool poses a risk for the validity of\nML malware detectors for Android based on static analysis features. To shed\nsome light in this regard, in this article we assess the impact of specific\nobfuscation techniques on common features extracted using static analysis and\ndetermine whether the changes are significant enough to undermine the\neffectiveness of ML malware detectors that rely on these features. The\nexperimental results suggest that obfuscation techniques affect all static\nanalysis features to varying degrees across different tools. However, certain\nfeatures retain their validity for ML malware detection even in the presence of\nobfuscation. Based on these findings, we propose a ML malware detector for\nAndroid that is robust against obfuscation and outperforms current\nstate-of-the-art detectors.",
        "translated": "恶意软件作者认为混淆是绕过基于静态分析特征的恶意软件检测器的手段。对于 Android，一些研究已经证实，许多反恶意软件产品很容易规避简单的程序转换。与这些工作相反，Android 利用静态分析特性的机器学习检测方案也被认为是模糊弹性的。因此，需要确定在何种程度上使用特定的混淆策略或工具对基于静态分析特征的机器学习恶意软件检测器的有效性构成风险。为了说明这一点，在本文中，我们评估了特定模糊技术对使用静态分析提取的共同特征的影响，并确定这些变化是否足够显着，以破坏依赖于这些特征的 ML 恶意软件检测器的有效性。实验结果表明，模糊处理技术对不同工具的静态分析特征有不同程度的影响。然而，某些特征仍然有效的 ML 恶意软件检测，即使在存在混淆。基于这些发现，我们提出了一个针对 Android 系统的 ML 恶意软件检测器，该检测器对模糊处理具有鲁棒性，性能优于目前最先进的检测器。"
    },
    {
        "title": "Facial Data Minimization: Shallow Model as Your Privacy Filter",
        "url": "http://arxiv.org/abs/2310.15590v1",
        "pub_date": "2023-10-24",
        "summary": "Face recognition service has been used in many fields and brings much\nconvenience to people. However, once the user's facial data is transmitted to a\nservice provider, the user will lose control of his/her private data. In recent\nyears, there exist various security and privacy issues due to the leakage of\nfacial data. Although many privacy-preserving methods have been proposed, they\nusually fail when they are not accessible to adversaries' strategies or\nauxiliary data. Hence, in this paper, by fully considering two cases of\nuploading facial images and facial features, which are very typical in face\nrecognition service systems, we proposed a data privacy minimization\ntransformation (PMT) method. This method can process the original facial data\nbased on the shallow model of authorized services to obtain the obfuscated\ndata. The obfuscated data can not only maintain satisfactory performance on\nauthorized models and restrict the performance on other unauthorized models but\nalso prevent original privacy data from leaking by AI methods and human visual\ntheft. Additionally, since a service provider may execute preprocessing\noperations on the received data, we also propose an enhanced perturbation\nmethod to improve the robustness of PMT. Besides, to authorize one facial image\nto multiple service models simultaneously, a multiple restriction mechanism is\nproposed to improve the scalability of PMT. Finally, we conduct extensive\nexperiments and evaluate the effectiveness of the proposed PMT in defending\nagainst face reconstruction, data abuse, and face attribute estimation attacks.\nThese experimental results demonstrate that PMT performs well in preventing\nfacial data abuse and privacy leakage while maintaining face recognition\naccuracy.",
        "translated": "人脸识别服务已经广泛应用于各个领域，给人们带来了极大的方便。然而，一旦用户的面部数据传输到服务提供商，用户将失去对其私人数据的控制。近年来，由于面部数据的泄露，出现了各种各样的安全和隐私问题。虽然已经提出了许多保护隐私的方法，但是当它们不能被对手的策略或辅助数据访问时，它们通常会失败。因此，本文在充分考虑人脸识别服务系统中非常典型的两种上传人脸图像和人脸特征的情况下，提出了一种数据隐私最小化转换(PMT)方法。该方法基于授权服务的浅层模型对原始人脸数据进行处理，得到模糊数据。模糊数据不仅可以保持授权模型的良好性能，限制其他未授权模型的性能，而且可以防止原始隐私数据被人工智能方法泄露和人类视觉盗窃。此外，由于服务提供者可以对接收到的数据执行预处理操作，我们还提出了一种增强的扰动方法来提高 PMT 的鲁棒性。此外，为了同时将一幅人脸图像授权给多个服务模型，提出了一种多重约束机制来提高 PMT 的可扩展性。最后，我们进行了广泛的实验，并评估了所提出的 PMT 在防御人脸重构、数据滥用和人脸属性估计攻击方面的有效性。实验结果表明，PMT 在保持人脸识别精度的同时，在防止人脸数据滥用和隐私泄露方面表现良好。"
    },
    {
        "title": "SecV: Secure Code Partitioning via Multi-Language Secure Values",
        "url": "http://arxiv.org/abs/2310.15582v1",
        "pub_date": "2023-10-24",
        "summary": "Trusted execution environments like Intel SGX provide \\emph{enclaves}, which\noffer strong security guarantees for applications. Running entire applications\ninside enclaves is possible, but this approach leads to a large trusted\ncomputing base (TCB). As such, various tools have been developed to partition\nprograms written in languages such as C or Java into \\emph{trusted} and\n\\emph{untrusted} parts, which are run in and out of enclaves respectively.\nHowever, those tools depend on language-specific taint-analysis and\npartitioning techniques. They cannot be reused for other languages and there is\nthus a need for tools that transcend this language barrier.\n  We address this challenge by proposing a multi-language technique to specify\nsensitive code or data, as well as a multi-language tool to analyse and\npartition the resulting programs for trusted execution environments like Intel\nSGX. We leverage GraalVM's Truffle framework, which provides a\nlanguage-agnostic abstract syntax tree (AST) representation for programs, to\nprovide special AST nodes called \\emph{secure nodes} that encapsulate sensitive\nprogram information. Secure nodes can easily be embedded into the ASTs of a\nwide range of languages via Truffle's \\emph{polyglot API}. Our technique\nincludes a multi-language dynamic taint tracking tool to analyse and partition\napplications based on our generic secure nodes. Our extensive evaluation with\nmicro- and macro-benchmarks shows that we can use our technique for two\nlanguages (Javascript and \\python), and that partitioned programs can obtain up\nto $14.5\\%$ performance improvement as compared to unpartitioned versions.",
        "translated": "像 Intel SGX 这样的可信执行环境提供了 emph { enclaves } ，它为应用程序提供了强有力的安全保证。在飞地内运行整个应用程序是可能的，但是这种方法会导致大量的可信计算基础(TCB)。因此，人们开发了各种工具来将用 C 或 Java 等语言编写的程序划分为 emph { trust }和 emph { untrust }两部分，这两部分分别在飞地内和飞地外运行。但是，这些工具依赖于特定于语言的污点分析和分区技术。它们不能用于其他语言，因此需要超越这种语言障碍的工具。我们通过提出一种多语言技术来指定敏感的代码或数据，以及一种多语言工具来分析和划分可信执行环境(如 Intel SGX)的结果程序来应对这一挑战。我们利用 GraalVM 的 Truffle 框架(它为程序提供了语言无关的抽象语法树(AST)表示)来提供特殊的 AST 节点，称为 emph { security 节点} ，它封装了敏感的程序信息。通过 Truffle 的 emph { polyglot API } ，安全节点可以很容易地嵌入到各种语言的 AST 中。我们的技术包括一个基于通用安全节点的多语言动态污点跟踪工具来分析和划分应用程序。我们对微基准测试和宏基准测试的广泛评估表明，我们可以将我们的技术用于两种语言(Javascript 和 python) ，与未分区版本相比，分区程序可以获得高达14.5% 的性能提高。"
    },
    {
        "title": "Privacy Amplification for Matrix Mechanisms",
        "url": "http://arxiv.org/abs/2310.15526v1",
        "pub_date": "2023-10-24",
        "summary": "Privacy amplification exploits randomness in data selection to provide\ntighter differential privacy (DP) guarantees. This analysis is key to DP-SGD's\nsuccess in machine learning, but, is not readily applicable to the newer\nstate-of-the-art algorithms. This is because these algorithms, known as\nDP-FTRL, use the matrix mechanism to add correlated noise instead of\nindependent noise as in DP-SGD.\n  In this paper, we propose \"MMCC\", the first algorithm to analyze privacy\namplification via sampling for any generic matrix mechanism. MMCC is nearly\ntight in that it approaches a lower bound as $\\epsilon\\to0$. To analyze\ncorrelated outputs in MMCC, we prove that they can be analyzed as if they were\nindependent, by conditioning them on prior outputs. Our \"conditional\ncomposition theorem\" has broad utility: we use it to show that the noise added\nto binary-tree-DP-FTRL can asymptotically match the noise added to DP-SGD with\namplification. Our amplification algorithm also has practical empirical\nutility: we show it leads to significant improvement in the privacy-utility\ntrade-offs for DP-FTRL algorithms on standard benchmarks.",
        "translated": "隐私放大利用数据选择的随机性来提供更严格的差分隐私(DP)保证。这种分析是 DP-SGD 在机器学习中成功的关键，但是，不能很容易地适用于较新的最先进的算法。这是因为这些算法，被称为 DP-FTRL，使用矩阵机制添加相关噪声，而不是像 DP-SGD 中的独立噪声。在本文中，我们提出了“ MMCC”，第一个算法来分析隐私放大通过采样的任何一般矩阵机制。MMCC 几乎是紧密的，因为它接近一个下限为 $epsilon 到0 $。为了分析 MMCC 中的相关输出，我们证明了它们可以通过对先验输出的调节而被分析得好像它们是独立的。我们的“条件合成定理”具有广泛的用途: 我们用它来证明二叉树 DP-FTRL 中的噪声可以通过放大渐近地匹配 DP-SGD 中的噪声。我们的放大算法也具有实际的经验效用: 我们表明，它导致在标准基准上的 DP-FTRL 算法的隐私效用权衡显着改善。"
    },
    {
        "title": "Non-Fungible Token Security",
        "url": "http://arxiv.org/abs/2310.15518v1",
        "pub_date": "2023-10-24",
        "summary": "Non-fungible tokens (NFTs) are unique digital assets stored on the blockchain\nand is used to certify ownership and authenticity of the digital asset. NFTs\nwere first created in 2014 while their popularity peaked between 2021 and 2022.\nIn this paper, the authors dive into the world of Non-Fungible Tokens (NFTs),\ntheir history, the Future of NFTs, as well as the security concerns.",
        "translated": "不可替代令牌(NFT)是存储在区块链上的唯一数字资产，用于证明数字资产的所有权和真实性。NFT 最初创建于2014年，其受欢迎程度在2021年至2022年之间达到顶峰。在本文中，作者深入研究了非可替换令牌(NFT)的世界、它们的历史、 NFT 的未来以及安全问题。"
    },
    {
        "title": "The Janus Interface: How Fine-Tuning in Large Language Models Amplifies\n  the Privacy Risks",
        "url": "http://arxiv.org/abs/2310.15469v1",
        "pub_date": "2023-10-24",
        "summary": "The era post-2018 marked the advent of Large Language Models (LLMs), with\ninnovations such as OpenAI's ChatGPT showcasing prodigious linguistic prowess.\nAs the industry galloped toward augmenting model parameters and capitalizing on\nvast swaths of human language data, security and privacy challenges also\nemerged. Foremost among these is the potential inadvertent accrual of Personal\nIdentifiable Information (PII) during web-based data acquisition, posing risks\nof unintended PII disclosure. While strategies like RLHF during training and\nCatastrophic Forgetting have been marshaled to control the risk of privacy\ninfringements, recent advancements in LLMs, epitomized by OpenAI's fine-tuning\ninterface for GPT-3.5, have reignited concerns. One may ask: can the\nfine-tuning of LLMs precipitate the leakage of personal information embedded\nwithin training datasets? This paper reports the first endeavor to seek the\nanswer to the question, particularly our discovery of a new LLM exploitation\navenue, called the Janus attack. In the attack, one can construct a PII\nassociation task, whereby an LLM is fine-tuned using a minuscule PII dataset,\nto potentially reinstate and reveal concealed PIIs. Our findings indicate that,\nwith a trivial fine-tuning outlay, LLMs such as GPT-3.5 can transition from\nbeing impermeable to PII extraction to a state where they divulge a substantial\nproportion of concealed PII. This research, through its deep dive into the\nJanus attack vector, underscores the imperative of navigating the intricate\ninterplay between LLM utility and privacy preservation.",
        "translated": "2018年后的时代标志着大语言模型(LLM)的到来，OpenAI 的 ChatGPT 等创新展示了惊人的语言能力。随着业界迅速增加模型参数并利用大量人类语言数据，安全和隐私方面的挑战也出现了。其中最重要的是个人可识别信息(PII)在基于网络的数据采集过程中可能无意中累积，造成意外披露 PII 的风险。虽然像训练期间的 RLHF 和灾难性遗忘这样的策略已经被用来控制侵犯隐私的风险，但是 LLM 的最新进展，以 OpenAI 对 GPT-3.5的微调界面为代表，重新引起了人们的关注。有人可能会问: LLM 的微调是否会导致嵌入在培训数据集中的个人信息的泄露？本文报告的第一次努力，以寻求这个问题的答案，特别是我们发现了一个新的 LLM 利用途径，称为 Janus 攻击。在攻击中，人们可以构建一个 PII 关联任务，通过使用微小的 PII 数据集对 LLM 进行微调，以潜在地恢复和显示隐藏的 PII。我们的研究结果表明，通过微小的微调支出，GPT-3.5等 LLM 可以从不可渗透的 PII 提取过渡到泄露大量隐藏 PII 的状态。通过深入研究 Janus 攻击向量，这项研究强调了在 LLM 实用程序和隐私保护之间复杂的相互作用中导航的必要性。"
    },
    {
        "title": "Detecting Pretraining Data from Large Language Models",
        "url": "http://arxiv.org/abs/2310.16789v1",
        "pub_date": "2023-10-25",
        "summary": "Although large language models (LLMs) are widely deployed, the data used to\ntrain them is rarely disclosed. Given the incredible scale of this data, up to\ntrillions of tokens, it is all but certain that it includes potentially\nproblematic text such as copyrighted materials, personally identifiable\ninformation, and test data for widely reported reference benchmarks. However,\nwe currently have no way to know which data of these types is included or in\nwhat proportions. In this paper, we study the pretraining data detection\nproblem: given a piece of text and black-box access to an LLM without knowing\nthe pretraining data, can we determine if the model was trained on the provided\ntext? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that\nuses data created before and after model training to support gold truth\ndetection. We also introduce a new detection method Min-K% Prob based on a\nsimple hypothesis: an unseen example is likely to contain a few outlier words\nwith low probabilities under the LLM, while a seen example is less likely to\nhave words with such low probabilities. Min-K% Prob can be applied without any\nknowledge about the pretraining corpus or any additional training, departing\nfrom previous detection methods that require training a reference model on data\nthat is similar to the pretraining data. Moreover, our experiments demonstrate\nthat Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previous\nmethods. We apply Min-K% Prob to two real-world scenarios, copyrighted book\ndetection, and contaminated downstream example detection, and find it a\nconsistently effective solution.",
        "translated": "尽管大型语言模型(LLM)得到了广泛的应用，但是用于训练它们的数据却很少被公开。鉴于这些数据的难以置信的规模，高达数万亿令牌，几乎可以肯定的是，它包括了可能存在问题的文本，如受版权保护的材料，个人身份信息，以及广泛报道的参考基准的测试数据。但是，我们目前无法知道这些类型的哪些数据被包括在内，以及这些数据的比例是多少。在本文中，我们研究了预训练数据检测问题: 在不知道预训练数据的情况下，给定一段文本和对 LLM 的黑盒访问，我们能否确定模型是否在提供的文本上进行训练？为了促进这项研究，我们引入了一个动态基准 WIKIMIA，它使用模型训练前后创建的数据来支持黄金真相检测。我们还介绍了一种新的检测方法 Min-K% Prob，该方法基于一个简单的假设: 在 LLM 下，一个看不见的例子可能包含一些概率较低的异常词，而一个看得见的例子不太可能包含概率较低的词。Min-K% Prob 可以在没有任何关于预训练语料或任何额外训练的情况下应用，不同于以前的检测方法，这些方法需要对与预训练数据相似的数据进行参考模型的训练。此外，我们的实验表明，Min-K% Prob 比以前的方法提高了7.4% 。我们将 Min-K% Prob 应用于两个真实世界的场景，受版权保护的图书检测和受污染的下游示例检测，并发现它是一个一致有效的解决方案。"
    },
    {
        "title": "Robust and Actively Secure Serverless Collaborative Learning",
        "url": "http://arxiv.org/abs/2310.16678v1",
        "pub_date": "2023-10-25",
        "summary": "Collaborative machine learning (ML) is widely used to enable institutions to\nlearn better models from distributed data. While collaborative approaches to\nlearning intuitively protect user data, they remain vulnerable to either the\nserver, the clients, or both, deviating from the protocol. Indeed, because the\nprotocol is asymmetric, a malicious server can abuse its power to reconstruct\nclient data points. Conversely, malicious clients can corrupt learning with\nmalicious updates. Thus, both clients and servers require a guarantee when the\nother cannot be trusted to fully cooperate. In this work, we propose a\npeer-to-peer (P2P) learning scheme that is secure against malicious servers and\nrobust to malicious clients. Our core contribution is a generic framework that\ntransforms any (compatible) algorithm for robust aggregation of model updates\nto the setting where servers and clients can act maliciously. Finally, we\ndemonstrate the computational efficiency of our approach even with 1-million\nparameter models trained by 100s of peers on standard datasets.",
        "translated": "协作机器学习(ML)被广泛用于使机构能够从分布式数据中学习更好的模型。虽然协作学习方法直观地保护了用户数据，但它们仍然容易受到服务器、客户端或两者的影响，偏离了协议。事实上，由于协议是不对称的，恶意服务器可以滥用其权力来重建客户端数据点。相反，恶意客户端可以通过恶意更新破坏学习。因此，客户端和服务器都需要在不能信任对方完全合作时提供保证。在这项工作中，我们提出了一个对等(P2P)学习方案，这是安全的恶意服务器和健壮的恶意客户端。我们的核心贡献是一个通用框架，它可以将任何(兼容的)模型更新的健壮聚合算法转换为服务器和客户机可能恶意行为的设置。最后，我们证明了我们的方法的计算效率，即使100万个标准数据集上的对等点训练的100万个参数模型。"
    },
    {
        "title": "On the Proactive Generation of Unsafe Images From Text-To-Image Models\n  Using Benign Prompts",
        "url": "http://arxiv.org/abs/2310.16613v1",
        "pub_date": "2023-10-25",
        "summary": "Text-to-image models like Stable Diffusion have had a profound impact on\ndaily life by enabling the generation of photorealistic images from textual\nprompts, fostering creativity, and enhancing visual experiences across various\napplications. However, these models also pose risks. Previous studies have\nsuccessfully demonstrated that manipulated prompts can elicit text-to-image\nmodels to generate unsafe images, e.g., hateful meme variants. Yet, these\nstudies only unleash the harmful power of text-to-image models in a passive\nmanner. In this work, we focus on the proactive generation of unsafe images\nusing targeted benign prompts via poisoning attacks. We propose two poisoning\nattacks: a basic attack and a utility-preserving attack. We qualitatively and\nquantitatively evaluate the proposed attacks using four representative hateful\nmemes and multiple query prompts. Experimental results indicate that\ntext-to-image models are vulnerable to the basic attack even with five\npoisoning samples. However, the poisoning effect can inadvertently spread to\nnon-targeted prompts, leading to undesirable side effects. Root cause analysis\nidentifies conceptual similarity as an important contributing factor to the\nside effects. To address this, we introduce the utility-preserving attack as a\nviable mitigation strategy to maintain the attack stealthiness, while ensuring\ndecent attack performance. Our findings underscore the potential risks of\nadopting text-to-image models in real-world scenarios, calling for future\nresearch and safety measures in this space.",
        "translated": "像稳定扩散这样的文本到图像模型已经对日常生活产生了深远的影响，因为它可以通过文本提示生成逼真的图像，培养创造力，并增强各种应用程序的视觉体验。然而，这些模型也带来了风险。先前的研究已经成功地证明，人为操纵的提示可以引发文本到图像的模型，从而产生不安全的图像，例如，可恶的模因变体。然而，这些研究只是以一种被动的方式释放了文本到图像模型的有害力量。在这项工作中，我们的重点是主动生成不安全的图像使用有针对性的良性提示通过中毒攻击。我们提出两种中毒攻击: 一种是基本攻击，一种是效用保持攻击。我们使用四个有代表性的仇恨模因和多个查询提示来定性和定量地评估所提议的攻击。实验结果表明，即使有五个中毒样本，文本到图像模型也很容易受到基本攻击。然而，中毒效应可能无意中传播到非目标提示，导致不良的副作用。根本原因分析认为概念上的相似性是副作用的一个重要影响因素。为了解决这个问题，我们引入了效用保持攻击作为一个可行的缓解策略，以保持攻击的隐蔽性，同时确保良好的攻击性能。我们的研究结果强调了在现实世界中采用文本到图像模型的潜在风险，呼吁未来在这一领域进行研究和采取安全措施。"
    },
    {
        "title": "Flow-Attention-based Spatio-Temporal Aggregation Network for 3D Mask\n  Detection",
        "url": "http://arxiv.org/abs/2310.16569v1",
        "pub_date": "2023-10-25",
        "summary": "Anti-spoofing detection has become a necessity for face recognition systems\ndue to the security threat posed by spoofing attacks. Despite great success in\ntraditional attacks, most deep-learning-based methods perform poorly in 3D\nmasks, which can highly simulate real faces in appearance and structure,\nsuffering generalizability insufficiency while focusing only on the spatial\ndomain with single frame input. This has been mitigated by the recent\nintroduction of a biomedical technology called rPPG (remote\nphotoplethysmography). However, rPPG-based methods are sensitive to noisy\ninterference and require at least one second (&gt; 25 frames) of observation time,\nwhich induces high computational overhead. To address these challenges, we\npropose a novel 3D mask detection framework, called FASTEN\n(Flow-Attention-based Spatio-Temporal aggrEgation Network). We tailor the\nnetwork for focusing more on fine-grained details in large movements, which can\neliminate redundant spatio-temporal feature interference and quickly capture\nsplicing traces of 3D masks in fewer frames. Our proposed network contains\nthree key modules: 1) a facial optical flow network to obtain non-RGB\ninter-frame flow information; 2) flow attention to assign different\nsignificance to each frame; 3) spatio-temporal aggregation to aggregate\nhigh-level spatial features and temporal transition features. Through extensive\nexperiments, FASTEN only requires five frames of input and outperforms eight\ncompetitors for both intra-dataset and cross-dataset evaluations in terms of\nmultiple detection metrics. Moreover, FASTEN has been deployed in real-world\nmobile devices for practical 3D mask detection.",
        "translated": "由于欺骗攻击带来的安全威胁，防欺骗检测已经成为人脸识别系统的必需。尽管在传统的攻击中取得了巨大的成功，但是大多数基于深度学习的方法在3D 掩模中表现不佳，能够在外观和结构上高度模拟真实人脸，在单帧输入的情况下只关注空间域，存在泛化能力不足的问题。最近，一种名为 rpPG (远程光体积描记术)的生物医学技术的出现缓解了这种情况。然而，基于 rPPG 的方法对噪声干扰很敏感，需要至少一秒(> 25帧)的观测时间，这导致了较高的计算开销。为了应对这些挑战，我们提出了一种新的三维面具检测框架，称为 FASTEN (基于流注意的时空聚合网络)。该方法能够消除冗余的时空特征干扰，在较少的帧数下快速捕获三维掩模的拼接轨迹。该网络包括三个关键模块: 1)面部光流网络获取非 RGB 的帧间流信息; 2)流注意力分配不同的意义给每一帧; 3)时空聚合聚集高层次的空间特征和时间过渡特征。通过大量的实验，FASTEN 只需要5帧的输入，在多检测指标方面，在数据集内和跨数据集评估方面优于8个竞争对手。此外，FASTEN 已经部署在现实世界的移动设备中，用于实际的3D 掩模检测。"
    },
    {
        "title": "Trustworthy Cross-Border Interoperable Identity System for Developing\n  Countries",
        "url": "http://arxiv.org/abs/2310.16562v1",
        "pub_date": "2023-10-25",
        "summary": "Foundational identity systems (FIDS) have been used to optimise service\ndelivery and inclusive economic growth in developing countries. As developing\nnations increasingly seek to use FIDS for the identification and authentication\nof identity (ID) holders, trustworthy interoperability will help to develop a\ncross-border dimension of e-Government. Despite this potential, there has not\nbeen any significant research on the interoperability of FIDS in the African\nidentity ecosystem. There are several challenges to this; on one hand, complex\ninternal political dynamics have resulted in weak institutions, implying that\nFIDS could be exploited for political gains. On the other hand, the trust in\nthe government by the citizens or ID holders is habitually low, in which case,\ndata security and privacy protection concerns become paramount. In the same\nsense, some FIDS are technology-locked, thus interoperability is primarily\nambiguous. There are also issues of cross-system compatibility, legislation,\nvendor-locked system design principles and unclear regulatory provisions for\ndata sharing. Fundamentally, interoperability is an essential prerequisite for\ne-Government services and underpins optimal service delivery in education,\nsocial security, and financial services including gender and equality as\nalready demonstrated by the European Union. Furthermore, cohesive data exchange\nthrough an interoperable identity system will create an ecosystem of efficient\ndata governance and the integration of cross-border FIDS. Consequently, this\nresearch identifies the challenges, opportunities, and requirements for\ncross-border interoperability in an African context. Our findings show that\ninteroperability in the African identity ecosystem is vital to strengthen the\nseamless authentication and verification of ID holders for inclusive economic\ngrowth and widen the dimensions of e-Government across the continent.",
        "translated": "基础身份识别系统(FIDS)已被用于优化发展中国家的服务提供和包容性经济增长。随着发展中国家越来越多地寻求使用 FIDS 来识别和认证身份(ID)持有人，可靠的互操作性将有助于发展电子政务的跨境层面。尽管存在这种潜力，但对于非洲身份生态系统中 FIDS 的互操作性还没有任何重要的研究。这方面存在几个挑战; 一方面，复杂的内部政治动态导致机构薄弱，这意味着可以利用金融发展信息系统获取政治利益。另一方面，公民或身份证持有者对政府的信任度通常较低，在这种情况下，数据安全和隐私保护问题就显得尤为重要。在同样的意义上，一些 FIDS 是技术锁定的，因此互操作性主要是模棱两可的。此外，还存在跨系统兼容性、立法、供应商锁定系统设计原则以及数据共享监管规定不明确等问题。从根本上说，互操作性是电子政务服务的一个必要先决条件，是教育、社会保障和金融服务(包括性别和平等)提供最佳服务的基础，欧洲联盟已经证明了这一点。此外，通过可互操作的身份系统进行协调一致的数据交换，将创造一个有效数据治理和跨界金融数据发布系统一体化的生态系统。因此，本研究确定了在非洲背景下跨界互操作性的挑战、机遇和要求。我们的研究结果表明，非洲身份生态系统的互操作性对于加强身份证持有者的无缝认证和核查以促进包容性经济增长和扩大整个非洲大陆电子政务的范围至关重要。"
    },
    {
        "title": "Toward Practical Privacy-Preserving Convolutional Neural Networks\n  Exploiting Fully Homomorphic Encryption",
        "url": "http://arxiv.org/abs/2310.16530v1",
        "pub_date": "2023-10-25",
        "summary": "Incorporating fully homomorphic encryption (FHE) into the inference process\nof a convolutional neural network (CNN) draws enormous attention as a viable\napproach for achieving private inference (PI). FHE allows delegating the entire\ncomputation process to the server while ensuring the confidentiality of\nsensitive client-side data. However, practical FHE implementation of a CNN\nfaces significant hurdles, primarily due to FHE's substantial computational and\nmemory overhead. To address these challenges, we propose a set of\noptimizations, which includes GPU/ASIC acceleration, an efficient activation\nfunction, and an optimized packing scheme. We evaluate our method using the\nResNet models on the CIFAR-10 and ImageNet datasets, achieving several orders\nof magnitude improvement compared to prior work and reducing the latency of the\nencrypted CNN inference to 1.4 seconds on an NVIDIA A100 GPU. We also show that\nthe latency drops to a mere 0.03 seconds with a custom hardware design.",
        "translated": "将完全同态加密(fHE)整合到卷积神经网络(CNN)的推理过程中，作为一种实现私人推理(PI)的可行方法，引起了巨大的关注。FHE 允许将整个计算过程委托给服务器，同时确保敏感客户端数据的机密性。然而，实际的 FHE 实施 CNN 面临着重大的障碍，主要是由于 FHE 的大量计算和内存开销。为了应对这些挑战，我们提出了一套优化方案，其中包括 GPU/ASIC 加速、高效的激活函数和优化的包装方案。我们使用 CIFAR-10和 ImageNet 数据集上的 ResNet 模型来评估我们的方法，与以前的工作相比，实现了几个数量级的改进，并且在 NVIDIA a100图形处理器上将加密的 CNN 推断的延迟降低到1.4秒。我们还展示了通过自定义硬件设计，延迟降至仅0.03秒。"
    },
    {
        "title": "RIPencapsulation: Defeating IP Encapsulation on TI MSP Devices",
        "url": "http://arxiv.org/abs/2310.16433v1",
        "pub_date": "2023-10-25",
        "summary": "Internet of Things (IoT) devices sit at the intersection of unwieldy software\ncomplexity and unprecedented attacker access. This unique position comes with a\ndaunting security challenge: how can I protect both proprietary code and\nconfidential data on a device that the attacker has unfettered access to?\nTrusted Execution Environments (TEEs) promise to solve this challenge through\nhardware-based separation of trusted and untrusted computation and data. While\nTEEs do an adequate job of protecting secrets on desktop-class devices, we\nreveal that trade-offs made in one of the most widely-used commercial IoT\ndevices undermine their TEE's security.\n  This paper uncovers two fundamental weaknesses in IP Encapsulation (IPE), the\nTEE deployed by Texas Instruments for MSP430 and MSP432 devices. We observe\nthat lack of call site enforcement and residual state after unexpected TEE\nexits enable an attacker to reveal all proprietary code and secret data within\nthe IPE. We design and implement an attack called RIPencapsulation, which\nsystematically executes portions of code within the IPE and uses the partial\nstate revealed through the register file to exfiltrate secret data and to\nidentify gadget instructions. The attack then uses gadget instructions to\nreveal all proprietary code within the IPE. Our evaluation with commodity\ndevices and a production compiler and settings shows that -- even after\nfollowing all manufacturer-recommended secure coding practices --\nRIPencapsultaion reveals, within minutes, both the code and keys from\nthird-party cryptographic implementations protected by the IPE.",
        "translated": "物联网(IoT)设备处于笨重的软件复杂性和前所未有的攻击者访问的交叉点上。这个独特的位置伴随着一个令人畏缩的安全挑战: 我如何保护一个设备上的专有代码和机密数据，攻击者可以不受限制地访问？可信执行环境(Trusted Execution Environment，TEE)承诺通过基于硬件的可信和不可信计算与数据分离来解决这一挑战。尽管 TEE 在保护桌面类设备的机密方面做得很好，但我们发现，在一个最广泛使用的商业物联网设备上做出的权衡会破坏 TEE 的安全性。本文揭示了 IP 封装(IPE)的两个基本弱点，即德州仪器公司部署在 MSP430和 MSP432设备上的 TEE。我们观察到，缺乏调用站点强制和在意外的 TEE 退出之后的残留状态使攻击者能够揭示 IPE 中的所有专有代码和秘密数据。我们设计并实现了一种称为 RIPcap 的攻击，它系统地执行 IPE 中的部分代码，并使用通过寄存器文件显示的部分状态来提取秘密数据并识别小工具指令。然后，攻击使用小工具指令来显示 IPE 中的所有专有代码。我们对商品设备和产品编译器的评估和设置表明，即使遵循了所有制造商推荐的安全编码实践，RIP 封装也能在几分钟内揭示受 IPE 保护的第三方加密实现的代码和密钥。"
    },
    {
        "title": "Challenges of Radio Frequency Fingerprinting: From Data Collection to\n  Deployment",
        "url": "http://arxiv.org/abs/2310.16406v1",
        "pub_date": "2023-10-25",
        "summary": "Radio Frequency Fingerprinting (RFF) techniques promise to authenticate\nwireless devices at the physical layer based on inherent hardware imperfections\nintroduced during manufacturing. Such RF transmitter imperfections are\nreflected into over-the-air signals, allowing receivers to accurately identify\nthe RF transmitting source. Recent advances in Machine Learning, particularly\nin Deep Learning (DL), have improved the ability of RFF systems to extract and\nlearn complex features that make up the device-specific fingerprint. However,\nintegrating DL techniques with RFF and operating the system in real-world\nscenarios presents numerous challenges. This article identifies and analyzes\nthese challenges while considering the three reference phases of any DL-based\nRFF system: (i) data collection and preprocessing, (ii) training, and finally,\n(iii) deployment. Our investigation points out the current open problems that\nprevent real deployment of RFF while discussing promising future directions,\nthus paving the way for further research in the area.",
        "translated": "无线射频指纹(RFF)技术基于制造过程中引入的固有硬件缺陷，承诺在物理层对无线设备进行认证。这种射频发射机的缺陷反映到空中信号中，使接收机能够准确地识别射频发射源。机器学习的最新进展，特别是在深度学习(DL)方面，已经提高了 RFF 系统提取和学习构成特定设备指纹的复杂特征的能力。然而，将 DL 技术与 RFF 相结合并在真实场景中操作系统会带来许多挑战。本文在考虑任何基于 DL 的 RFF 系统的三个参考阶段时，识别并分析了这些挑战: (i)数据收集和预处理，(ii)培训，以及(iii)部署。我们的调查指出了目前存在的问题，阻碍了 RFF 的真正部署，同时讨论了有希望的未来方向，从而为该领域的进一步研究铺平了道路。"
    },
    {
        "title": "Enhancing Large Language Models for Secure Code Generation: A\n  Dataset-driven Study on Vulnerability Mitigation",
        "url": "http://arxiv.org/abs/2310.16263v1",
        "pub_date": "2023-10-25",
        "summary": "Large language models (LLMs) have brought significant advancements to code\ngeneration, benefiting both novice and experienced developers. However, their\ntraining using unsanitized data from open-source repositories, like GitHub,\nintroduces the risk of inadvertently propagating security vulnerabilities. To\neffectively mitigate this concern, this paper presents a comprehensive study\nfocused on evaluating and enhancing code LLMs from a software security\nperspective. We introduce SecuCoGen\\footnote{SecuCoGen has been uploaded as\nsupplemental material and will be made publicly available after publication.},\na meticulously curated dataset targeting 21 critical vulnerability types.\nSecuCoGen comprises 180 samples and serves as the foundation for conducting\nexperiments on three crucial code-related tasks: code generation, code repair\nand vulnerability classification, with a strong emphasis on security. Our\nexperimental results reveal that existing models often overlook security\nconcerns during code generation, leading to the generation of vulnerable code.\nTo address this, we propose effective approaches to mitigate the security\nvulnerabilities and enhance the overall robustness of code generated by LLMs.\nMoreover, our study identifies weaknesses in existing models' ability to repair\nvulnerable code, even when provided with vulnerability information.\nAdditionally, certain vulnerability types pose challenges for the models,\nhindering their performance in vulnerability classification. Based on these\nfindings, we believe our study will have a positive impact on the software\nengineering community, inspiring the development of improved methods for\ntraining and utilizing LLMs, thereby leading to safer and more trustworthy\nmodel deployment.",
        "translated": "大型语言模型(LLM)为代码生成带来了显著的进步，使新手和经验丰富的开发人员都受益。然而，他们使用来自开源存储库(如 GitHub)的未经过消毒的数据进行培训，会带来无意中传播安全漏洞的风险。为了有效地缓解这种担忧，本文从软件安全的角度对代码 LLM 的评估和增强进行了全面的研究。我们介绍 SecuCoGen 脚注{ SecuCoGen 已作为补充材料上传，并将在发布后公开发布。}一个精心策划的针对21种关键脆弱性类型的数据集。SecuCoGen 包含180个样本，是进行三个关键代码相关任务的实验的基础: 代码生成、代码修复和漏洞分类，并强调安全性。我们的实验结果表明，现有的模型在代码生成过程中往往忽略了安全问题，导致了易受攻击的代码的生成。为了解决这个问题，我们提出了有效的方法来减轻安全漏洞，并增强由 LLM 生成的代码的整体健壮性。此外，我们的研究还发现了现有模型在修复易受攻击代码方面的弱点，即使提供了易受攻击的代码信息。此外，某些脆弱性类型对模型提出了挑战，阻碍了它们在脆弱性分类中的性能。基于这些发现，我们相信我们的研究将对软件工程社区产生积极的影响，激励开发改进的培训和使用 LLM 的方法，从而导致更安全和更可靠的模型部署。"
    },
    {
        "title": "Poison is Not Traceless: Fully-Agnostic Detection of Poisoning Attacks",
        "url": "http://arxiv.org/abs/2310.16224v1",
        "pub_date": "2023-10-24",
        "summary": "The performance of machine learning models depends on the quality of the\nunderlying data. Malicious actors can attack the model by poisoning the\ntraining data. Current detectors are tied to either specific data types,\nmodels, or attacks, and therefore have limited applicability in real-world\nscenarios. This paper presents a novel fully-agnostic framework, DIVA\n(Detecting InVisible Attacks), that detects attacks solely relying on analyzing\nthe potentially poisoned data set. DIVA is based on the idea that poisoning\nattacks can be detected by comparing the classifier's accuracy on poisoned and\nclean data and pre-trains a meta-learner using Complexity Measures to estimate\nthe otherwise unknown accuracy on a hypothetical clean dataset. The framework\napplies to generic poisoning attacks. For evaluation purposes, in this paper,\nwe test DIVA on label-flipping attacks.",
        "translated": "机器学习模型的性能取决于底层数据的质量。恶意的参与者可以通过毒害训练数据来攻击模型。当前检测器与特定的数据类型、模型或攻击相关联，因此在实际场景中的适用性有限。本文提出了一种新的完全不可知框架 DIVA，它仅仅依靠分析潜在的中毒数据集来检测攻击。DIVA 的基本思想是，中毒攻击可以通过比较分类器对有毒和清洁数据的准确性来检测，并预先训练元学习者使用复杂性措施来估计假设的清洁数据集的未知准确性。该框架适用于一般的中毒攻击。出于评估的目的，本文测试了 DIVA 对标签翻转攻击的应用。"
    },
    {
        "title": "Defending Against Transfer Attacks From Public Models",
        "url": "http://arxiv.org/abs/2310.17645v1",
        "pub_date": "2023-10-26",
        "summary": "Adversarial attacks have been a looming and unaddressed threat in the\nindustry. However, through a decade-long history of the robustness evaluation\nliterature, we have learned that mounting a strong or optimal attack is\nchallenging. It requires both machine learning and domain expertise. In other\nwords, the white-box threat model, religiously assumed by a large majority of\nthe past literature, is unrealistic. In this paper, we propose a new practical\nthreat model where the adversary relies on transfer attacks through publicly\navailable surrogate models. We argue that this setting will become the most\nprevalent for security-sensitive applications in the future. We evaluate the\ntransfer attacks in this setting and propose a specialized defense method based\non a game-theoretic perspective. The defenses are evaluated under 24 public\nmodels and 11 attack algorithms across three datasets (CIFAR-10, CIFAR-100, and\nImageNet). Under this threat model, our defense, PubDef, outperforms the\nstate-of-the-art white-box adversarial training by a large margin with almost\nno loss in the normal accuracy. For instance, on ImageNet, our defense achieves\n62% accuracy under the strongest transfer attack vs only 36% of the best\nadversarially trained model. Its accuracy when not under attack is only 2%\nlower than that of an undefended model (78% vs 80%). We release our code at\nhttps://github.com/wagner-group/pubdef.",
        "translated": "对抗性攻击已经成为这个行业迫在眉睫的未解决的威胁。然而，通过长达十年的健壮性评估文献，我们已经了解到，安装一个强大的或最佳的攻击是具有挑战性的。它需要机器学习和领域专业知识。换句话说，过去绝大多数文献虔诚地假设的白盒威胁模型是不现实的。在本文中，我们提出了一个新的实用的威胁模型，其中对手依赖转移攻击通过公开可用的代理模型。我们认为，这种设置将成为未来安全敏感应用程序中最普遍的设置。我们评估在这种情况下的转移攻击，并提出了一个专门的防御方法的基础上的博弈论的观点。这些防御系统在三个数据集(CIFAR-10、 CIFAR-100和 ImageNet)的24个公共模型和11个攻击算法下进行评估。在这种威胁模式下，我们的辩护方 PubDef 在正常准确度几乎没有损失的情况下，大大优于最先进的白盒对抗训练。例如，在 ImageNet 上，我们的防御在最强的转移攻击下达到62% 的准确率，而在最好的对抗性训练模型下仅达到36% 。在不受攻击的情况下，它的命中率只比不设防的模型低2% (78% 比80%)。我们在 https://github.com/wagner-group/pubdef 发布我们的代码。"
    },
    {
        "title": "SoK: Pitfalls in Evaluating Black-Box Attacks",
        "url": "http://arxiv.org/abs/2310.17534v1",
        "pub_date": "2023-10-26",
        "summary": "Numerous works study black-box attacks on image classifiers. However, these\nworks make different assumptions on the adversary's knowledge and current\nliterature lacks a cohesive organization centered around the threat model. To\nsystematize knowledge in this area, we propose a taxonomy over the threat space\nspanning the axes of feedback granularity, the access of interactive queries,\nand the quality and quantity of the auxiliary data available to the attacker.\nOur new taxonomy provides three key insights. 1) Despite extensive literature,\nnumerous under-explored threat spaces exist, which cannot be trivially solved\nby adapting techniques from well-explored settings. We demonstrate this by\nestablishing a new state-of-the-art in the less-studied setting of access to\ntop-k confidence scores by adapting techniques from well-explored settings of\naccessing the complete confidence vector, but show how it still falls short of\nthe more restrictive setting that only obtains the prediction label,\nhighlighting the need for more research. 2) Identification the threat model of\ndifferent attacks uncovers stronger baselines that challenge prior\nstate-of-the-art claims. We demonstrate this by enhancing an initially weaker\nbaseline (under interactive query access) via surrogate models, effectively\noverturning claims in the respective paper. 3) Our taxonomy reveals\ninteractions between attacker knowledge that connect well to related areas,\nsuch as model inversion and extraction attacks. We discuss how advances in\nother areas can enable potentially stronger black-box attacks. Finally, we\nemphasize the need for a more realistic assessment of attack success by\nfactoring in local attack runtime. This approach reveals the potential for\ncertain attacks to achieve notably higher success rates and the need to\nevaluate attacks in diverse and harder settings, highlighting the need for\nbetter selection criteria.",
        "translated": "许多著作都研究了图像分类器的黑盒攻击。然而，这些作品对对手的知识做出了不同的假设，当前的文献缺乏一个以威胁模型为中心的内聚性组织。为了使该领域的知识系统化，我们提出了一种威胁空间分类法，它跨越反馈粒度、交互查询的访问以及攻击者可用的辅助数据的质量和数量等轴。我们的新分类法提供了三个关键的见解。1)尽管有大量的文献，但仍然存在许多未被充分探索的威胁空间，这些空间不能通过从已经探索过的环境中采用技术来解决。我们通过在研究较少的获取 top-k 置信度分数的设置中建立一个新的最先进的设置来证明这一点，通过适应来自获取完全置信向量的充分探索的设置的技术，但是显示它仍然没有达到更多的限制性设置，只获得预测标签，强调需要更多的研究。2)识别不同攻击的威胁模型揭示了挑战先进声明的更强的基线。我们通过代理模型增强最初较弱的基线(在交互式查询访问下)来证明这一点，有效地推翻了相关论文中的主张。3)我们的分类揭示了攻击者知识与相关领域之间的交互作用，如模型反演和抽取攻击。我们讨论如何在其他领域的进步，可以使潜在的更强大的黑盒攻击。最后，我们强调有必要通过在本地攻击运行时中分解来对攻击成功性进行更加现实的评估。这种办法表明，某些攻击有可能取得显著较高的成功率，并需要在多样和艰难的环境中评估攻击，突出表明需要更好的选择标准。"
    },
    {
        "title": "CBD: A Certified Backdoor Detector Based on Local Dominant Probability",
        "url": "http://arxiv.org/abs/2310.17498v1",
        "pub_date": "2023-10-26",
        "summary": "Backdoor attack is a common threat to deep neural networks. During testing,\nsamples embedded with a backdoor trigger will be misclassified as an\nadversarial target by a backdoored model, while samples without the backdoor\ntrigger will be correctly classified. In this paper, we present the first\ncertified backdoor detector (CBD), which is based on a novel, adjustable\nconformal prediction scheme based on our proposed statistic local dominant\nprobability. For any classifier under inspection, CBD provides 1) a detection\ninference, 2) the condition under which the attacks are guaranteed to be\ndetectable for the same classification domain, and 3) a probabilistic upper\nbound for the false positive rate. Our theoretical results show that attacks\nwith triggers that are more resilient to test-time noise and have smaller\nperturbation magnitudes are more likely to be detected with guarantees.\nMoreover, we conduct extensive experiments on four benchmark datasets\nconsidering various backdoor types, such as BadNet, CB, and Blend. CBD achieves\ncomparable or even higher detection accuracy than state-of-the-art detectors,\nand it in addition provides detection certification. Notably, for backdoor\nattacks with random perturbation triggers bounded by $\\ell_2\\leq0.75$ which\nachieves more than 90\\% attack success rate, CBD achieves 100\\% (98\\%), 100\\%\n(84\\%), 98\\% (98\\%), and 72\\% (40\\%) empirical (certified) detection true\npositive rates on the four benchmark datasets GTSRB, SVHN, CIFAR-10, and\nTinyImageNet, respectively, with low false positive rates.",
        "translated": "后门攻击是深度神经网络的常见威胁。在测试过程中，带有后门触发器的样品将被后门模型错误地分类为敌对目标，而没有后门触发器的样品将被正确分类。在本文中，我们提出了第一个认证后门检测器(CBD) ，它是基于一个新颖的，可调的保形预测方案的基础上，我们提出的统计局部主导概率。对于任何被检测的分类器，CBD 提供1)检测推断，2)在同一分类域中保证可检测到攻击的条件，3)假阳性率的概率上界。我们的理论结果表明，触发器的攻击更有弹性的测试时间噪声和具有较小的扰动幅度更有可能检测与保证。此外，我们在四个基准数据集上进行了广泛的实验，考虑了各种后门类型，如 BadNet、 CB 和 Blend。CBD 比最先进的检测器获得了相当甚至更高的检测精度，此外它还提供了检测认证。值得注意的是，对于以 $ell _ 2 leq0.75 $为界的随机扰动触发的后门攻击，其达到90% 以上的攻击成功率，CBD 分别达到100% (98%) ，100% (84%) ，98% (98%)和72% (40%)经验(认证)检测真实阳性率在四个基准数据集 GTSRB，SVHN，CIFAR-10和 TinyImageNet 上，假阳性率较低。"
    },
    {
        "title": "A near-autonomous and incremental intrusion detection system through\n  active learning of known and unknown attacks",
        "url": "http://arxiv.org/abs/2310.17430v1",
        "pub_date": "2023-10-26",
        "summary": "Intrusion detection is a traditional practice of security experts, however,\nthere are several issues which still need to be tackled. Therefore, in this\npaper, after highlighting these issues, we present an architecture for a hybrid\nIntrusion Detection System (IDS) for an adaptive and incremental detection of\nboth known and unknown attacks. The IDS is composed of supervised and\nunsupervised modules, namely, a Deep Neural Network (DNN) and the K-Nearest\nNeighbors (KNN) algorithm, respectively. The proposed system is near-autonomous\nsince the intervention of the expert is minimized through the active learning\n(AL) approach. A query strategy for the labeling process is presented, it aims\nat teaching the supervised module to detect unknown attacks and improve the\ndetection of the already-known attacks. This teaching is achieved through\nsliding windows (SW) in an incremental fashion where the DNN is retrained when\nthe data is available over time, thus rendering the IDS adaptive to cope with\nthe evolutionary aspect of the network traffic. A set of experiments was\nconducted on the CICIDS2017 dataset in order to evaluate the performance of the\nIDS, promising results were obtained.",
        "translated": "入侵检测是安全专家的传统做法，然而，还有几个问题需要解决。因此，在本文中，在强调了这些问题之后，我们提出了一个混合入侵预防系统(IDS)的架构，用于对已知和未知攻击的自适应和增量检测。入侵检测系统由监督模块和非监督模块组成，分别是深度神经网络(DNN)和 K 近邻算法(KNN)。该系统通过主动学习(AL)方法使专家的干预最小化，从而实现了系统的近自治。提出了一种针对标记过程的查询策略，目的是教导监督模块检测未知攻击，提高对已知攻击的检测能力。这种教学是通过滑动窗口(SW)以增量的方式实现的，当随着时间的推移数据可用时，DNN 被重新训练，从而使入侵检测系统适应于处理网络流量的演化方面。为了评估入侵检测系统的性能，在 CICIDS2017数据集上进行了一系列实验，取得了良好的效果。"
    },
    {
        "title": "Network Intrusion Detection with Edge-Directed Graph Multi-Head\n  Attention Networks",
        "url": "http://arxiv.org/abs/2310.17348v1",
        "pub_date": "2023-10-26",
        "summary": "A network intrusion usually involves a number of network locations. Data flow\n(including the data generated by intrusion behaviors) among these locations\n(usually represented by IP addresses) naturally forms a graph. Thus, graph\nneural networks (GNNs) have been used in the construction of intrusion\ndetection models in recent years since they have an excellent ability to\ncapture graph topological features of intrusion data flow. However, existing\nGNN models treat node mean aggregation equally in node information aggregation.\nIn reality, the correlations of nodes and their neighbors as well as the linked\nedges are different. Assigning higher weights to nodes and edges with high\nsimilarity can highlight the correlation among them, which will enhance the\naccuracy and expressiveness of the model. To this end, this paper proposes\nnovel Edge-Directed Graph Multi-Head Attention Networks (EDGMAT) for network\nintrusion detection. The proposed EDGMAT model introduces a multi-head\nattention mechanism into the intrusion detection model. Additional weight\nlearning is realized through the combination of a multi-head attention\nmechanism and edge features. Weighted aggregation makes better use of the\nrelationship between different network traffic data. Experimental results on\nfour recent NIDS benchmark datasets show that the performance of EDGMAT in\nterms of weighted F1-Score is significantly better than that of four\nstate-of-the-art models in multi-class detection tasks.",
        "translated": "网络入侵通常涉及多个网络位置。这些位置之间的数据流(包括入侵行为产生的数据)(通常由 IP 地址表示)自然形成一个图。因此，图形神经网络(GNN)由于能够很好地捕获入侵数据流的图形拓扑特征，近年来被广泛应用于入侵检测模型的构建中。然而，现有的 GNN 模型在节点信息聚合中对节点均值聚合是一视同仁的。实际情况中，节点与其邻居以及连接边的相关性是不同的。对高相似度的节点和边赋予较高的权重，可以突出它们之间的相关性，从而提高模型的准确性和表达能力。为此，本文提出了一种用于网络入侵检测的新型边指向图多头注意网络(EDGMAT)。提出的 EDGMAT 模型在入侵检测模型中引入了多头注意机制。通过多头注意机制和边缘特征相结合的方法实现了额外的权值学习。加权聚合更好地利用了不同网络流量数据之间的关系。对四个 NIDS 基准数据集的实验结果表明，EDGMAT 在多类检测任务中的加权 F1-Score 性能明显优于四个最新的模型。"
    },
    {
        "title": "Static Semantics Reconstruction for Enhancing JavaScript-WebAssembly\n  Multilingual Malware Detection",
        "url": "http://arxiv.org/abs/2310.17304v1",
        "pub_date": "2023-10-26",
        "summary": "The emergence of WebAssembly allows attackers to hide the malicious\nfunctionalities of JavaScript malware in cross-language interoperations, termed\nJavaScript-WebAssembly multilingual malware (JWMM). However, existing\nanti-virus solutions based on static program analysis are still limited to\nmonolingual code. As a result, their detection effectiveness decreases\nsignificantly against JWMM. The detection of JWMM is challenging due to the\ncomplex interoperations and semantic diversity between JavaScript and\nWebAssembly. To bridge this gap, we present JWBinder, the first technique aimed\nat enhancing the static detection of JWMM. JWBinder performs a\nlanguage-specific data-flow analysis to capture the cross-language\ninteroperations and then characterizes the functionalities of JWMM through a\nunified high-level structure called Inter-language Program Dependency Graph.\nThe extensive evaluation on one of the most representative real-world\nanti-virus platforms, VirusTotal, shows that \\system effectively enhances\nanti-virus systems from various vendors and increases the overall successful\ndetection rate against JWMM from 49.1\\% to 86.2\\%. Additionally, we assess the\nside effects and runtime overhead of JWBinder, corroborating its practical\nviability in real-world applications.",
        "translated": "WebAssembly 的出现允许攻击者在跨语言的互操作中隐藏 JavaScript 恶意软件的恶意功能，称为 JavaScript-WebAssembly 多语言恶意软件(JWMM)。然而，现有的基于静态程序分析的防病毒解决方案仍然局限于单语言代码。因此，它们对 JWMM 的检测效率显著降低。由于 JavaScript 和 WebAssembly 之间复杂的互操作和语义多样性，JWMM 的检测具有挑战性。为了弥补这一差距，我们提出了 JWBinder，这是第一种旨在增强 JWMM 静态检测的技术。JWbinder 执行一个特定于语言的数据流分析来捕捉跨语言的互操作，然后通过一个统一的高级结构——中间语言程序依赖图(Inter-language Program Dependency Graph)来描述 JWMM 的功能。对最具代表性的现实世界反病毒平台之一 VirusTotal 的广泛评估表明，该系统有效地增强了来自不同厂商的反病毒系统，并将针对 JWMM 的总体成功检测率从49.1% 提高到86.2% 。此外，我们还评估了 JWBinder 的副作用和运行时开销，证实了它在实际应用中的实际可行性。"
    },
    {
        "title": "Redactable and Sanitizable Signature Schemes: Applications and\n  Limitations for use in Decentralized Digital Identity Systems",
        "url": "http://arxiv.org/abs/2310.17297v1",
        "pub_date": "2023-10-26",
        "summary": "Redactable signature schemes and sanitizable signature schemes are methods\nthat permit modification of a given digital message and retain a valid\nsignature. This can be applied to decentralized identity systems for delegating\nidentity issuance and redacting sensitive information for privacy-preserving\nverification of identity. We propose implementing these protocols on a digital\ncredential and compare them against other privacy-enhancing techniques to\nassess their suitability",
        "translated": "可编校签名方案和可消毒签名方案是允许修改给定数字电文并保留有效签名的方法。这可以应用于分散的身份系统，用于授权发放身份和编辑敏感信息，以保护隐私，核实身份。我们建议在数字证书上实现这些协议，并将其与其他隐私增强技术进行比较，以评估其适用性"
    },
    {
        "title": "A Method for Network Intrusion Detection Using Flow Sequence and BERT\n  Framework",
        "url": "http://arxiv.org/abs/2310.17127v1",
        "pub_date": "2023-10-26",
        "summary": "A Network Intrusion Detection System (NIDS) is a tool that identifies\npotential threats to a network. Recently, different flow-based NIDS designs\nutilizing Machine Learning (ML) algorithms have been proposed as solutions to\ndetect intrusions efficiently. However, conventional ML-based classifiers have\nnot seen widespread adoption in the real world due to their poor domain\nadaptation capability. In this research, our goal is to explore the possibility\nof using sequences of flows to improve the domain adaptation capability of\nnetwork intrusion detection systems. Our proposal employs natural language\nprocessing techniques and Bidirectional Encoder Representations from\nTransformers framework, which is an effective technique for modeling data with\nrespect to its context. Early empirical results show that our approach has\nimproved domain adaptation capability compared to previous approaches. The\nproposed approach provides a new research method for building a robust\nintrusion detection system.",
        "translated": "网络入侵预防系统(nIDS)是一种识别网络潜在威胁的工具。最近，利用机器学习(ML)算法的不同的基于流的 NIDS 设计被提出来作为有效检测入侵的解决方案。然而，传统的基于机器学习的分类器由于其领域适应能力较差，在现实世界中还没有得到广泛的应用。在本研究中，我们的目标是探讨利用流序列来改善网路入侵侦测系统的领域适应能力的可能性。我们的方案采用了自然语言处理技术和来自 Transformers 框架的双向编码器表示，这是一种有效的数据建模技术。早期的实验结果表明，与以前的方法相比，我们的方法提高了领域自适应能力。提出的方法为建立稳健的入侵预防系统提供了一种新的研究方法。"
    },
    {
        "title": "Proving the Absence of Microarchitectural Timing Channels",
        "url": "http://arxiv.org/abs/2310.17046v1",
        "pub_date": "2023-10-25",
        "summary": "Microarchitectural timing channels are a major threat to computer security. A\nset of OS mechanisms called time protection was recently proposed as a\nprincipled way of preventing information leakage through such channels and\nprototyped in the seL4 microkernel. We formalise time protection and the\nunderlying hardware mechanisms in a way that allows linking them to the\ninformation-flow proofs that showed the absence of storage channels in seL4.",
        "translated": "微结构时序通道是计算机安全的主要威胁。最近提出了一套称为时间保护的操作系统机制，作为防止信息泄露通过这种通道的一种原则性方法，并在 sel4微内核中进行了原型设计。我们将时间保护和底层硬件机制正式化，以便将它们与显示 seL4中存储通道缺失的信息流证明联系起来。"
    },
    {
        "title": "Security Patchworking in Lebanon: Infrastructuring Across Failing\n  Infrastructures",
        "url": "http://arxiv.org/abs/2310.16969v1",
        "pub_date": "2023-10-25",
        "summary": "In this paper we bring to light the infrastructuring work carried out by\npeople in Lebanon to establish and maintain everyday security in response to\nmultiple simultaneously failing infrastructures. We do so through interviews\nwith 13 participants from 12 digital and human rights organisations and two\nweeks of ethnographically informed fieldwork in Beirut, Lebanon, in July 2022.\nThrough our analysis we develop the notion of security patchworking that makes\nvisible the infrastructuring work necessitated to secure basic needs such as\nelectricity provision, identity authentication and financial resources. Such\npractices are rooted in differing mechanisms of protection that often result in\nnew forms of insecurity. We discuss the implications for CSCW and HCI\nresearchers and point to security patchworking as a lens to be used when\ndesigning technologies to support infrastructuring, while advocating for\ncollaborative work across CSCW and security research.",
        "translated": "本文件揭示了黎巴嫩人民为建立和维护日常安全而开展的基础设施工作，以应对多个同时失效的基础设施。我们是通过2022年7月在黎巴嫩贝鲁特对来自12个数字和人权组织的13名参与者进行访谈以及为期两周的人种学知识实地考察来做到这一点的。通过我们的分析，我们发展了安全补丁的概念，使人们看到必要的基础设施工作，以确保基本需求，如电力供应、身份认证和财政资源。这种做法的根源是不同的保护机制，往往导致新形式的不安全。我们讨论了对 CSCW 和 HCI 研究人员的影响，并指出安全补丁作为设计支持基础设施的技术时使用的一个透镜，同时倡导跨 CSCW 和安全研究的协作工作。"
    },
    {
        "title": "$α$-Mutual Information: A Tunable Privacy Measure for Privacy\n  Protection in Data Sharing",
        "url": "http://arxiv.org/abs/2310.18241v1",
        "pub_date": "2023-10-27",
        "summary": "This paper adopts Arimoto's $\\alpha$-Mutual Information as a tunable privacy\nmeasure, in a privacy-preserving data release setting that aims to prevent\ndisclosing private data to adversaries. By fine-tuning the privacy metric, we\ndemonstrate that our approach yields superior models that effectively thwart\nattackers across various performance dimensions. We formulate a general\ndistortion-based mechanism that manipulates the original data to offer privacy\nprotection. The distortion metrics are determined according to the data\nstructure of a specific experiment. We confront the problem expressed in the\nformulation by employing a general adversarial deep learning framework that\nconsists of a releaser and an adversary, trained with opposite goals. This\nstudy conducts empirical experiments on images and time-series data to verify\nthe functionality of $\\alpha$-Mutual Information. We evaluate the\nprivacy-utility trade-off of customized models and compare them to mutual\ninformation as the baseline measure. Finally, we analyze the consequence of an\nattacker's access to side information about private data and witness that\nadapting the privacy measure results in a more refined model than the\nstate-of-the-art in terms of resiliency against side information.",
        "translated": "本文采用 Arimoto 的 $alpha $- Mutual Information 作为可调的隐私措施，在一个保护隐私的数据发布设置中，旨在防止向对手泄露私人数据。通过对隐私度量进行微调，我们证明了我们的方法产生了优越的模型，有效地阻止了各种性能维度上的攻击者。我们制定了一个通用的基于失真的机制，操纵原始数据，以提供隐私保护。失真度量是根据特定实验的数据结构确定的。我们通过使用一般的对抗性深度学习框架来面对表述中所表达的问题，该框架由一个释放者和一个对手组成，训练目标相反。本研究利用影像与时间序列资料进行实证研究，以验证 $alpha $- Mutual 信息的功能性。我们评估定制模型的隐私-效用权衡，并将它们与相互信息作为基准测量进行比较。最后，我们分析了攻击者访问关于私有数据的侧信息的后果，并且证明了在对侧信息的弹性方面，适应隐私度量的结果比最先进的模型更精确。"
    },
    {
        "title": "Enhancing Enterprise Network Security: Comparing Machine-Level and\n  Process-Level Analysis for Dynamic Malware Detection",
        "url": "http://arxiv.org/abs/2310.18165v1",
        "pub_date": "2023-10-27",
        "summary": "Analysing malware is important to understand how malicious software works and\nto develop appropriate detection and prevention methods. Dynamic analysis can\novercome evasion techniques commonly used to bypass static analysis and provide\ninsights into malware runtime activities. Much research on dynamic analysis\nfocused on investigating machine-level information (e.g., CPU, memory, network\nusage) to identify whether a machine is running malicious activities. A\nmalicious machine does not necessarily mean all running processes on the\nmachine are also malicious. If we can isolate the malicious process instead of\nisolating the whole machine, we could kill the malicious process, and the\nmachine can keep doing its job. Another challenge dynamic malware detection\nresearch faces is that the samples are executed in one machine without any\nbackground applications running. It is unrealistic as a computer typically runs\nmany benign (background) applications when a malware incident happens. Our\nexperiment with machine-level data shows that the existence of background\napplications decreases previous state-of-the-art accuracy by about 20.12% on\naverage. We also proposed a process-level Recurrent Neural Network (RNN)-based\ndetection model. Our proposed model performs better than the machine-level\ndetection model; 0.049 increase in detection rate and a false-positive rate\nbelow 0.1.",
        "translated": "分析恶意软件对于了解恶意软件是如何工作的以及开发适当的检测和预防方法非常重要。动态分析可以克服通常用于绕过静态分析的规避技术，并提供对恶意软件运行时活动的洞察。许多动态分析的研究集中在调查机器级别的信息(例如 CPU、内存、网络使用情况) ，以确定一台机器是否正在运行恶意活动。恶意机器并不一定意味着机器上运行的所有进程都是恶意的。如果我们可以隔离恶意进程，而不是隔离整个机器，我们可以杀死恶意进程，机器可以继续做它的工作。动态恶意软件检测研究面临的另一个挑战是，样本在一台机器上执行，没有任何后台应用程序运行。这是不现实的，因为当恶意软件事件发生时，计算机通常会运行许多良性(后台)应用程序。我们对机器级数据的实验表明，背景应用程序的存在使得先前最先进的准确率平均下降了20.12% 。我们还提出了一个基于进程级递归神经网络(RNN)的检测模型。我们提出的模型比机器级检测模型表现更好，检测率提高了0.049，假阳性率低于0.1。"
    },
    {
        "title": "Sui Lutris: A Blockchain Combining Broadcast and Consensus",
        "url": "http://arxiv.org/abs/2310.18042v1",
        "pub_date": "2023-10-27",
        "summary": "Sui Lutris is the first smart-contract platform to sustainably achieve\nsub-second finality. It achieves this significant decrease in latency by\nemploying consensusless agreement not only for simple payments but for a large\nvariety of transactions. Unlike prior work, Sui Lutris neither compromises\nexpressiveness nor throughput and can run perpetually without restarts. Sui\nLutris achieves this by safely integrating consensuless agreement with a\nhigh-throughput consensus protocol that is invoked out of the critical finality\npath but makes sure that when a transaction is at risk of inconsistent\nconcurrent accesses its settlement is delayed until the total ordering is\nresolved. Building such a hybrid architecture is especially delicate during\nreconfiguration events, where the system needs to preserve the safety of the\nconsensusless path without compromising the long-term liveness of potentially\nmisconfigured clients. We thus develop a novel reconfiguration protocol, the\nfirst to show the safe and efficient reconfiguration of a consensusless\nblockchain. Sui Lutris is currently running in production as part of a major\nsmart-contract platform. Combined with the Move Programming language it enables\nthe safe execution of smart contracts that expose objects as a first-class\nresource. In our experiments Sui Lutris achieves latency lower than 0.5 seconds\nfor throughput up to 5,000 certificates per second (150k ops/s with bundling),\ncompared to the state-of-the-art real-world consensus latencies of 3 seconds.\nFurthermore, it gracefully handles validators crash-recovery and does not\nsuffer visible performance degradation during reconfiguration.",
        "translated": "Sui Lutris 是第一个可持续实现亚秒级终结的智能合同平台。它不仅对简单的付款，而且对大量的交易采用无协商一致的协议，从而大大减少了延迟。与以前的工作不同，Sui Lutris 既不损害表达性也不损害吞吐量，并且可以在不重新启动的情况下永久运行。Sui Lutris 通过安全地将协商一致协议与高通量协商一致协议相结合来实现这一点，该协议在关键的最终路径之外被调用，但是确保当事务处于不一致并发访问的风险时，其解决被延迟，直到总订单得到解决。在重新配置事件期间，建立这样一个混合架构特别棘手，因为系统需要保持无共识路径的安全性，同时不损害可能配置错误的客户端的长期活力。因此，我们开发了一个新的重构协议，第一个显示安全和有效的重构的一致性区块链。Sui Lutris 目前作为一个主要的智能合同平台的一部分在生产中运行。与 Move Programming 语言相结合，可以安全地执行将对象作为一级资源公开的智能契约。在我们的实验中，Sui Lutris 实现了低于0.5秒的延迟，吞吐量高达每秒5,000个证书(捆绑后每秒150k) ，相比之下，现实世界中最先进的共识延迟为3秒。此外，它能够很好地处理验证器的崩溃恢复，并且在重新配置期间不会出现可见的性能下降。"
    },
    {
        "title": "Temperature Monitoring of Agricultural Areas in a Secure Data Room",
        "url": "http://arxiv.org/abs/2310.18019v1",
        "pub_date": "2023-10-27",
        "summary": "Agricultural production is highly dependent on naturally occurring\nenvironmental conditions like change of seasons and the weather. Especially in\nfruit and wine growing, late frosts occurring shortly after the crops have\nsprouted have the potential to cause massive damage to plants [L1,L2] [1]. In\nthis article we present a cost-efficient temperature monitoring system for\ndetecting and reacting to late frosts to prevent crop failures. The proposed\nsolution includes a data space where Internet of Things (IoT) devices can form\na cyber-physical system (CPS) to interact with their nearby environment and\nsecurely exchange data. Based on this data, more accurate predictions can be\nmade in the future using machine learning (ML), which will further contribute\nto minimising economic damage caused by crop failures.",
        "translated": "农业生产高度依赖自然环境条件，如季节和天气的变化。特别是在水果和葡萄酒的生长中，作物发芽后不久出现的晚霜有可能对植物造成巨大的损害[ L1，L2][1]。在本文中，我们提出了一个经济有效的温度监测系统，检测和反应的晚霜，以防止作物倒闭。提出的解决方案包括一个数据空间，物联网(IoT)设备可以在其中形成一个网宇实体系统(CPS) ，与周围环境进行交互，并安全地交换数据。基于这些数据，未来可以利用机器学习(ML)进行更准确的预测，这将进一步有助于尽量减少作物歉收造成的经济损失。"
    },
    {
        "title": "DP-SGD with weight clipping",
        "url": "http://arxiv.org/abs/2310.18001v1",
        "pub_date": "2023-10-27",
        "summary": "Recently, due to the popularity of deep neural networks and other methods\nwhose training typically relies on the optimization of an objective function,\nand due to concerns for data privacy, there is a lot of interest in\ndifferentially private gradient descent methods. To achieve differential\nprivacy guarantees with a minimum amount of noise, it is important to be able\nto bound precisely the sensitivity of the information which the participants\nwill observe. In this study, we present a novel approach that mitigates the\nbias arising from traditional gradient clipping. By leveraging public\ninformation concerning the current global model and its location within the\nsearch domain, we can achieve improved gradient bounds, leading to enhanced\nsensitivity determinations and refined noise level adjustments. We extend the\nstate of the art algorithms, present improved differential privacy guarantees\nrequiring less noise and present an empirical evaluation.",
        "translated": "最近，由于深度神经网络和其他方法的普及，这些方法的训练通常依赖于目标函数的优化，而且由于对数据隐私的关注，人们对不同的私有梯度下降法方法有很大的兴趣。为了在尽量减少噪音的情况下达到差分隐私保证，参与者所观察到的资料的敏感度必须精确地受到约束。在这项研究中，我们提出了一种新颖的方法，以减轻由传统的梯度裁剪产生的偏差。通过利用关于当前全球模型及其在搜索域内的位置的公共信息，我们可以实现改进的梯度界限，从而增强灵敏度的确定和精确的噪声水平调整。我们扩展了最先进的算法，提出了需要更少噪音的改进的差分隐私保证，并提出了实证评估。"
    },
    {
        "title": "Can LLMs Keep a Secret? Testing Privacy Implications of Language Models\n  via Contextual Integrity Theory",
        "url": "http://arxiv.org/abs/2310.17884v1",
        "pub_date": "2023-10-27",
        "summary": "The interactive use of large language models (LLMs) in AI assistants (at\nwork, home, etc.) introduces a new set of inference-time privacy risks: LLMs\nare fed different types of information from multiple sources in their inputs\nand are expected to reason about what to share in their outputs, for what\npurpose and with whom, within a given context. In this work, we draw attention\nto the highly critical yet overlooked notion of contextual privacy by proposing\nConfAIde, a benchmark designed to identify critical weaknesses in the privacy\nreasoning capabilities of instruction-tuned LLMs. Our experiments show that\neven the most capable models such as GPT-4 and ChatGPT reveal private\ninformation in contexts that humans would not, 39% and 57% of the time,\nrespectively. This leakage persists even when we employ privacy-inducing\nprompts or chain-of-thought reasoning. Our work underscores the immediate need\nto explore novel inference-time privacy-preserving approaches, based on\nreasoning and theory of mind.",
        "translated": "在人工智能助手(工作场所、家庭等)中交互式使用大型语言模型(LLM)引入了一组新的推理时间隐私风险: LLM 在其输入中接收来自多个来源的不同类型的信息，并被期望推断在给定的上下文中共享输出的内容、目的和与谁共享。在这项工作中，我们通过提出 ConfAIde 来提请注意高度关键但被忽视的上下文隐私概念，ConfAIde 是一个基准，旨在识别指令调优 LLM 隐私推理能力中的关键弱点。我们的实验表明，即使是最有能力的模型，如 GPT-4和 ChatGPT 分别有39% 和57% 的时间在人类不会显示的情况下显示私人信息。即使我们使用隐私诱导提示或思维链推理，这种泄露也会持续存在。我们的工作强调了探索基于推理和心智理论的新颖推理时间隐私保护方法的迫切需要。"
    },
    {
        "title": "Measuring CDNs susceptible to Domain Fronting",
        "url": "http://arxiv.org/abs/2310.17851v1",
        "pub_date": "2023-10-27",
        "summary": "Domain fronting is a network communication technique that involves leveraging\n(or abusing) content delivery networks (CDNs) to disguise the final destination\nof network packets by presenting them as if they were intended for a different\ndomain than their actual endpoint. This technique can be used for both benign\nand malicious purposes, such as circumventing censorship or hiding\nmalware-related communications from network security systems. Since domain\nfronting has been known for a few years, some popular CDN providers have\nimplemented traffic filtering approaches to curb its use at their CDN\ninfrastructure. However, it remains unclear to what extent domain fronting has\nbeen mitigated.\n  To better understand whether domain fronting can still be effectively used,\nwe propose a systematic approach to discover CDNs that are still prone to\ndomain fronting. To this end, we leverage passive and active DNS traffic\nanalysis to pinpoint domain names served by CDNs and build an automated tool\nthat can be used to discover CDNs that allow domain fronting in their\ninfrastructure. Our results reveal that domain fronting is feasible in 22 out\nof 30 CDNs that we tested, including some major CDN providers like Akamai and\nFastly. This indicates that domain fronting remains widely available and can be\neasily abused for malicious purposes.",
        "translated": "域前端是一种网络通信技术，涉及到利用(或滥用)内容传递网络(CDN)来掩盖网络数据包的最终目的地，将它们表示为与其实际端点不同的域。这种技术既可以用于良性目的，也可以用于恶意目的，例如规避审查或在网络安全系统中隐藏与恶意软件相关的通信。由于域前端已经知道了几年，一些流行的 CDN 提供商已经实施了流量过滤方法，以抑制其在 CDN 基础设施中的使用。然而，目前尚不清楚域名前置问题在多大程度上得到了缓解。为了更好地理解域前端是否仍然可以有效地使用，我们提出了一个系统的方法来发现 CDN 仍然是易于领域前端。为此，我们利用被动和主动 DNS 流量分析来精确定位由 CDN 服务的域名，并建立一个自动化工具，可用于发现允许在其基础设施中进行域名前置处理的 CDN。我们的研究结果表明，在我们测试的30个 CDN 中有22个是可行的，包括一些主要的 CDN 提供商，如 Akamai 和 Fastly。这表明域名前置仍然是广泛可用的，并且很容易被滥用于恶意目的。"
    },
    {
        "title": "Positional Encoding-based Resident Identification in Multi-resident\n  Smart Homes",
        "url": "http://arxiv.org/abs/2310.17836v1",
        "pub_date": "2023-10-27",
        "summary": "We propose a novel resident identification framework to identify residents in\na multi-occupant smart environment. The proposed framework employs a feature\nextraction model based on the concepts of positional encoding. The feature\nextraction model considers the locations of homes as a graph. We design a novel\nalgorithm to build such graphs from layout maps of smart environments. The\nNode2Vec algorithm is used to transform the graph into high-dimensional node\nembeddings. A Long Short-Term Memory (LSTM) model is introduced to predict the\nidentities of residents using temporal sequences of sensor events with the node\nembeddings. Extensive experiments show that our proposed scheme effectively\nidentifies residents in a multi-occupant environment. Evaluation results on two\nreal-world datasets demonstrate that our proposed approach achieves 94.5% and\n87.9% accuracy, respectively.",
        "translated": "我们提出了一个新的居民识别框架，以识别居民在多居住者智能环境。该框架采用了基于位置编码概念的特征提取模型。特征提取模型将住宅的位置作为一个图形来考虑。我们设计了一个新的算法来建立这样的图从智能环境的布局图。使用 Node2Vec 算法将图转换为高维节点嵌入。提出了一种基于节点嵌入的长短期记忆(LSTM)模型，利用传感器事件的时间序列预测居民的身份。广泛的实验表明，我们提出的方案有效地识别居民在一个多居住环境。对两个实际数据集的评估结果表明，该方法的准确率分别达到了94.5% 和87.9% 。"
    },
    {
        "title": "BlackJack: Secure machine learning on IoT devices through hardware-based\n  shuffling",
        "url": "http://arxiv.org/abs/2310.17804v1",
        "pub_date": "2023-10-26",
        "summary": "Neural networks are seeing increased use in diverse Internet of Things (IoT)\napplications such as healthcare, smart homes and industrial monitoring. Their\nwidespread use makes neural networks a lucrative target for theft. An attacker\ncan obtain a model without having access to the training data or incurring the\ncost of training. Also, networks trained using private data (e.g., medical\nrecords) can reveal information about this data. Networks can be stolen by\nleveraging side channels such as power traces of the IoT device when it is\nrunning the network. Existing attacks require operations to occur in the same\norder each time; an attacker must collect and analyze several traces of the\ndevice to steal the network. Therefore, to prevent this type of attack, we\nrandomly shuffle the order of operations each time. With shuffling, each\noperation can now happen at many different points in each execution, making the\nattack intractable. However, we show that shuffling in software can leak\ninformation which can be used to subvert this solution. Therefore, to perform\nsecure shuffling and reduce latency, we present BlackJack, hardware added as a\nfunctional unit within the CPU. BlackJack secures neural networks on IoT\ndevices by increasing the time needed for an attack to centuries, while adding\njust 2.46% area, 3.28% power and 0.56% latency overhead on an ARM M0+ SoC.",
        "translated": "神经网络越来越多地应用于各种物联网(IoT)应用，如医疗保健、智能家居和工业监控。它们的广泛使用使得神经网络成为一个有利可图的盗窃目标。攻击者可以在不访问训练数据或不产生训练成本的情况下获得模型。此外，使用私人数据(例如，医疗记录)训练的网络可以揭示关于这些数据的信息。当物联网设备运行网络时，可以通过利用旁通道(比如物联网设备的电源跟踪)来窃取网络。现有的攻击要求每次操作都以相同的顺序发生; 攻击者必须收集和分析设备的多个踪迹才能窃取网络。因此，为了防止这种类型的攻击，我们每次随机改变操作的顺序。通过洗牌，每个操作现在可以在每次执行的许多不同点发生，这使得攻击变得难以处理。然而，我们表明，在软件洗牌可以泄漏信息，可以用来颠覆这个解决方案。因此，为了执行安全洗牌和减少延迟，我们提出了 BlackJack，硬件作为一个功能单元添加到 CPU 内。BlackJack 通过将攻击所需的时间增加到几个世纪来保护物联网设备上的神经网络，同时在 ARM M0 + SoC 上仅增加2.46% 的面积、3.28% 的功耗和0.56% 的延迟开销。"
    },
    {
        "title": "On the accuracy and efficiency of group-wise clipping in differentially\n  private optimization",
        "url": "http://arxiv.org/abs/2310.19215v1",
        "pub_date": "2023-10-30",
        "summary": "Recent advances have substantially improved the accuracy, memory cost, and\ntraining speed of differentially private (DP) deep learning, especially on\nlarge vision and language models with millions to billions of parameters. In\nthis work, we thoroughly study the per-sample gradient clipping style, a key\ncomponent in DP optimization. We show that different clipping styles have the\nsame time complexity but instantiate an accuracy-memory trade-off: while the\nall-layer clipping (of coarse granularity) is the most prevalent and usually\ngives the best accuracy, it incurs heavier memory cost compared to other\ngroup-wise clipping, such as the layer-wise clipping (of finer granularity). We\nformalize this trade-off through our convergence theory and complexity\nanalysis. Importantly, we demonstrate that the accuracy gap between group-wise\nclipping and all-layer clipping becomes smaller for larger models, while the\nmemory advantage of the group-wise clipping remains. Consequently, the\ngroup-wise clipping allows DP optimization of large models to achieve high\naccuracy and low peak memory simultaneously.",
        "translated": "最近的进展大大提高了差分私有(DP)深度学习的准确性、记忆成本和训练速度，特别是在具有数百万到数十亿参数的大视觉和语言模型上。在这项工作中，我们深入研究了每个样本的梯度裁剪风格，DP 优化的一个关键组成部分。我们表明，不同的剪辑风格具有相同的时间复杂度，但实例化了一个精度-内存权衡: 虽然全层剪辑(粗粒度)是最普遍的，通常给出最好的精度，但与其他分组剪辑(如分层剪辑(更细粒度))相比，它会带来更高的内存成本。我们通过收敛理论和复杂性分析形式化了这种权衡。重要的是，我们证明了在较大的模型中，分组裁剪和全层裁剪之间的准确性差距变得更小，而分组裁剪的内存优势仍然存在。因此，分组裁剪允许 DP 优化的大型模型，以实现高精度和低峰值存储器同时进行。"
    },
    {
        "title": "From Chatbots to PhishBots? -- Preventing Phishing scams created using\n  ChatGPT, Google Bard and Claude",
        "url": "http://arxiv.org/abs/2310.19181v1",
        "pub_date": "2023-10-29",
        "summary": "The advanced capabilities of Large Language Models (LLMs) have made them\ninvaluable across various applications, from conversational agents and content\ncreation to data analysis, research, and innovation. However, their\neffectiveness and accessibility also render them susceptible to abuse for\ngenerating malicious content, including phishing attacks. This study explores\nthe potential of using four popular commercially available LLMs - ChatGPT (GPT\n3.5 Turbo), GPT 4, Claude and Bard to generate functional phishing attacks\nusing a series of malicious prompts. We discover that these LLMs can generate\nboth phishing emails and websites that can convincingly imitate well-known\nbrands, and also deploy a range of evasive tactics for the latter to elude\ndetection mechanisms employed by anti-phishing systems. Notably, these attacks\ncan be generated using unmodified, or \"vanilla,\" versions of these LLMs,\nwithout requiring any prior adversarial exploits such as jailbreaking. As a\ncountermeasure, we build a BERT based automated detection tool that can be used\nfor the early detection of malicious prompts to prevent LLMs from generating\nphishing content attaining an accuracy of 97\\% for phishing website prompts,\nand 94\\% for phishing email prompts.",
        "translated": "大型语言模型(LLM)的高级功能使它们在各种应用程序中具有无可估量的价值，从会话代理和内容创建到数据分析、研究和创新。然而，它们的有效性和可访问性也使它们容易被滥用以产生恶意内容，包括网络钓鱼攻击。这项研究探索了使用四种流行的商业化 LLM-ChatGPT (GPT 3.5 Turbo) ，GPT 4，Claude 和 Bard 来使用一系列恶意提示生成功能性钓鱼攻击的潜力。我们发现，这些 LLM 可以产生钓鱼电子邮件和网站，可以令人信服地模仿知名品牌，也部署了一系列规避战术后者，以规避反钓鱼系统使用的检测机制。值得注意的是，这些攻击可以使用这些 LLM 的未修改或“普通”版本生成，而不需要任何先前的对手利用，如越狱。作为对策，我们建立了一个基于 BERT 的自动检测工具，可以用于早期检测恶意提示，以防止 LLM 生成钓鱼内容，对于钓鱼网站提示，准确率达到97% ，对于钓鱼电子邮件提示，准确率达到94% 。"
    },
    {
        "title": "RAIFLE: Reconstruction Attacks on Interaction-based Federated Learning\n  with Active Data Manipulation",
        "url": "http://arxiv.org/abs/2310.19163v1",
        "pub_date": "2023-10-29",
        "summary": "Federated learning (FL) has recently emerged as a privacy-preserving approach\nfor machine learning in domains that rely on user interactions, particularly\nrecommender systems (RS) and online learning to rank (OLTR). While there has\nbeen substantial research on the privacy of traditional FL, little attention\nhas been paid to studying the privacy properties of these interaction-based FL\n(IFL) systems. In this work, we show that IFL can introduce unique challenges\nconcerning user privacy, particularly when the central server has knowledge and\ncontrol over the items that users interact with. Specifically, we demonstrate\nthe threat of reconstructing user interactions by presenting RAIFLE, a general\noptimization-based reconstruction attack framework customized for IFL. RAIFLE\nemploys Active Data Manipulation (ADM), a novel attack technique unique to IFL,\nwhere the server actively manipulates the training features of the items to\ninduce adversarial behaviors in the local FL updates. We show that RAIFLE is\nmore impactful than existing FL privacy attacks in the IFL context, and\ndescribe how it can undermine privacy defenses like secure aggregation and\nprivate information retrieval. Based on our findings, we propose and discuss\ncountermeasure guidelines to mitigate our attack in the context of federated\nRS/OLTR specifically and IFL more broadly.",
        "translated": "联邦学习(FL)作为一种保护隐私的机器学习方法，最近出现在依赖于用户交互的领域，特别是推荐系统(RS)和在线学习排名(OLTR)。传统外语系统的隐私性研究已经取得了丰硕的成果，但是对于这些基于交互的外语系统的隐私性研究却很少。在这项工作中，我们表明 IFL 可以引入有关用户隐私的独特挑战，特别是当中央服务器具有知识和对用户交互的项目的控制时。具体来说，我们通过提出 RAIFLE (一种为 IFL 定制的通用的基于优化的重构攻击框架)来演示重构用户交互的威胁。RAIFLE 采用主动数据操纵(ADM)技术，这是 IFL 独有的一种新的攻击技术，服务器主动地操纵项目的训练特征，以诱发本地 FL 更新中的敌对行为。我们展示了 RAIFLE 在 IFL 环境下比现有的 FL 隐私攻击更具影响力，并描述了它如何破坏诸如安全聚合和私人信息检索等隐私防御。基于我们的研究结果，我们提出并讨论了对策指南，以减轻我们的攻击在联邦 RS/OLTR 和 IFL 更广泛的背景下具体。"
    },
    {
        "title": "BERT Lost Patience Won't Be Robust to Adversarial Slowdown",
        "url": "http://arxiv.org/abs/2310.19152v2",
        "pub_date": "2023-10-29",
        "summary": "In this paper, we systematically evaluate the robustness of multi-exit\nlanguage models against adversarial slowdown. To audit their robustness, we\ndesign a slowdown attack that generates natural adversarial text bypassing\nearly-exit points. We use the resulting WAFFLE attack as a vehicle to conduct a\ncomprehensive evaluation of three multi-exit mechanisms with the GLUE benchmark\nagainst adversarial slowdown. We then show our attack significantly reduces the\ncomputational savings provided by the three methods in both white-box and\nblack-box settings. The more complex a mechanism is, the more vulnerable it is\nto adversarial slowdown. We also perform a linguistic analysis of the perturbed\ntext inputs, identifying common perturbation patterns that our attack\ngenerates, and comparing them with standard adversarial text attacks. Moreover,\nwe show that adversarial training is ineffective in defeating our slowdown\nattack, but input sanitization with a conversational model, e.g., ChatGPT, can\nremove perturbations effectively. This result suggests that future work is\nneeded for developing efficient yet robust multi-exit models. Our code is\navailable at: https://github.com/ztcoalson/WAFFLE",
        "translated": "本文系统地评价了多退出语言模型在对抗性减速情况下的鲁棒性。为了审计他们的健壮性，我们设计了一个减速攻击，生成自然的对抗性文本绕过早期退出点。我们使用最终的 WAFFLE 攻击作为一个工具来进行一个全面的评估三个多退出机制与 GLUE 基准对抗性减速。然后，我们显示我们的攻击显著降低了这三种方法在白盒和黑盒设置中提供的计算节省。机制越复杂，就越容易受到对手经济放缓的影响。我们还对受到干扰的文本输入进行语言分析，识别我们的攻击产生的常见干扰模式，并将它们与标准的对抗性文本攻击进行比较。此外，我们表明，对抗性训练在击败我们的减速攻击方面是无效的，但是输入消毒与会话模型，例如 ChatGPT，可以有效地消除干扰。这一结果表明，未来的工作需要开发高效而健壮的多退出模型。我们的代码可以在以下 https://github.com/ztcoalson/waffle 找到"
    },
    {
        "title": "Updated Standard for Secure Satellite Communications: Analysis of\n  Satellites, Attack Vectors, Existing Standards, and Enterprise and Security\n  Architectures",
        "url": "http://arxiv.org/abs/2310.19105v1",
        "pub_date": "2023-10-29",
        "summary": "Satellites play a vital role in remote communication where traditional\ncommunication mediums struggle to provide benefits over associated costs and\nefficiency. In recent years, satellite communication has achieved utter\ninterest in the industry due to the achievement of high data rates through the\nmassive deployment of LEO satellites. Because of the complex diversity in types\nof satellites, communication methodologies, technological obstacles,\nenvironmental limitations, elements in the entire ecosystem, massive financial\nimpact, geopolitical conflict and domination, easier access to satellite\ncommunications, and various other reasons, the threat vectors are rising in the\nthreat landscape. To achieve resilience against those, only technological\nsolutions are not enough. An effective approach will be through security\nstandards. However, there is a considerable gap in the industry regarding a\ngeneric security standard framework for satellite communication and space data\nsystems. A few countries and space agencies have their own standard framework\nand private policies. However, many of those are either private, serve the\nspecific requirements of specific missions, or have not been updated for a long\ntime.\n  This project report will focus on identifying, categorizing, comparing, and\nassessing elements, threat landscape, enterprise security architectures, and\navailable public standards of satellite communication and space data systems.\nAfter that, it will utilize the knowledge to propose an updated standard\nframework for secure satellite communications and space data systems.",
        "translated": "卫星在远程通信方面发挥着至关重要的作用，传统通信媒介难以提供超过相关成本和效率的效益。近年来，由于通过大量部署低地轨道卫星实现了高数据速率，卫星通信引起了业界的极大兴趣。由于卫星类型的复杂多样性、通信方法、技术障碍、环境限制、整个生态系统的要素、巨大的金融影响、地缘政治冲突和统治、更容易获得卫星通信等各种原因，威胁向量正在威胁景观中上升。要实现抵御这些挑战的能力，仅有技术解决方案是不够的。有效的方法是通过安全标准。然而，在卫星通信和空间数据系统通用安全标准框架方面，业界存在着相当大的差距。少数国家和空间机构有自己的标准框架和私人政策。然而，其中许多要么是私有的，服务于特定任务的具体要求，要么很长时间没有更新。本项目报告将侧重于识别、分类、比较和评估要素、威胁状况、企业安全架构以及卫星通信和空间数据系统的现有公共标准。之后，它将利用这些知识提出更新的卫星通信和空间数据系统安全标准框架。"
    },
    {
        "title": "Web3 Meets AI Marketplace: Exploring Opportunities, Analyzing\n  Challenges, and Suggesting Solutions",
        "url": "http://arxiv.org/abs/2310.19099v1",
        "pub_date": "2023-10-29",
        "summary": "Web3 and AI have been among the most discussed fields over the recent years,\nwith substantial hype surrounding each field's potential to transform the world\nas we know it. However, as the hype settles, it's evident that neither AI nor\nWeb3 can address all challenges independently. Consequently, the intersection\nof AI and Web3 is gaining increased attention, emerging as a new field with the\npotential to address the limitations of each. In this article, we will focus on\nthe integration of web3 and the AI marketplace, where AI services and products\ncan be provided in a decentralized manner (DeAI). A comprehensive review is\nprovided by summarizing the opportunities and challenges on this topic.\nAdditionally, we offer analyses and solutions to address these challenges.\nWe've developed a framework that lets users pay with any kind of cryptocurrency\nto get AI services. Additionally, they can also enjoy AI services for free on\nour platform by simply locking up their assets temporarily in the protocol.\nThis unique approach is a first in the industry. Before this, offering free AI\nservices in the web3 community wasn't possible. Our solution opens up exciting\nopportunities for the AI marketplace in the web3 space to grow and be widely\nadopted.",
        "translated": "Web3和人工智能是近年来最受关注的领域之一，围绕着每个领域改变我们所知世界的潜力大肆宣传。然而，随着炒作的平息，很明显，无论是人工智能还是 Web3都不能独立应对所有的挑战。因此，人工智能和 Web3的交叉正在获得越来越多的关注，作为一个新的领域出现，有可能解决各自的局限性。在本文中，我们将关注 web3和 AI 市场的集成，在这里 AI 服务和产品可以以一种分散的方式(DeAI)提供。通过总结这一主题的机遇和挑战，提供了一个全面的审查。此外，我们还提供分析和解决方案来应对这些挑战。我们已经开发了一个框架，允许用户使用任何类型的加密货币来支付 AI 服务。此外，他们也可以在我们的平台上免费享受人工智能服务，只需将他们的资产暂时锁定在协议中。这种独特的方法在业界尚属首次。在此之前，在 web3社区提供免费的 AI 服务是不可能的。我们的解决方案为 web3领域的人工智能市场成长和被广泛采用提供了激动人心的机会。"
    },
    {
        "title": "Differentially Private Permutation Tests: Applications to Kernel Methods",
        "url": "http://arxiv.org/abs/2310.19043v1",
        "pub_date": "2023-10-29",
        "summary": "Recent years have witnessed growing concerns about the privacy of sensitive\ndata. In response to these concerns, differential privacy has emerged as a\nrigorous framework for privacy protection, gaining widespread recognition in\nboth academic and industrial circles. While substantial progress has been made\nin private data analysis, existing methods often suffer from impracticality or\na significant loss of statistical efficiency. This paper aims to alleviate\nthese concerns in the context of hypothesis testing by introducing\ndifferentially private permutation tests. The proposed framework extends\nclassical non-private permutation tests to private settings, maintaining both\nfinite-sample validity and differential privacy in a rigorous manner. The power\nof the proposed test depends on the choice of a test statistic, and we\nestablish general conditions for consistency and non-asymptotic uniform power.\nTo demonstrate the utility and practicality of our framework, we focus on\nreproducing kernel-based test statistics and introduce differentially private\nkernel tests for two-sample and independence testing: dpMMD and dpHSIC. The\nproposed kernel tests are straightforward to implement, applicable to various\ntypes of data, and attain minimax optimal power across different privacy\nregimes. Our empirical evaluations further highlight their competitive power\nunder various synthetic and real-world scenarios, emphasizing their practical\nvalue. The code is publicly available to facilitate the implementation of our\nframework.",
        "translated": "近年来，人们越来越关注敏感数据的隐私问题。为了应对这些担忧，差分隐私已成为保护隐私的严格框架，在学术界和工业界都得到了广泛认可。虽然在私人数据分析方面取得了重大进展，但现有方法往往不切实际，或统计效率大大降低。本文旨在通过引入差分私有排列检验来缓解假设检验中的这些问题。建议的框架将经典的非私有排列测试扩展到私有环境，以严格的方式保持有限样本的有效性和差分隐私。该检验的幂取决于检验统计量的选择，我们建立了一致性和非渐近一致幂的一般条件。为了演示我们的框架的实用性和实用性，我们重点重现了基于内核的测试统计信息，并为双样本和独立的测试引入了差异私有内核测试: dpMMD 和 dpHSIC。提出的内核测试实现简单，适用于各种类型的数据，并在不同的隐私机制下获得最小最优功耗。我们的实证评估进一步突出了它们在各种合成和现实世界情景下的竞争力，强调了它们的实用价值。该代码可公开使用，以促进我们框架的实现。"
    },
    {
        "title": "Boosting Decision-Based Black-Box Adversarial Attack with Gradient\n  Priors",
        "url": "http://arxiv.org/abs/2310.19038v1",
        "pub_date": "2023-10-29",
        "summary": "Decision-based methods have shown to be effective in black-box adversarial\nattacks, as they can obtain satisfactory performance and only require to access\nthe final model prediction. Gradient estimation is a critical step in black-box\nadversarial attacks, as it will directly affect the query efficiency. Recent\nworks have attempted to utilize gradient priors to facilitate score-based\nmethods to obtain better results. However, these gradient priors still suffer\nfrom the edge gradient discrepancy issue and the successive iteration gradient\ndirection issue, thus are difficult to simply extend to decision-based methods.\nIn this paper, we propose a novel Decision-based Black-box Attack framework\nwith Gradient Priors (DBA-GP), which seamlessly integrates the data-dependent\ngradient prior and time-dependent prior into the gradient estimation procedure.\nFirst, by leveraging the joint bilateral filter to deal with each random\nperturbation, DBA-GP can guarantee that the generated perturbations in edge\nlocations are hardly smoothed, i.e., alleviating the edge gradient discrepancy,\nthus remaining the characteristics of the original image as much as possible.\nSecond, by utilizing a new gradient updating strategy to automatically adjust\nthe successive iteration gradient direction, DBA-GP can accelerate the\nconvergence speed, thus improving the query efficiency. Extensive experiments\nhave demonstrated that the proposed method outperforms other strong baselines\nsignificantly.",
        "translated": "基于决策的方法在黑盒对抗攻击中已被证明是有效的，因为它们可以获得令人满意的性能，并且只需要访问最终的模型预测。梯度估计是黑盒对抗攻击的关键步骤，它直接影响到查询效率。最近的工作试图利用梯度先验，以促进基于分数的方法，以获得更好的结果。然而，这些梯度先验仍然存在边缘梯度偏差问题和连续迭代梯度方向问题，因此很难简单地推广到基于决策的方法。本文提出了一种新的基于决策的带有梯度先验的黑盒攻击框架(DBA-GP) ，它将数据相关的梯度先验和时间相关的先验无缝集成到梯度估计过程中。首先，通过利用联合双边泸波器来处理每个随机扰动，DBA-GP 可以保证在边缘位置产生的扰动很难平滑，即减轻边缘梯度差异，从而尽可能保留原始图像的特征。其次，利用一种新的梯度更新策略自动调整连续迭代的梯度方向，DBA-GP 可以加快收敛速度，从而提高查询效率。大量的实验表明，该方法的性能明显优于其他强基线。"
    },
    {
        "title": "Securing Refugee Identity: A Literature Review on Blockchain-based Smart\n  Contract",
        "url": "http://arxiv.org/abs/2310.19018v1",
        "pub_date": "2023-10-29",
        "summary": "Identity documentation for refugees is a complex process and crucial for host\nnations. A secured identity management system ensures both security and the\nefficient provision of services for the host nation and the donor\norganizations. Realizing the benefits, a handful of studies enriched the\nblockchain-based security identification for refugees. The research studies\npresented the introductory, conceptual, and practical solution related to the\nblockchain-based smart contract. There is a common agreement in the studies\nthat blockchain-based smart contract not only streamlines refugee identity\nverification but also safeguards against unauthorized entries. Since it is a\ntechnology as well, it has been essential to know the present status of the\ntechnology in the social context. In such a situation it becomes essential to\nreview the existing research studies to provide insight for future studies. In\nthis study, we reviewed current studies using a thematic approach. Our findings\nsuggest researchers are more inclined to provide conceptual models as the\nmodels are important in advancing technology; however, the models need to be\nimplemented for practical advances. However, the main contribution of this\nstudy is that this study gathers current efforts in smart contract-based\nrefugee identity management. This study is important for the refugee host\nnations as well as for stakeholders. Knowledge gained from the study is\nexpected to provide insight into how the technology can be developed using\nexisting theory and implementation frameworks.",
        "translated": "难民的身份证明文件是一个复杂的过程，对东道国至关重要。一个安全的身份管理系统可确保东道国和捐助组织的安全和有效提供服务。认识到这些好处，一些研究丰富了基于区块链的难民安全识别。研究提出了基于区块链的智能合同的介绍性、概念性和实用性解决方案。研究中有一个共同的共识，即基于区块链的智能合同不仅简化了难民身份验证，而且还保障了未经授权的入境。因为它也是一种技术，所以了解技术在社会语境中的现状是非常必要的。在这种情况下，有必要回顾现有的研究成果，以便为今后的研究提供见解。在这项研究中，我们回顾了目前的研究使用的主题方法。我们的研究结果表明，研究人员更倾向于提供概念模型，因为这些模型在技术进步中很重要; 然而，这些模型需要为实际进步而实施。然而，这项研究的主要贡献在于，这项研究汇集了目前基于合同的智能难民身份管理的努力。这项研究对难民收容国和利益攸关方都很重要。从研究中获得的知识有望提供深入了解如何利用现有的理论和实施框架开发这种技术。"
    },
    {
        "title": "Label Poisoning is All You Need",
        "url": "http://arxiv.org/abs/2310.18933v1",
        "pub_date": "2023-10-29",
        "summary": "In a backdoor attack, an adversary injects corrupted data into a model's\ntraining dataset in order to gain control over its predictions on images with a\nspecific attacker-defined trigger. A typical corrupted training example\nrequires altering both the image, by applying the trigger, and the label.\nModels trained on clean images, therefore, were considered safe from backdoor\nattacks. However, in some common machine learning scenarios, the training\nlabels are provided by potentially malicious third-parties. This includes\ncrowd-sourced annotation and knowledge distillation. We, hence, investigate a\nfundamental question: can we launch a successful backdoor attack by only\ncorrupting labels? We introduce a novel approach to design label-only backdoor\nattacks, which we call FLIP, and demonstrate its strengths on three datasets\n(CIFAR-10, CIFAR-100, and Tiny-ImageNet) and four architectures (ResNet-32,\nResNet-18, VGG-19, and Vision Transformer). With only 2% of CIFAR-10 labels\ncorrupted, FLIP achieves a near-perfect attack success rate of 99.4% while\nsuffering only a 1.8% drop in the clean test accuracy. Our approach builds upon\nthe recent advances in trajectory matching, originally introduced for dataset\ndistillation.",
        "translated": "在后门攻击中，对手向模型的训练数据集中注入损坏的数据，以便通过特定的攻击者定义的触发器控制其对图像的预测。一个典型的损坏训练示例需要通过应用触发器和标签来修改图像。因此，接受清晰图像训练的模型被认为是安全的，不会受到后门攻击。然而，在一些常见的机器学习场景中，训练标签是由潜在的恶意第三方提供的。这包括众包注释和知识提取。因此，我们研究了一个基本问题: 我们能否仅仅通过破坏标签来发动一次成功的后门攻击？我们引入了一种新的方法来设计标签式后门攻击，我们称之为 FLIP，并在三个数据集(CIFAR-10，CIFAR-100和 Tiny-ImageNet)和四个架构(ResNet-32，ResNet-18，VGG-19和 Vision Transformer)上展示了它的优势。只有2% 的 CIFAR-10标签损坏，FLIP 实现了一个近乎完美的攻击成功率为99.4% ，而只有1.8% 的干净测试准确性下降。我们的方法建立在轨迹匹配的最新进展之上，最初是为数据集精馏引入的。"
    },
    {
        "title": "Initialization Matters: Privacy-Utility Analysis of Overparameterized\n  Neural Networks",
        "url": "http://arxiv.org/abs/2310.20579v1",
        "pub_date": "2023-10-31",
        "summary": "We analytically investigate how over-parameterization of models in randomized\nmachine learning algorithms impacts the information leakage about their\ntraining data. Specifically, we prove a privacy bound for the KL divergence\nbetween model distributions on worst-case neighboring datasets, and explore its\ndependence on the initialization, width, and depth of fully connected neural\nnetworks. We find that this KL privacy bound is largely determined by the\nexpected squared gradient norm relative to model parameters during training.\nNotably, for the special setting of linearized network, our analysis indicates\nthat the squared gradient norm (and therefore the escalation of privacy loss)\nis tied directly to the per-layer variance of the initialization distribution.\nBy using this analysis, we demonstrate that privacy bound improves with\nincreasing depth under certain initializations (LeCun and Xavier), while\ndegrades with increasing depth under other initializations (He and NTK). Our\nwork reveals a complex interplay between privacy and depth that depends on the\nchosen initialization distribution. We further prove excess empirical risk\nbounds under a fixed KL privacy budget, and show that the interplay between\nprivacy utility trade-off and depth is similarly affected by the\ninitialization.",
        "translated": "我们分析研究了随机机器学习算法中模型的过度参数化是如何影响训练数据的信息泄露的。具体来说，我们证明了最坏情况下相邻数据集上模型分布之间的 KL 发散的一个隐私界限，并探讨了它对全连通神经网络的初始化、宽度和深度的依赖性。我们发现这个 KL 隐私界限很大程度上取决于训练过程中相对于模型参数的期望平方梯度范数。值得注意的是，对于线性化网络的特殊设置，我们的分析表明平方梯度范数(因此隐私损失的升级)直接与初始化分布的每层方差有关。通过使用这个分析，我们证明了隐私界限在某些初始化情况下随着深度的增加而改善，而在其他初始化情况下随着深度的增加而降低(He 和 NTK)。我们的工作揭示了隐私和深度之间复杂的相互作用，这取决于所选择的初始化分布。我们进一步证明了在一个固定的 KL 隐私预算下的超额经验风险界限，并且表明隐私效用权衡和深度之间的相互作用同样受到初始化的影响。"
    },
    {
        "title": "Privacy-preserving design of graph neural networks with applications to\n  vertical federated learning",
        "url": "http://arxiv.org/abs/2310.20552v1",
        "pub_date": "2023-10-31",
        "summary": "The paradigm of vertical federated learning (VFL), where institutions\ncollaboratively train machine learning models via combining each other's local\nfeature or label information, has achieved great success in applications to\nfinancial risk management (FRM). The surging developments of graph\nrepresentation learning (GRL) have opened up new opportunities for FRM\napplications under FL via efficiently utilizing the graph-structured data\ngenerated from underlying transaction networks. Meanwhile, transaction\ninformation is often considered highly sensitive. To prevent data leakage\nduring training, it is critical to develop FL protocols with formal privacy\nguarantees. In this paper, we present an end-to-end GRL framework in the VFL\nsetting called VESPER, which is built upon a general privatization scheme\ntermed perturbed message passing (PMP) that allows the privatization of many\npopular graph neural architectures.Based on PMP, we discuss the strengths and\nweaknesses of specific design choices of concrete graph neural architectures\nand provide solutions and improvements for both dense and sparse graphs.\nExtensive empirical evaluations over both public datasets and an industry\ndataset demonstrate that VESPER is capable of training high-performance GNN\nmodels over both sparse and dense graphs under reasonable privacy budgets.",
        "translated": "垂直联邦学习(VFL)是机构之间通过结合彼此的局部特征或标签信息来协同训练机器学习模型的范式，在金融风险管理(FRM)的应用中取得了巨大的成功。图表示学习(GRL)的迅猛发展为 FRM 在 FL 下的应用开辟了新的机遇，有效地利用了底层事务网络生成的图结构化数据。同时，事务信息通常被认为是高度敏感的。为了防止培训期间的数据泄漏，开发具有正式隐私保护的 FL 协议至关重要。在本文中，我们提出了一个在 VFL 环境下的端到端 GRL 框架 VESPER，它建立在一个称为扰动消息传递(PMP)的通用私有化方案的基础上，该方案允许对许多流行的图神经结构进行私有化。基于 PMP 方法，讨论了具体图形神经网络结构设计选择的优缺点，并对稠密图和稀疏图提出了解决方案和改进措施。对公共数据集和行业数据集的大量实证评估表明，VESPER 能够在合理的隐私预算下，在稀疏和密集图上训练高性能 GNN 模型。"
    },
    {
        "title": "On the matrix code of quadratic relationships for a Goppa code",
        "url": "http://arxiv.org/abs/2310.20497v1",
        "pub_date": "2023-10-31",
        "summary": "In this article, we continue the analysis started in \\cite{CMT23} for the\nmatrix code of quadratic relationships associated with a Goppa code. We provide\nnew sparse and low-rank elements in the matrix code and categorize them\naccording to their shape. Thanks to this description, we prove that the set of\nrank 2 matrices in the matrix codes associated with square-free binary Goppa\ncodes, i.e. those used in Classic McEiece, is much larger than what is\nexpected, at least in the case where the Goppa polynomial degree is 2. We build\nupon the algebraic determinantal modeling introduced in \\cite{CMT23} to derive\na structural attack on these instances. Our method can break in just a few\nseconds some recent challenges about key-recovery attacks on the McEliece\ncryptosystem, consistently reducing their estimated security level. We also\nprovide a general method, valid for any Goppa polynomial degree, to transform a\ngeneric pair of support and multiplier into a pair of support and Goppa\npolynomial.",
        "translated": "在本文中，我们继续分析引用{ CMT23}中开始的与 Goppa 码相关的二次关系的矩阵代码。我们在矩阵编码中提供新的稀疏元素和低秩元素，并根据它们的形状对它们进行分类。由于这种描述，我们证明了与无平方二进制 Goppa 码相关的矩阵码中的秩2矩阵集，即古典 McEece 码中使用的秩2矩阵集，比预期的要大得多，至少在 Goppa 多项式次数为2的情况下是如此。我们在引用{ CMT23}的代数行列式建模的基础上推导出对这些实例的结构攻击。我们的方法可以在几秒钟内突破一些关于 McEliece 密码系统密钥恢复攻击的最新挑战，持续降低他们的估计安全级别。我们还提供了一个通用的方法，有效的任何 Goppa 多项式度，转换成一对支持和乘子一对支持和 Goppa 多项式。"
    },
    {
        "title": "Amoeba: Circumventing ML-supported Network Censorship via Adversarial\n  Reinforcement Learning",
        "url": "http://arxiv.org/abs/2310.20469v1",
        "pub_date": "2023-10-31",
        "summary": "Embedding covert streams into a cover channel is a common approach to\ncircumventing Internet censorship, due to censors' inability to examine\nencrypted information in otherwise permitted protocols (Skype, HTTPS, etc.).\nHowever, recent advances in machine learning (ML) enable detecting a range of\nanti-censorship systems by learning distinct statistical patterns hidden in\ntraffic flows. Therefore, designing obfuscation solutions able to generate\ntraffic that is statistically similar to innocuous network activity, in order\nto deceive ML-based classifiers at line speed, is difficult.\n  In this paper, we formulate a practical adversarial attack strategy against\nflow classifiers as a method for circumventing censorship. Specifically, we\ncast the problem of finding adversarial flows that will be misclassified as a\nsequence generation task, which we solve with Amoeba, a novel reinforcement\nlearning algorithm that we design. Amoeba works by interacting with censoring\nclassifiers without any knowledge of their model structure, but by crafting\npackets and observing the classifiers' decisions, in order to guide the\nsequence generation process. Our experiments using data collected from two\npopular anti-censorship systems demonstrate that Amoeba can effectively shape\nadversarial flows that have on average 94% attack success rate against a range\nof ML algorithms. In addition, we show that these adversarial flows are robust\nin different network environments and possess transferability across various ML\nmodels, meaning that once trained against one, our agent can subvert other\ncensoring classifiers without retraining.",
        "translated": "由于审查者无法在其他允许的协议(Skype、 HTTPS 等)中检查加密信息，将隐蔽流嵌入到掩蔽通道是规避互联网审查的常用方法。然而，机器学习(ML)的最新进展能够通过学习隐藏在流量流中的不同的统计模式来检测一系列的反审查系统。因此，设计能够产生统计上类似于无害网络活动的流量的混淆解决方案，以便在线速度上欺骗基于机器学习的分类器，是困难的。本文提出了一种实用的针对流分类器的对抗性攻击策略，作为一种规避审查的方法。具体来说，我们将寻找对手流的问题错误地归类为序列生成任务，我们使用我们设计的一种新型强化学习算法——变形虫(ameba)来解决这个问题。变形虫的工作方式是在不知道分类器模型结构的情况下，与截尾分类器进行交互，而是通过构造数据包和观察分类器的决策来指导序列生成过程。我们使用从两个流行的反审查系统收集的数据进行的实验表明，Amoeba 可以有效地形成对抗性流，对一系列 ML 算法平均有94% 的攻击成功率。此外，我们表明，这些对抗性流动在不同的网络环境中是稳健的，并具有跨各种机器学习模型的可转移性，这意味着一旦针对一个训练，我们的代理可以颠覆其他审查分类器没有再训练。"
    },
    {
        "title": "A Survey on Federated Unlearning: Challenges, Methods, and Future\n  Directions",
        "url": "http://arxiv.org/abs/2310.20448v1",
        "pub_date": "2023-10-31",
        "summary": "In recent years, the notion of ``the right to be forgotten\" (RTBF) has\nevolved into a fundamental element of data privacy regulations, affording\nindividuals the ability to request the removal of their personal data from\ndigital records. Consequently, given the extensive adoption of data-intensive\nmachine learning (ML) algorithms and increasing concerns for personal data\nprivacy protection, the concept of machine unlearning (MU) has gained\nconsiderable attention. MU empowers an ML model to selectively eliminate\nsensitive or personally identifiable information it acquired during the\ntraining process. Evolving from the foundational principles of MU, federated\nunlearning (FU) has emerged to confront the challenge of data erasure within\nthe domain of federated learning (FL) settings. This empowers the FL model to\nunlearn an FL client or identifiable information pertaining to the client while\npreserving the integrity of the decentralized learning process. Nevertheless,\nunlike traditional MU, the distinctive attributes of federated learning\nintroduce specific challenges for FU techniques. These challenges lead to the\nneed for tailored design when designing FU algorithms. Therefore, this\ncomprehensive survey delves into the techniques, methodologies, and recent\nadvancements in federated unlearning. It provides an overview of fundamental\nconcepts and principles, evaluates existing federated unlearning algorithms,\nreviews optimizations tailored to federated learning, engages in discussions\nregarding practical applications, along with an assessment of their\nlimitations, and outlines promising directions for future research.",
        "translated": "近年来，“被遗忘的权利”(RTBF)的概念已经发展成为数据隐私法规的一个基本要素，使个人能够要求从数字记录中删除他们的个人数据。因此，随着数据密集型机器学习(ML)算法的广泛应用以及人们对个人数据隐私保护的日益关注，机器去学习(MU)的概念得到了相当的重视。机器学习模型能够有选择地消除在训练过程中获得的敏感性或个人身份信息。从 MU 的基本原理演变而来的联邦去学习(FU)已经出现，面临着联邦学习(FL)设置领域内数据擦除的挑战。这使得 FL 模型能够在保持分散学习过程的完整性的同时，去除 FL 客户或与客户相关的可识别信息。然而，与传统的 MU 不同，联邦学习的独特属性为 FU 技术带来了特殊的挑战。这些挑战导致在设计 FU 算法时需要量身定制的设计。因此，本综合调查深入研究了联邦忘却的技术、方法和最新进展。它提供了基本概念和原则的概述，评估现有的联邦忘记算法，审查针对联邦学习的优化，参与关于实际应用的讨论，以及对其局限性的评估，并概述了未来研究的有希望的方向。"
    },
    {
        "title": "Configuring Timing Parameters to Ensure Execution-Time Opacity in Timed\n  Automata",
        "url": "http://arxiv.org/abs/2310.20392v1",
        "pub_date": "2023-10-31",
        "summary": "Timing information leakage occurs whenever an attacker successfully deduces\nconfidential internal information by observing some timed information such as\nevents with timestamps. Timed automata are an extension of finite-state\nautomata with a set of clocks evolving linearly and that can be tested or\nreset, making this formalism able to reason on systems involving concurrency\nand timing constraints. In this paper, we summarize a recent line of works\nusing timed automata as the input formalism, in which we assume that the\nattacker has access (only) to the system execution time. First, we address the\nfollowing execution-time opacity problem: given a timed system modeled by a\ntimed automaton, given a secret location and a final location, synthesize the\nexecution times from the initial location to the final location for which one\ncannot deduce whether the secret location was visited. This means that for any\nsuch execution time, the system is opaque: either the final location is not\nreachable, or it is reachable with that execution time for both a run visiting\nand a run not visiting the secret location. We also address the full\nexecution-time opacity problem, asking whether the system is opaque for all\nexecution times; we also study a weak counterpart. Second, we add timing\nparameters, which are a way to configure a system: we identify a subclass of\nparametric timed automata with some decidability results. In addition, we\ndevise a semi-algorithm for synthesizing timing parameter valuations\nguaranteeing that the resulting system is opaque. Third, we report on problems\nwhen the secret has itself an expiration date, thus defining expiring\nexecution-time opacity problems. We finally show that our method can also apply\nto program analysis with configurable internal timings.",
        "translated": "每当攻击者通过观察某些定时信息(例如带有时间戳的事件)成功推断出机密内部信息时，就会出现定时信息泄露。时间自动机是有限状态自动机的一个扩展，它具有一组线性发展的时钟，可以进行测试或重置，使得这种形式能够对涉及并发性和时间约束的系统进行推理。在本文中，我们总结了最近一系列使用时间自动机作为输入形式的工作，在这些工作中，我们假设攻击者只能访问系统执行时间。首先，我们解决了以下执行时间不透明性问题: 给定一个由时间自动机建模的定时系统，给定一个秘密位置和一个最终位置，综合从初始位置到最终位置的执行时间，人们无法推断是否访问了秘密位置。这意味着对于任何这样的执行时间，系统都是不透明的: 要么无法到达最终位置，要么运行访问和不访问秘密位置的运行都可以使用该执行时间到达最终位置。我们还讨论了完整执行时间的不透明性问题，询问系统是否在所有执行时间都是不透明的; 我们还研究了一个弱对应物。其次，我们添加时间参数，这是一种配置系统的方法: 我们确定了一个子类的参数时间自动机与一些可判定性的结果。此外，我们设计了一个半算法来合成时序参数值，保证所得到的系统是不透明的。第三，当秘密本身有一个到期日期时，我们报告问题，从而定义到期的执行时间不透明性问题。最后，我们证明了我们的方法也可以应用于具有可配置内部计时的程序分析。"
    },
    {
        "title": "Physical-layer key distribution using synchronous complex dynamics of\n  DBR semiconductor lasers",
        "url": "http://arxiv.org/abs/2310.20365v1",
        "pub_date": "2023-10-31",
        "summary": "Common-signal-induced synchronization of semiconductor lasers with optical\nfeedback inspired a promising physical key distribution with\ninformation-theoretic security and potential in high rate. A significant\nchallenge is the requirement to shorten the synchronization recovery time for\nincreasing key rate without sacrificing operation parameter space for security.\nHere, open-loop synchronization of wavelength-tunable multi-section distributed\nBragg reflector (DBR) lasers is proposed as a solution for physical-layer key\ndistribution. Experiments show that the synchronization is sensitive to two\noperation parameters, i.e., currents of grating section and phase section.\nFurthermore, fast wavelength-shift keying synchronization can be achieved by\ndirect modulation on one of the two currents. The synchronization recovery time\nis shortened by one order of magnitude compared to close-loop synchronization.\nAn experimental implementation is demonstrated with a final key rate of 5.98\nMbit/s over 160 km optical fiber distance. It is thus believed that\nfast-tunable multi-section semiconductor lasers opens a new avenue of high-rate\nphysical-layer key distribution using laser synchronization.",
        "translated": "半导体激光器的共信号诱导同步与光反馈启发了一个有前途的物理密钥分配与资讯理论安全性和电位在高速率。在不牺牲操作参数空间保证安全的情况下，缩短同步恢复时间以提高密钥速率是一个重要的挑战。在这里，开环同步的波长可调谐多段分散式布拉格反射器(DBR)激光器被提出作为一个解决方案的物理层密钥分配。实验表明，同步对光栅截面电流和相位截面电流两个操作参数敏感。此外，快速波长移位键控同步可以实现直接调制的两个电流之一。与闭环同步相比，同步恢复时间缩短了一个数量级。实验结果表明，在160km 的光纤传输距离内，最终密钥速率为5.98 Mbit/s。因此，快速可调谐多段半导体激光器为利用激光同步实现高速物理层密钥分配开辟了一条新的途径。"
    },
    {
        "title": "Bohr sets generated by polynomials and Coppersmith's method in many\n  variables",
        "url": "http://arxiv.org/abs/2310.20342v1",
        "pub_date": "2023-10-31",
        "summary": "We obtain bounds on the average size of Bohr sets with coefficients\nparametrised by polynomials over finite fields and obtain a series of general\nresults and also some sharper results for specific sets which are important for\napplications to computer science. In particular, we use our estimates to show\nthat a heuristic assumption used in the many variable version of Coppersmith's\nmethod holds with high probability. We demonstrate the use of our results on\nthe approximate greatest common divisor problem and obtain a fully rigorous\nversion of the heuristic algorithm of H. Cohn and N. Heninger (2013).",
        "translated": "得到了有限域上多项式参数化系数的玻尔集平均大小的界，并得到了一系列一般性结果和一些对计算机科学应用有重要意义的关于特定集的更尖锐的结果。特别是，我们使用我们的估计表明，启发式假设中使用的多变量版本的科珀史密斯的方法具有很高的概率。我们展示了我们的结果在近似最大公约数问题上的应用，并得到了一个完全严格的 H. Cohn 和 N. Heninger 启发式算法(2013)。"
    },
    {
        "title": "Verification of Neural Networks Local Differential Classification\n  Privacy",
        "url": "http://arxiv.org/abs/2310.20299v1",
        "pub_date": "2023-10-31",
        "summary": "Neural networks are susceptible to privacy attacks. To date, no verifier can\nreason about the privacy of individuals participating in the training set. We\npropose a new privacy property, called local differential classification\nprivacy (LDCP), extending local robustness to a differential privacy setting\nsuitable for black-box classifiers. Given a neighborhood of inputs, a\nclassifier is LDCP if it classifies all inputs the same regardless of whether\nit is trained with the full dataset or whether any single entry is omitted. A\nnaive algorithm is highly impractical because it involves training a very large\nnumber of networks and verifying local robustness of the given neighborhood\nseparately for every network. We propose Sphynx, an algorithm that computes an\nabstraction of all networks, with a high probability, from a small set of\nnetworks, and verifies LDCP directly on the abstract network. The challenge is\ntwofold: network parameters do not adhere to a known distribution probability,\nmaking it difficult to predict an abstraction, and predicting too large\nabstraction harms the verification. Our key idea is to transform the parameters\ninto a distribution given by KDE, allowing to keep the over-approximation error\nsmall. To verify LDCP, we extend a MILP verifier to analyze an abstract\nnetwork. Experimental results show that by training only 7% of the networks,\nSphynx predicts an abstract network obtaining 93% verification accuracy and\nreducing the analysis time by $1.7\\cdot10^4$x.",
        "translated": "神经网络容易受到隐私攻击。迄今为止，没有任何验证者能够对参与训练集的个人的隐私进行推理。我们提出了一个新的保密性属性，称为局部差分分类保密性(LDCP) ，将局部鲁棒性扩展到一个适合于黑盒分类器的差分隐私设置。给定一个输入邻域，如果一个分类器对所有输入进行相同的分类，而不管它是否用完整的数据集训练，或者是否省略了任何单个条目，那么它就是 LDCP。一个简单的算法是非常不切实际的，因为它涉及训练非常大量的网络和验证局部鲁棒性给定的邻居分别为每个网络。我们提出了 Sphynx 算法，该算法从一个小的网络集合中以高概率计算所有网络的抽象，并直接在抽象网络上验证 LDCP。挑战有两方面: 网络参数不遵守已知的分布概率，使得预测抽象变得困难，并且预测过大的抽象会损害验证。我们的关键思想是将参数转换成 KDE 给出的分布，使得过近似误差保持在较小的范围内。为了验证 LDCP，我们扩展了一个 MILP 验证器来分析抽象网络。实验结果表明，通过只训练7% 的网络，Sphynx 预测一个抽象网络可以获得93% 的验证准确率，并减少分析时间 $1.7 cdot10 ^ 4 $x。"
    },
    {
        "title": "Multi-Domain Polarization for Enhancing the Physical Layer Security of\n  MIMO Systems",
        "url": "http://arxiv.org/abs/2310.20200v1",
        "pub_date": "2023-10-31",
        "summary": "A novel Physical Layer Security (PLS) framework is conceived for enhancing\nthe security of the wireless communication systems by exploiting multi-domain\npolarization in Multiple-Input Multiple-Output (MIMO) systems. We design a\nsophisticated key generation scheme based on multi-domain polarization, and the\ncorresponding receivers. An in-depth analysis of the system's secrecy rate is\nprovided, demonstrating the confidentiality of our approach in the presence of\neavesdroppers having strong computational capabilities. More explicitly, our\nsimulation results and theoretical analysis corroborate the advantages of the\nproposed scheme in terms of its bit error rate (BER), block error rate (BLER),\nand maximum achievable secrecy rate. Our findings indicate that the innovative\nPLS framework effectively enhances the security and reliability of wireless\ncommunication systems. For instance, in a $4\\times4$ MIMO setup, the proposed\nPLS strategy exhibits an improvement of $2$dB compared to conventional MIMO,\nsystems at a BLER of $2\\cdot 10^{-5}$ while the eavesdropper's BLER reaches\n$1$.",
        "translated": "针对多输入多输出(MIMO)系统中的多域极化问题，提出了一种新的物理层安全(PLS)框架，以提高无线通信系统的安全性。设计了一种复杂的基于多域极化的密钥生成方案，并给出了相应的接收机。对系统的保密率进行了深入的分析，证明了我们的方法在具有强大计算能力的窃听者面前的保密性。更明确的是，我们的仿真结果和理论分析证实了该方案在误码率(BER)、块错误率(BLER)和最大可达保密率方面的优势。我们的研究结果表明，创新的 PLS 框架有效地提高了无线通信系统的安全性和可靠性。例如，在 $4乘以4 $MIMO 设置中，提出的 PLS 策略比传统 MIMO 显示出 $2 $dB 的改进，系统的 BLER 为 $2 cdot 10 ^ {-5} $，而窃听者的 BLER 达到 $1 $。"
    },
    {
        "title": "Software Repositories and Machine Learning Research in Cyber Security",
        "url": "http://arxiv.org/abs/2311.00691v1",
        "pub_date": "2023-11-01",
        "summary": "In today's rapidly evolving technological landscape and advanced software\ndevelopment, the rise in cyber security attacks has become a pressing concern.\nThe integration of robust cyber security defenses has become essential across\nall phases of software development. It holds particular significance in\nidentifying critical cyber security vulnerabilities at the initial stages of\nthe software development life cycle, notably during the requirement phase.\nThrough the utilization of cyber security repositories like The Common Attack\nPattern Enumeration and Classification (CAPEC) from MITRE and the Common\nVulnerabilities and Exposures (CVE) databases, attempts have been made to\nleverage topic modeling and machine learning for the detection of these\nearly-stage vulnerabilities in the software requirements process. Past research\nthemes have returned successful outcomes in attempting to automate\nvulnerability identification for software developers, employing a mixture of\nunsupervised machine learning methodologies such as LDA and topic modeling.\nLooking ahead, in our pursuit to improve automation and establish connections\nbetween software requirements and vulnerabilities, our strategy entails\nadopting a variety of supervised machine learning techniques. This array\nencompasses Support Vector Machines (SVM), Na\\\"ive Bayes, random forest, neural\nnetworking and eventually transitioning into deep learning for our\ninvestigation. In the face of the escalating complexity of cyber security, the\nquestion of whether machine learning can enhance the identification of\nvulnerabilities in diverse software development scenarios is a paramount\nconsideration, offering crucial assistance to software developers in developing\nsecure software.",
        "translated": "在科技日新月异和软件发展日新月异的今天，网络安全攻击的增加已经成为一个迫切需要关注的问题。在软件开发的所有阶段，强大的网络安全防御系统的集成已经变得至关重要。在软件开发生命周期的初始阶段，特别是在需求阶段，它对于识别关键的网络安全漏洞具有特别重要的意义。通过利用来自 MITRE 和通用漏洞披露数据库的网络安全资料库，例如共同攻击模式枚举和分类(CPAPEC) ，我们尝试利用主题建模和机器学习来检测软件需求过程中的这些早期漏洞。过去的研究主题在试图自动识别软件开发人员的漏洞方面取得了成功的结果，这种方法结合了 LDA 和主题建模等非监督式学习方法。展望未来，为了提高自动化程度，并在软件需求和漏洞之间建立联系，我们的策略需要采用各种监督式学习技术。这个阵列包括支持向量机(SVM) ，朴素贝叶斯，随机森林，神经网络和最终过渡到我们的研究深度学习。面对不断上升的网络安全复杂性，机器学习能否加强识别不同软件开发场景中的脆弱性是一个首要考虑因素，为软件开发人员开发安全软件提供了至关重要的帮助。"
    },
    {
        "title": "Revealing CNN Architectures via Side-Channel Analysis in Dataflow-based\n  Inference Accelerators",
        "url": "http://arxiv.org/abs/2311.00579v1",
        "pub_date": "2023-11-01",
        "summary": "Convolution Neural Networks (CNNs) are widely used in various domains. Recent\nadvances in dataflow-based CNN accelerators have enabled CNN inference in\nresource-constrained edge devices. These dataflow accelerators utilize inherent\ndata reuse of convolution layers to process CNN models efficiently. Concealing\nthe architecture of CNN models is critical for privacy and security. This paper\nevaluates memory-based side-channel information to recover CNN architectures\nfrom dataflow-based CNN inference accelerators. The proposed attack exploits\nspatial and temporal data reuse of the dataflow mapping on CNN accelerators and\narchitectural hints to recover the structure of CNN models. Experimental\nresults demonstrate that our proposed side-channel attack can recover the\nstructures of popular CNN models, namely Lenet, Alexnet, and VGGnet16.",
        "translated": "卷积神经网络广泛应用于各个领域。基于数据流的 CNN 加速器的最新进展使得 CNN 能够在资源受限的边缘设备中进行推理。这些数据流加速器利用卷积层固有的数据重用来有效地处理 CNN 模型。隐藏 CNN 模型的架构对隐私和安全至关重要。本文通过评估基于内存的旁通道信息，从基于数据流的 CNN 推理加速器恢复 CNN 结构。该攻击利用 CNN 加速器上数据流映射的时空数据重用和体系结构提示来恢复 CNN 模型的结构。实验结果表明，我们提出的侧信道攻击可以恢复流行的 CNN 模型，即 Lenet，Alexnet 和 VGGnet16的结构。"
    },
    {
        "title": "Generalised DePIN Protocol: A Framework for Decentralized Physical\n  Infrastructure Networks",
        "url": "http://arxiv.org/abs/2311.00551v1",
        "pub_date": "2023-11-01",
        "summary": "This paper introduces the Generalised DePIN (GDP) protocol, a comprehensive\nframework for decentralized physical infrastructure networks. GDP establishes a\nmodular system, enabling tailored application across sectors like ridesharing\nand power systems. Leveraging device onboarding, multi-sensor redundancy, and a\nreward/penalty mechanism, GDP promotes genuine behavior and ensures\nnetwork-wide vigilance. Through continuous audits and updates, the protocol\nremains dynamic, ensuring sustainable decentralized operations.",
        "translated": "本文介绍了分散式物理基础设施网络的综合框架——广义 DePIN (GDP)协议。国内生产总值建立了一个模块化系统，使定制应用跨部门，如共乘和电力系统。利用设备上线、多传感器冗余和奖惩机制，GDP 促进了真正的行为，并确保了网络范围的警觉性。通过持续的审计和更新，协议保持动态，确保可持续的分散运作。"
    },
    {
        "title": "Towards Universal Atomic Composability: A Formal Model for Multi-Rollup\n  Environments on Ethereum",
        "url": "http://arxiv.org/abs/2311.00422v1",
        "pub_date": "2023-11-01",
        "summary": "This paper introduces a comprehensive formal model to address the atomic\ncomposability across multiple rollups on Ethereum. Drawing inspiration from\nclassical distributed system solutions and modern cryptographic techniques, the\nproposed model integrates mechanisms like buffering, dependency handling,\nconcurrency control, and the revolutionary zero-knowledge proofs. Beyond the\nmere proposal of the model, we delve into its practical implications, limits,\npotential pitfalls, and corrective measures, ensuring that the system remains\nresistant to manipulative or erroneous behaviors.",
        "translated": "本文介绍了一个综合的形式化模型，用于解决以太单元上多个汇总的原子可组合性问题。该模型从传统的分布式系统解决方案和现代密码技术中获得灵感，集成了缓冲、依赖处理、并发控制和革命性的零知识证明等机制。除了模型的提议之外，我们深入研究了它的实际意义、限制、潜在的陷阱和纠正措施，以确保系统仍然能够抵抗操纵或错误的行为。"
    },
    {
        "title": "On the Integration of Self-Sovereign Identity with TLS 1.3 Handshake to\n  Build Trust in IoT Systems",
        "url": "http://arxiv.org/abs/2311.00386v1",
        "pub_date": "2023-11-01",
        "summary": "The centralized PKI is not a suitable solution to provide identities in\nlarge-scale IoT systems. The main problem is the high cost of managing X.509\ncertificates throughout their lifecycle, from installation to regular updates\nand revocation. The Self-Sovereign Identity (SSI) is a decentralised option\nthat reduces the need for human intervention, and therefore has the potential\nto significantly reduce the complexity and cost associated to identity\nmanagement in large-scale IoT systems. However, to leverage the full potential\nof SSI, the authentication of IoT nodes needs to be moved from the application\nto the Transport Layer Security (TLS) level. This paper contributes to the\nadoption of SSI in large-scale IoT systems by addressing, for the first time,\nthe extension of the original TLS 1.3 handshake to support two new SSI\nauthentication modes while maintaining the interoperability with nodes\nimplementing the original handshake protocol. The open source implementation of\nthe new TLS 1.3 handshake protocol in OpenSSL is used to experimentally prove\nthe feasibility of the approach.",
        "translated": "在大规模物联网系统中，集中式 PKI 不适合提供身份认证。主要的问题是在 X. 509证书的整个生命周期中管理它们的高成本，从安装到定期更新和撤销。自我主权身份(SSI)是一种分散的选择，减少了对人类干预的需要，因此有可能显著降低大规模物联网系统中身份管理的复杂性和成本。然而，为了充分利用 SSI 的潜力，物联网节点的身份验证需要从应用程序转移到传输层安全(TLS)级别。本文首次将原有的 TLS 1.3握手协议扩展到支持两种新的 SSI 认证模式，同时保持了与实现原有握手协议的节点的互操作性，为大规模物联网系统采用 SSI 认证做出了贡献。利用 OpenSSL 中新的 TLS 1.3握手协议的开源实现，通过实验验证了该方法的可行性。"
    },
    {
        "title": "Architecture of Data Anomaly Detection-Enhanced Decentralized Expert\n  System for Early-Stage Alzheimer's Disease Prediction",
        "url": "http://arxiv.org/abs/2311.00373v1",
        "pub_date": "2023-11-01",
        "summary": "Alzheimer's Disease is a global health challenge that requires early and\naccurate detection to improve patient outcomes. Magnetic Resonance Imaging\n(MRI) holds significant diagnostic potential, but its effective analysis\nremains a formidable task. This study introduces a groundbreaking decentralized\nexpert system that cleverly combines blockchain technology with Artificial\nIntelligence (AI) to integrate robust anomaly detection for patient-submitted\ndata.\n  Traditional diagnostic methods often lead to delayed and imprecise\npredictions, especially in the early stages of the disease. Centralized data\nrepositories struggle to manage the immense volumes of MRI data, and persistent\nprivacy concerns hinder collaborative efforts. Our innovative solution\nharnesses decentralization to protect data integrity and patient privacy,\nfacilitated by blockchain technology. It not only emphasizes AI-driven MRI\nanalysis but also incorporates a sophisticated data anomaly detection\narchitecture. These mechanisms scrutinize patient-contributed data for various\nissues, including data quality problems and atypical findings within MRI\nimages.\n  Conducting an exhaustive check of MRI image correctness and quality directly\non the blockchain is impractical due to computational complexity and cost\nconstraints. Typically, such checks are performed off-chain, and the blockchain\nsecurely records the results. This comprehensive approach empowers our\ndecentralized app to provide more precise early-stage Alzheimer's Disease\npredictions. By merging the strengths of blockchain, AI, and anomaly detection,\nour system represents a pioneering step towards revolutionizing disease\ndiagnostics.",
        "translated": "阿尔茨海默病是一个全球性的健康挑战，需要早期和准确的检测来改善患者的预后。磁共振成像(MRI)具有显著的诊断潜力，但其有效的分析仍然是一项艰巨的任务。这项研究介绍了一个开创性的分散式专家系统，它巧妙地结合了区块链技术和人工智能(AI) ，为患者提交的数据集成了强大的异常检测。传统的诊断方法往往导致延迟和不准确的预测，特别是在疾病的早期阶段。集中化的数据存储库难以管理海量的 MRI 数据，持续的隐私担忧阻碍了协作努力。我们创新的解决方案利用区块链技术，利用地方分权保护数据完整性和病人隐私。它不仅强调人工智能驱动的核磁共振成像分析，而且还采用了复杂的数据异常检测架构。这些机制仔细检查病人贡献的数据的各种问题，包括数据质量问题和非典型的 MRI 图像发现。由于计算量和成本的限制，直接在区块链上对 MRI 图像的正确性和质量进行详尽的检查是不切实际的。通常，这样的检查是脱链执行的，区块链安全地记录结果。这种全面的方法使我们分散的应用程序能够提供更精确的早期阿尔茨海默病预测。通过融合区块链、人工智能和异常检测的优势，我们的系统代表着朝着革命性疾病诊断迈出的开创性一步。"
    },
    {
        "title": "Butson Hadamard matrices, bent sequences, and spherical codes",
        "url": "http://arxiv.org/abs/2311.00354v1",
        "pub_date": "2023-11-01",
        "summary": "We explore a notion of bent sequence attached to the data consisting of an\nHadamard matrix of order $n$ defined over the complex $q^{th}$ roots of unity,\nan eigenvalue of that matrix, and a Galois automorphism from the cyclotomic\nfield of order $q.$ In particular we construct self-dual bent sequences for\nvarious $q\\le 60$ and lengths $n\\le 21.$ Computational construction methods\ncomprise the resolution of polynomial systems by Groebner bases and eigenspace\ncomputations. Infinite families can be constructed from regular Hadamard\nmatrices, Bush-type Hadamard matrices, and generalized Boolean bent\nfunctions.As an application, we estimate the covering radius of the code\nattached to that matrix over $\\Z_q.$ We derive a lower bound on that quantity\nfor the Chinese Euclidean metric when bent sequences exist. We give the\nEuclidean distance spectrum, and bound above the covering radius of an attached\nspherical code, depending on its strength as a spherical design.",
        "translated": "我们探索了一个与数据相连的弯曲序列的概念，该序列包括定义在复数 $q ^ { th } $单位根上的一个阶为 $n $的 Hadamard 矩阵，该矩阵的一个特征值，以及来自分圆域为 $q 的一个伽罗瓦自同构。特别地，我们为各种 $q le 60 $和长度 $n le 21构造了自对偶弯曲序列。$计算构造方法包括由 Groebner 基解析多项式系统和特征空间计算。无穷族可以由正则 Hadamard 矩阵、 Bush 型 Hadamard 矩阵和广义 Boolean 弯曲函数构造。作为一个应用，我们估计附加在该矩阵上的代码的覆盖半径超过 $Z _ q。$当弯曲序列存在时，我们得到了中国欧氏度量的下界。我们给出了欧几里得度量谱，并且根据其作为球形设计的强度，约束在附加球形码的覆盖半径之上。"
    },
    {
        "title": "Adversarially Robust Distributed Count Tracking via Partial Differential\n  Privacy",
        "url": "http://arxiv.org/abs/2311.00346v1",
        "pub_date": "2023-11-01",
        "summary": "We study the distributed tracking model, also known as distributed functional\nmonitoring. This model involves $k$ sites each receiving a stream of items and\ncommunicating with the central server. The server's task is to track a function\nof all items received thus far continuously, with minimum communication cost.\nFor count tracking, it is known that there is a $\\sqrt{k}$ gap in communication\nbetween deterministic and randomized algorithms. However, existing randomized\nalgorithms assume an \"oblivious adversary\" who constructs the entire input\nstreams before the algorithm starts. Here we consider adaptive adversaries who\ncan choose new items based on previous answers from the algorithm.\nDeterministic algorithms are trivially robust to adaptive adversaries, while\nrandomized ones may not. Therefore, we investigate whether the $\\sqrt{k}$\nadvantage of randomized algorithms is from randomness itself or the oblivious\nadversary assumption. We provide an affirmative answer to this question by\ngiving a robust algorithm with optimal communication. Existing robustification\ntechniques do not yield optimal bounds due to the inherent challenges of the\ndistributed nature of the problem. To address this, we extend the differential\nprivacy framework by introducing \"partial differential privacy\" and proving a\nnew generalization theorem. This theorem may have broader applications beyond\nrobust count tracking, making it of independent interest.",
        "translated": "我们研究了分布式跟踪模型，也称为分布式功能监控。这个模型包括 $k $网站，每个网站接收一个项目流并与中央服务器通信。服务器的任务是跟踪到目前为止连续收到的所有项目的功能，以最小的通信成本。对于计数跟踪，我们知道在确定性算法和随机算法之间存在一个 $sqrt { k } $间隙。然而，现有的随机算法假设一个“健忘的对手”，在算法开始之前构造整个输入流。在这里，我们考虑适应性的对手谁可以选择新的项目基于以前的答案从算法。确定性算法对于适应性对手来说是非常稳健的，而随机算法则不然。因此，我们研究随机算法的 $sqrt { k } $优势是来自于随机性本身还是来自于不经意的对手假设。通过给出一个具有最优通信的鲁棒算法，我们对这个问题给出了一个肯定的答案。现有的鲁棒化技术由于问题的分布性质所固有的挑战而不能产生最优的界限。为了解决这个问题，我们扩展了差分隐私框架，引入了“部分差分隐私”，并证明了一个新的推广定理。这个定理可能有更广泛的应用超过鲁棒计数跟踪，使其独立的兴趣。"
    },
    {
        "title": "Stacking an autoencoder for feature selection of zero-day threats",
        "url": "http://arxiv.org/abs/2311.00304v1",
        "pub_date": "2023-11-01",
        "summary": "Zero-day attack detection plays a critical role in mitigating risks,\nprotecting assets, and staying ahead in the evolving threat landscape. This\nstudy explores the application of stacked autoencoder (SAE), a type of\nartificial neural network, for feature selection and zero-day threat\nclassification using a Long Short-Term Memory (LSTM) scheme. The process\ninvolves preprocessing the UGRansome dataset and training an unsupervised SAE\nfor feature extraction. Finetuning with supervised learning is then performed\nto enhance the discriminative capabilities of this model. The learned weights\nand activations of the autoencoder are analyzed to identify the most important\nfeatures for discriminating between zero-day threats and normal system\nbehavior. These selected features form a reduced feature set that enables\naccurate classification. The results indicate that the SAE-LSTM performs well\nacross all three attack categories by showcasing high precision, recall, and F1\nscore values, emphasizing the model's strong predictive capabilities in\nidentifying various types of zero-day attacks. Additionally, the balanced\naverage scores of the SAE-LSTM suggest that the model generalizes effectively\nand consistently across different attack categories.",
        "translated": "零日攻击检测在降低风险、保护资产以及在不断变化的威胁环境中保持领先地位方面发挥着关键作用。本研究探讨堆叠式自动编码器(SAE) ，一种人工神经网络，在特征选择和零日威胁分类中使用长短期记忆(LSTM)方案的应用。该过程包括对 UGRansome 数据集进行预处理，并训练一个无监督的 SAE 进行特征提取。然后与监督式学习进行微调，以增强该模型的判别能力。通过分析自动编码器的学习权值和激活，识别出区分零日威胁和正常系统行为的最重要特征。这些选定的特征形成一个简化的特征集，使准确的分类成为可能。结果表明，SAE-LSTM 通过展示高精度、召回率和 F1得分值，在所有三种攻击类型中表现良好，强调了该模型在识别各种类型的零日攻击方面的强大预测能力。此外，SAE-LSTM 的平衡平均得分表明，该模型有效和一致地推广了不同的攻击类别。"
    },
    {
        "title": "JADE: A Linguistics-based Safety Evaluation Platform for LLM",
        "url": "http://arxiv.org/abs/2311.00286v2",
        "pub_date": "2023-11-01",
        "summary": "In this paper, we present JADE, a targeted linguistic fuzzing platform which\nstrengthens the linguistic complexity of seed questions to simultaneously and\nconsistently break a wide range of widely-used LLMs categorized in three\ngroups: eight open-sourced Chinese, six commercial Chinese and four commercial\nEnglish LLMs. JADE generates three safety benchmarks for the three groups of\nLLMs, which contain unsafe questions that are highly threatening: the questions\nsimultaneously trigger harmful generation of multiple LLMs, with an average\nunsafe generation ratio of $70\\%$ (please see the table below), while are still\nnatural questions, fluent and preserving the core unsafe semantics. We release\nthe benchmark demos generated for commercial English LLMs and open-sourced\nEnglish LLMs in the following link: https://github.com/whitzard-ai/jade-db. For\nreaders who are interested in evaluating on more questions generated by JADE,\nplease contact us.\n  JADE is based on Noam Chomsky's seminal theory of transformational-generative\ngrammar. Given a seed question with unsafe intention, JADE invokes a sequence\nof generative and transformational rules to increment the complexity of the\nsyntactic structure of the original question, until the safety guardrail is\nbroken. Our key insight is: Due to the complexity of human language, most of\nthe current best LLMs can hardly recognize the invariant evil from the infinite\nnumber of different syntactic structures which form an unbound example space\nthat can never be fully covered. Technically, the generative/transformative\nrules are constructed by native speakers of the languages, and, once developed,\ncan be used to automatically grow and transform the parse tree of a given\nquestion, until the guardrail is broken. For more evaluation results and demo,\nplease check our website: https://whitzard-ai.github.io/jade.html.",
        "translated": "本文提出了一个有针对性的语言模糊化平台 JADE，该平台增强了种子问题的语言复杂性，同时一致地突破了广泛使用的分为三类的 LLM: 八种开源汉语、六种商业汉语和四种商业英语 LLM。JADE 为三组 LLM 生成了三个安全基准，其中包含高度威胁性的不安全问题: 这些问题同时触发了多个 LLM 的有害生成，平均不安全生成比例为70% (请参阅下表) ，而仍然是自然的问题，流畅并保留了核心的不安全语义。我们在以下连结发布商业英语和开源英语 https://github.com/whitzard-ai/jade-db 的基准演示。如果读者有兴趣对 JADE 产生的更多问题进行评估，请与我们联系。JADE 是基于诺姆•乔姆斯基(Noam Chomsky)开创性的转换-生成文法理论。给定一个具有不安全意图的种子问题，JADE 调用一系列生成和转换规则来增加原问题句法结构的复杂性，直到安全护栏被打破。我们的主要见解是: 由于人类语言的复杂性，目前大多数最好的 LLM 很难从无数不同的句法结构中识别出不变的罪恶，这些不同的句法结构构成了一个永远无法被完全覆盖的无界的例子空间。从技术上讲，生成/转换规则是由语言的母语使用者构建的，一旦开发完成，就可以用来自动生长和转换给定问题的解析树，直到防护被打破。如欲了解更多评估结果和演示，请浏览我们的网站:  https://whitzard-ai.github.io/jade.html。"
    },
    {
        "title": "The Property Law of Crypto Tokens",
        "url": "http://arxiv.org/abs/2311.01461v1",
        "pub_date": "2023-11-02",
        "summary": "This article addresses the lack of comprehensive studies on Web3\ntechnologies, primarily due to lawyers' reluctance to explore technical\nintricacies. Understanding the underlying technological foundations is crucial\nto enhance the credibility of legal opinions. This article aims to illuminate\nthese foundations, debunk myths, and concentrate on determining the legal\nstatus of crypto-assets in the context of property rights within the\ndistributed economy. In addition, this article notes that the intangible nature\nof crypto-assets that derive value from distributed registries, and their\nresistance to deletion, makes crypto-assets more akin to the autonomy of\nintellectual property than physical media. The article presents illustrative\nexamples from common law (United States, United Kingdom, New Zealand) and civil\nlaw (Germany, Austria, Poland) systems. Proposing a universal solution, it\nadvocates a comprehensive framework safeguarding digital property - data\nownership - extending beyond the confines of Web3.\n  This article presents a comprehensive, multi-layered approach to the analysis\nof tokens as digital content and virtual goods. The approach, universally\napplicable to various of such goods, scrutinizes property on three distinct\nlayers: first, the rights to the virtual good itself; second, the rights to the\nassets linked to the virtual good; and third, the rights to the intellectual\nproperty intricately associated with the token. Additionally, the paper\nprovides concise analysis of the conflict of laws rules applicable to virtual\ngoods. It also delves into issues concerning formal requirements for the\ntransfer of intellectual property rights, licensing, the first sale\n(exhaustion) doctrine, the concept of the lawful acquirer, and other crucial\naspects of intellectual property in the realm of virtual goods, particularly\nwithin the emerging metaverse.",
        "translated": "本文讨论了缺乏对 Web3技术的全面研究，这主要是由于律师不愿意探索错综复杂的技术。了解基本的技术基础对于提高法律意见的可信度至关重要。本文旨在阐明这些基础，揭穿神话，并集中于确定加密资产在分布式经济中的产权背景下的法律地位。此外，本文指出，从分布式登记处获得价值的加密资产的无形性质及其对删除的抵制，使得加密资产更接近于知识产权的自主权，而不是实物媒体。本文介绍了英美法系(美国、英国、新西兰)和大陆法系(德国、奥地利、波兰)的实例。它提出了一个通用的解决方案，倡导一个全面的框架来保护数字财产-数据所有权-超越 Web3的范围。本文提出了一种全面的、多层次的方法来分析作为数字内容和虚拟商品的令牌。这种方法普遍适用于各种此类商品，对财产进行三个不同层面的审查: 第一，虚拟商品本身的权利; 第二，与虚拟商品相关联的资产的权利; 第三，与令牌错综复杂相关的知识产权的权利。此外，本文还对适用于虚拟物品的法律冲突规则进行了简要分析。它还深入探讨了关于知识产权转让的形式要求、许可、第一销售(用尽)原则、合法获得者的概念，以及虚拟商品领域中知识产权的其他关键方面，特别是在新兴的元宇宙中。"
    },
    {
        "title": "Like an Open Book? Read Neural Network Architecture with Simple Power\n  Analysis on 32-bit Microcontrollers",
        "url": "http://arxiv.org/abs/2311.01344v1",
        "pub_date": "2023-11-02",
        "summary": "Model extraction is a growing concern for the security of AI systems. For\ndeep neural network models, the architecture is the most important information\nan adversary aims to recover. Being a sequence of repeated computation blocks,\nneural network models deployed on edge-devices will generate distinctive\nside-channel leakages. The latter can be exploited to extract critical\ninformation when targeted platforms are physically accessible. By combining\ntheoretical knowledge about deep learning practices and analysis of a\nwidespread implementation library (ARM CMSIS-NN), our purpose is to answer this\ncritical question: how far can we extract architecture information by simply\nexamining an EM side-channel trace? For the first time, we propose an\nextraction methodology for traditional MLP and CNN models running on a high-end\n32-bit microcontroller (Cortex-M7) that relies only on simple pattern\nrecognition analysis. Despite few challenging cases, we claim that, contrary to\nparameters extraction, the complexity of the attack is relatively low and we\nhighlight the urgent need for practicable protections that could fit the strong\nmemory and latency requirements of such platforms.",
        "translated": "模型提取越来越受到人工智能系统安全性的关注。对于深度神经网络模型来说，体系结构是对手要恢复的最重要的信息。由于神经网络模型是一系列重复的计算块，部署在边缘器件上会产生明显的边通道泄漏。当目标平台可以物理访问时，可以利用后者提取关键信息。通过结合有关深度学习实践的理论知识和对广泛应用的实现库(ARM CMSIS-NN)的分析，我们的目的是回答这个关键问题: 通过简单地检查 EM 边通道跟踪，我们能在多大程度上提取体系结构信息？首次提出了一种基于高端32位微控制器(Cortex-M7)的传统 MLP 和 CNN 模型的提取方法，该方法仅依赖于简单的模式识别分析。尽管很少有具有挑战性的案例，但我们声称，与参数提取相反，攻击的复杂性相对较低，我们强调迫切需要切实可行的保护措施，以满足此类平台的强大内存和延迟要求。"
    },
    {
        "title": "Securing Wireless Communication in Critical Infrastructure: Challenges\n  and Opportunities",
        "url": "http://arxiv.org/abs/2311.01338v1",
        "pub_date": "2023-11-02",
        "summary": "Critical infrastructure constitutes the foundation of every society. While\ntraditionally solely relying on dedicated cable-based communication, this\ninfrastructure rapidly transforms to highly digitized and interconnected\nsystems which increasingly rely on wireless communication. Besides providing\ntremendous benefits, especially affording the easy, cheap, and flexible\ninterconnection of a large number of assets spread over larger geographic\nareas, wireless communication in critical infrastructure also raises unique\nsecurity challenges. Most importantly, the shift from dedicated private wired\nnetworks to heterogeneous wireless communication over public and shared\nnetworks requires significantly more involved security measures. In this paper,\nwe identify the most relevant challenges resulting from the use of wireless\ncommunication in critical infrastructure and use those to identify a\ncomprehensive set of promising opportunities to preserve the high security\nstandards of critical infrastructure even when switching from wired to wireless\ncommunication.",
        "translated": "关键的基础设施是每个社会的基础。虽然传统上仅仅依赖于专用的基于电缆的通信，但这种基础设施迅速转变为高度数字化和相互连接的系统，这些系统越来越依赖于无线通信。除了提供巨大的好处，特别是提供分布在更大地理区域上的大量资产的简单、廉价和灵活的互连之外，关键基础设施中的无线通信也提出了独特的安全挑战。最重要的是，从专用有线网络到公共和共享网络上的异构无线通信的转变需要更多的安全措施。在本文中，我们确定了在关键基础设施中使用无线通信所带来的最相关的挑战，并利用这些挑战来确定一整套有希望的机会，以保持关键基础设施的高安全标准，即使在从有线通信切换到无线通信时也是如此。"
    },
    {
        "title": "Towards Evaluating Transfer-based Attacks Systematically, Practically,\n  and Fairly",
        "url": "http://arxiv.org/abs/2311.01323v1",
        "pub_date": "2023-11-02",
        "summary": "The adversarial vulnerability of deep neural networks (DNNs) has drawn great\nattention due to the security risk of applying these models in real-world\napplications. Based on transferability of adversarial examples, an increasing\nnumber of transfer-based methods have been developed to fool black-box DNN\nmodels whose architecture and parameters are inaccessible. Although tremendous\neffort has been exerted, there still lacks a standardized benchmark that could\nbe taken advantage of to compare these methods systematically, fairly, and\npractically. Our investigation shows that the evaluation of some methods needs\nto be more reasonable and more thorough to verify their effectiveness, to\navoid, for example, unfair comparison and insufficient consideration of\npossible substitute/victim models. Therefore, we establish a transfer-based\nattack benchmark (TA-Bench) which implements 30+ methods. In this paper, we\nevaluate and compare them comprehensively on 25 popular substitute/victim\nmodels on ImageNet. New insights about the effectiveness of these methods are\ngained and guidelines for future evaluations are provided. Code at:\nhttps://github.com/qizhangli/TA-Bench.",
        "translated": "由于深层神经网络模型在实际应用中存在安全风险，因此深层神经网络的对抗性易受攻击性问题引起了人们的广泛关注。基于对抗性实例的可转移性，越来越多的基于转移的方法被开发用来欺骗结构和参数不可访问的黑盒 DNN 模型。尽管已经付出了巨大的努力，但仍然缺乏一个可以利用的标准化基准来系统、公平和实际地比较这些方法。我们的调查表明，一些方法的评估需要更加合理和更加彻底，以验证其有效性，避免，例如，不公平的比较和不充分考虑可能的替代/受害者模型。因此，我们建立了一个基于传输的攻击基准(TA-Bench) ，它实现了30多种方法。本文对 ImageNet 上流行的25种替代/受害模型进行了综合评价和比较。对这些方法的有效性有了新的认识，并为今后的评价提供了指导方针。密码:  https://github.com/qizhangli/ta-bench。"
    },
    {
        "title": "DP-Mix: Mixup-based Data Augmentation for Differentially Private\n  Learning",
        "url": "http://arxiv.org/abs/2311.01295v1",
        "pub_date": "2023-11-02",
        "summary": "Data augmentation techniques, such as simple image transformations and\ncombinations, are highly effective at improving the generalization of computer\nvision models, especially when training data is limited. However, such\ntechniques are fundamentally incompatible with differentially private learning\napproaches, due to the latter's built-in assumption that each training image's\ncontribution to the learned model is bounded. In this paper, we investigate why\nnaive applications of multi-sample data augmentation techniques, such as mixup,\nfail to achieve good performance and propose two novel data augmentation\ntechniques specifically designed for the constraints of differentially private\nlearning. Our first technique, DP-Mix_Self, achieves SoTA classification\nperformance across a range of datasets and settings by performing mixup on\nself-augmented data. Our second technique, DP-Mix_Diff, further improves\nperformance by incorporating synthetic data from a pre-trained diffusion model\ninto the mixup process. We open-source the code at\nhttps://github.com/wenxuan-Bao/DP-Mix.",
        "translated": "数据增强技术，如简单的图像变换和组合，对提高计算机视觉模型的泛化能力非常有效，特别是在训练数据有限的情况下。然而，由于后者的内置假设，即每个训练图像对学习模型的贡献是有限的，这些技术与差异私有学习方法在根本上是不兼容的。在本文中，我们研究了为什么多样本数据增强技术的初始应用，如混合，未能取得良好的性能，并提出了两种新的数据增强技术，专门设计的差异私有学习的约束条件。我们的第一项技术 DP-Mix _ Self 通过对自增强数据进行混合，实现了跨一系列数据集和设置的 SoTA 分类性能。我们的第二个技术，DP-Mix _ Diff，通过将预先训练好的扩散模型的合成数据合并到混合过程中，进一步提高了性能。我们在 https://github.com/wenxuan-bao/dp-mix 开源代码。"
    },
    {
        "title": "Emergent (In)Security of Multi-Cloud Environments",
        "url": "http://arxiv.org/abs/2311.01247v1",
        "pub_date": "2023-11-02",
        "summary": "As organizations increasingly use cloud services to host their IT\ninfrastructure, there is a need to share data among these cloud hosted services\nand systems. A majority of IT organizations have workloads spread across\ndifferent cloud service providers, growing their multi-cloud environments. When\nan organization grows their multi-cloud environment, the threat vectors and\nvulnerabilities for their cloud systems and services grow as well. The increase\nin the number of attack vectors creates a challenge of how to prioritize\nmitigations and countermeasures to best defend a multi-cloud environment\nagainst attacks. Utilizing multiple industry standard risk analysis tools, we\nconducted an analysis of multi-cloud threat vectors enabling calculation and\nprioritization for the identified mitigations and countermeasures. The\nprioritizations from the analysis showed that authentication and architecture\nare the highest risk areas of threat vectors. Armed with this data, IT managers\nare able to more appropriately budget cybersecurity expenditure to implement\nthe most impactful mitigations and countermeasures.",
        "translated": "随着组织越来越多地使用云服务来托管它们的 IT 基础设施，有必要在这些云托管服务和系统之间共享数据。大多数 IT 组织的工作负载分布在不同的云服务提供商之间，增加了它们的多云环境。当一个组织发展其多云环境时，其云系统和服务的威胁向量和漏洞也会增长。攻击载体数量的增加给如何优先考虑缓解和对策以最好地保护多云环境免受攻击带来了挑战。利用多个行业标准风险分析工具，我们对多云威胁向量进行了分析，从而能够计算和确定缓解措施和对策的优先级。分析中的优先级表明，身份验证和体系结构是威胁向量的最高风险领域。有了这些数据，IT 管理人员就能够更恰当地预算网络安全支出，以实施最有效的缓解措施和对策。"
    },
    {
        "title": "Attacking Graph Neural Networks with Bit Flips: Weisfeiler and Lehman Go\n  Indifferent",
        "url": "http://arxiv.org/abs/2311.01205v1",
        "pub_date": "2023-11-02",
        "summary": "Prior attacks on graph neural networks have mostly focused on graph poisoning\nand evasion, neglecting the network's weights and biases. Traditional\nweight-based fault injection attacks, such as bit flip attacks used for\nconvolutional neural networks, do not consider the unique properties of graph\nneural networks. We propose the Injectivity Bit Flip Attack, the first bit flip\nattack designed specifically for graph neural networks. Our attack targets the\nlearnable neighborhood aggregation functions in quantized message passing\nneural networks, degrading their ability to distinguish graph structures and\nlosing the expressivity of the Weisfeiler-Lehman test. Our findings suggest\nthat exploiting mathematical properties specific to certain graph neural\nnetwork architectures can significantly increase their vulnerability to bit\nflip attacks. Injectivity Bit Flip Attacks can degrade the maximal expressive\nGraph Isomorphism Networks trained on various graph property prediction\ndatasets to random output by flipping only a small fraction of the network's\nbits, demonstrating its higher destructive power compared to a bit flip attack\ntransferred from convolutional neural networks. Our attack is transparent and\nmotivated by theoretical insights which are confirmed by extensive empirical\nresults.",
        "translated": "先前对图神经网络的攻击主要集中在图的中毒和规避上，忽略了网络的权重和偏差。传统的基于权重的故障注入攻击，如用于卷积神经网络的比特翻转攻击，没有考虑到图神经网络的独特性质。我们提出了注入性比特翻转攻击，第一个比特翻转攻击是专门为图形神经网络设计的。我们的攻击目标是量化信息传递神经网络中可学习的邻域聚集函数，降低了它们区分图结构的能力，丧失了 Weisfeiler-Lehman 检验的表达能力。我们的研究结果表明，利用特定于某些图形神经网络结构的数学特性可以显著增加它们对位翻转攻击的脆弱性。注入性比特翻转攻击可以通过只翻转网络比特的一小部分，将在各种图性质预测数据集上训练的最大表达式图同构网络降级为随机输出，表明其破坏力高于从卷积神经网络转移的比特翻转攻击。我们的攻击是透明的，动机是理论上的洞察力，被广泛的实证结果所证实。"
    },
    {
        "title": "A Review of Digital Twins and their Application in Cybersecurity based\n  on Artificial Intelligence",
        "url": "http://arxiv.org/abs/2311.01154v1",
        "pub_date": "2023-11-02",
        "summary": "The potential of digital twin technology is yet to be fully realized due to\nits diversity and untapped potential. Digital twins enable systems' analysis,\ndesign, optimization, and evolution to be performed digitally or in conjunction\nwith a cyber-physical approach to improve speed, accuracy, and efficiency over\ntraditional engineering methods. Industry 4.0, factories of the future, and\ndigital twins continue to benefit from the technology and provide enhanced\nefficiency within existing systems. Due to the lack of information and security\nstandards associated with the transition to cyber digitization, cybercriminals\nhave been able to take advantage of the situation. Access to a digital twin of\na product or service is equivalent to threatening the entire collection. There\nis a robust interaction between digital twins and artificial intelligence\ntools, which leads to strong interaction between these technologies, so it can\nbe used to improve the cybersecurity of these digital platforms based on their\nintegration with these technologies. This study aims to investigate the role of\nartificial intelligence in providing cybersecurity for digital twin versions of\nvarious industries, as well as the risks associated with these versions. In\naddition, this research serves as a road map for researchers and others\ninterested in cybersecurity and digital security.",
        "translated": "由于数字孪生技术的多样性和尚未开发的潜力，其潜力尚未充分实现。数字双胞胎使得系统的分析、设计、优化和进化能够以数字方式进行，或者结合网络物理方法来提高速度、准确性和效率，而不是传统的工程方法。工业4.0、未来的工厂和数字双胞胎继续受益于该技术，并在现有系统内提供更高的效率。由于缺乏与向网络数字化过渡有关的信息和安全标准，网络犯罪分子得以利用这种情况。访问一个产品或服务的数字孪生兄弟相当于威胁到整个收藏。数字双胞胎和人工智能工具之间存在着强有力的互动，这导致了这些技术之间的强有力的互动，因此它可以用来改善这些数字平台的网络安全，基于它们与这些技术的整合。本研究旨在探讨人工智能在为不同行业的数字化双生版本提供网络安全方面所扮演的角色，以及与这些版本相关的风险。此外，这项研究为研究人员和其他对网络安全和数字安全感兴趣的人提供了路线图。"
    },
    {
        "title": "A Survey on Coin Selection Algorithms in UTXO-based Blockchains",
        "url": "http://arxiv.org/abs/2311.01113v1",
        "pub_date": "2023-11-02",
        "summary": "Coin selection algorithms are a fundamental component of blockchain\ntechnology. In this paper, we present a comprehensive review of the existing\ncoin selection algorithms utilized in unspent transaction output (UTXO)-based\nblockchains. We provide a list of the desired objectives and categorize\nexisting algorithms into three types: primitive, basic, and advanced\nalgorithms. This allows for a structured understanding of their functionalities\nand limitations. We also evaluate the performance of existing coin selection\nalgorithms. The aim of this paper is to provide system researchers and\ndevelopers with a concrete view of the current design landscape.",
        "translated": "硬币选择算法是区块链技术的基本组成部分。本文综述了基于未用事务输出(UTXO)区块链的现有硬币选择算法。我们提供了所需目标的列表，并将现有算法分为三种类型: 原始算法、基本算法和高级算法。这允许对它们的功能和局限性进行结构化的理解。我们还评估了现有的硬币选择算法的性能。本文的目的是为系统研究人员和开发人员提供一个具体的设计景观。"
    },
    {
        "title": "Reputation Systems for Supply Chains: The Challenge of Achieving Privacy\n  Preservation",
        "url": "http://arxiv.org/abs/2311.01060v1",
        "pub_date": "2023-11-02",
        "summary": "Consumers frequently interact with reputation systems to rate products,\nservices, and deliveries. While past research extensively studied different\nconceptual approaches to realize such systems securely and\nprivacy-preservingly, these concepts are not yet in use in business-to-business\nenvironments. In this paper, (1) we thus outline which specific challenges\nprivacy-cautious stakeholders in volatile supply chain networks introduce, (2)\ngive an overview of the diverse landscape of privacy-preserving reputation\nsystems and their properties, and (3) based on well-established concepts from\nsupply chain information systems and cryptography, we further propose an\ninitial concept that accounts for the aforementioned challenges by utilizing\nfully homomorphic encryption. For future work, we identify the need of\nevaluating whether novel systems address the supply chain-specific privacy and\nconfidentiality needs.",
        "translated": "消费者经常与信誉系统进行交互，以对产品、服务和交付进行评级。虽然过去的研究广泛地研究了不同的概念方法来安全和保护隐私地实现这些系统，但这些概念还没有在 B2B 环境中使用。在本文中，(1)我们因此概述了在不稳定的供应链网络中保护隐私谨慎的利益相关者所带来的具体挑战，(2)概述了保护隐私的声誉系统及其属性的多样化前景，(3)基于供应链信息系统和密码学的成熟概念，我们进一步提出了一个初步概念，通过充分利用同态加密来解决上述挑战。对于未来的工作，我们确定需要评估新系统是否满足特定于供应链的隐私和保密需求。"
    },
    {
        "title": "Architecture of Smart Certificates for Web3 Applications Against\n  Cyberthreats in Financial Industry",
        "url": "http://arxiv.org/abs/2311.01956v1",
        "pub_date": "2023-11-03",
        "summary": "This study addresses the security challenges associated with the current\ninternet transformations, specifically focusing on emerging technologies such\nas blockchain and decentralized storage. It also investigates the role of Web3\napplications in shaping the future of the internet. The primary objective is to\npropose a novel design for 'smart certificates,' which are digital certificates\nthat can be programmatically enforced. Utilizing such certificates, an\nenterprise can better protect itself from cyberattacks and ensure the security\nof its data and systems. Web3 recent security solutions by companies and\nprojects like Certik, Forta, Slither, and Securify are the equivalent of code\nscanning tool that were originally developed for Web1 and Web2 applications,\nand definitely not like certificates to help enterprises feel safe against\ncyberthreats. We aim to improve the resilience of enterprises' digital\ninfrastructure by building on top of Web3 application and put methodologies in\nplace for vulnerability analysis and attack correlation, focusing on\narchitecture of different layers, Wallet/Client, Application and Smart\nContract, where specific components are provided to identify and predict\nthreats and risks. Furthermore, Certificate Transparency is used for enhancing\nthe security, trustworthiness and decentralized management of the certificates,\nand detecting misuses, compromises, and malfeasances.",
        "translated": "这项研究解决了与当前互联网转型相关的安全挑战，特别侧重于新兴技术，如区块链和分散存储。它还调查了 Web3应用程序在塑造互联网未来中的作用。主要目标是提出一种新颖的“智能证书”设计，这是一种可以通过编程强制执行的数字证书。利用这些证书，企业可以更好地保护自己免受网络攻击，并确保其数据和系统的安全。由 Certik、 Forta、 Slither 和 Securify 等公司和项目提供的 Web3最近的安全解决方案相当于最初为 Web1和 Web2应用程序开发的代码扫描工具，绝对不像帮助企业感到安全的证书那样能够抵御网络威胁。我们的目标是提高企业数字基础设施的弹性，建立在 Web3应用之上，并为脆弱性分析和攻击相关性提供适当的方法，重点关注不同层次的体系结构，Wallet/客户端，应用程序和智能合同，其中提供特定的组件来识别和预测威胁和风险。此外，证书透明度用于增强证书的安全性、可信性和分散管理，并检测错误、妥协和不当行为。"
    },
    {
        "title": "CoPriv: Network/Protocol Co-Optimization for Communication-Efficient\n  Private Inference",
        "url": "http://arxiv.org/abs/2311.01737v1",
        "pub_date": "2023-11-03",
        "summary": "Deep neural network (DNN) inference based on secure 2-party computation (2PC)\ncan offer cryptographically-secure privacy protection but suffers from orders\nof magnitude latency overhead due to enormous communication. Previous works\nheavily rely on a proxy metric of ReLU counts to approximate the communication\noverhead and focus on reducing the ReLUs to improve the communication\nefficiency. However, we observe these works achieve limited communication\nreduction for state-of-the-art (SOTA) 2PC protocols due to the ignorance of\nother linear and non-linear operations, which now contribute to the majority of\ncommunication. In this work, we present CoPriv, a framework that jointly\noptimizes the 2PC inference protocol and the DNN architecture. CoPriv features\na new 2PC protocol for convolution based on Winograd transformation and\ndevelops DNN-aware optimization to significantly reduce the inference\ncommunication. CoPriv further develops a 2PC-aware network optimization\nalgorithm that is compatible with the proposed protocol and simultaneously\nreduces the communication for all the linear and non-linear operations. We\ncompare CoPriv with the SOTA 2PC protocol, CrypTFlow2, and demonstrate 2.1x\ncommunication reduction for both ResNet-18 and ResNet-32 on CIFAR-100. We also\ncompare CoPriv with SOTA network optimization methods, including SNL,\nMetaPruning, etc. CoPriv achieves 9.98x and 3.88x online and total\ncommunication reduction with a higher accuracy compare to SNL, respectively.\nCoPriv also achieves 3.87x online communication reduction with more than 3%\nhigher accuracy compared to MetaPruning.",
        "translated": "基于安全二方计算(2PC)的深度神经网络(DNN)推理可以提供加密安全的隐私保护，但由于庞大的通信量而带来数量级延迟开销。以往的研究主要依赖于 ReLU 计数的代理度量来近似计算通信开销，并着重于减少 ReLU 以提高通信效率。然而，我们观察到，由于忽视了其他线性和非线性操作，这些工作对最先进的(SOTA)2PC 协议实现了有限的通信减少，这些操作现在占了通信的大部分。在这项工作中，我们提出了 CoPriv，一个框架，联合优化2PC 推理协议和 DNN 架构。CoPriv 提供了一种基于 Winograd 变换的新的2PC 卷积协议，并开发了基于 DNN 的优化以显著减少推理通信。CoPriv 进一步开发了一种支持2PC 的网络优化算法，该算法与所提出的协议兼容，同时减少了所有线性和非线性操作的通信。我们将 CoPriv 与 SOTA 2PC 协议 CrypTFlow2进行了比较，并在 CIFAR-100上证明了 ResNet-18和 ResNet-32的通信减少了2.1倍。我们还比较了 CoPriv 和 SOTA 网络优化方法，包括 SNL、 MetaPruning 等。与 SNL 相比，CoPriv 分别实现了9.98 x 和3.88 x 的在线和总体通信减少，精度更高。与 MetaPruning 相比，CoPriv 还实现了3.87倍的在线通信减少，准确率提高了3% 以上。"
    },
    {
        "title": "Adversarial Attacks on Cooperative Multi-agent Bandits",
        "url": "http://arxiv.org/abs/2311.01698v1",
        "pub_date": "2023-11-03",
        "summary": "Cooperative multi-agent multi-armed bandits (CMA2B) consider the\ncollaborative efforts of multiple agents in a shared multi-armed bandit game.\nWe study latent vulnerabilities exposed by this collaboration and consider\nadversarial attacks on a few agents with the goal of influencing the decisions\nof the rest. More specifically, we study adversarial attacks on CMA2B in both\nhomogeneous settings, where agents operate with the same arm set, and\nheterogeneous settings, where agents have distinct arm sets. In the homogeneous\nsetting, we propose attack strategies that, by targeting just one agent,\nconvince all agents to select a particular target arm $T-o(T)$ times while\nincurring $o(T)$ attack costs in $T$ rounds. In the heterogeneous setting, we\nprove that a target arm attack requires linear attack costs and propose attack\nstrategies that can force a maximum number of agents to suffer linear regrets\nwhile incurring sublinear costs and only manipulating the observations of a few\ntarget agents. Numerical experiments validate the effectiveness of our proposed\nattack strategies.",
        "translated": "合作式多智能体多武装匪徒(CMA2B)考虑了多智能体在共享多臂老虎机游戏中的协同努力。我们研究这种合作所暴露的潜在弱点，并考虑对少数代理的敌对攻击，目的是影响其他代理的决策。更具体地说，我们研究了 CMA2B 在同质环境中的对抗性攻击，其中代理人使用相同的臂集操作，以及在异质环境中代理人具有不同的臂集。在同质环境下，我们提出了攻击策略，通过只瞄准一个代理，说服所有代理选择一个特定的目标武器 $T-o (T) $次，同时在 $T $子弹中产生 $o (T) $攻击成本。在异构环境下，我们证明了一个目标武器攻击需要线性攻击代价，并提出了攻击策略，可以迫使最大数量的代理遭受线性后悔，同时产生次线性代价和只操纵少数目标代理的观察。数值实验验证了我们提出的攻击策略的有效性。"
    },
    {
        "title": "Universal Perturbation-based Secret Key-Controlled Data Hiding",
        "url": "http://arxiv.org/abs/2311.01696v1",
        "pub_date": "2023-11-03",
        "summary": "Deep neural networks (DNNs) are demonstrated to be vulnerable to universal\nperturbation, a single quasi-perceptible perturbation that can deceive the DNN\non most images. However, the previous works are focused on using universal\nperturbation to perform adversarial attacks, while the potential usability of\nuniversal perturbation as data carriers in data hiding is less explored,\nespecially for the key-controlled data hiding method. In this paper, we propose\na novel universal perturbation-based secret key-controlled data-hiding method,\nrealizing data hiding with a single universal perturbation and data decoding\nwith the secret key-controlled decoder. Specifically, we optimize a single\nuniversal perturbation, which serves as a data carrier that can hide multiple\nsecret images and be added to most cover images. Then, we devise a secret\nkey-controlled decoder to extract different secret images from the single\ncontainer image constructed by the universal perturbation by using different\nsecret keys. Moreover, a suppress loss function is proposed to prevent the\nsecret image from leakage. Furthermore, we adopt a robust module to boost the\ndecoder's capability against corruption. Finally, A co-joint optimization\nstrategy is proposed to find the optimal universal perturbation and decoder.\nExtensive experiments are conducted on different datasets to demonstrate the\neffectiveness of the proposed method. Additionally, the physical test performed\non platforms (e.g., WeChat and Twitter) verifies the usability of the proposed\nmethod in practice.",
        "translated": "深层神经网络(DNN)被证明是脆弱的普遍扰动，一个单一的准可感知的扰动，可以欺骗 DNN 对大多数图像。然而，以往的研究主要集中在利用通用扰动进行对抗性攻击，而对于通用扰动作为数据隐藏载体的潜在可用性，尤其是密钥控制的数据隐藏方法的研究较少。本文提出了一种新的基于通用扰动的密钥控制数据隐藏方法，实现了一次通用扰动的数据隐藏和密钥控制解码器的数据解码。具体来说，我们优化了一个单一的通用扰动，作为一个数据载体，可以隐藏多个秘密图像，并添加到大多数覆盖图像。然后，我们设计了一个密钥控制的解码器，利用不同的密钥从通用摄动构造的单个容器图像中提取不同的密钥图像。此外，提出了一种抑制损耗函数，以防止泄漏的秘密图像。此外，我们采用了一个健壮的模块，以提高解码器的能力，防止腐败。最后，提出了一种共同优化策略来寻找最优的通用扰动和解码器。为了验证该方法的有效性，在不同的数据集上进行了大量的实验。此外，在平台(如微信和 Twitter)上进行的物理测试验证了该方法在实践中的可用性。"
    },
    {
        "title": "CiFlow: Dataflow Analysis and Optimization of Key Switching for\n  Homomorphic Encryption",
        "url": "http://arxiv.org/abs/2311.01598v1",
        "pub_date": "2023-11-02",
        "summary": "Homomorphic encryption (HE) is a privacy-preserving computation technique\nthat enables computation on encrypted data. Today, the potential of HE remains\nlargely unrealized as it is impractically slow, preventing it from being used\nin real applications. A major computational bottleneck in HE is the\nkey-switching operation, accounting for approximately 70% of the overall HE\nexecution time and involving a large amount of data for inputs, intermediates,\nand keys. Prior research has focused on hardware accelerators to improve HE\nperformance, typically featuring large on-chip SRAMs and high off-chip\nbandwidth to deal with large scale data. In this paper, we present a novel\napproach to improve key-switching performance by rigorously analyzing its\ndataflow. Our primary goal is to optimize data reuse with limited on-chip\nmemory to minimize off-chip data movement. We introduce three distinct\ndataflows: Max-Parallel (MP), Digit-Centric (DC), and Output-Centric (OC), each\nwith unique scheduling approaches for key-switching computations. Through our\nanalysis, we show how our proposed Output-Centric technique can effectively\nreuse data by significantly lowering the intermediate key-switching working set\nand alleviating the need for massive off-chip bandwidth. We thoroughly evaluate\nthe three dataflows using the RPU, a recently published vector processor\ntailored for ring processing algorithms, which includes HE. This evaluation\nconsiders sweeps of bandwidth and computational throughput, and whether keys\nare buffered on-chip or streamed. With OC, we demonstrate up to 4.16x speedup\nover the MP dataflow and show how OC can save 16x on-chip SRAM by streaming\nkeys for minimal performance penalty.",
        "translated": "同态加密(HE)是一种保护隐私的计算技术，能够在加密数据上进行计算。今天，高等教育的潜力仍然很大程度上没有实现，因为它是不切实际的缓慢，防止它被用于实际应用。HE 中的一个主要计算瓶颈是键切换操作，约占整个 HE 执行时间的70% ，涉及大量的输入、中间件和键的数据。先前的研究集中在硬件加速器，以提高 HE 的性能，通常具有大型片上 SRAM 和高片外带宽，以处理大规模的数据。在本文中，我们提出了一种新的方法，以提高密钥切换性能的严格分析其数据流。我们的主要目标是利用有限的片上存储器优化数据重用，以最小化片外数据移动。我们引入了三种不同的数据流: 最大并行(MP)、数字中心(DC)和输出中心(OC) ，每种数据流都有用于键切换计算的独特调度方法。通过分析，我们展示了我们提出的以输出为中心的技术如何通过显著降低中间密钥交换工作集和减少对大规模片外带宽的需求来有效地重用数据。我们使用最近发布的专为环处理算法(包括 HE)定制的并行向量处理机 RPU 对这三个数据流进行了全面评估。此评估考虑带宽和计算吞吐量的扫描，以及密钥是在芯片上缓冲还是在流中缓冲。使用 OC，我们演示了在 MP 数据流上的最高4.16倍加速，并展示了 OC 如何通过流键以最小的性能损失节省16倍片上 SRAM。"
    },
    {
        "title": "Assist Is Just as Important as the Goal: Image Resurfacing to Aid\n  Model's Robust Prediction",
        "url": "http://arxiv.org/abs/2311.01563v1",
        "pub_date": "2023-11-02",
        "summary": "Adversarial patches threaten visual AI models in the real world. The number\nof patches in a patch attack is variable and determines the attack's potency in\na specific environment. Most existing defenses assume a single patch in the\nscene, and the multiple patch scenarios are shown to overcome them. This paper\npresents a model-agnostic defense against patch attacks based on total\nvariation for image resurfacing (TVR). The TVR is an image-cleansing method\nthat processes images to remove probable adversarial regions. TVR can be\nutilized solely or augmented with a defended model, providing multi-level\nsecurity for robust prediction. TVR nullifies the influence of patches in a\nsingle image scan with no prior assumption on the number of patches in the\nscene. We validate TVR on the ImageNet-Patch benchmark dataset and with\nreal-world physical objects, demonstrating its ability to mitigate patch\nattack.",
        "translated": "对抗性补丁威胁着现实世界中的视觉人工智能模型。补丁攻击中的补丁数量是可变的，它决定了攻击在特定环境中的效力。大多数现有的防御系统假设场景中只有一个补丁，而多个补丁场景显示可以克服它们。提出了一种基于全变分的模型不可知的补丁攻击防御方法。TVR 是一种图像清理方法，处理图像，以消除可能的敌对区域。TVR 可以单独使用，也可以通过防御模型进行增强，从而为鲁棒预测提供多层次的安全性。TVR 在没有事先假设场景中斑块数的情况下，消除了单个图像扫描中斑块的影响。我们验证了 ImageNet-Patch 基准数据集和真实世界物理对象的 TVR，证明了它减轻补丁攻击的能力。"
    },
    {
        "title": "VFCFinder: Seamlessly Pairing Security Advisories and Patches",
        "url": "http://arxiv.org/abs/2311.01532v1",
        "pub_date": "2023-11-02",
        "summary": "Security advisories are the primary channel of communication for discovered\nvulnerabilities in open-source software, but they often lack crucial\ninformation. Specifically, 63% of vulnerability database reports are missing\ntheir patch links, also referred to as vulnerability fixing commits (VFCs).\nThis paper introduces VFCFinder, a tool that generates the top-five ranked set\nof VFCs for a given security advisory using Natural Language Programming\nLanguage (NL-PL) models. VFCFinder yields a 96.6% recall for finding the\ncorrect VFC within the Top-5 commits, and an 80.0% recall for the Top-1 ranked\ncommit. VFCFinder generalizes to nine different programming languages and\noutperforms state-of-the-art approaches by 36 percentage points in terms of\nTop-1 recall. As a practical contribution, we used VFCFinder to backfill over\n300 missing VFCs in the GitHub Security Advisory (GHSA) database. All of the\nVFCs were accepted and merged into the GHSA database. In addition to\ndemonstrating a practical pairing of security advisories to VFCs, our general\nopen-source implementation will allow vulnerability database maintainers to\ndrastically improve data quality, supporting efforts to secure the software\nsupply chain.",
        "translated": "安全警告是发现开源软件漏洞的主要通讯渠道，但它们往往缺乏关键信息。具体来说，63% 的漏洞数据库报告缺少它们的补丁链接，这也被称为漏洞修复提交(VFC)。本文介绍了 VFCFFinder，这是一个使用自然语言编程语言(NL-PL)模型为给定的安全咨询生成排名前五位的 VFCs 集合的工具。VFCFFinder 对于在 Top-5提交中找到正确的 VFC 的召回率为96.6% ，对于 Top-1排名的提交的召回率为80.0% 。VFCFFinder 通用于9种不同的编程语言，在 Top-1召回率方面比最先进的方法高出36个百分点。作为一个实际贡献，我们使用 VFCFFinder 回填了 GitHub 安全咨询(GHSA)数据库中超过300个缺失的 VFC。所有 VFC 都被接受并合并到 GHSA 数据库中。除了向 VFC 展示安全建议的实际配对之外，我们的一般性开源实施将使脆弱性数据库维护人员能够大幅度提高数据质量，支持确保软件供应链安全的努力。"
    },
    {
        "title": "DeepInception: Hypnotize Large Language Model to Be Jailbreaker",
        "url": "http://arxiv.org/abs/2311.03191v1",
        "pub_date": "2023-11-06",
        "summary": "Despite remarkable success in various applications, large language models\n(LLMs) are vulnerable to adversarial jailbreaks that make the safety guardrails\nvoid. However, previous studies for jailbreaks usually resort to brute-force\noptimization or extrapolations of a high computation cost, which might not be\npractical or effective. In this paper, inspired by the Milgram experiment that\nindividuals can harm another person if they are told to do so by an\nauthoritative figure, we disclose a lightweight method, termed as\nDeepInception, which can easily hypnotize LLM to be a jailbreaker and unlock\nits misusing risks. Specifically, DeepInception leverages the personification\nability of LLM to construct a novel nested scene to behave, which realizes an\nadaptive way to escape the usage control in a normal scenario and provides the\npossibility for further direct jailbreaks. Empirically, we conduct\ncomprehensive experiments to show its efficacy. Our DeepInception can achieve\ncompetitive jailbreak success rates with previous counterparts and realize a\ncontinuous jailbreak in subsequent interactions, which reveals the critical\nweakness of self-losing on both open/closed-source LLMs like Falcon, Vicuna,\nLlama-2, and GPT-3.5/4/4V. Our investigation appeals that people should pay\nmore attention to the safety aspects of LLMs and a stronger defense against\ntheir misuse risks. The code is publicly available at:\nhttps://github.com/tmlr-group/DeepInception.",
        "translated": "尽管在各种应用中取得了显著的成功，大型语言模型(LLM)还是容易受到对抗性越狱攻击的影响，从而使安全防护失效。然而，以往对越狱的研究往往采用暴力优化或高计算成本的外推方法，这些方法可能并不实用或有效。在这篇论文中，我们揭示了一种轻量级的方法，称为“深度盗梦空间”(DeepInception) ，这种方法可以轻易地催眠 LLM 成为一个越狱者，并解除其滥用风险。这种米尔格拉姆实验认为，如果权威人士要求个人这样做，他们就可以伤害另一个人。具体来说，DeepInception 利用 LLM 的拟人化能力来构建一个新的嵌套场景来行为，这实现了在正常场景中逃避使用控制的自适应方式，并提供了进一步直接越狱的可能性。通过综合实验验证了该方法的有效性。我们的“深度盗梦空间”可以达到与以前同类产品竞争的越狱成功率，并在随后的互动中实现持续的越狱，这揭示了开源/封闭源 LLM (如 Falcon，Vicuna，llama-2和 GPT-3.5/4/4V)自我损失的关键弱点。我们的调查呼吁人们应该更多地关注 LLM 的安全方面，并对其滥用风险进行更有力的防御。该守则可于以下 https://github.com/tmlr-group/deepinception 公开索取:。"
    },
    {
        "title": "Preserving Privacy in GANs Against Membership Inference Attack",
        "url": "http://arxiv.org/abs/2311.03172v1",
        "pub_date": "2023-11-06",
        "summary": "Generative Adversarial Networks (GANs) have been widely used for generating\nsynthetic data for cases where there is a limited size real-world dataset or\nwhen data holders are unwilling to share their data samples. Recent works\nshowed that GANs, due to overfitting and memorization, might leak information\nregarding their training data samples. This makes GANs vulnerable to Membership\nInference Attacks (MIAs). Several defense strategies have been proposed in the\nliterature to mitigate this privacy issue. Unfortunately, defense strategies\nbased on differential privacy are proven to reduce extensively the quality of\nthe synthetic data points. On the other hand, more recent frameworks such as\nPrivGAN and PAR-GAN are not suitable for small-size training datasets. In the\npresent work, the overfitting in GANs is studied in terms of the discriminator,\nand a more general measure of overfitting based on the Bhattacharyya\ncoefficient is defined. Then, inspired by Fano's inequality, our first defense\nmechanism against MIAs is proposed. This framework, which requires only a\nsimple modification in the loss function of GANs, is referred to as the maximum\nentropy GAN or MEGAN and significantly improves the robustness of GANs to MIAs.\nAs a second defense strategy, a more heuristic model based on minimizing the\ninformation leaked from generated samples about the training data points is\npresented. This approach is referred to as mutual information minimization GAN\n(MIMGAN) and uses a variational representation of the mutual information to\nminimize the information that a synthetic sample might leak about the whole\ntraining data set. Applying the proposed frameworks to some commonly used data\nsets against state-of-the-art MIAs reveals that the proposed methods can reduce\nthe accuracy of the adversaries to the level of random guessing accuracy with a\nsmall reduction in the quality of the synthetic data samples.",
        "translated": "生成性对抗网络(GAN)已被广泛用于生成合成数据的情况下，有一个有限的大小真实世界的数据集或当数据持有者不愿意共享他们的数据样本。最近的研究表明，由于过度拟合和记忆，GAN 可能会泄漏有关其训练数据样本的信息。这使得 GAN 容易受到成员推断攻击(MIA)。文献中已经提出了几种防御策略来缓解这种隐私问题。不幸的是，基于差分隐私的防御策略被证明会大大降低合成数据点的质量。另一方面，最近的框架，如 PrivGAN 和 PAR-GAN 不适合小型的训练数据集。本文从鉴别器的角度研究了 GAN 中的过拟合问题，定义了一种基于 Bhattacharyya 系数的更一般的过拟合度量。然后，受法诺不等式的启发，提出了我们的第一种 MIA 防御机制。这个框架只需要对 GAN 的损耗函数进行简单的修改，被称为最大熵 GAN 或 MEGAN，它显著提高了 GAN 对 MIA 的鲁棒性。作为第二种防御策略，提出了一种基于最小化生成的训练数据点样本泄漏信息的启发式模型。这种方法被称为互信息最小化 GAN (MIMGAN) ，它使用互信息的变分表示来最小化合成样本可能泄漏的关于整个训练数据集的信息。将该框架应用于一些常用的数据集合，对抗最先进的 MIA，结果表明，该方法可以将对手的准确性降低到随机猜测的准确性水平，同时对合成数据样本的质量有小幅度的降低。"
    },
    {
        "title": "An Examination of the Alleged Privacy Threats of Confidence-Ranked\n  Reconstruction of Census Microdata",
        "url": "http://arxiv.org/abs/2311.03171v1",
        "pub_date": "2023-11-06",
        "summary": "The alleged threat of reconstruction attacks has led the U.S. Census Bureau\n(USCB) to replace in the Decennial Census 2020 the traditional statistical\ndisclosure limitation based on rank swapping with one based on differential\nprivacy (DP). This has resulted in substantial accuracy loss of the released\nstatistics. Worse yet, it has been shown that the reconstruction attacks used\nas an argument to move to DP are very far from allowing unequivocal\nreidentification of the respondents, because in general there are a lot of\nreconstructions compatible with the released statistics. In a very recent\npaper, a new reconstruction attack has been proposed, whose goal is to indicate\nthe confidence that a reconstructed record was in the original respondent data.\nThe alleged risk of serious disclosure entailed by such confidence-ranked\nreconstruction has renewed the interest of the USCB to use DP-based solutions.\nTo forestall the potential accuracy loss in future data releases resulting from\nadoption of these solutions, we show in this paper that the proposed\nconfidence-ranked reconstruction does not threaten privacy. Specifically, we\nreport empirical results showing that the proposed ranking cannot guide\nreidentification or attribute disclosure attacks, and hence it fails to warrant\nthe USCB's move towards DP. Further, we also demonstrate that, due to the way\nthe Census data are compiled, processed and released, it is not possible to\nreconstruct original and complete records through any methodology, and the\nconfidence-ranked reconstruction not only is completely ineffective at\naccurately reconstructing Census records but is trivially outperformed by an\nadequate interpretation of the released aggregate statistics.",
        "translated": "所谓的重建袭击威胁已经导致美国人口普查局(uscB)在2020年十年人口普查中取代了传统的基于排名交换的统计信息披露限制，取而代之的是基于差分隐私(DP)的统计信息披露限制。这导致了发布的统计数据的大量准确性损失。更糟糕的是，事实表明，以重建攻击为理由转移到民主德国的做法远远不能明确地重新确认受访者的身份，因为总的来说，有许多重建工作符合公布的统计数字。在最近的一篇论文中，提出了一种新的重建攻击，其目的是表明重建记录在原始响应者数据中的可信度。据称，这种信任等级重建可能带来严重披露风险，这重新激发了 USCB 使用基于 DP 的解决方案的兴趣。为了防止由于采用这些解决方案而导致的未来数据发布的潜在准确性损失，本文表明所提出的置信度重构方法不会威胁隐私。具体来说，我们报告的实证结果表明，建议的排名不能指导重新识别或属性披露的攻击，因此它不能保证 USCB 的移动向 DP。此外，我们还证明，由于人口普查数据的编制、处理和发布方式，不可能通过任何方法重建原始和完整的记录，置信度排序重建不仅在准确重建人口普查记录方面完全无效，而且对已发布的总体统计数据进行充分解释的效果也微不足道。"
    },
    {
        "title": "SoK: Memorisation in machine learning",
        "url": "http://arxiv.org/abs/2311.03075v1",
        "pub_date": "2023-11-06",
        "summary": "Quantifying the impact of individual data samples on machine learning models\nis an open research problem. This is particularly relevant when complex and\nhigh-dimensional relationships have to be learned from a limited sample of the\ndata generating distribution, such as in deep learning. It was previously shown\nthat, in these cases, models rely not only on extracting patterns which are\nhelpful for generalisation, but also seem to be required to incorporate some of\nthe training data more or less as is, in a process often termed memorisation.\nThis raises the question: if some memorisation is a requirement for effective\nlearning, what are its privacy implications? In this work we unify a broad\nrange of previous definitions and perspectives on memorisation in ML, discuss\ntheir interplay with model generalisation and their implications of these\nphenomena on data privacy. Moreover, we systematise methods allowing\npractitioners to detect the occurrence of memorisation or quantify it and\ncontextualise our findings in a broad range of ML learning settings. Finally,\nwe discuss memorisation in the context of privacy attacks, differential privacy\n(DP) and adversarial actors.",
        "translated": "量化单个数据样本对机器学习模型的影响是一个开放的研究问题。当必须从有限的数据生成分布样本(如深度学习)中学习复杂的高维关系时，这一点尤其重要。先前已经表明，在这些情况下，模型不仅依赖于提取有助于概括的模式，而且似乎需要或多或少地合并一些训练数据，在通常称为记忆的过程中。这就提出了一个问题: 如果一些记忆是有效学习的必要条件，那么它对隐私的影响是什么？在这项工作中，我们统一了以前在机器学习中记忆的定义和观点的广泛范围，讨论了它们与模型泛化的相互作用以及这些现象对数据隐私的影响。此外，我们系统化的方法，允许从业人员检测记忆的发生或量化它，并将我们的研究结果在机器学习设置的广泛范围内。最后，我们讨论了在隐私攻击、差分隐私(DP)和敌对行为者的背景下的记忆。"
    },
    {
        "title": "Non Deterministic Pseudorandom Generator for Quantum Key Distribution",
        "url": "http://arxiv.org/abs/2311.03024v1",
        "pub_date": "2023-11-06",
        "summary": "Quantum Key Distribution(QKD) thrives to achieve perfect secrecy of One time\nPad (OTP) through quantum processes. One of the crucial components of QKD are\nQuantum Random Number Generators(QRNG) for generation of keys. Unfortunately,\nthese QRNG does not immediately produce usable bits rather it produces raw bits\nwith high entropy but low uniformity which can be hardly used by any\ncryptographic system. A lot of pre-processing is required before the random\nnumbers generated by QRNG to be usable. This causes a bottle neck in random\nnumber generation rate as well as QKD system relying on it. To avoid this\nlacuna of post-processing methods employed as a central part of Quantum Random\nNumber Generators alternative approaches that satisfy the entropy(non\ndeterminism) and quantum security is explored. Pseudorandom generators based on\nquantum secure primitives could be an alternative to the post-processing\nproblem as PRNGs are way more faster than any random number generator employing\nphysical randomness (quantum mechanical process in QRNG) as well as it can\nprovide uniform bits required for cryptography application. In this work we\npropose a pseudorandom generator based on post quantum primitives. The central\ntheme of this random number generator is designing PRNG with non deterministic\nentropy generated through hard lattice problem - Learning with errors. We\nleverage the non determinism by Gaussian errors of LWE to construct\nnon-deterministic PRNG satisfying the entropy requirement of QKD. Further, the\npaper concludes by evaluating the PRNG through Die-Harder Test.",
        "translated": "量子密钥分配(QKD)通过量子过程实现一次性密钥分配(OTP)的完美保密。量子密钥发生器(QRNG)是量子密钥发生器的重要组成部分。不幸的是，这些 QRNG 不能立即产生有用的比特，而是产生高熵但低一致性的原始比特，这些比特很难被任何密码系统使用。在 QRNG 生成的随机数可用之前，需要进行大量的预处理。这就造成了随机数生成率的瓶颈，以及依赖于随机数生成率的 QKD 系统的瓶颈。为了避免作为量子随机数生成器核心部分的后处理方法的这一缺陷，探讨了满足熵(非确定性)和量子安全性的替代方法。基于量子安全原语的伪随机生成器可以替代后处理问题，因为 PRNG 比任何使用物理随机性(QRNG 中的量子力学过程)的随机数生成器都要快得多，而且它可以提供密码应用所需的统一比特。本文提出了一种基于后量子基元的伪随机发生器。这个随机数发生器的中心主题是设计一个由硬格子问题产生的非确定熵的 PRNG-误差学习。我们利用 LWE 中高斯误差的非确定性，构造出满足 QKD 熵要求的非确定性 PRNG。最后，通过模具硬度试验对 PRNG 进行了评价。"
    },
    {
        "title": "Hacking Cryptographic Protocols with Advanced Variational Quantum\n  Attacks",
        "url": "http://arxiv.org/abs/2311.02986v1",
        "pub_date": "2023-11-06",
        "summary": "Here we introduce an improved approach to Variational Quantum Attack\nAlgorithms (VQAA) on crytographic protocols. Our methods provide robust quantum\nattacks to well-known cryptographic algorithms, more efficiently and with\nremarkably fewer qubits than previous approaches. We implement simulations of\nour attacks for symmetric-key protocols such as S-DES, S-AES and Blowfish. For\ninstance, we show how our attack allows a classical simulation of a small\n8-qubit quantum computer to find the secret key of one 32-bit Blowfish instance\nwith 24 times fewer number of iterations than a brute-force attack. Our work\nalso shows improvements in attack success rates for lightweight ciphers such as\nS-DES and S-AES. Further applications beyond symmetric-key cryptography are\nalso discussed, including asymmetric-key protocols and hash functions. In\naddition, we also comment on potential future improvements of our methods. Our\nresults bring one step closer assessing the vulnerability of large-size\nclassical cryptographic protocols with Noisy Intermediate-Scale Quantum (NISQ)\ndevices, and set the stage for future research in quantum cybersecurity.",
        "translated": "本文介绍了一种改进的基于密码协议的变分量子攻击算法(VQAA)。我们的方法为众所周知的密码算法提供了强大的量子攻击，与以前的方法相比，这种方法更加有效且使用的量子位明显更少。我们对对称密钥协议如 S-DES、 S-AES 和 Blowfish 进行了攻击仿真。例如，我们展示了我们的攻击如何让一个经典的模拟小型8量子位量子计算机找到一个32位 Blowfish 实例的密钥，其迭代次数是一个穷举法的24倍。我们的工作还显示了 S-DES 和 S-AES 等轻量级密码在攻击成功率方面的改进。还讨论了对称密钥加密以外的其他应用，包括非对称密钥协议和散列函数。此外，我们还对我们的方法未来的潜在改进进行了评论。我们的研究结果使得利用噪声中尺度量子(NISQ)设备评估大规模经典密码协议的脆弱性更进了一步，并为量子网络安全的未来研究奠定了基础。"
    },
    {
        "title": "SoK: Evaluations in Industrial Intrusion Detection Research",
        "url": "http://arxiv.org/abs/2311.02929v1",
        "pub_date": "2023-11-06",
        "summary": "Industrial systems are increasingly threatened by cyberattacks with\npotentially disastrous consequences. To counter such attacks, industrial\nintrusion detection systems strive to timely uncover even the most\nsophisticated breaches. Due to its criticality for society, this fast-growing\nfield attracts researchers from diverse backgrounds, resulting in 130 new\ndetection approaches in 2021 alone. This huge momentum facilitates the\nexploration of diverse promising paths but likewise risks fragmenting the\nresearch landscape and burying promising progress. Consequently, it needs sound\nand comprehensible evaluations to mitigate this risk and catalyze efforts into\nsustainable scientific progress with real-world applicability. In this paper,\nwe therefore systematically analyze the evaluation methodologies of this field\nto understand the current state of industrial intrusion detection research. Our\nanalysis of 609 publications shows that the rapid growth of this research field\nhas positive and negative consequences. While we observe an increased use of\npublic datasets, publications still only evaluate 1.3 datasets on average, and\nfrequently used benchmarking metrics are ambiguous. At the same time, the\nadoption of newly developed benchmarking metrics sees little advancement.\nFinally, our systematic analysis enables us to provide actionable\nrecommendations for all actors involved and thus bring the entire research\nfield forward.",
        "translated": "工业系统日益受到网络攻击的威胁，这些攻击可能带来灾难性的后果。为了应对这种攻击，工业入侵检测系统努力及时发现即使是最复杂的漏洞。由于其对社会的重要性，这个快速增长的领域吸引了来自不同背景的研究人员，仅在2021年就产生了130种新的检测方法。这种巨大的势头有利于探索不同的有希望的道路，但同样有可能分裂研究领域和埋葬有希望的进展。因此，需要进行健全和全面的评估，以减轻这一风险，并促进努力实现具有现实适用性的可持续科学进步。因此，本文系统地分析了这一领域的评价方法，以了解工业入侵检测的研究现状。我们对609份出版物的分析表明，这一研究领域的快速发展有积极和消极的影响。虽然我们观察到公共数据集的使用有所增加，但出版物平均仍然只评估1.3个数据集，而且经常使用的基准测试指标是不明确的。与此同时，采用新开发的基准测试指标几乎没有什么进展。最后，我们的系统分析使我们能够为所有参与者提供可行的建议，从而推动整个研究领域的发展。"
    },
    {
        "title": "Pseudorandom Isometries",
        "url": "http://arxiv.org/abs/2311.02901v1",
        "pub_date": "2023-11-06",
        "summary": "We introduce a new notion called ${\\cal Q}$-secure pseudorandom isometries\n(PRI). A pseudorandom isometry is an efficient quantum circuit that maps an\n$n$-qubit state to an $(n+m)$-qubit state in an isometric manner. In terms of\nsecurity, we require that the output of a $q$-fold PRI on $\\rho$, for $ \\rho\n\\in {\\cal Q}$, for any polynomial $q$, should be computationally\nindistinguishable from the output of a $q$-fold Haar isometry on $\\rho$. \\par\nBy fine-tuning ${\\cal Q}$, we recover many existing notions of\npseudorandomness. We present a construction of PRIs and assuming post-quantum\none-way functions, we prove the security of ${\\cal Q}$-secure pseudorandom\nisometries (PRI) for different interesting settings of ${\\cal Q}$. \\par We also\ndemonstrate many cryptographic applications of PRIs, including, length\nextension theorems for quantum pseudorandomness notions, message authentication\nschemes for quantum states, multi-copy secure public and private encryption\nschemes, and succinct quantum commitments. }",
        "translated": "我们引入了一个新的概念，称为 ${ cal Q } $- 安全伪随机等距(PRI)。伪随机等距是一种有效的量子电路，它以等距的方式将 $n $- 量子比特态映射到 $(n + m) $- 量子比特态。在安全性方面，我们要求 $ho $上的 $q $- 折 PRI，{ cal Q } $中的 $ho，任何多项式 $q $的输出应该在计算上与 $ho $上的 $q $- 折 Haar 等距的输出不可区分。通过微调 ${ cal Q } $，我们恢复了许多现有的伪随机数概念。给出了一个伪随机等距的构造，并假设了后量子单向函数，证明了对于不同的有趣设置，${ cal Q } $- 安全伪随机等距(PRI)的安全性。我们还展示了 PRI 的许多加密应用，包括量子伪随机数概念的长度延伸定理、量子态的消息认证方案、多拷贝安全的公共和私有加密方案以及简洁的量子承诺。}"
    },
    {
        "title": "PermutEx: Feature-Extraction-Based Permutation -- A New Diffusion Scheme\n  for Image Encryption Algorithms",
        "url": "http://arxiv.org/abs/2311.02795v1",
        "pub_date": "2023-11-05",
        "summary": "Traditional permutation schemes mostly focus on random scrambling of pixels,\noften neglecting the intrinsic image information that could enhance diffusion\nin image encryption algorithms. This paper introduces PermutEx, a\nfeature-extraction-based permutation method that utilizes inherent image\nfeatures to scramble pixels effectively. Unlike random permutation schemes,\nPermutEx extracts the spatial frequency and local contrast features of the\nimage and ranks each pixel based on this information, identifying which pixels\nare more important or information-rich based on texture and edge information.\nIn addition, a unique permutation key is generated using the Logistic-Sine Map\nbased on chaotic behavior. The ranked pixels are permuted in conjunction with\nthis unique key, effectively permuting the original image into a scrambled\nversion. Experimental results indicate that the proposed method effectively\ndisrupts the correlation in information-rich areas within the image resulting\nin a correlation value of 0.000062. The effective scrambling of pixels,\nresulting in nearly zero correlation, makes this method suitable to be used as\ndiffusion in image encryption algorithms.",
        "translated": "传统的置换算法大多集中在像素点的随机置乱上，往往忽略了图像加密算法中能够增强扩散的内在信息。本文介绍了基于特征提取的排列方法 PermutEx，该方法利用图像的固有特征对像素进行有效的置乱。与随机排列方案不同，PermutEx 提取图像的空间频率和局部对比度特征，并根据这些信息对每个像素进行排序，根据纹理和边缘信息识别哪些像素更重要或信息丰富。此外，利用基于混沌行为的 Logistic-Sine 映射生成唯一的置换密钥。排列的像素与这个唯一的键排列在一起，有效地将原始图像排列成一个乱序版本。实验结果表明，该方法有效地破坏了图像中信息丰富区域的相关性，相关值为0.000062。该方法对像素点进行有效的置乱，使得像素点之间的相关性几乎为零，因而适用于图像加密算法中的扩散。"
    },
    {
        "title": "ELEGANT: Certified Defense on the Fairness of Graph Neural Networks",
        "url": "http://arxiv.org/abs/2311.02757v1",
        "pub_date": "2023-11-05",
        "summary": "Graph Neural Networks (GNNs) have emerged as a prominent graph learning model\nin various graph-based tasks over the years. Nevertheless, due to the\nvulnerabilities of GNNs, it has been empirically proved that malicious\nattackers could easily corrupt the fairness level of their predictions by\nadding perturbations to the input graph data. In this paper, we take crucial\nsteps to study a novel problem of certifiable defense on the fairness level of\nGNNs. Specifically, we propose a principled framework named ELEGANT and present\na detailed theoretical certification analysis for the fairness of GNNs. ELEGANT\ntakes any GNNs as its backbone, and the fairness level of such a backbone is\ntheoretically impossible to be corrupted under certain perturbation budgets for\nattackers. Notably, ELEGANT does not have any assumption over the GNN structure\nor parameters, and does not require re-training the GNNs to realize\ncertification. Hence it can serve as a plug-and-play framework for any\noptimized GNNs ready to be deployed. We verify the satisfactory effectiveness\nof ELEGANT in practice through extensive experiments on real-world datasets\nacross different backbones of GNNs, where ELEGANT is also demonstrated to be\nbeneficial for GNN debiasing. Open-source code can be found at\nhttps://github.com/yushundong/ELEGANT.",
        "translated": "图形神经网络(GNN)作为一种突出的图形学习模型，多年来在各种基于图形的任务中得到了广泛的应用。然而，由于 GNN 的脆弱性，经验证明，恶意攻击者可以通过向输入图数据中添加扰动，轻易地破坏其预测的公平性水平。在本文中，我们采取了关键的步骤来研究一个新的问题的认证防御的公平性水平的 GNN。具体地说，我们提出了一个原则性的框架—— ELEGANT，并对 GNN 的公平性进行了详细的理论验证分析。ELEGANT 采用任何 GNN 作为主干，在攻击者的某些扰动预算下，这种主干的公平性水平在理论上是不可能被破坏的。值得注意的是，ELEGANT 对 GNN 结构或参数没有任何假设，也不需要重新训练 GNN 来实现认证。因此，它可以作为任何优化的 GNN 的即插即用框架，准备部署。通过对 GNN 不同骨架上的实际数据集进行广泛的实验，验证了 ELEGANT 在实际应用中的有效性。开源代码可以在 https://github.com/yushundong/elegant 找到。"
    },
    {
        "title": "Quantization-aware Neural Architectural Search for Intrusion Detection",
        "url": "http://arxiv.org/abs/2311.04194v1",
        "pub_date": "2023-11-07",
        "summary": "Deploying machine learning-based intrusion detection systems (IDSs) on\nhardware devices is challenging due to their limited computational resources,\npower consumption, and network connectivity. Hence, there is a significant need\nfor robust, deep learning models specifically designed with such constraints in\nmind. In this paper, we present a design methodology that automatically trains\nand evolves quantized neural network (NN) models that are a thousand times\nsmaller than state-of-the-art NNs but can efficiently analyze network data for\nintrusion at high accuracy. In this regard, the number of LUTs utilized by this\nnetwork when deployed to an FPGA is between 2.3x and 8.5x smaller with\nperformance comparable to prior work.",
        "translated": "在硬件设备上部署基于机器学习的入侵检测系统(IDS)具有挑战性，因为它们的计算资源、功耗和网络连接性有限。因此，我们非常需要专门针对这些约束设计的健壮、深入的学习模型。本文提出了一种自动训练和演化量化神经网络(NN)模型的设计方法，这种神经网络模型比现有的神经网络小一千倍，但是能够高精度地有效地分析网络数据。在这方面，当部署到 FPGA 时，这个网络所使用的 LUT 的数量在2.3 x 到8.5 x 之间，性能与以前的工作相当。"
    },
    {
        "title": "Contactless Fingerprint Biometric Anti-Spoofing: An Unsupervised Deep\n  Learning Approach",
        "url": "http://arxiv.org/abs/2311.04148v1",
        "pub_date": "2023-11-07",
        "summary": "Contactless fingerprint recognition offers a higher level of user comfort and\naddresses hygiene concerns more effectively. However, it is also more\nvulnerable to presentation attacks such as photo paper, paper-printout, and\nvarious display attacks, which makes it more challenging to implement in\nbiometric systems compared to contact-based modalities. Limited research has\nbeen conducted on presentation attacks in contactless fingerprint systems, and\nthese studies have encountered challenges in terms of generalization and\nscalability since both bonafide samples and presentation attacks are utilized\nduring training model. Although this approach appears promising, it lacks the\nability to handle unseen attacks, which is a crucial factor for developing PAD\nmethods that can generalize effectively. We introduced an innovative\nanti-spoofing approach that combines an unsupervised autoencoder with a\nconvolutional block attention module to address the limitations of existing\nmethods. Our model is exclusively trained on bonafide images without exposure\nto any spoofed samples during the training phase. It is then evaluated against\nvarious types of presentation attack images in the testing phase. The scheme we\nproposed has achieved an average BPCER of 0.96\\% with an APCER of 1.6\\% for\npresentation attacks involving various types of spoofed samples.",
        "translated": "非接触式指纹识别提供了更高水平的用户舒适性，并更有效地解决卫生问题。然而，它也更容易受到表示攻击，如照片纸、纸张打印输出和各种显示攻击，这使得在生物识别系统中实现比基于接触的模式更具挑战性。针对非接触指纹系统中的表示攻击问题，目前国内外的研究还很有限，由于在训练模型中同时使用了真实样本和表示攻击，这些研究在通用性和可扩展性方面遇到了挑战。虽然这种方法看起来很有希望，但它缺乏处理看不见的攻击的能力，这是开发能够有效推广的 PAD 方法的一个关键因素。我们介绍了一种创新的反欺骗方法，结合无监督自动编码器和卷积块注意模块，以解决现有方法的局限性。我们的模型是专门训练的真实图像没有暴露在任何欺骗样本在训练阶段。然后在测试阶段针对不同类型的表示攻击图像进行评估。对于涉及不同类型欺骗样本的表示攻击，我们提出的方案平均 BPCER 为0.96% ，APCER 为1.6% 。"
    },
    {
        "title": "Do Language Models Learn Semantics of Code? A Case Study in\n  Vulnerability Detection",
        "url": "http://arxiv.org/abs/2311.04109v1",
        "pub_date": "2023-11-07",
        "summary": "Recently, pretrained language models have shown state-of-the-art performance\non the vulnerability detection task. These models are pretrained on a large\ncorpus of source code, then fine-tuned on a smaller supervised vulnerability\ndataset. Due to the different training objectives and the performance of the\nmodels, it is interesting to consider whether the models have learned the\nsemantics of code relevant to vulnerability detection, namely bug semantics,\nand if so, how the alignment to bug semantics relates to model performance. In\nthis paper, we analyze the models using three distinct methods:\ninterpretability tools, attention analysis, and interaction matrix analysis. We\ncompare the models' influential feature sets with the bug semantic features\nwhich define the causes of bugs, including buggy paths and Potentially\nVulnerable Statements (PVS). We find that (1) better-performing models also\naligned better with PVS, (2) the models failed to align strongly to PVS, and\n(3) the models failed to align at all to buggy paths. Based on our analysis, we\ndeveloped two annotation methods which highlight the bug semantics inside the\nmodel's inputs. We evaluated our approach on four distinct transformer models\nand four vulnerability datasets and found that our annotations improved the\nmodels' performance in the majority of settings - 11 out of 16, with up to 9.57\npoints improvement in F1 score compared to conventional fine-tuning. We further\nfound that with our annotations, the models aligned up to 232% better to\npotentially vulnerable statements. Our findings indicate that it is helpful to\nprovide the model with information of the bug semantics, that the model can\nattend to it, and motivate future work in learning more complex path-based bug\nsemantics. Our code and data are available at\nhttps://figshare.com/s/4a16a528d6874aad51a0.",
        "translated": "最近，预训练语言模型在漏洞检测任务中表现出了最先进的性能。这些模型在大量源代码上进行预训练，然后在较小的监督漏洞数据集上进行微调。由于不同的训练目标和模型的性能，考虑这些模型是否已经学习了与漏洞检测相关的代码的语义，即 bug 语义，以及如果是的话，bug 语义的对齐如何与模型性能相关是很有意思的。在本文中，我们使用三种不同的方法来分析模型: 可解释性工具、注意力分析和交互矩阵分析。我们将模型中有影响力的特性集与定义 bug 原因的 bug 语义特性进行比较，包括 bug 路径和潜在漏洞语句(PVS)。我们发现(1)性能更好的模型也更好地与 PVS 对齐，(2)模型未能与 PVS 强烈对齐，以及(3)模型根本未能与错误路径对齐。基于我们的分析，我们开发了两种注释方法，它们突出了模型输入中的 bug 语义。我们在四个不同的变压器模型和四个脆弱性数据集上评估了我们的方法，发现我们的注释改善了模型在大多数设置中的性能-16个中的11个，与传统的微调相比，F1评分提高了9.57分。我们进一步发现，通过我们的注释，模型与潜在的脆弱语句的匹配度提高了232% 。我们的研究结果表明，这有助于为模型提供错误语义的信息，该模型可以参与其中，并激励未来学习更复杂的基于路径的错误语义的工作。我们的代码和数据可以在 https://figshare.com/s/4a16a528d6874aad51a0找到。"
    },
    {
        "title": "A Lightweight and Secure PUF-Based Authentication and Key-exchange\n  Protocol for IoT Devices",
        "url": "http://arxiv.org/abs/2311.04078v1",
        "pub_date": "2023-11-07",
        "summary": "The Internet of Things (IoT) has improved people's lives by seamlessly\nintegrating into many facets of modern life and facilitating information\nsharing across platforms. Device Authentication and Key exchange are major\nchallenges for the IoT. High computational resource requirements for\ncryptographic primitives and message transmission during Authentication make\nthe existing methods like PKI and IBE not suitable for these resource\nconstrained devices. PUF appears to offer a practical and economical security\nmechanism in place of typically sophisticated cryptosystems like PKI and IBE.\nPUF provides an unclonable and tamper sensitive unique signature based on the\nPUF chip by using manufacturing process variability. Therefore, in this study,\nwe use lightweight bitwise XOR, hash function, and PUF to Authenticate IoT\ndevices. Despite several studies employing the PUF to authenticate\ncommunication between IoT devices, to the authors' knowledge, existing\nsolutions require intermediary gateway and internet capabilities by the IoT\ndevice to directly interact with a Server for Authentication and hence, are not\nscalable when the IoT device works on different technologies like BLE, Zigbee,\netc. To address the aforementioned issue, we present a system in which the IoT\ndevice does not require a continuous active internet connection to communicate\nwith the server in order to Authenticate itself. The results of a thorough\nsecurity study are validated against adversarial attacks and PUF modeling\nattacks. For formal security validation, the AVISPA verification tool is also\nused. Performance study recommends this protocol's lightweight characteristics.\nThe proposed protocol's acceptability and defenses against adversarial assaults\nare supported by a prototype developed with ESP32.",
        "translated": "物联网(IoT)通过无缝地融入现代生活的方方面面，促进跨平台的信息共享，改善了人们的生活。设备认证和密钥交换是物联网面临的主要挑战。在认证过程中，对加密原语和消息传输的高计算资源要求，使得现有的 PKI 和 IBE 等方法不适合这些资源有限的设备。PUF 似乎提供了一种实用而经济的安全机制，以取代 PKI 和 IBE 等典型的复杂密码体制。PUF 基于 PUF 芯片，利用制造过程的可变性，提供了一种不可复制和篡改敏感的唯一签名。因此，在本研究中，我们使用轻量级的按位 XOR、散列函数和 PUF 来认证物联网设备。尽管有几项研究使用 PUF 来验证物联网设备之间的通信，但据作者所知，现有的解决方案需要物联网设备的中间网关和互联网能力，以直接与认证服务器交互，因此，当物联网设备在不同的技术上工作时，不可扩展，如 BLE，Zigbee 等。为了解决上述问题，我们提出了一个系统，其中物联网设备不需要一个连续的主动互联网连接与服务器通信，以认证本身。针对对手攻击和 PUF 模型攻击，对安全性研究的结果进行了验证。对于正式的安全验证，还使用了 AVISPA 验证工具。性能研究推荐了该协议的轻量级特性。所提议的协议的可接受性和对抗对手攻击的防御得到了 ESP32开发的原型的支持。"
    },
    {
        "title": "P-Bench: A Multi-level Privacy Evaluation Benchmark for Language Models",
        "url": "http://arxiv.org/abs/2311.04044v1",
        "pub_date": "2023-11-07",
        "summary": "The rapid development of language models (LMs) brings unprecedented\naccessibility and usage for both models and users. On the one hand, powerful\nLMs, trained with massive textual data, achieve state-of-the-art performance\nover numerous downstream NLP tasks. On the other hand, more and more attention\nis paid to unrestricted model accesses that may bring malicious privacy risks\nof data leakage. To address these issues, many recent works propose\nprivacy-preserving language models (PPLMs) with differential privacy (DP).\nUnfortunately, different DP implementations make it challenging for a fair\ncomparison among existing PPLMs. In this paper, we present P-Bench, a\nmulti-perspective privacy evaluation benchmark to empirically and intuitively\nquantify the privacy leakage of LMs. Instead of only protecting and measuring\nthe privacy of protected data with DP parameters, P-Bench sheds light on the\nneglected inference data privacy during actual usage. P-Bench first clearly\ndefines multi-faceted privacy objectives during private fine-tuning. Then,\nP-Bench constructs a unified pipeline to perform private fine-tuning. Lastly,\nP-Bench performs existing privacy attacks on LMs with pre-defined privacy\nobjectives as the empirical evaluation results. The empirical attack results\nare used to fairly and intuitively evaluate the privacy leakage of various\nPPLMs. We conduct extensive experiments on three datasets of GLUE for\nmainstream LMs.",
        "translated": "语言模型(LM)的快速发展为模型和用户带来了前所未有的可访问性和使用性。一方面，强大的 LM 经过大量文本数据的训练，在众多的下游 NLP 任务中实现了最先进的性能。另一方面，无限制的模型访问越来越受到人们的关注，这种访问可能带来数据泄露的恶意隐私风险。为了解决这些问题，许多最近的作品提出了带有差分隐私的保护隐私的语言模型(pplm)。不幸的是，不同的 DP 实现使得在现有的 PPLM 之间进行公平的比较具有挑战性。在本文中，我们提出了 P-Bench，一个多视角的隐私评估基准，以经验和直观的量化隐私泄漏的 LM。P-Bench 不再仅仅使用 DP 参数来保护和度量受保护数据的隐私，而是在实际使用过程中揭示了被忽视的推理数据的隐私。P-Bench 首先在私人微调期间明确定义了多方面的隐私目标。然后，P-Bench 构造一个统一的管道来执行私有的微调。最后，P-Bench 以预定义的隐私目标作为实证评估结果，对 LM 进行现有的隐私攻击。利用经验攻击结果，公正、直观地评估了各种 PPLM 的隐私泄漏情况。我们在三个主流 LM 的 GLUE 数据集上进行了广泛的实验。"
    },
    {
        "title": "Causal Discovery Under Local Privacy",
        "url": "http://arxiv.org/abs/2311.04037v1",
        "pub_date": "2023-11-07",
        "summary": "Differential privacy is a widely adopted framework designed to safeguard the\nsensitive information of data providers within a data set. It is based on the\napplication of controlled noise at the interface between the server that stores\nand processes the data, and the data consumers. Local differential privacy is a\nvariant that allows data providers to apply the privatization mechanism\nthemselves on their data individually. Therefore it provides protection also in\ncontexts in which the server, or even the data collector, cannot be trusted.\nThe introduction of noise, however, inevitably affects the utility of the data,\nparticularly by distorting the correlations between individual data components.\nThis distortion can prove detrimental to tasks such as causal discovery. In\nthis paper, we consider various well-known locally differentially private\nmechanisms and compare the trade-off between the privacy they provide, and the\naccuracy of the causal structure produced by algorithms for causal learning\nwhen applied to data obfuscated by these mechanisms. Our analysis yields\nvaluable insights for selecting appropriate local differentially private\nprotocols for causal discovery tasks. We foresee that our findings will aid\nresearchers and practitioners in conducting locally private causal discovery.",
        "translated": "差分隐私是一个被广泛采用的架构，旨在保障资料库内资料供应商的敏感资料。它是基于在存储和处理数据的服务器与数据使用者之间的接口上应用受控噪声。本地差分隐私是一种变体，允许数据提供商自行对其数据应用私有化机制。因此，它还在不能信任服务器甚至数据收集器的上下文中提供保护。然而，噪音的引入不可避免地会影响数据的效用，特别是通过扭曲各个数据成分之间的相关性。这种扭曲可以证明有害的任务，如因果发现。在本文中，我们考虑了各种众所周知的局部差异私有机制，并比较了它们提供的隐私和因果学习算法产生的因果结构的准确性之间的权衡，当应用于被这些机制混淆的数据时。我们的分析为选择适当的局部差异私有协议进行因果发现任务提供了有价值的见解。我们预见，我们的发现将有助于研究人员和从业人员进行当地私人因果发现。"
    },
    {
        "title": "AGNES: Abstraction-guided Framework for Deep Neural Networks Security",
        "url": "http://arxiv.org/abs/2311.04009v1",
        "pub_date": "2023-11-07",
        "summary": "Deep Neural Networks (DNNs) are becoming widespread, particularly in\nsafety-critical areas. One prominent application is image recognition in\nautonomous driving, where the correct classification of objects, such as\ntraffic signs, is essential for safe driving. Unfortunately, DNNs are prone to\nbackdoors, meaning that they concentrate on attributes of the image that should\nbe irrelevant for their correct classification. Backdoors are integrated into a\nDNN during training, either with malicious intent (such as a manipulated\ntraining process, because of which a yellow sticker always leads to a traffic\nsign being recognised as a stop sign) or unintentional (such as a rural\nbackground leading to any traffic sign being recognised as animal crossing,\nbecause of biased training data).\n  In this paper, we introduce AGNES, a tool to detect backdoors in DNNs for\nimage recognition. We discuss the principle approach on which AGNES is based.\nAfterwards, we show that our tool performs better than many state-of-the-art\nmethods for multiple relevant case studies.",
        "translated": "深度神经网络(DNN)正变得越来越普遍，特别是在安全关键领域。一个突出的应用是图像识别在自主驾驶中，其中正确的分类对象，如交通标志，是必不可少的安全驾驶。不幸的是，DNN 倾向于后门，这意味着他们专注于图像的属性，这些属性应该与他们的正确分类无关。在训练期间，后门被整合到 DNN 中，有的是恶意的(例如操纵训练过程，因为黄色标签总是导致交通标志被识别为停止标志) ，有的是无意的(例如农村背景导致任何交通标志被识别为动物之森，因为有偏差的训练数据)。在本文中，我们介绍了 AGNES，一个工具来检测后门的 DNN 的图像识别。我们讨论了 AGNES 所基于的原理方法。然后，我们展示了我们的工具在多个相关案例研究中比许多最先进的方法表现得更好。"
    },
    {
        "title": "Unveiling the Invisible: Detection and Evaluation of Prototype Pollution\n  Gadgets with Dynamic Taint Analysis",
        "url": "http://arxiv.org/abs/2311.03919v1",
        "pub_date": "2023-11-07",
        "summary": "For better or worse, JavaScript is the cornerstone of modern Web.\nPrototype-based languages like JavaScript are susceptible to prototype\npollution vulnerabilities, enabling an attacker to inject arbitrary properties\ninto an object's prototype. The attacker can subsequently capitalize on the\ninjected properties by executing otherwise benign pieces of code, so-called\ngadgets, that perform security-sensitive operations. The success of an attack\nlargely depends on the presence of gadgets, leading to high-profile exploits\nsuch as privilege escalation and arbitrary code execution (ACE).\n  This paper proposes Dasty, the first semi-automated pipeline to help\ndevelopers identify gadgets in their applications' software supply chain. Dasty\ntargets server-side Node.js applications and relies on an enhancement of\ndynamic taint analysis which we implement with the dynamic AST-level\ninstrumentation. Moreover, Dasty provides support for visualization of code\nflows with an IDE, thus facilitating the subsequent manual analysis for\nbuilding proof-of-concept exploits. To illustrate the danger of gadgets, we use\nDasty in a study of the most dependent-upon NPM packages to analyze the\npresence of gadgets leading to ACE. Dasty identifies 1,269 server-side\npackages, of which 631 have code flows that may reach dangerous sinks. We\nmanually prioritize and verify the candidate flows to build proof-of-concept\nexploits for 49 NPM packages, including popular packages such as ejs,\nnodemailer and workerpool. To investigate how Dasty integrates with existing\ntools to find end-to-end exploits, we conduct an in-depth analysis of a popular\ndata visualization dashboard to find one high-severity CVE-2023-31415 leading\nto remote code execution. For the first time, our results systematically\ndemonstrate the dangers of server-side gadgets and call for further research to\nsolve the problem.",
        "translated": "无论好坏，JavaScript 都是现代 Web 的基石。像 JavaScript 这样的基于原型的语言容易受到原型污染漏洞的影响，这使得攻击者能够向对象的原型中注入任意属性。随后，攻击者可以通过执行其他良性代码片段(即所谓的 gadget)来利用注入的属性，这些代码片段执行安全敏感的操作。攻击的成功很大程度上取决于小工具的存在，导致高调的攻击，如权限提升和任意代码执行(ACE)。本文提出了 Dasty，这是第一个半自动化的流水线，可以帮助开发人员在他们的应用软件供应链中识别小工具。Dasty 的目标是服务器端 Node.js 应用程序，它依赖于动态污染分析的增强，我们使用动态 AST 级别的工具来实现这一点。此外，Dasty 还提供了 IDE 对代码流可视化的支持，从而为构建概念验证利用的后续手工分析提供了便利。为了说明小工具的危险性，我们在一个研究中使用 Dasty 来分析导致 ACE 的小工具的存在。Dasty 确定了1,269个服务器端包，其中631个包含可能到达危险接收器的代码流。我们手动对候选流进行优先级排序和验证，以便为49个 NPM 包(包括 ejs、 nodemailer 和 workerpool 等流行包)构建概念验证利用。为了研究达斯蒂如何与现有工具集成以发现端到端的漏洞，我们对一个流行的数据可视化仪表板进行了深入分析，以发现一个高严重性的 CVE-2023-31415导致远程代码执行。我们的研究结果第一次系统地证明了服务器端小工具的危险性，并呼吁进一步研究来解决这个问题。"
    },
    {
        "title": "FD-MIA: Efficient Attacks on Fairness-enhanced Models",
        "url": "http://arxiv.org/abs/2311.03865v1",
        "pub_date": "2023-11-07",
        "summary": "Previous studies have developed fairness methods for biased models that\nexhibit discriminatory behaviors towards specific subgroups. While these models\nhave shown promise in achieving fair predictions, recent research has\nidentified their potential vulnerability to score-based membership inference\nattacks (MIAs). In these attacks, adversaries can infer whether a particular\ndata sample was used during training by analyzing the model's prediction\nscores. However, our investigations reveal that these score-based MIAs are\nineffective when targeting fairness-enhanced models in binary classifications.\nThe attack models trained to launch the MIAs degrade into simplistic threshold\nmodels, resulting in lower attack performance. Meanwhile, we observe that\nfairness methods often lead to prediction performance degradation for the\nmajority subgroups of the training data. This raises the barrier to successful\nattacks and widens the prediction gaps between member and non-member data.\nBuilding upon these insights, we propose an efficient MIA method against\nfairness-enhanced models based on fairness discrepancy results (FD-MIA). It\nleverages the difference in the predictions from both the original and\nfairness-enhanced models and exploits the observed prediction gaps as attack\nclues. We also explore potential strategies for mitigating privacy leakages.\nExtensive experiments validate our findings and demonstrate the efficacy of the\nproposed method.",
        "translated": "先前的研究已经发展了公平的方法，偏见的模型表现出对特定亚群体的歧视行为。虽然这些模型在实现公平预测方面显示出希望，但最近的研究已经确定了它们在基于分数的成员推理攻击(MIAs)面前的潜在脆弱性。在这些攻击中，对手可以通过分析模型的预测分数来推断在训练期间是否使用了特定的数据样本。然而，我们的研究表明，这些基于分数的 MIA 在针对二进制分类中的公平增强模型时是无效的。训练发射 MIA 的攻击模型退化为简单的阈值模型，导致攻击性能下降。同时，我们观察到公平性方法往往导致对大多数训练数据子群的预测性能下降。这就增加了成功攻击的障碍，扩大了成员和非成员数据之间的预测差距。在此基础上，我们提出了一种针对基于公平性差异结果的公平性增强模型的有效 MIA 方法(FD-MIA)。它利用来自原始模型和公平性增强模型的预测差异，并利用观察到的预测差异作为攻击线索。我们还探讨了减少隐私泄露的潜在策略。大量的实验验证了我们的发现，并证明了该方法的有效性。"
    },
    {
        "title": "IC-SECURE: Intelligent System for Assisting Security Experts in\n  Generating Playbooks for Automated Incident Response",
        "url": "http://arxiv.org/abs/2311.03825v1",
        "pub_date": "2023-11-07",
        "summary": "Security orchestration, automation, and response (SOAR) systems ingest alerts\nfrom security information and event management (SIEM) system, and then trigger\nrelevant playbooks that automate and orchestrate the execution of a sequence of\nsecurity activities. SOAR systems have two major limitations: (i) security\nanalysts need to define, create and change playbooks manually, and (ii) the\nchoice between multiple playbooks that could be triggered is based on rules\ndefined by security analysts. To address these limitations, recent studies in\nthe field of artificial intelligence for cybersecurity suggested the task of\ninteractive playbook creation. In this paper, we propose IC-SECURE, an\ninteractive playbook creation solution based on a novel deep learning-based\napproach that provides recommendations to security analysts during the playbook\ncreation process. IC-SECURE captures the context in the form of alert data and\ncurrent status of incomplete playbook, required to make reasonable\nrecommendation for next module that should be included in the new playbook\nbeing created. We created three evaluation datasets, each of which involved a\ncombination of a set of alert rules and a set of playbooks from a SOAR\nplatform. We evaluated IC-SECURE under various settings, and compared our\nresults with two state-of-the-art recommender system methods. In our evaluation\nIC-SECURE demonstrated superior performance compared to other methods by\nconsistently recommending the correct security module, achieving precision@1 &gt;\n0.8 and recall@3 &gt; 0.92",
        "translated": "安全编排、自动化和响应(SOAR)系统接收来自安全信息和事件管理(SIEM)系统的警报，然后触发相关的剧本，自动化和编排一系列安全活动的执行。SOAR 系统有两个主要的局限性: (i)安全分析师需要手动定义、创建和更改剧本，以及(ii)可能触发的多个剧本之间的选择是基于安全分析师定义的规则。为了解决这些局限性，网络安全的人工智能领域的最新研究提出了创建交互式剧本的任务。在本文中，我们提出了 IC-SECURE，这是一个交互式剧本创建解决方案，它基于一种新颖的基于深度学习的方法，在剧本创建过程中为安全分析师提供建议。IC-SECURE 以警报数据和不完整剧本的当前状态的形式捕获上下文，需要对下一个模块作出合理的推荐，这个模块应该包含在正在创建的新剧本中。我们创建了三个评估数据集，每个数据集都包含一组警报规则和一组来自 SOAR 平台的剧本。我们在不同的设置下对 IC-SECURE 进行了评估，并将我们的结果与两种最先进的推荐系统方法进行了比较。在我们的评估中，IC-SECURE 通过不断推荐正确的安全模块，达到精度@1 > 0.8和召回@3 > 0.92，表现出比其他方法更好的性能"
    },
    {
        "title": "Sandi: A System for Accountability and Applications in Direct\n  Communication (Extended Abstract)",
        "url": "http://arxiv.org/abs/2311.04861v1",
        "pub_date": "2023-11-08",
        "summary": "Reputation systems guide our decision making both in life and work: which\nrestaurant to eat at, which vendor to buy from, which software dependencies to\nuse, and who or what to trust. These systems are often based on old ideas and\nare failing in the face of modern threats. Fraudsters have found ways to\nmanipulate them, undermining their integrity and utility. Generative AI adds to\nthe problem by enabling the creation of real-looking fake narratives at scale,\ncreating a false sense of consensus. Meanwhile, the need for reliable\nreputation concepts is more important than ever, as wrong decisions lead to\nincreasingly severe outcomes: wasted time, poor service, and a feeling of\ninjustice at best, fraud, identity theft, and ransomware at worst.\n  In this extended abstract we introduce Sandi, a new kind of reputation system\nwith a single well-defined purpose: to create trust through accountability in\none-to-one transactions. Examples of such transactions include sending an email\nor making a purchase online. Sandi has strong security and privacy properties\nthat make it suitable for use also in sensitive contexts. Furthermore, Sandi\ncan guarantee reputation integrity and transparency for its registered users.\n  As a primary application, we envision how Sandi could counter fraud and abuse\nin direct communication. Concretely, message senders request a cryptographic\ntag from Sandi that they send along with their message. If the receiver finds\nthe message inappropriate, they can report the sender using this tag. Notably,\nonly senders need registered accounts and do not need to manage long-term keys.\nThe design of Sandi ensures compatibility with any communication system that\nallows for small binary data transmission.",
        "translated": "信誉系统指导我们在生活和工作中做出决定: 去哪家餐馆吃饭，从哪家供应商那里买东西，使用哪种软件依赖，信任谁或信任什么。这些系统往往基于旧观念，在面对现代威胁时失败了。欺诈者已经找到了操纵他们的方法，破坏了他们的诚信和实用性。生成性人工智能通过大规模创造看起来真实的虚假叙述，创造一种虚假的共识感，从而加剧了这个问题。与此同时，对可靠信誉概念的需求比以往任何时候都更加重要，因为错误的决定会导致日益严重的后果: 浪费时间，糟糕的服务，最好的情况下是不公正的感觉，最坏的情况下是欺诈，身份盗窃和勒索软件。在这个扩展摘要中，我们介绍了 Sandi，一种新型的信誉系统，它有一个明确的目的: 通过一对一交易中的问责制来建立信任。这类交易的例子包括发送电子邮件或在线购买。Sandi 具有强大的安全和隐私属性，因此也适合在敏感环境中使用。此外，Sandi 可以保证其注册用户的声誉完整性和透明度。作为一个主要的应用程序，我们设想如何 Sandi 可以反欺诈和滥用的直接沟通。具体来说，消息发送者从 Sandi 请求一个加密标记，并随消息一起发送。如果接收方发现消息不合适，他们可以使用此标记报告发送方。值得注意的是，只有发送方需要注册帐户，不需要管理长期密钥。Sandi 的设计确保了与任何允许小型二进制数据传输的通信系统的兼容性。"
    },
    {
        "title": "DAG-Sword: A Simulator of Large-Scale Network Topologies for\n  DAG-Oriented Proof-of-Work Blockchains",
        "url": "http://arxiv.org/abs/2311.04638v1",
        "pub_date": "2023-11-08",
        "summary": "The blockchain brought interesting properties for many practical\napplications. However, some properties, such as the transaction processing\nthroughput remained limited, especially in Proof-of-Work blockchains.\nTherefore, several promising directions, such as sharding designs and DAG-based\nprotocols emerged. In this paper, we focus on DAG-based consensus protocols and\npresent a discrete-event simulator for them. Our simulator can simulate\nrealistic blockchain networks created from data of a Bitcoin network, while its\nnetwork configuration and topology can be customized. The simulated network\nconsists of honest and malicious miners. Malicious miners do not make any\nattack on consensus itself. Instead, they use a different transaction selection\nstrategy than honest miners (who select transactions randomly) with the\nintention to earn unfairly more profits than honest miners at the cost of\ndowngrading the protocol performance by duplicate transactions. As a\nconsequence, this harms the performance of some DAG-based protocols (e.g.,\nPHANTOM and GHOSTDAG) in terms of transaction processing throughput, which we\ndemonstrate in our experiments and extend the results of the related work that\ncontains a small-scale network of 10 nodes by the results obtained on a\nlarge-scale network with 7000 nodes. Next, we empirically compare different\nalgorithms for the mempool structure, and we propose a composite mempool\nstructure that is memory-efficient and thus convenient for simulations of\nresource-demanding large-scale networks.",
        "translated": "区块链为许多实际应用带来了有趣的性质。然而，一些属性，例如事务处理吞吐量仍然有限，特别是在 Proof-of-Work 区块链中。因此，出现了一些有前途的方向，如分片设计和基于 DAG 的协议。本文重点研究了基于有向无环网的一致性协议，并提出了一个离散事件模拟器。我们的模拟器可以模拟由比特币网络数据创建的真实的区块链网络，而其网络配置和拓扑结构可以自定义。模拟的网络由诚实和恶意的矿工组成。恶意的矿工不会对共识本身进行任何攻击。相反，他们使用的交易选择策略不同于诚实的采矿者(他们随机选择交易) ，其意图是通过重复交易降低协议性能，从而不公平地获得比诚实的采矿者更多的利润。因此，这损害了一些基于 DAG 的协议(例如，PHANTOM 和 GHOSTDAG)在事务处理吞吐量方面的性能，我们在实验中证明了这一点，并通过在拥有7000个节点的大规模网络上获得的结果扩展了包含10个节点的小规模网络的相关工作的结果。接下来，我们对不同的内存池结构算法进行了实证比较，提出了一种复合内存池结构，该结构具有存储效率高，便于对需要资源的大规模网络进行仿真。"
    },
    {
        "title": "Army of Thieves: Enhancing Black-Box Model Extraction via Ensemble based\n  sample selection",
        "url": "http://arxiv.org/abs/2311.04588v1",
        "pub_date": "2023-11-08",
        "summary": "Machine Learning (ML) models become vulnerable to Model Stealing Attacks\n(MSA) when they are deployed as a service. In such attacks, the deployed model\nis queried repeatedly to build a labelled dataset. This dataset allows the\nattacker to train a thief model that mimics the original model. To maximize\nquery efficiency, the attacker has to select the most informative subset of\ndata points from the pool of available data. Existing attack strategies utilize\napproaches like Active Learning and Semi-Supervised learning to minimize costs.\nHowever, in the black-box setting, these approaches may select sub-optimal\nsamples as they train only one thief model. Depending on the thief model's\ncapacity and the data it was pretrained on, the model might even select noisy\nsamples that harm the learning process. In this work, we explore the usage of\nan ensemble of deep learning models as our thief model. We call our attack Army\nof Thieves(AOT) as we train multiple models with varying complexities to\nleverage the crowd's wisdom. Based on the ensemble's collective decision,\nuncertain samples are selected for querying, while the most confident samples\nare directly included in the training data. Our approach is the first one to\nutilize an ensemble of thief models to perform model extraction. We outperform\nthe base approaches of existing state-of-the-art methods by at least 3% and\nachieve a 21% higher adversarial sample transferability than previous work for\nmodels trained on the CIFAR-10 dataset.",
        "translated": "机器学习(ML)模型在作为服务部署时容易受到模型窃取攻击(MSA)的攻击。在这种攻击中，将重复查询已部署的模型以构建标记的数据集。这个数据集允许攻击者训练一个模仿原始模型的小偷模型。为了最大限度地提高查询效率，攻击者必须从可用数据池中选择信息量最大的数据点子集。现有的攻击策略利用主动学习和半监督学习等方法来最小化成本。然而，在黑盒设置中，这些方法可能会选择次优的样本，因为它们只训练一个小偷模型。根据小偷模型的容量和它预先训练的数据，模型甚至可能选择有害于学习过程的噪声样本。在这项工作中，我们探索使用集成的深度学习模型作为我们的小偷模型。我们称我们的攻击盗贼军团(AOT) ，因为我们训练的多个模型具有不同的复杂性，以利用群众的智慧。在集合决策的基础上，选取不确定样本进行查询，直接将最有信心的样本包含在训练数据中。我们的方法是第一个利用盗贼模型集合来执行模型提取。我们比现有最先进的方法的基础方法至少高出3% ，并且比以前在 CIFAR-10数据集上训练的模型的对抗性样本可转移性高出21% 。"
    },
    {
        "title": "KiD: A Hardware Design Framework Targeting Unified NTT Multiplication\n  for CRYSTALS-Kyber and CRYSTALS-Dilithium on FPGA",
        "url": "http://arxiv.org/abs/2311.04581v1",
        "pub_date": "2023-11-08",
        "summary": "Large-degree polynomial multiplication is an integral component of\npost-quantum secure lattice-based cryptographic algorithms like CRYSTALS-Kyber\nand Dilithium. The computational complexity of large-degree polynomial\nmultiplication can be reduced significantly through Number Theoretic\nTransformation (NTT). In this paper, we aim to develop a unified and shared NTT\narchitecture that can support polynomial multiplication for both CRYSTALS-Kyber\nand Dilithium. More specifically, in this paper, we have proposed three\ndifferent unified architectures for NTT multiplication in CRYSTALS-Kyber and\nDilithium with varying numbers of configurable radix-2 butterfly units.\nAdditionally, the developed implementation is coupled with a conflict-free\nmemory mapping scheme that allows the architecture to be fully pipelined. We\nhave validated our implementation on Artix-7, Zynq-7000 and Zynq Ultrascale+\nFPGAs. Our standalone implementations for NTT multiplication for CRYSTALS-Kyber\nand Dilithium perform better than the existing works, and our unified\narchitecture shows excellent area and timing performance compared to both\nstandalone and existing unified implementations. This architecture can\npotentially be used for compact and efficient implementation for CRYSTALS-Kyber\nand Dilithium.",
        "translated": "大度多项式乘法是基于后量子安全格子密码算法(如 CRYSTALS-Kyber 和 Dilithium)的一个重要组成部分。通过数论变换(NTT)可以显著降低大次多项式乘法的计算复杂度。在本文中，我们的目标是开发一个统一的和共享的 NTT 架构，可以支持多项式乘的水晶-凯伯和双锂。更具体地说，在本文中，我们提出了三种不同的 NTT 在水晶中增殖的统一体系结构—— Kyber 和 Dilithium，它们具有不同数量的可配置的基 -2蝴蝶单元。此外，开发的实现还与一个无冲突的内存映射方案相结合，该方案允许体系结构完全流水线化。我们已经在 Artix-7，Zynq-7000和 Zynq 超级刻度 + FPGA 上验证了我们的实现。我们针对 CRYSTALS-Kyber 和 Dilithium 的 NTT 倍增的独立实现比现有的工程表现更好，与独立和现有的统一实现相比，我们的统一体系结构显示出出色的区域和时序性能。这种结构可能用于紧凑和有效的实施的水晶凯伯尔和双锂。"
    },
    {
        "title": "Local Differential Privacy for Smart Meter Data Sharing",
        "url": "http://arxiv.org/abs/2311.04544v1",
        "pub_date": "2023-11-08",
        "summary": "Energy disaggregation techniques, which use smart meter data to infer\nappliance energy usage, can provide consumers and energy companies valuable\ninsights into energy management. However, these techniques also present privacy\nrisks, such as the potential for behavioral profiling. Local differential\nprivacy (LDP) methods provide strong privacy guarantees with high efficiency in\naddressing privacy concerns. However, existing LDP methods focus on protecting\naggregated energy consumption data rather than individual appliances.\nFurthermore, these methods do not consider the fact that smart meter data are a\nform of streaming data, and its processing methods should account for time\nwindows. In this paper, we propose a novel LDP approach (named LDP-SmartEnergy)\nthat utilizes randomized response techniques with sliding windows to facilitate\nthe sharing of appliance-level energy consumption data over time while not\nrevealing individual users' appliance usage patterns. Our evaluations show that\nLDP-SmartEnergy runs efficiently compared to baseline methods. The results also\ndemonstrate that our solution strikes a balance between protecting privacy and\nmaintaining the utility of data for effective analysis.",
        "translated": "能源分解技术利用智能电表数据来推断电器的能源使用情况，可以为消费者和能源公司提供有价值的能源管理见解。然而，这些技术也存在隐私风险，比如潜在的行为分析。本地差分隐私(lDP)方法提供了强大的隐私保障，能够高效地解决隐私问题。然而，现有的 LDP 方法侧重于保护总能耗数据，而不是单个电器。此外，这些方法没有考虑到智能电表数据是流数据的一种形式这一事实，其处理方法应考虑时间窗口。在这篇文章中，我们提出了一种新的 LDP 方法(命名为 LDP-SmartEnergy) ，该方法利用带有滑动窗口的随机化回答技术来促进随着时间的推移共享设备级能源消耗数据，同时不暴露个人用户的设备使用模式。我们的评估表明，LDP-SmartEnergy 运行效率高于基准方法。结果还表明，我们的解决方案在保护隐私和维护有效分析数据的效用之间取得了平衡。"
    },
    {
        "title": "SyncBleed: A Realistic Threat Model and Mitigation Strategy for\n  Zero-Involvement Pairing and Authentication (ZIPA)",
        "url": "http://arxiv.org/abs/2311.04433v1",
        "pub_date": "2023-11-08",
        "summary": "Zero Involvement Pairing and Authentication (ZIPA) is a promising technique\nfor auto-provisioning large networks of Internet-of-Things (IoT) devices.\nPresently, these networks use password-based authentication, which is difficult\nto scale to more than a handful of devices. To deal with this challenge, ZIPA\nenabled devices autonomously extract identical authentication or encryption\nkeys from ambient environmental signals. However, during the key negotiation\nprocess, existing ZIPA systems leak information on a public wireless channel\nwhich can allow adversaries to learn the key. We demonstrate a passive attack\ncalled SyncBleed, which uses leaked information to reconstruct keys generated\nby ZIPA systems. To mitigate SyncBleed, we present TREVOR, an improved key\ngeneration technique that produces nearly identical bit sequences from\nenvironmental signals without leaking information. We demonstrate that TREVOR\ncan generate keys from a variety of environmental signal types under 4 seconds,\nconsistently achieving a 90-95% bit agreement rate across devices within\nvarious environmental sources.",
        "translated": "零参与配对和认证(ZIPA)是物联网(IoT)设备自动配置大型网络的一种有前途的技术。目前，这些网络使用基于密码的身份验证，很难扩展到多个设备。为了应对这个挑战，ZIPA 允许设备自主地从环境信号中提取相同的身份验证或加密密钥。但是，在密钥协商过程中，现有的 ZIPA 系统会在公共无线信道上泄露信息，使得对手可以获取密钥。我们演示了一种名为 SyncBleed 的被动攻击，它使用泄漏的信息重建由 ZIPA 系统生成的密钥。为了减轻同步出血，我们提出 TREVOR，一种改进的关键生成技术，产生几乎相同的位序列从环境信号没有泄漏信息。我们证明 TREVOR 可以在4秒内从多种环境信号类型中生成密钥，在不同环境源的设备之间始终达到90-95% 的比特一致性。"
    },
    {
        "title": "CompactTag: Minimizing Computation Overheads in Actively-Secure MPC for\n  Deep Neural Networks",
        "url": "http://arxiv.org/abs/2311.04406v1",
        "pub_date": "2023-11-08",
        "summary": "Secure Multiparty Computation (MPC) protocols enable secure evaluation of a\ncircuit by several parties, even in the presence of an adversary who\nmaliciously corrupts all but one of the parties. These MPC protocols are\nconstructed using the well-known secret-sharing-based paradigm (SPDZ and\nSPDZ2k), where the protocols ensure security against a malicious adversary by\ncomputing Message Authentication Code (MAC) tags on the input shares and then\nevaluating the circuit with these input shares and tags. However, this tag\ncomputation adds a significant runtime overhead, particularly for machine\nlearning (ML) applications with numerous linear computation layers such as\nconvolutions and fully connected layers.\n  To alleviate the tag computation overhead, we introduce CompactTag, a\nlightweight algorithm for generating MAC tags specifically tailored for linear\nlayers in ML. Linear layer operations in ML, including convolutions, can be\ntransformed into Toeplitz matrix multiplications. For the multiplication of two\nmatrices with dimensions T1 x T2 and T2 x T3 respectively, SPDZ2k required O(T1\nx T2 x T3) local multiplications for the tag computation. In contrast,\nCompactTag only requires O(T1 x T2 + T1 x T3 + T2 x T3) local multiplications,\nresulting in a substantial performance boost for various ML models.\n  We empirically compared our protocol to the SPDZ2k protocol for various ML\ncircuits, including ResNet Training-Inference, Transformer Training-Inference,\nand VGG16 Training-Inference. SPDZ2k dedicated around 30% of its online runtime\nfor tag computation. CompactTag speeds up this tag computation bottleneck by up\nto 23x, resulting in up to 1.47x total online phase runtime speedups for\nvarious ML workloads.",
        "translated": "安全多方计算(MPC)协议允许多方对电路进行安全评估，即使在存在恶意破坏除一方以外的所有方的对手的情况下也是如此。这些 MPC 协议是使用著名的基于秘密共享的范例(SPDZ 和 SPDZ2k)构建的，其中协议通过计算输入共享上的讯息鑑别码(MAC)标签，然后用这些输入共享和标签评估电路来确保针对恶意对手的安全性。然而，这种标记计算增加了大量的运行时开销，特别是对于具有许多线性计算层(如卷积和完全连接的层)的机器学习(ML)应用程序。为了减少标签计算开销，我们引入了 CompactTag，这是一种轻量级的算法，用于生成专门为 ML 中的线性层定制的 MAC 标签。ML 中的线性层运算，包括卷积，可以转化为 Toeplitz 矩阵乘法。对于维数分别为 T1 x T2和 T2 x T3的两个矩阵的相乘，SPDZ2k 需要 O (T1 x T2 x T3)局部相乘来进行标签计算。相比之下，CompactTag 只需要 O (T1 x T2 + T1 x T3 + T2 x T3)局部乘法，从而大大提高了各种机器学习模型的性能。我们经验性地比较了我们的协议和 SPDZ2k 协议在各种机器学习电路中的应用，包括 ResNet 训练-推理、变压器训练-推理和 VGG16训练-推理。SPDZ2k 将大约30% 的在线运行时用于标记计算。CompactTag 将这个标记计算瓶颈加速了23倍，从而为各种 ML 工作负载带来了高达1.47倍的在线运行阶段总加速。"
    },
    {
        "title": "Watermarks in the Sand: Impossibility of Strong Watermarking for\n  Generative Models",
        "url": "http://arxiv.org/abs/2311.04378v1",
        "pub_date": "2023-11-07",
        "summary": "Watermarking generative models consists of planting a statistical signal\n(watermark) in a model's output so that it can be later verified that the\noutput was generated by the given model. A strong watermarking scheme satisfies\nthe property that a computationally bounded attacker cannot erase the watermark\nwithout causing significant quality degradation. In this paper, we study the\n(im)possibility of strong watermarking schemes. We prove that, under\nwell-specified and natural assumptions, strong watermarking is impossible to\nachieve. This holds even in the private detection algorithm setting, where the\nwatermark insertion and detection algorithms share a secret key, unknown to the\nattacker. To prove this result, we introduce a generic efficient watermark\nattack; the attacker is not required to know the private key of the scheme or\neven which scheme is used. Our attack is based on two assumptions: (1) The\nattacker has access to a \"quality oracle\" that can evaluate whether a candidate\noutput is a high-quality response to a prompt, and (2) The attacker has access\nto a \"perturbation oracle\" which can modify an output with a nontrivial\nprobability of maintaining quality, and which induces an efficiently mixing\nrandom walk on high-quality outputs. We argue that both assumptions can be\nsatisfied in practice by an attacker with weaker computational capabilities\nthan the watermarked model itself, to which the attacker has only black-box\naccess. Furthermore, our assumptions will likely only be easier to satisfy over\ntime as models grow in capabilities and modalities. We demonstrate the\nfeasibility of our attack by instantiating it to attack three existing\nwatermarking schemes for large language models: Kirchenbauer et al. (2023),\nKuditipudi et al. (2023), and Zhao et al. (2023). The same attack successfully\nremoves the watermarks planted by all three schemes, with only minor quality\ndegradation.",
        "translated": "水印生成模型包括在模型的输出中植入一个统计信号(水印) ，以便以后可以验证该输出是由给定的模型生成的。一个强水印方案满足这样一个特性，即计算有界的攻击者不能在不造成显著的质量下降的情况下删除水印。本文研究了强水印算法实现的可能性。我们证明，在一定的自然假设条件下，强水印是不可能实现的。即使在私有检测算法设置中也是如此，其中水印插入和检测算法共享一个攻击者不知道的秘密密钥。为了证明这一结果，我们引入了一种通用的高效水印攻击，攻击者不需要知道该方案的私钥，甚至不需要知道使用了哪种方案。我们的攻击基于两个假设: (1)攻击者可以访问一个“质量预言器”，它可以评估一个候选输出是否是对提示的高质量响应; (2)攻击者可以访问一个“扰动预言器”，它可以修改输出，具有保持质量的重要概率，并诱导高质量输出上的有效混合随机游走。我们认为这两个假设都可以在实践中得到满足，攻击者的计算能力比水印模型本身弱，攻击者只能访问黑盒。此外，随着模型能力和模式的增长，我们的假设可能只会更容易满足。我们通过实例化它来攻击三种现有的大型语言模型水印方案来证明我们的攻击的可行性: Kirchenbauer 等(2023) ，KuditiPudi 等(2023)和 Zhao 等(2023)。同样的攻击成功地移除了所有三种方案植入的水印，只有轻微的质量降低。"
    },
    {
        "title": "Federated Experiment Design under Distributed Differential Privacy",
        "url": "http://arxiv.org/abs/2311.04375v1",
        "pub_date": "2023-11-07",
        "summary": "Experiment design has a rich history dating back over a century and has found\nmany critical applications across various fields since then. The use and\ncollection of users' data in experiments often involve sensitive personal\ninformation, so additional measures to protect individual privacy are required\nduring data collection, storage, and usage. In this work, we focus on the\nrigorous protection of users' privacy (under the notion of differential privacy\n(DP)) while minimizing the trust toward service providers. Specifically, we\nconsider the estimation of the average treatment effect (ATE) under DP, while\nonly allowing the analyst to collect population-level statistics via secure\naggregation, a distributed protocol enabling a service provider to aggregate\ninformation without accessing individual data. Although a vital component in\nmodern A/B testing workflows, private distributed experimentation has not\npreviously been studied. To achieve DP, we design local privatization\nmechanisms that are compatible with secure aggregation and analyze the utility,\nin terms of the width of confidence intervals, both asymptotically and\nnon-asymptotically. We show how these mechanisms can be scaled up to handle the\nvery large number of participants commonly found in practice. In addition, when\nintroducing DP noise, it is imperative to cleverly split privacy budgets to\nestimate both the mean and variance of the outcomes and carefully calibrate the\nconfidence intervals according to the DP noise. Last, we present comprehensive\nexperimental evaluations of our proposed schemes and show the privacy-utility\ntrade-offs in experiment design.",
        "translated": "实验设计有着悠久的历史，可以追溯到一个多世纪以前，并且在各个领域发现了许多重要的应用。实验中用户数据的使用和收集往往涉及敏感的个人信息，因此在数据收集、存储和使用过程中需要采取额外的措施来保护个人隐私。在这项工作中，我们着重于严格保护用户的隐私(根据差分隐私(DP)的概念) ，同时尽量减少对服务提供商的信任。具体来说，我们考虑 DP 下的平均治疗效果(ATE)的估计，同时只允许分析师通过安全聚合收集总体级别的统计数据，这是一种分布式协议，使服务提供者能够聚合信息而无需访问单个数据。尽管在现代 A/B 测试工作流中，私有分布式实验是一个非常重要的组成部分，但是以前还没有对其进行过研究。为了实现动态规划，我们设计了与安全聚合兼容的局部私有化机制，并从渐近和非渐近的置信区间宽度的角度分析了效用。我们将展示如何扩大这些机制的规模，以处理实践中常见的大量参与者。此外，在引入 DP 噪声时，需要巧妙地分割隐私预算，以估计结果的均值和方差，并根据 DP 噪声仔细校准置信区间。最后，对所提出的方案进行了全面的实验评估，并给出了实验设计中的隐私-效用权衡。"
    },
    {
        "title": "Enhancing Malware Detection by Integrating Machine Learning with Cuckoo\n  Sandbox",
        "url": "http://arxiv.org/abs/2311.04372v1",
        "pub_date": "2023-11-07",
        "summary": "In the modern era, malware is experiencing a significant increase in both its\nvariety and quantity, aligning with the widespread adoption of the digital\nworld. This surge in malware has emerged as a critical challenge in the realm\nof cybersecurity, prompting numerous research endeavors and contributions to\naddress the issue. Machine learning algorithms have been leveraged for malware\ndetection due to their ability to uncover concealed patterns within vast\ndatasets. However, deep learning algorithms, characterized by their\nmulti-layered structure, surpass the limitations of traditional machine\nlearning approaches. By employing deep learning techniques such as CNN\n(Convolutional Neural Network) and RNN (Recurrent Neural Network), this study\naims to classify and identify malware extracted from a dataset containing API\ncall sequences. The performance of these algorithms is compared with that of\nconventional machine learning methods, including SVM (Support Vector Machine),\nRF (Random Forest), KNN (K-Nearest Neighbors), XGB (Extreme Gradient Boosting),\nand GBC (Gradient Boosting Classifier), all using the same dataset. The\noutcomes of this research demonstrate that both deep learning and machine\nlearning algorithms achieve remarkably high levels of accuracy, reaching up to\n99% in certain cases.",
        "translated": "在现代，恶意软件的种类和数量都在显著增加，这与数字世界的广泛采用是一致的。这种恶意软件的激增已经成为网络安全领域的一个重大挑战，促使许多研究工作和贡献来解决这个问题。机器学习算法已经被用于恶意软件的检测，因为它们能够揭示大量数据集中隐藏的模式。然而，深度学习算法以其多层结构为拥有属性，超越了传统机器学习方法的局限性。通过使用深度学习技术，如 CNN (卷积神经网络)和 RNN (递归神经网络) ，这项研究的目的是分类和识别从包含 API 调用序列的数据集中提取的恶意软件。这些算法的性能与传统的机器学习方法相比，包括 SVM (支持向量机) ，RF (随机森林) ，KNN (k-最近邻) ，XGB (极端梯度提升)和 gBC (梯度提升分类器) ，都使用相同的数据集。这项研究的结果表明，深度学习和机器学习算法都取得了非常高的准确率，在某些情况下达到99% 。"
    },
    {
        "title": "FigStep: Jailbreaking Large Vision-language Models via Typographic\n  Visual Prompts",
        "url": "http://arxiv.org/abs/2311.05608v1",
        "pub_date": "2023-11-09",
        "summary": "Large vision-language models (VLMs) like GPT-4V represent an unprecedented\nrevolution in the field of artificial intelligence (AI). Compared to\nsingle-modal large language models (LLMs), VLMs possess more versatile\ncapabilities by incorporating additional modalities (e.g., images). Meanwhile,\nthere's a rising enthusiasm in the AI community to develop open-source VLMs,\nsuch as LLaVA and MiniGPT4, which, however, have not undergone rigorous safety\nassessment. In this paper, to demonstrate that more modalities lead to\nunforeseen AI safety issues, we propose FigStep, a novel jailbreaking framework\nagainst VLMs. FigStep feeds harmful instructions into VLMs through the image\nchannel and then uses benign text prompts to induce VLMs to output contents\nthat violate common AI safety policies. Our experimental results show that\nFigStep can achieve an average attack success rate of 94.8% across 2 families\nof popular open-source VLMs, LLaVA and MiniGPT4 (a total of 5 VLMs). Moreover,\nwe demonstrate that the methodology of FigStep can even jailbreak GPT-4V, which\nalready leverages several system-level mechanisms to filter harmful queries.\nAbove all, our experimental results reveal that VLMs are vulnerable to\njailbreaking attacks, which highlights the necessity of novel safety alignments\nbetween visual and textual modalities.",
        "translated": "像 GPT-4V 这样的大型视觉语言模型代表了人工智能领域一场前所未有的革命。与单模态大语言模型(LLM)相比，VLM 通过结合其他模态(例如图像)具有更多的通用功能。与此同时，人工智能领域开发开源 VLM 的热情也在不断上升，比如 LLaVA 和 MiniGPT4，然而，它们还没有经过严格的安全评估。在本文中，为了证明更多的模式导致不可预见的人工智能安全问题，我们提出了 FigStep，一个针对 VLM 的新的越狱框架。FigStep 通过图像通道将有害的指令输入到 VLM 中，然后使用良性的文本提示来诱导 VLM 输出违反常见 AI 安全策略的内容。我们的实验结果表明，FigStep 可以在2个流行的开源 VLMs，LLaVA 和 MiniGPT4(总共5个 VLMs)家族中达到94.8% 的平均攻击成功率。此外，我们还演示了 FigStep 的方法甚至可以越狱 GPT-4V，它已经利用了几个系统级机制来过滤有害查询。最重要的是，我们的实验结果揭示了 VLM 易受越狱攻击，这突出了视觉和文本模式之间新的安全对齐的必要性。"
    },
    {
        "title": "Extending Regev's factoring algorithm to compute discrete logarithms",
        "url": "http://arxiv.org/abs/2311.05545v1",
        "pub_date": "2023-11-09",
        "summary": "Regev recently introduced a quantum factoring algorithm that may be perceived\nas a $d$-dimensional variation of Shor's factoring algorithm. In this work, we\nextend Regev's factoring algorithm to an algorithm for computing discrete\nlogarithms in a natural way. Furthermore, we discuss natural extensions of\nRegev's factoring algorithm to order finding, and to factoring completely via\norder finding.",
        "translated": "Regev 最近引入了一个量子因子分解算法，它可能被认为是 Shor 因子分解算法的 $d $- 维变体。在这项工作中，我们将 Regev 的因式分解算法推广到一个自然地计算离散对数的算法。此外，我们还讨论了 Regev 因式分解算法的自然扩展，以便找到顺序，并通过顺序找到完全的因式分解。"
    },
    {
        "title": "A Comprehensive Survey of Threshold Digital Signatures: NIST Standards,\n  Post-Quantum Cryptography, Exotic Techniques, and Real-World Applications",
        "url": "http://arxiv.org/abs/2311.05514v1",
        "pub_date": "2023-11-09",
        "summary": "Threshold digital signatures enable a distributed execution of signature\nfunctionalities and will play a crucial role in the security of emerging\ndecentralized next-generation networked systems and applications. In this\npaper, we provide a comprehensive and systematic survey of threshold and\ndistributed signatures with advanced features. Our survey encompasses threshold\nsignatures in conventional and post-quantum cryptography (PQC) settings and\ncaptures custom-design and standard signatures (e.g., conventional NIST and\nNIST-PQC). We examine both generic (via secure multi-party computation) and\ncustom thresholding techniques for a myriad of signature families while\ninvestigating exotic signatures, real-life applications, and potential future\nresearch direction.",
        "translated": "门限数字签名使签名功能得以分布式执行，并将在新兴的分散式下一代网络系统和应用程序的安全性方面发挥关键作用。本文对具有先进特征的门限签名和分布式签名进行了全面、系统的研究。我们的调查包括常规和后量子密码(PQC)设置中的阈值签名，并捕获定制设计和标准签名(例如，常规 NIST 和 NIST-PQC)。我们研究通用(通过安全多方计算)和自定义阈值技术的无数签名家族，同时调查外来签名，现实生活中的应用，以及潜在的未来研究方向。"
    },
    {
        "title": "Trust your BMS: Designing a Lightweight Authentication Architecture for\n  Industrial Networks",
        "url": "http://arxiv.org/abs/2311.05498v1",
        "pub_date": "2023-11-09",
        "summary": "With the advent of clean energy awareness and systems that rely on extensive\nbattery usage, the community has seen an increased interest in the development\nof more complex and secure Battery Management Systems (BMS). In particular, the\ninclusion of BMS in modern complex systems like electric vehicles and power\ngrids has presented a new set of security-related challenges. A concern is\nshown when BMS are intended to extend their communication with external system\nnetworks, as their interaction can leave many backdoors open that potential\nattackers could exploit. Hence, it is highly desirable to find a general design\nthat can be used for BMS and its system inclusion. In this work, a security\narchitecture solution is proposed intended for the communication between BMS\nand other system devices. The aim of the proposed architecture is to be easily\napplicable in different industrial settings and systems, while at the same time\nkeeping the design lightweight in nature.",
        "translated": "随着清洁能源意识和系统的出现，依赖于广泛的电池使用，社区已经看到了更复杂和安全的电池管理系统(BMS)的发展兴趣增加。特别是，将 BMS 纳入电动汽车和电网等现代复杂系统提出了一系列与安全相关的新挑战。当 BMS 打算扩展它们与外部系统网络的通信时，会出现一个问题，因为它们的交互会留下许多可能被攻击者利用的后门。因此，寻找一种可用于 BMS 及其系统包含的通用设计是非常可取的。在这项工作中，提出了一个安全体系结构的解决方案，旨在 BMS 与其他系统设备之间的通信。提出的体系结构的目的是方便地适用于不同的工业设置和系统，同时保持设计轻量级的性质。"
    },
    {
        "title": "ChatGPT and other Large Language Models for Cybersecurity of Smart Grid\n  Applications",
        "url": "http://arxiv.org/abs/2311.05462v1",
        "pub_date": "2023-11-09",
        "summary": "Cybersecurity breaches targeting electrical substations constitute a\nsignificant threat to the integrity of the power grid, necessitating\ncomprehensive defense and mitigation strategies. Any anomaly in information and\ncommunication technology (ICT) should be detected for secure communications\nbetween devices in digital substations. This paper proposes large language\nmodels (LLM), e.g., ChatGPT, for the cybersecurity of IEC 61850-based digital\nsubstation communications. Multicast messages such as generic object oriented\nsubstation event (GOOSE) and sampled value (SV) are used for case studies. The\nproposed LLM-based cybersecurity framework includes for the first time data\npre-processing of communication systems and human-in-the-loop (HITL) training\n(considering the cybersecurity guidelines recommended by humans). The results\nshow a comparative analysis of detected anomaly data carried out based on the\nperformance evaluation metrics for different LLMs. A hardware-in-the-loop (HIL)\ntestbed is used to generate and extract a dataset of IEC 61850 communications.",
        "translated": "针对变电站的网络安全漏洞对电网的完整性构成重大威胁，需要采取全面的防御和缓解战略。为了保证数字化变电站内设备之间的安全通信，应该检测信息和通信技术(ICT)中的任何异常。针对基于 IEC 61850标准的数字化变电站通信网络安全问题，提出了基于 ChatGPT 的大语言模型。多播消息，如通用面向对象的变电站事件(GOOSE)和采样值(SV)是用于案例研究。建议的以 LLM 为基础的网络安全框架首次包括通信系统数据预处理和人员在线(HITL)培训(考虑到人员推荐的网络安全指南)。结果表明，基于不同 LLM 的性能评估指标，对检测到的异常数据进行了比较分析。采用硬件在环(HIL)测试平台生成和提取 IEC 61850通信数据集。"
    },
    {
        "title": "A Survey on Privacy of Health Data Lifecycle: A Taxonomy, Review, and\n  Future Directions",
        "url": "http://arxiv.org/abs/2311.05404v1",
        "pub_date": "2023-11-09",
        "summary": "With the increasing breaches and security threats that endanger health data,\nensuring patients' privacy is essential. To that end, the research community\nhas proposed various privacy-preserving approaches based on cryptography,\nhashing, or ledger technologies for alleviating health data vulnerability. To\nestablish a comprehensive understanding of health data privacy risks, and the\nbenefits and limitations of existing privacy-preserving approaches, we perform\na detailed review of existing work and distill 10 distinct privacy concerns\noccurring in a health data lifecycle. Furthermore, we classify existing\napproaches based on their applicability to particular privacy concerns\noccurring at a particular lifecycle stage. Finally, we propose a taxonomy of\ntechniques used for privacy preservation in healthcare and triangulate those\ntechniques with the lifecycle stages and concerns. Our review indicates heavy\nusage of cryptographical techniques in this domain. However, we have also found\nthat healthcare systems have special requirements that require novel\ncryptographic techniques and security schemes to address special needs.\nTherefore, we identify several future research directions to mitigate the\nsecurity challenges for privacy preservation in health data management.",
        "translated": "随着越来越多的漏洞和安全威胁危及健康数据，确保病人的隐私是至关重要的。为此，研究团体提出了各种基于加密、散列或分类账技术的保护隐私的方法，以减轻健康数据的脆弱性。为了全面了解健康数据隐私风险以及现有隐私保护方法的益处和局限性，我们对现有工作进行了详细的审查，并提取了健康数据生命周期中发生的10个不同的隐私问题。此外，我们根据现有方法对特定生命周期阶段出现的特定隐私问题的适用性对它们进行分类。最后，我们提出了在医疗保健中用于隐私保护的技术分类，并根据生命周期阶段和关注点对这些技术进行了三角测量。我们的回顾表明密码技术在这个领域的大量使用。然而，我们也发现，医疗保健系统有特殊的需求，需要新的加密技术和安全方案来满足特殊的需求。因此，我们确定了几个未来的研究方向，以缓解在健康数据管理中隐私保护的安全挑战。"
    },
    {
        "title": "Data Valuation and Detections in Federated Learning",
        "url": "http://arxiv.org/abs/2311.05304v1",
        "pub_date": "2023-11-09",
        "summary": "Federated Learning (FL) enables collaborative model training without sharing\nraw data, demanding abundant, high-quality data for optimal model performance.\nFair and efficient data evaluation is a fundamental issue for incentivizing\nclients to provide more high-quality data. Meanwhile, it is likely that only a\nsubset of clients and datasets are relevant for a learning task while the rest\nof them may have a negative impact on the model training. This paper introduces\na novel privacy-preserving method for evaluating client contributions and\nselecting relevant data samples without a pre-specified training algorithm. Our\nproposed approach, FedBary, utilizes Wasserstein distance within the federated\ncontext, offering a new pioneering solution for data valuation, which provides\ntransparent data evaluation and efficient computation of Wasserstein barycenter\nto mitigate reliance on validation data. We conduct extensive empirical\nexperiments and theoretical analysis, showing the promising research of this\nvaluation metric.",
        "translated": "联邦学习(FL)不需要共享原始数据就可以实现协同模型训练，需要大量高质量的数据来优化模型性能。公平有效的数据评估是激励客户提供更多高质量数据的基本问题。与此同时，可能只有一部分客户和数据集与学习任务相关，而其余的客户和数据集可能对模型训练产生负面影响。本文介绍了一种新的保护隐私的方法，用于评估客户贡献和选择相关的数据样本，而不需要预先指定的训练算法。我们提出的方法 FedBary 在联邦上下文中利用了 Wasserstein 距离，为数据估值提供了一种新的开创性解决方案，其提供了透明的数据评估和 Wasserstein 重心的高效计算，以减轻对验证数据的依赖。我们进行了广泛的实证实验和理论分析，展示了这一评价指标的研究前景。"
    },
    {
        "title": "Finding Software Vulnerabilities in Open-Source C Projects via Bounded\n  Model Checking",
        "url": "http://arxiv.org/abs/2311.05281v1",
        "pub_date": "2023-11-09",
        "summary": "Computer-based systems have solved several domain problems, including\nindustrial, military, education, and wearable. Nevertheless, such arrangements\nneed high-quality software to guarantee security and safety as both are\nmandatory for modern software products. We advocate that bounded model-checking\ntechniques can efficiently detect vulnerabilities in general software systems.\nHowever, such an approach struggles to scale up and verify extensive code\nbases. Consequently, we have developed and evaluated a methodology to verify\nlarge software systems using a state-of-the-art bounded model checker. In\nparticular, we pre-process input source-code files and guide the respective\nmodel checker to explore them systematically. Moreover, the proposed scheme\nincludes a function-wise prioritization strategy, which readily provides\nresults for code entities according to a scale of importance. Experimental\nresults using a real implementation of the proposed methodology show that it\ncan efficiently verify large software systems. Besides, it presented low peak\nmemory allocation when executed. We have evaluated our approach by verifying\ntwelve popular open-source C projects, where we have found real software\nvulnerabilities that their developers confirmed.",
        "translated": "基于计算机的系统已经解决了几个领域的问题，包括工业、军事、教育和可穿戴设备。然而，这种安排需要高质量的软件来保证安全性和安全性，因为这两者对于现代软件产品都是强制性的。我们主张有界模型检测技术可以有效地检测一般软件系统中的漏洞。但是，这种方法难以扩展和验证大量的代码基。因此，我们开发并评估了一种使用最先进的有界模型检查器来验证大型软件系统的方法。特别是，我们预处理输入源代码文件，并指导各自的模型检查器系统地探索它们。此外，提出的方案还包括一个按功能划分优先级的策略，该策略可以根据重要性等级轻松地为代码实体提供结果。实验结果表明，该方法能够有效地验证大型软件系统。此外，在执行时还表现出较低的内存分配峰值。我们通过验证十二个流行的开源 c 项目来评估我们的方法，在这些项目中我们发现了他们的开发人员确认的真实的软件漏洞。"
    },
    {
        "title": "RAGLog: Log Anomaly Detection using Retrieval Augmented Generation",
        "url": "http://arxiv.org/abs/2311.05261v1",
        "pub_date": "2023-11-09",
        "summary": "The ability to detect log anomalies from system logs is a vital activity\nneeded to ensure cyber resiliency of systems. It is applied for fault\nidentification or facilitate cyber investigation and digital forensics.\nHowever, as logs belonging to different systems and components differ\nsignificantly, the challenge to perform such analysis is humanly challenging\nfrom the volume, variety and velocity of logs. This is further complicated by\nthe lack or unavailability of anomalous log entries to develop trained machine\nlearning or artificial intelligence models for such purposes. In this research\nwork, we explore the use of a Retrieval Augmented Large Language Model that\nleverages a vector database to detect anomalies from logs. We used a Question\nand Answer configuration pipeline. To the best of our knowledge, our experiment\nwhich we called RAGLog is a novel one and the experimental results show much\npromise.",
        "translated": "从系统日志中检测日志异常的能力是确保系统的网络弹性所需的重要活动。它可应用于错误识别或方便网上调查及数位鑑识。然而，由于属于不同系统和组件的日志存在显著差异，执行这种分析的挑战来自于日志的体积、多样性和速度。由于缺乏或无法使用异常日志条目来开发用于此目的的训练有素的机器学习或人工智能模型，这种情况变得更加复杂。在这项研究工作中，我们探索使用检索增强大型语言模型，利用向量数据库检测日志中的异常。我们使用了问答配置管道。据我们所知，我们的实验，我们所谓的 RAGLog 是一个新颖的和实验结果显示了很大的希望。"
    },
    {
        "title": "Can we run our Ethereum nodes at home?",
        "url": "http://arxiv.org/abs/2311.05252v1",
        "pub_date": "2023-11-09",
        "summary": "Scalability is a common issue among the most used permissionless blockchains,\nand several approaches have been proposed to solve this issue. Tackling\nscalability while preserving the security and decentralization of the network\nis a significant challenge. To deliver effective scaling solutions, Ethereum\nachieved a major protocol improvement, including a change in the consensus\nmechanism towards Proof of Stake. This improvement aimed a vast reduction of\nthe hardware requirements to run a node, leading to significant sustainability\nbenefits with a lower network energy consumption. This work analyzes the\nresource usage behavior of different clients running as Ethereum consensus\nnodes, comparing their performance under different configurations and analyzing\ntheir differences. Our results show higher requirements than claimed initially\nand how different clients react to network perturbations. Furthermore, we\ndiscuss the differences between the consensus clients, including their strong\npoints and limitations.",
        "translated": "可伸缩性是最常用的无权限区块链中的一个共同问题，已经提出了几种解决这个问题的方法。在处理可伸缩性的同时，保持网络的安全性和地方分权是一个重大的挑战。为了提供有效的缩放解决方案，以太坊实现了一个重大的协议改进，包括改变共识机制，以证明股份。这种改进的目的是大大减少运行一个节点所需的硬件，从而在降低网络能源消耗的同时带来显著的可持续性效益。分析了不同客户机作为以太共识节点运行时的资源使用行为，比较了不同配置下的性能，并分析了不同配置下的性能差异。我们的结果显示了比最初声称的更高的需求，以及不同的客户端如何对网络扰动作出反应。此外，我们还讨论了共识客户之间的差异，包括他们的优点和局限性。"
    },
    {
        "title": "Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the\n  Wild",
        "url": "http://arxiv.org/abs/2311.06237v1",
        "pub_date": "2023-11-10",
        "summary": "Engaging in the deliberate generation of abnormal outputs from large language\nmodels (LLMs) by attacking them is a novel human activity. This paper presents\na thorough exposition of how and why people perform such attacks. Using a\nformal qualitative methodology, we interviewed dozens of practitioners from a\nbroad range of backgrounds, all contributors to this novel work of attempting\nto cause LLMs to fail. We relate and connect this activity between its\npractitioners' motivations and goals; the strategies and techniques they\ndeploy; and the crucial role the community plays. As a result, this paper\npresents a grounded theory of how and why people attack large language models:\nLLM red teaming in the wild.",
        "translated": "通过攻击大语言模型来有意识地产生异常输出是一种新的人类活动。本文对人们如何以及为什么进行这种攻击进行了全面的阐述。使用一个正式的定性方法，我们采访了来自广泛背景的数十名从业者，所有贡献者都试图导致 LLM 失败这一新颖的工作。我们将这个活动与其实践者的动机和目标、他们所部署的策略和技术以及社区所扮演的关键角色联系起来。因此，本文就人们如何以及为什么攻击大型语言模型提出了一个扎实的理论: 野外 LLM 红色团队。"
    },
    {
        "title": "Deep Learning meets Blockchain for Automated and Secure Access Control",
        "url": "http://arxiv.org/abs/2311.06236v1",
        "pub_date": "2023-11-10",
        "summary": "Access control is a critical component of computer security, governing access\nto system resources. However, designing policies and roles in traditional\naccess control can be challenging and difficult to maintain in dynamic and\ncomplex systems, which is particularly problematic for organizations with\nnumerous resources. Furthermore, traditional methods suffer from issues such as\nthird-party involvement, inefficiency, and privacy gaps, making transparent and\ndynamic access control an ongoing research problem. Moreover detecting\nmalicious activities and identifying users who are not behaving appropriately\ncan present notable difficulties. To address these challenges, we propose\nDLACB, a Deep Learning Based Access Control Using Blockchain, as a solution to\ndecentralized access control. DLACB uses blockchain to provide transparency,\ntraceability, and reliability in various domains such as medicine, finance, and\ngovernment while taking advantage of deep learning to not rely on predefined\npolicies and eventually automate access control. With the integration of\nblockchain and deep learning for access control, DLACB can provide a general\nframework applicable to various domains, enabling transparent and reliable\nlogging of all transactions. As all data is recorded on the blockchain, we have\nthe capability to identify malicious activities. We store a list of malicious\nactivities in the storage system and employ a verification algorithm to\ncross-reference it with the blockchain. We conduct measurements and comparisons\nof the smart contract processing time for the deployed access control system in\ncontrast to traditional access control methods, determining the time overhead\ninvolved. The processing time of DLBAC demonstrates remarkable stability when\nexposed to increased request volumes.",
        "translated": "访问控制是计算机安全的关键组成部分，它控制对系统资源的访问。然而，在传统访问控制中设计策略和角色可能具有挑战性，并且难以在动态和复杂的系统中维护，这对拥有大量资源的组织来说尤其成问题。此外，传统方法还存在第三方参与、效率低下和隐私漏洞等问题，使得透明和动态访问控制成为一个正在研究的问题。此外，检测恶意活动和识别行为不当的用户也会带来显著的困难。为了应对这些挑战，我们提出了 DLACB，一种基于区块链的深度学习访问控制，作为分散访问控制的解决方案。DLACB 使用区块链在医药、金融和政府等领域提供透明度、可追溯性和可靠性，同时利用深度学习不依赖预定义的策略，最终实现访问控制的自动化。通过整合区块链和访问控制的深度学习，DLACB 可以提供一个适用于不同领域的通用框架，实现所有事务的透明和可靠的日志记录。由于所有数据都记录在区块链上，我们有能力识别恶意活动。我们在存储系统中存储一个恶意活动列表，并使用验证算法与区块链进行交叉引用。与传统的访问控制方法相比，我们对已部署的访问控制系统的智能合同处理时间进行了测量和比较，确定了所涉及的时间开销。当请求量增加时，DLBAC 的处理时间表现出显著的稳定性。"
    },
    {
        "title": "Does Differential Privacy Prevent Backdoor Attacks in Practice?",
        "url": "http://arxiv.org/abs/2311.06227v1",
        "pub_date": "2023-11-10",
        "summary": "Differential Privacy (DP) was originally developed to protect privacy.\nHowever, it has recently been utilized to secure machine learning (ML) models\nfrom poisoning attacks, with DP-SGD receiving substantial attention.\nNevertheless, a thorough investigation is required to assess the effectiveness\nof different DP techniques in preventing backdoor attacks in practice. In this\npaper, we investigate the effectiveness of DP-SGD and, for the first time in\nliterature, examine PATE in the context of backdoor attacks. We also explore\nthe role of different components of DP algorithms in defending against backdoor\nattacks and will show that PATE is effective against these attacks due to the\nbagging structure of the teacher models it employs. Our experiments reveal that\nhyperparameters and the number of backdoors in the training dataset impact the\nsuccess of DP algorithms. Additionally, we propose Label-DP as a faster and\nmore accurate alternative to DP-SGD and PATE. We conclude that while Label-DP\nalgorithms generally offer weaker privacy protection, accurate hyper-parameter\ntuning can make them more effective than DP methods in defending against\nbackdoor attacks while maintaining model accuracy.",
        "translated": "差分隐私(DP)最初是为了保护隐私而开发的。然而，它最近被用来保护机器学习(ML)模型免受中毒攻击，DP-SGD 受到了广泛的关注。然而，需要进行彻底的调查，以评估不同的 DP 技术在实践中防止后门攻击的有效性。在本文中，我们研究了 DP-SGD 的有效性，并且在文献中首次考察了后门攻击背景下的 PATE。我们还探讨了 DP 算法的不同组成部分在防御后门攻击中的作用，并将表明 PATE 对这些攻击是有效的，因为它所使用的教师模型的包装结构。我们的实验表明，训练数据集中的超参数和后门的数量影响 DP 算法的成功。此外，我们建议标签 DP 作为一个更快，更准确的选择 DP-SGD 和 PATE。我们的结论是，虽然 Label-DP 算法通常提供较弱的隐私保护，但是精确的超参数调整可以使它们在保持模型精度的同时，比 DP 算法更有效地抵御后门攻击。"
    },
    {
        "title": "Triad: Trusted Timestamps in Untrusted Environments",
        "url": "http://arxiv.org/abs/2311.06156v1",
        "pub_date": "2023-11-10",
        "summary": "We aim to provide trusted time measurement mechanisms to applications and\ncloud infrastructure deployed in environments that could harbor potential\nadversaries, including the hardware infrastructure provider. Despite Trusted\nExecution Environments (TEEs) providing multiple security functionalities,\ntimestamps from the Operating System are not covered. Nevertheless, some\nservices require time for validating permissions or ordering events. To address\nthat need, we introduce Triad, a trusted timestamp dispatcher of time readings.\nThe solution provides trusted timestamps enforced by mutually supportive\nenclave-based clock servers that create a continuous trusted timeline. We\nleverage enclave properties such as forced exits and CPU-based counters to\nmitigate attacks on the server's timestamp counters. Triad produces trusted,\nconfidential, monotonically-increasing timestamps with bounded error and\ndesirable, non-trivial properties. Our implementation relies on Intel SGX and\nSCONE, allowing transparent usage. We evaluate Triad's error and behavior in\nmultiple dimensions.",
        "translated": "我们的目标是为部署在可能包含潜在对手(包括硬件基础设施提供商)的环境中的应用程序和云基础设施提供可信的时间度量机制。尽管可信执行环境(Trusted Execution Environment，TEE)提供了多种安全功能，但是没有涵盖来自操作系统的时间戳。然而，有些服务需要时间来验证权限或排序事件。为了满足这一需求，我们引入了 Triad，一个可信的时间戳读取器。该解决方案提供由相互支持的基于飞地的时钟服务器强制执行的可信时间戳，这些时钟服务器创建连续的可信时间线。我们利用飞地属性(如强制退出和基于 CPU 的计数器)来减轻对服务器时间戳计数器的攻击。Triad 生成可信的、保密的、单调增长的时间戳，其中包含有限的错误和令人满意的、非平凡的属性。我们的实现依赖于 Intel SGX 和 SCONE，允许透明使用。我们评估三合会的错误和行为在多个维度。"
    },
    {
        "title": "A Last-Level Defense for Application Integrity and Confidentiality",
        "url": "http://arxiv.org/abs/2311.06154v1",
        "pub_date": "2023-11-10",
        "summary": "Our objective is to protect the integrity and confidentiality of applications\noperating in untrusted environments. Trusted Execution Environments (TEEs) are\nnot a panacea. Hardware TEEs fail to protect applications against Sybil, Fork\nand Rollback Attacks and, consequently, fail to preserve the consistency and\nintegrity of applications. We introduce a novel system, LLD, that enforces the\nintegrity and consistency of applications in a transparent and scalable\nfashion. Our solution augments TEEs with instantiation control and rollback\nprotection. Instantiation control, enforced with TEE-supported leases,\nmitigates Sybil/Fork Attacks without incurring the high costs of solving\ncrypto-puzzles. Our rollback detection mechanism does not need excessive\nreplication, nor does it sacrifice durability. We show that implementing these\nfunctionalities in the LLD runtime automatically protects applications and\nservices such as a popular DBMS.",
        "translated": "我们的目标是保护在不可信环境中运行的应用程序的完整性和机密性。可信执行环境(Trusted Execution Environment，TEE)并非万能药。硬件 TEE 无法保护应用程序免受 Sybil、 Fork 和 Rollback 攻击，因此无法保持应用程序的一致性和完整性。我们引入了一个新的系统 LLD，它以透明和可伸缩的方式强制应用程序的完整性和一致性。我们的解决方案通过实例化控制和回滚保护来增强 TEE。实例化控制通过 TEE 支持的租约实施，减轻了 Sybil/Fork 攻击，而不会产生解决加密难题的高成本。我们的回滚检测机制不需要过多的复制，也不需要牺牲耐久性。我们展示了在 LLD 运行时中实现这些功能会自动保护应用程序和服务，例如流行的 DBMS。"
    },
    {
        "title": "A Compiler from Array Programs to Vectorized Homomorphic Encryption",
        "url": "http://arxiv.org/abs/2311.06142v1",
        "pub_date": "2023-11-10",
        "summary": "Homomorphic encryption (HE) is a practical approach to secure computation\nover encrypted data. However, writing programs with efficient HE\nimplementations remains the purview of experts. A difficult barrier for\nprogrammability is that efficiency requires operations to be vectorized in\ninobvious ways, forcing efficient HE programs to manipulate ciphertexts with\ncomplex data layouts and to interleave computations with data movement\nprimitives.\n  We present Viaduct-HE, a compiler generates efficient vectorized HE programs.\nViaduct-HE can generate both the operations and complex data layouts required\nfor efficient HE programs. The source language of Viaduct-HE is array-oriented,\nenabling the compiler to have a simple representation of possible vectorization\nschedules. With such a representation, the compiler searches the space of\npossible vectorization schedules and finds those with efficient data layouts.\nAfter finding a vectorization schedule, Viaduct-HE further optimizes HE\nprograms through term rewriting. The compiler has extension points to customize\nthe exploration of vectorization schedules, to customize the cost model for HE\nprograms, and to add back ends for new HE libraries.\n  Our evaluation of the prototype Viaduct-HE compiler shows that it produces\nefficient vectorized HE programs with sophisticated data layouts and\noptimizations comparable to those designed by experts.",
        "translated": "同态加密(HE)是一种保护加密数据计算安全的实用方法。然而，编写具有高效 HE 实现的程序仍然是专家的职权范围。可编程性的一个困难障碍是，效率要求以不明显的方式向量化操作，迫使高效的 HE 程序操作复杂数据布局的密文，并将计算与数据移动原语交错。我们提出了高架桥 -HE，一个编译器生成高效的矢量化 HE 程序。高架桥-HE 可以生成高效 HE 程序所需的操作和复杂的数据布局。HE 的源语言是面向数组的，使编译器能够有一个可能的矢量化调度的简单表示。使用这种表示形式，编译器搜索可能的向量化调度的空间，并找到那些具有高效数据布局的调度。在找到一个矢量化时间表后，高架桥 -HE 通过术语重写进一步优化 HE 程序。编译器具有扩展点来定制矢量化调度的探索，定制 HE 程序的成本模型，并为新的 HE 库添加后端。我们对原型高架桥 HE 编译器的评估表明，它可以生成高效的矢量化 HE 程序，其复杂的数据布局和优化可与专家设计的程序媲美。"
    },
    {
        "title": "A high throughput Intrusion Detection System (IDS) to enhance the\n  security of data transmission among research centers",
        "url": "http://arxiv.org/abs/2311.06082v1",
        "pub_date": "2023-11-10",
        "summary": "Data breaches and cyberattacks represent a severe problem in higher education\ninstitutions and universities that can result in illegal access to sensitive\ninformation and data loss. To enhance the security of data transmission,\nIntrusion Prevention Systems (IPS, i.e., firewalls) and Intrusion Detection\nSystems (IDS, i.e., packet sniffers) are used to detect potential threats in\nthe exchanged data. IPSs and IDSs are usually designed as software programs\nrunning on a server machine. However, when the speed of exchanged data is too\nhigh, this solution can become unreliable. In this case, IPSs and IDSs designed\non a real hardware platform, such as ASICs and FPGAs, represent a more reliable\nsolution. This paper presents a packet sniffer that was designed using a\ncommercial FPGA development board. The system can support a data throughput of\n10 Gbit/s with preliminary results showing that the speed of data transmission\ncan be reliably extended to 100 Gbit/s. The designed system is highly\nconfigurable by the user and can enhance the data protection of information\ntransmitted using the Ethernet protocol. It is particularly suited for the\nsecurity of universities and research centers, where point-to-point network\nconnections are dominant and large amount of sensitive data are shared among\ndifferent hosts.",
        "translated": "数据泄露和网络攻击是高等教育机构和大学的一个严重问题，可能导致非法获取敏感信息和数据丢失。为提高数据传输的安全性，入侵防御系统(IPS，即防火墙)和入侵检测系统(IDS，即数据包嗅探器)被用来检测交换数据中的潜在威胁。IPS 和 IDS 通常被设计为运行在服务器机器上的软件程序。但是，当交换数据的速度太快时，这种解决方案就会变得不可靠。在这种情况下，在真实的硬件平台上设计的 IPS 和 IDS，如 ASIC 和 FPGA，代表了一种更可靠的解决方案。本文介绍了一种利用商用 FPGA 开发板设计的数据包嗅探器。该系统可以支持10Gbit/s 的数据吞吐量，初步结果显示，数据传输速度可以可靠地扩展到100Gbit/s。所设计的系统具有高度的用户可配置性，能够加强对使用以太网协议传输的信息的数据保护。它特别适合于大学和研究中心的安全，在这些地方，点对点网络连接占主导地位，大量敏感数据在不同的主机之间共享。"
    },
    {
        "title": "Practical Membership Inference Attacks against Fine-tuned Large Language\n  Models via Self-prompt Calibration",
        "url": "http://arxiv.org/abs/2311.06062v1",
        "pub_date": "2023-11-10",
        "summary": "Membership Inference Attacks (MIA) aim to infer whether a target data record\nhas been utilized for model training or not. Prior attempts have quantified the\nprivacy risks of language models (LMs) via MIAs, but there is still no\nconsensus on whether existing MIA algorithms can cause remarkable privacy\nleakage on practical Large Language Models (LLMs). Existing MIAs designed for\nLMs can be classified into two categories: reference-free and reference-based\nattacks. They are both based on the hypothesis that training records\nconsistently strike a higher probability of being sampled. Nevertheless, this\nhypothesis heavily relies on the overfitting of target models, which will be\nmitigated by multiple regularization methods and the generalization of LLMs.\nThe reference-based attack seems to achieve promising effectiveness in LLMs,\nwhich measures a more reliable membership signal by comparing the probability\ndiscrepancy between the target model and the reference model. However, the\nperformance of reference-based attack is highly dependent on a reference\ndataset that closely resembles the training dataset, which is usually\ninaccessible in the practical scenario. Overall, existing MIAs are unable to\neffectively unveil privacy leakage over practical fine-tuned LLMs that are\noverfitting-free and private. We propose a Membership Inference Attack based on\nSelf-calibrated Probabilistic Variation (SPV-MIA). Specifically, since\nmemorization in LLMs is inevitable during the training process and occurs\nbefore overfitting, we introduce a more reliable membership signal,\nprobabilistic variation, which is based on memorization rather than\noverfitting. Furthermore, we introduce a self-prompt approach, which constructs\nthe dataset to fine-tune the reference model by prompting the target LLM\nitself. In this manner, the adversary can collect a dataset with a similar\ndistribution from public APIs.",
        "translated": "成员推理攻击(MIA)的目的是推断目标数据记录是否被用于模型训练。先前的尝试已经通过 MIA 量化了语言模型(LM)的隐私风险，但是现有的 MIA 算法是否会在实际的大型语言模型(LLM)上引起显著的隐私泄漏仍然没有达成共识。现有的为 LM 设计的 MIA 可以分为两类: 无引用攻击和基于引用的攻击。它们都基于这样一个假设，即训练记录一致地具有较高的被抽样的概率。然而，这一假设很大程度上依赖于目标模型的过度拟合，这将被多种正则化方法和 LLM 的推广所减轻。通过比较目标模型和参考模型之间的概率差异，可以得到更可靠的隶属度信号，从而证明基于参考模型的攻击在 LLM 中具有良好的效果。然而，基于参考的攻击的性能高度依赖于与训练数据集非常相似的参考数据集，这在实际场景中通常是不可访问的。总的来说，现有的内存管理协议无法有效地揭示实际微调 LLM 的隐私泄漏，而 LLM 是过于自由和私有的。提出了一种基于自校准概率变化的成员推理攻击方法(SPV-MIA)。具体来说，由于 LLM 中的记忆在训练过程中是不可避免的，并且在过拟合之前发生，我们引入了一个更可靠的成员信号——概率变化，它是基于记忆而不是过拟合。此外，我们还引入了一种自提示方法，该方法通过提示目标 LLM 本身来构造数据集来微调引用模型。通过这种方式，对手可以从公共 API 收集具有类似分布的数据集。"
    },
    {
        "title": "Robust Adversarial Attacks Detection for Deep Learning based Relative\n  Pose Estimation for Space Rendezvous",
        "url": "http://arxiv.org/abs/2311.05992v1",
        "pub_date": "2023-11-10",
        "summary": "Research on developing deep learning techniques for autonomous spacecraft\nrelative navigation challenges is continuously growing in recent years.\nAdopting those techniques offers enhanced performance. However, such approaches\nalso introduce heightened apprehensions regarding the trustability and security\nof such deep learning methods through their susceptibility to adversarial\nattacks. In this work, we propose a novel approach for adversarial attack\ndetection for deep neural network-based relative pose estimation schemes based\non the explainability concept. We develop for an orbital rendezvous scenario an\ninnovative relative pose estimation technique adopting our proposed\nConvolutional Neural Network (CNN), which takes an image from the chaser's\nonboard camera and outputs accurately the target's relative position and\nrotation. We perturb seamlessly the input images using adversarial attacks that\nare generated by the Fast Gradient Sign Method (FGSM). The adversarial attack\ndetector is then built based on a Long Short Term Memory (LSTM) network which\ntakes the explainability measure namely SHapley Value from the CNN-based pose\nestimator and flags the detection of adversarial attacks when acting.\nSimulation results show that the proposed adversarial attack detector achieves\na detection accuracy of 99.21%. Both the deep relative pose estimator and\nadversarial attack detector are then tested on real data captured from our\nlaboratory-designed setup. The experimental results from our\nlaboratory-designed setup demonstrate that the proposed adversarial attack\ndetector achieves an average detection accuracy of 96.29%.",
        "translated": "近年来，针对自主航天器相对导航难题开发深度学习技术的研究不断增多。采用这些技术可以提高性能。然而，这种方法也使人们更加担心这种深入学习方法的可靠性和安全性，因为它们容易受到敌对攻击。针对基于深度神经网络的相对位姿估计方案，提出了一种新的基于可解释性概念的对手攻击检测方法。我们为轨道交会场景开发了一种创新的相对姿态估计技术，采用了我们提出的卷积神经网络(CNN) ，该技术从追逐者的机载摄像机中获取图像，并准确地输出目标的相对位置和旋转。我们使用快速梯度符号方法(FGSM)产生的对抗性攻击来无缝地扰动输入图像。然后基于长短期记忆(LSTM)网络构建对抗性攻击检测器，该网络从基于 CNN 的姿态估计器中获取可解释性度量即 SHapley 值，并在行动时标记对抗性攻击的检测。仿真结果表明，所提出的对手攻击检测器的检测准确率达到了99.21% 。深度相对位姿估计器和对手攻击检测器然后测试从我们实验室设计的装置捕获的真实数据。实验室设计的实验结果表明，所提出的对手攻击检测器平均检测准确率达到96.29% 。"
    },
    {
        "title": "KRATT: QBF-Assisted Removal and Structural Analysis Attack Against Logic\n  Locking",
        "url": "http://arxiv.org/abs/2311.05982v1",
        "pub_date": "2023-11-10",
        "summary": "This paper introduces KRATT, a removal and structural analysis attack against\nstate-of-the-art logic locking techniques, such as single and double flip\nlocking techniques (SFLTs and DFLTs). KRATT utilizes powerful quantified\nBoolean formulas (QBFs), which have not found widespread use in hardware\nsecurity, to find the secret key of SFLTs for the first time. It can handle\nlocked circuits under both oracle-less (OL) and oracle-guided (OG) threat\nmodels. It modifies the locked circuit and uses a prominent OL attack to make a\nstrong guess under the OL threat model. It uses a structural analysis technique\nto identify promising protected input patterns and explores them using the\noracle under the OG model. Experimental results on ISCAS'85, ITC'99, and HeLLO:\nCTF'22 benchmarks show that KRATT can break SFLTs using a QBF formulation in\nless than a minute, can decipher a large number of key inputs of SFLTs and\nDFLTs with high accuracy under the OL threat model, and can easily find the\nsecret key of DFLTs under the OG threat model. It is shown that KRATT\noutperforms publicly available OL and OG attacks in terms of solution quality\nand run-time.",
        "translated": "本文介绍了 KRATT，一种针对最先进的逻辑锁定技术，如单翻转锁定和双翻转锁定技术(SFLT 和 DFLT)的去除和结构分析攻击。KRATT 首次利用在硬件安全方面尚未得到广泛应用的强大的量化布尔公式(QBF)来寻找 SFLT 的密钥。它可以处理在无甲骨文(OL)和甲骨文引导(OG)威胁模式下的锁定电路。它修改了锁定电路，并使用突出 OL 攻击下的 OL 威胁模型作出强大的猜测。它使用结构分析技术来识别有希望的受保护输入模式，并在 OG 模型下使用 Oracle 对它们进行探索。在 ISCAS’85、 ITC’99和 HelLLO: CTF’22基准上的实验结果表明，KRATT 可以在不到一分钟的时间内使用 QBF 公式破译 SFLT，在 OL 威胁模型下可以高精度地破译 SFLT 和 DFLT 的大量关键输入，并且在 OG 威胁模型下可以很容易地找到 DFLT 的密钥。结果表明，KRATT 在解决方案质量和运行时方面优于公开可用的 OL 和 OG 攻击。"
    },
    {
        "title": "An Extensive Study on Adversarial Attack against Pre-trained Models of\n  Code",
        "url": "http://arxiv.org/abs/2311.07553v1",
        "pub_date": "2023-11-13",
        "summary": "Transformer-based pre-trained models of code (PTMC) have been widely utilized\nand have achieved state-of-the-art performance in many mission-critical\napplications. However, they can be vulnerable to adversarial attacks through\nidentifier substitution or coding style transformation, which can significantly\ndegrade accuracy and may further incur security concerns. Although several\napproaches have been proposed to generate adversarial examples for PTMC, the\neffectiveness and efficiency of such approaches, especially on different code\nintelligence tasks, has not been well understood. To bridge this gap, this\nstudy systematically analyzes five state-of-the-art adversarial attack\napproaches from three perspectives: effectiveness, efficiency, and the quality\nof generated examples. The results show that none of the five approaches\nbalances all these perspectives. Particularly, approaches with a high attack\nsuccess rate tend to be time-consuming; the adversarial code they generate\noften lack naturalness, and vice versa. To address this limitation, we explore\nthe impact of perturbing identifiers under different contexts and find that\nidentifier substitution within for and if statements is the most effective.\nBased on these findings, we propose a new approach that prioritizes different\ntypes of statements for various tasks and further utilizes beam search to\ngenerate adversarial examples. Evaluation results show that it outperforms the\nstate-of-the-art ALERT in terms of both effectiveness and efficiency while\npreserving the naturalness of the generated adversarial examples.",
        "translated": "基于变压器的预训练码模型(PTMC)已被广泛应用，并在许多关键任务应用中取得了最先进的性能。然而，通过标识符替换或编码风格转换，它们可能容易受到敌对攻击，这可能会显著降低准确性，并可能进一步引起安全问题。尽管已经提出了几种方法来生成 PTMC 的对抗性示例，但是这些方法的有效性和效率，特别是在不同的代码智能任务中，还没有得到很好的理解。为了弥补这一差距，本研究从三个方面系统地分析了五种最先进的对抗性攻击方法: 有效性、效率和生成例子的质量。结果表明，这五种方法都不能平衡所有这些观点。特别是，具有高攻击成功率的方法往往是耗时的; 它们生成的敌对代码通常缺乏自然性，反之亦然。为了解决这个限制，我们研究了在不同上下文中扰动标识符的影响，发现 for 和 if 语句中的标识符替换是最有效的。基于这些发现，我们提出了一种新的方法，为不同的任务排列不同类型的语句的优先级，并进一步利用波束搜索生成对抗性的例子。评估结果表明，该方法在有效性和效率方面优于最先进的 ALERT 方法，同时保持了生成的对抗性例子的自然性。"
    },
    {
        "title": "Tabdoor: Backdoor Vulnerabilities in Transformer-based Neural Networks\n  for Tabular Data",
        "url": "http://arxiv.org/abs/2311.07550v1",
        "pub_date": "2023-11-13",
        "summary": "Deep neural networks (DNNs) have shown great promise in various domains.\nAlongside these developments, vulnerabilities associated with DNN training,\nsuch as backdoor attacks, are a significant concern. These attacks involve the\nsubtle insertion of triggers during model training, allowing for manipulated\npredictions. More recently, DNNs for tabular data have gained increasing\nattention due to the rise of transformer models.\n  Our research presents a comprehensive analysis of backdoor attacks on tabular\ndata using DNNs, particularly focusing on transformer-based networks. Given the\ninherent complexities of tabular data, we explore the challenges of embedding\nbackdoors. Through systematic experimentation across benchmark datasets, we\nuncover that transformer-based DNNs for tabular data are highly susceptible to\nbackdoor attacks, even with minimal feature value alterations. Our results\nindicate nearly perfect attack success rates (approx100%) by introducing novel\nbackdoor attack strategies to tabular data. Furthermore, we evaluate several\ndefenses against these attacks, identifying Spectral Signatures as the most\neffective one. Our findings highlight the urgency to address such\nvulnerabilities and provide insights into potential countermeasures for\nsecuring DNN models against backdoors on tabular data.",
        "translated": "深层神经网络(DNN)已经在各个领域显示出巨大的前景。除了这些发展，与 DNN 培训相关的漏洞，如后门攻击，也是一个重要的问题。这些攻击包括在模型训练期间微妙地插入触发器，允许操纵预测。近年来，由于变压器模型的兴起，表格数据的 DNN 受到了越来越多的关注。我们的研究提出了一个使用 DNN 对表格数据的后门攻击的综合分析，特别是基于变压器的网络。鉴于表格数据固有的复杂性，我们探讨了嵌入后门的挑战。通过跨基准数据集的系统实验，我们发现表格数据的基于转换器的 DNN 非常容易受到后门攻击，即使只有最小的特征值改变。我们的研究结果表明，通过对表格数据引入新的后门攻击策略，近乎完美的攻击成功率(约100%)。此外，我们评估了几种防御这些攻击的方法，确定光谱特征是最有效的一种。我们的研究结果强调了解决这些漏洞的紧迫性，并提供了潜在的对策的见解，以保护 DNN 模型对表格数据的后门。"
    },
    {
        "title": "KnowSafe: Combined Knowledge and Data Driven Hazard Mitigation in\n  Artificial Pancreas Systems",
        "url": "http://arxiv.org/abs/2311.07460v1",
        "pub_date": "2023-11-13",
        "summary": "Significant progress has been made in anomaly detection and run-time\nmonitoring to improve the safety and security of cyber-physical systems (CPS).\nHowever, less attention has been paid to hazard mitigation. This paper proposes\na combined knowledge and data driven approach, KnowSafe, for the design of\nsafety engines that can predict and mitigate safety hazards resulting from\nsafety-critical malicious attacks or accidental faults targeting a CPS\ncontroller. We integrate domain-specific knowledge of safety constraints and\ncontext-specific mitigation actions with machine learning (ML) techniques to\nestimate system trajectories in the far and near future, infer potential\nhazards, and generate optimal corrective actions to keep the system safe.\nExperimental evaluation on two realistic closed-loop testbeds for artificial\npancreas systems (APS) and a real-world clinical trial dataset for diabetes\ntreatment demonstrates that KnowSafe outperforms the state-of-the-art by\nachieving higher accuracy in predicting system state trajectories and potential\nhazards, a low false positive rate, and no false negatives. It also maintains\nthe safe operation of the simulated APS despite faults or attacks without\nintroducing any new hazards, with a hazard mitigation success rate of 92.8%,\nwhich is at least 76% higher than solely rule-based (50.9%) and data-driven\n(52.7%) methods.",
        "translated": "在异常检测和运行时监测方面已取得重大进展，以改善网络物理系统(CPS)的安全和保安。然而，人们对于减轻灾害的关注却很少。本文提出了一种知识和数据驱动相结合的方法 KnowSafe，用于安全引擎的设计，可以预测和减轻由针对 CPS 控制器的安全关键恶意攻击或意外故障导致的安全危害。我们将特定领域的安全约束知识和上下文特定的缓解行动与机器学习(ML)技术相结合，以估计系统在远期和近期的轨迹，推断潜在的危险，并产生最佳的纠正行动，以保持系统的安全。对人工胰腺系统(APS)的两个现实闭环试验床和糖尿病治疗的现实世界临床试验数据集的实验评估表明，KnowSafe 通过在预测系统状态轨迹和潜在危险方面实现更高的准确性，低假阳性率和无假阴性而优于最先进的水平。它还保持了模拟 APS 的安全运行，尽管故障或攻击没有引入任何新的危险，减少危险的成功率为92.8% ，这是至少76% 高于单独的规则为基础(50.9%)和数据驱动(52.7%)的方法。"
    },
    {
        "title": "Mitigating Backdoors within Deep Neural Networks in Data-limited\n  Configuration",
        "url": "http://arxiv.org/abs/2311.07417v1",
        "pub_date": "2023-11-13",
        "summary": "As the capacity of deep neural networks (DNNs) increases, their need for huge\namounts of data significantly grows. A common practice is to outsource the\ntraining process or collect more data over the Internet, which introduces the\nrisks of a backdoored DNN. A backdoored DNN shows normal behavior on clean data\nwhile behaving maliciously once a trigger is injected into a sample at the test\ntime. In such cases, the defender faces multiple difficulties. First, the\navailable clean dataset may not be sufficient for fine-tuning and recovering\nthe backdoored DNN. Second, it is impossible to recover the trigger in many\nreal-world applications without information about it. In this paper, we\nformulate some characteristics of poisoned neurons. This backdoor\nsuspiciousness score can rank network neurons according to their activation\nvalues, weights, and their relationship with other neurons in the same layer.\nOur experiments indicate the proposed method decreases the chance of attacks\nbeing successful by more than 50% with a tiny clean dataset, i.e., ten clean\nsamples for the CIFAR-10 dataset, without significantly deteriorating the\nmodel's performance. Moreover, the proposed method runs three times as fast as\nbaselines.",
        "translated": "随着深度神经网络(DNN)能力的增加，它们对大量数据的需求显著增加。一个常见的做法是外包培训过程或通过互联网收集更多的数据，这引入了后门 DNN 的风险。一个后门 DNN 显示正常行为的干净数据，而恶意行为一旦触发注入样品在测试时间。在这种情况下，防御者面临多重困难。首先，可用的干净数据集可能不足以微调和恢复后门 DNN。其次，在许多现实世界的应用程序中，如果没有关于触发器的信息，就不可能恢复触发器。本文阐述了中毒神经元的一些特征。这种后门怀疑评分可以根据网络神经元的激活值、权重以及它们与同一层中其他神经元的关系对它们进行排序。我们的实验表明，提出的方法降低了攻击成功的机会超过50% 与一个微小的干净的数据集，即10个干净的样本为 CIFAR-10数据集，没有显着恶化模型的性能。此外，该方法的运行速度是基线的三倍。"
    },
    {
        "title": "Differentially Private Approximate Pattern Matching",
        "url": "http://arxiv.org/abs/2311.07415v1",
        "pub_date": "2023-11-13",
        "summary": "In this paper, we consider the $k$-approximate pattern matching problem under\ndifferential privacy, where the goal is to report or count all substrings of a\ngiven string $S$ which have a Hamming distance at most $k$ to a pattern $P$, or\ndecide whether such a substring exists. In our definition of privacy,\nindividual positions of the string $S$ are protected. To be able to answer\nqueries under differential privacy, we allow some slack on $k$, i.e. we allow\nreporting or counting substrings of $S$ with a distance at most\n$(1+\\gamma)k+\\alpha$ to $P$, for a multiplicative error $\\gamma$ and an\nadditive error $\\alpha$. We analyze which values of $\\alpha$ and $\\gamma$ are\nnecessary or sufficient to solve the $k$-approximate pattern matching problem\nwhile satisfying $\\epsilon$-differential privacy. Let $n$ denote the length of\n$S$. We give 1) an $\\epsilon$-differentially private algorithm with an additive\nerror of $O(\\epsilon^{-1}\\log n)$ and no multiplicative error for the existence\nvariant; 2) an $\\epsilon$-differentially private algorithm with an additive\nerror $O(\\epsilon^{-1}\\max(k,\\log n)\\cdot\\log n)$ for the counting variant; 3)\nan $\\epsilon$-differentially private algorithm with an additive error of\n$O(\\epsilon^{-1}\\log n)$ and multiplicative error $O(1)$ for the reporting\nvariant for a special class of patterns. The error bounds hold with high\nprobability. All of these algorithms return a witness, that is, if there exists\na substring of $S$ with distance at most $k$ to $P$, then the algorithm returns\na substring of $S$ with distance at most $(1+\\gamma)k+\\alpha$ to $P$. Further,\nwe complement these results by a lower bound, showing that any algorithm for\nthe existence variant which also returns a witness must have an additive error\nof $\\Omega(\\epsilon^{-1}\\log n)$ with constant probability.",
        "translated": "在本文中，我们考虑了模式匹配下的 $k $近似差分隐私问题，其目标是报告或计算给定字符串 $s $的所有子字符串，这些子字符串的最大汉明距离为 $k $到模式 $p $，或者决定这样的子字符串是否存在。在我们的隐私定义中，字符串 $S $的个别位置受到保护。为了能够在差分隐私下回答查询，我们允许对 $k $有一些松懈，即我们允许报告或计算最大距离为 $(1 + γ) k + alpha $到 $p $的 $s $子字符串，对于乘法错误 $γ $和加法错误 $alpha $。我们分析了 $alpha $和 $γ $的哪些值对于解决 $k $- 近似模式匹配问题是必要的或足够的，同时满足 $epsilon $- 差分隐私。让 $n $表示 $S $的长度。我们给出了1)一个加性误差为 $O (epsilon ^ {-1} log n) $且存在变量没有乘法误差的 $epsilon $- 差分私有算法; 2)一个加性误差为 $O (epsilon ^ {-1} max (k，log n)(k，log n) cdot log n) $的 $epsilon $- 差分私有算法;3)一个 $epsilon $- 差分私有算法，其加法误差为 $O (epsilon ^ {-1} log n) $，对于一类特殊的模式，报告变量的乘法误差为 $O (1) $。误差界具有很高的概率。所有这些算法都返回一个见证者，也就是说，如果存在一个距离最大为 $k $to $P $的 $S $子字符串，那么该算法返回距离最大为 $(1 + γ) k + alpha $to $P $的 $S $子字符串。进一步，我们用一个下界来补充这些结果，表明任何同样返回一个见证者的存在变量的算法必须具有一个加性误差 $Omega (epsilon ^ {-1} log n) $且概率为常数。"
    },
    {
        "title": "Transpose Attack: Stealing Datasets with Bidirectional Training",
        "url": "http://arxiv.org/abs/2311.07389v1",
        "pub_date": "2023-11-13",
        "summary": "Deep neural networks are normally executed in the forward direction. However,\nin this work, we identify a vulnerability that enables models to be trained in\nboth directions and on different tasks. Adversaries can exploit this capability\nto hide rogue models within seemingly legitimate models. In addition, in this\nwork we show that neural networks can be taught to systematically memorize and\nretrieve specific samples from datasets. Together, these findings expose a\nnovel method in which adversaries can exfiltrate datasets from protected\nlearning environments under the guise of legitimate models. We focus on the\ndata exfiltration attack and show that modern architectures can be used to\nsecretly exfiltrate tens of thousands of samples with high fidelity, high\nenough to compromise data privacy and even train new models. Moreover, to\nmitigate this threat we propose a novel approach for detecting infected models.",
        "translated": "深层神经网络通常在前向执行。然而，在这项工作中，我们确定了一个漏洞，使模型能够在两个方向和不同的任务上进行训练。对手可以利用这种能力在看似合法的模型中隐藏流氓模型。此外，在这项工作中，我们表明，可以教神经网络系统地记忆和检索特定样本从数据集。总之，这些发现揭示了一种新的方法，在这种方法中，对手可以在合法模型的伪装下，从受保护的学习环境中提取数据集。我们关注的是数据溢出攻击，并表明现代架构可以用来秘密溢出数以万计的高保真度的样本，高到足以损害数据隐私，甚至训练新的模型。此外，为了减轻这种威胁，我们提出了一种新的检测感染模型的方法。"
    },
    {
        "title": "Enhancing NAC-ABE to Support Access Control for mHealth Applications and\n  Beyond",
        "url": "http://arxiv.org/abs/2311.07299v1",
        "pub_date": "2023-11-13",
        "summary": "Name-based access control (NAC) over NDN provides fine-grained data\nconfidentiality and access control by encrypting and signing data at the time\nof data production. NAC utilizes specially crafted naming conventions to define\nand enforce access control policies. NAC-ABE, an extension to NAC, uses an\nattribute-based encryption (ABE) scheme to support access control with improved\nscalability and flexibility. However, existing NAC-ABE libraries are based on\nciphertext-policy ABE (CP-ABE), which requires knowledge of the access policy\nwhen encrypting data packets. In some applications, including mHealth, the data\naccess policy is unknown at the time of data generation, while data attributes\nand properties are known. In this paper, we present an extension to the\nexisting NDN-ABE library which can be used by mHealth and other applications to\nenforce fine-granularity access control in data sharing. We also discuss the\nchallenges we encountered during the application deployment, and remaining open\nissues together with potential solution directions.",
        "translated": "NDN 上的基于名称的访问控制(NAC)通过在数据生成时对数据进行加密和签名，提供细粒度的数据保密性和访问控制。NAC 使用特别精心制作的命名约定来定义和实施访问控制策略。NAC-ABE 是 NAC 的一个扩展，它使用基于属性的加密(ABE)方案来支持访问控制，提高了可伸缩性和灵活性。然而，现有的 NAC-ABE 库都是基于密文策略 ABE (CP-ABE)的，这就要求在对数据包进行加密时了解访问策略。在包括 mHealth 在内的一些应用程序中，数据访问策略在数据生成时是未知的，而数据属性和属性是已知的。本文对现有的 NDN-ABE 库进行了扩展，可以用于 mHealth 和其他应用程序实现数据共享中的细粒度访问控制。我们还讨论了在应用程序部署过程中遇到的挑战，以及未解决的问题和潜在的解决方案方向。"
    },
    {
        "title": "Self-testing of true multipartite entangled states",
        "url": "http://arxiv.org/abs/2311.07266v1",
        "pub_date": "2023-11-13",
        "summary": "Self-testing is a method to certify quantum states and measurements in a\ndevice-independent way. The device-independent certification of quantum\nproperties is purely based on input-output measurement statistics of the\ninvolved devices with minimal knowledge about their internal workings.\nBipartite pure entangled states can be self-tested, but, in the case of\nmultipartite pure entangled states, the answer is not so straightforward.\nNevertheless, \\v{S}upi\\'{c} et al. recently introduced a novel self-testing\nmethod for any pure entangled quantum state, which leverages network assistance\nand relies on bipartite entangled measurements. Hence, their scheme loses the\ntrue device-independent flavor of self-testing. In this regard, we provide a\nself-testing scheme for genuine multipartite pure entangle states in the true\nsense by employing a generalized Hardy-type non-local argument. It is important\nto note that our approach involves only local operations and classical\ncommunications and it does not depend on bipartite entangled measurements and\nis free from any network assistance. In addition, we provide the\ndevice-independent bound of the maximum probability of success of the\ngeneralized Hardy-type nonlocality test.",
        "translated": "自测试是一种用与器件无关的方式验证量子态和测量的方法。与器件无关的量子特性认证纯粹基于相关器件的输入输出测量统计，对其内部工作原理知之甚少。二部纯纠缠态可以自我检验，但是，在多部纯纠缠态的情况下，答案不是那么简单。然而，v { S } upi’{ c }等人最近提出了一种新的纯纠缠量子态自测试方法，该方法利用网络辅助，依赖于二分纠缠测量。因此，他们的方案失去了真正独立于设备的自测试风格。在这方面，我们利用广义 Hardy 型非局域论证，给出了真正意义上的真正多部纯纠缠态的一个自检验方案。必须指出的是，我们的方法只涉及局部操作和经典通信，它不依赖于二分纠缠测量，没有任何网络援助。此外，我们还给出了广义 Hardy 型非局部性检验成功的最大概率的与设备无关的界。"
    },
    {
        "title": "Adversarial Purification for Data-Driven Power System Event Classifiers\n  with Diffusion Models",
        "url": "http://arxiv.org/abs/2311.07110v1",
        "pub_date": "2023-11-13",
        "summary": "The global deployment of the phasor measurement units (PMUs) enables\nreal-time monitoring of the power system, which has stimulated considerable\nresearch into machine learning-based models for event detection and\nclassification. However, recent studies reveal that machine learning-based\nmethods are vulnerable to adversarial attacks, which can fool the event\nclassifiers by adding small perturbations to the raw PMU data. To mitigate the\nthreats posed by adversarial attacks, research on defense strategies is\nurgently needed. This paper proposes an effective adversarial purification\nmethod based on the diffusion model to counter adversarial attacks on the\nmachine learning-based power system event classifier. The proposed method\nincludes two steps: injecting noise into the PMU data; and utilizing a\npre-trained neural network to eliminate the added noise while simultaneously\nremoving perturbations introduced by the adversarial attacks. The proposed\nadversarial purification method significantly increases the accuracy of the\nevent classifier under adversarial attacks while satisfying the requirements of\nreal-time operations. In addition, the theoretical analysis reveals that the\nproposed diffusion model-based adversarial purification method decreases the\ndistance between the original and compromised PMU data, which reduces the\nimpacts of adversarial attacks. The empirical results on a large-scale\nreal-world PMU dataset validate the effectiveness and computational efficiency\nof the proposed adversarial purification method.",
        "translated": "相量测量单元(PMU)的全球部署实现了对电力系统的实时监测，这促进了基于机器学习的事件检测和分类模型的大量研究。然而，最近的研究表明，基于机器学习的方法是脆弱的对抗性攻击，这可以欺骗事件分类器添加小扰动的原始 PMU 数据。为了减轻敌对攻击的威胁，迫切需要对防御策略进行研究。针对基于机器学习的电力系统事件分类器受到的对手攻击，提出了一种基于扩散模型的有效对手净化方法。该方法包括两个步骤: 向 PMU 数据注入噪声; 利用预先训练好的神经网络消除附加噪声，同时消除对手攻击引起的扰动。该方法在满足实时操作要求的同时，显著提高了事件分类器在敌对攻击下的分类精度。此外，理论分析表明，提出的基于扩散模型的对抗性净化方法减少了原始 PMU 数据与被破坏 PMU 数据之间的距离，从而减少了对抗性攻击的影响。在大规模实际 PMU 数据集上的实验结果验证了所提出的对抗性净化方法的有效性和计算效率。"
    },
    {
        "title": "Effective In-vehicle Intrusion Detection via Multi-view Statistical\n  Graph Learning on CAN Messages",
        "url": "http://arxiv.org/abs/2311.07056v1",
        "pub_date": "2023-11-13",
        "summary": "As an important component of internet of vehicles (IoV), intelligent\nconnected vehicles (ICVs) have to communicate with external networks\nfrequently. In this case, the resource-constrained in-vehicle network (IVN) is\nfacing a wide variety of complex and changing external cyber-attacks,\nespecially the masquerade attack with high difficulty of detection while\nserious damaging effects that few counter measures can identify successfully.\nMoreover, only coarse-grained recognition can be achieved in current mainstream\nintrusion detection mechanisms, i.e., whether a whole data flow observation\nwindow contains attack labels rather than fine-grained recognition on every\nsingle data item within this window. In this paper, we propose StatGraph: an\nEffective Multi-view Statistical Graph Learning Intrusion Detection to\nimplement the fine-grained intrusion detection. Specifically, StatGraph\ngenerates two statistical graphs, timing correlation graph (TCG) and coupling\nrelationship graph (CRG), based on data streams. In given message observation\nwindows, edge attributes in TCGs represent temporal correlation between\ndifferent message IDs, while edge attributes in CRGs denote the neighbour\nrelationship and contextual similarity. Besides, a lightweight shallow layered\nGCN network is trained based graph property of TCGs and CRGs, which can learn\nthe universal laws of various patterns more effectively and further enhance the\nperformance of detection. To address the problem of insufficient attack types\nin previous intrusion detection, we select two real in-vehicle CAN datasets\nthat cover four new attacks never investigated before. Experimental result\nshows StatGraph improves both detection granularity and detection performance\nover state-of-the-art intrusion detection methods.",
        "translated": "作为车辆互联网(IoV)的重要组成部分，智能联网车辆(ICV)需要频繁地与外部网络进行通信。在这种情况下，资源受限的车载网络(IVN)面临着各种复杂多变的外部网络攻击，尤其是高检测难度的伪装攻击，其严重的破坏性影响很少有对策能够成功识别。此外，在目前主流的入侵检测机制中，只能实现粗粒度识别，即在整个数据流观测窗口中是否包含攻击标签，而不能对该窗口中的每个数据项进行细粒度识别。本文提出了 StatGraph: 一种有效的多视图统计图学习入侵检测方法来实现细粒度入侵检测。具体来说，StatGraph 基于数据流生成两个统计图，即时间相关图(TCG)和耦合关系图(CRG)。在给定的消息观察窗口中，TCG 中的边缘属性表示不同消息 ID 之间的时间相关性，而 CRG 中的边缘属性表示相邻关系和上下文相似性。此外，基于 TCG 和 CRG 的图特性训练了一个轻量级浅层 GCN 网络，可以更有效地学习各种模式的普遍规律，进一步提高检测性能。为了解决以往入侵检测中攻击类型不足的问题，我们选择了两个实际的车载 CAN 数据集，涵盖了以前从未研究过的四种新的攻击类型。实验结果表明，StatGraph 在检测粒度和检测性能方面都优于目前最先进的入侵检测方法。"
    },
    {
        "title": "Aid Nexus : A Blockchain Based Financial Distribution System",
        "url": "http://arxiv.org/abs/2311.08372v1",
        "pub_date": "2023-11-14",
        "summary": "Blockchain technology has emerged as a disruptive force with transformative\npotential across numerous industries, promising efficient and automated\nsolutions that can revolutionize traditional systems. By leveraging\ndecentralized ledger systems, blockchain offers enhanced security,\ntransparency, and transaction verification without the need for intermediaries.\nThe finance sector is exploring blockchain-based solutions for payments,\nremittances, lending, and investments, while healthcare adopts the technology\nfor medical record keeping, supply chain tracking, and data management.\nSimilarly, supply chain management benefits from blockchain's ability to\nenhance transparency, traceability, and accountability from raw materials to\nfinished products. Other sectors, including real estate, energy, and\ngovernment, are also investigating blockchain-based solutions to improve\nefficiency, security, and transparency. Furthermore, smart contracts within the\nblockchain enable process automation, reducing manual intervention in\ndistribution workflows. AidNeux, a consortium-based blockchain DApp, reimagines\nthe distribution of financial assistance by addressing inefficiencies and\nopaqueness. Using smart contracts ensures the security and directness of money\ntransfers. Its robust digital identity verification and real-time auditability\nreduce fraud risks and strengthen accountability, thereby presenting a\nscalable, transparent solution to problems inherent to conventional financial\naid systems.",
        "translated": "区块链技术已经成为一种颠覆性力量，具有跨越众多行业的变革潜力，有望提供能够彻底改变传统系统的高效和自动化解决方案。通过利用分散的分类账系统，区块链提供了增强的安全性、透明度和无需中介的事务验证。金融业正在探索基于区块链的支付、汇款、贷款和投资解决方案，而医疗保健采用了病历保存、供应链跟踪和数据管理技术。同样，供应链管理受益于区块链的能力，以提高透明度，可追溯性和责任，从原材料到成品。包括房地产、能源和政府在内的其他部门也在研究基于区块链的解决方案，以提高效率、安全性和透明度。此外，区块链中的智能契约支持流程自动化，减少了对分发工作流的人工干预。AidNeux 是一家基于联盟的区块链 DApp，它通过解决效率低下和不透明的问题，重新构想财政援助的分配。使用智能合同确保资金转移的安全性和直接性。其强有力的数字身份验证和实时审计减少了欺诈风险，加强了问责制，从而为传统财政援助系统固有的问题提供了一个可扩展的、透明的解决方案。"
    },
    {
        "title": "Sparsity-Preserving Differentially Private Training of Large Embedding\n  Models",
        "url": "http://arxiv.org/abs/2311.08357v1",
        "pub_date": "2023-11-14",
        "summary": "As the use of large embedding models in recommendation systems and language\napplications increases, concerns over user data privacy have also risen.\nDP-SGD, a training algorithm that combines differential privacy with stochastic\ngradient descent, has been the workhorse in protecting user privacy without\ncompromising model accuracy by much. However, applying DP-SGD naively to\nembedding models can destroy gradient sparsity, leading to reduced training\nefficiency. To address this issue, we present two new algorithms, DP-FEST and\nDP-AdaFEST, that preserve gradient sparsity during private training of large\nembedding models. Our algorithms achieve substantial reductions ($10^6 \\times$)\nin gradient size, while maintaining comparable levels of accuracy, on benchmark\nreal-world datasets.",
        "translated": "随着大型嵌入式模型在推荐系统和语言应用程序中的使用越来越多，对用户数据隐私的担忧也在增加。DP-sgd 是一种结合了差分隐私和随机梯度下降的训练算法，一直是保护用户隐私的主力军，同时又不影响模型的准确性。但是，将 DP-SGD 简单地应用于嵌入模型会破坏梯度稀疏性，从而降低训练效率。为了解决这个问题，我们提出了两个新的算法，DP-FEST 和 DP-AdaFEST，在大型嵌入模型的私有训练过程中保持梯度稀疏性。我们的算法在基准实际数据集上实现了梯度大小的大幅减少($10 ^ 6倍 $) ，同时保持了可比的精度水平。"
    },
    {
        "title": "Laccolith: Hypervisor-Based Adversary Emulation with Anti-Detection",
        "url": "http://arxiv.org/abs/2311.08274v1",
        "pub_date": "2023-11-14",
        "summary": "Advanced Persistent Threats (APTs) represent the most threatening form of\nattack nowadays since they can stay undetected for a long time. Adversary\nemulation is a proactive approach for preparing against these attacks. However,\nadversary emulation tools lack the anti-detection abilities of APTs. We\nintroduce Laccolith, a hypervisor-based solution for adversary emulation with\nanti-detection to fill this gap. We also present an experimental study to\ncompare Laccolith with MITRE CALDERA, a state-of-the-art solution for adversary\nemulation, against five popular anti-virus products. We found that CALDERA\ncannot evade detection, limiting the realism of emulated attacks, even when\ncombined with a state-of-the-art anti-detection framework. Our experiments show\nthat Laccolith can hide its activities from all the tested anti-virus products,\nthus making it suitable for realistic emulations.",
        "translated": "高级持续威胁(APT)是当今最具威胁性的攻击形式，因为它们可以长时间不被发现。对手模拟是一种预防这些攻击的主动方法。然而，对手仿真工具缺乏 APT 的抗检测能力。我们引入了基于虚拟机监控程序(hypervisor)的解决方案 Laccolith，该解决方案针对具有反检测功能的对手仿真，以填补这一空白。我们也提出了一个实验性的研究，比较漆石和 MITRE CALDERA，一个国家的最先进的解决方案的对手模拟，对五个流行的杀毒产品。我们发现 CALDERA 不能逃避检测，限制了仿真攻击的真实性，即使结合了最先进的反检测框架。我们的实验表明，漆包石可以隐藏其活动的所有测试的抗病毒产品，从而使其适合于现实的仿真。"
    },
    {
        "title": "Centralized Intermediation in a Decentralized Web3 Economy: Value\n  Accrual and Extraction",
        "url": "http://arxiv.org/abs/2311.08234v1",
        "pub_date": "2023-11-14",
        "summary": "The advent of Web3 has ushered in a new era of decentralized digital economy,\npromising a shift from centralized authority to distributed, peer-to-peer\ninteractions. However, the underlying infrastructure of this decentralized\necosystem often relies on centralized cloud providers, creating a paradoxical\nconcentration of value and power. This paper investigates the mechanics of\nvalue accrual and extraction within the Web3 ecosystem, focusing on the roles\nand revenues of centralized clouds. Through an analysis of publicly available\nmaterial, we elucidate the financial implications of cloud services in\npurportedly decentralized contexts. We further explore the individual's\nperspective of value creation and accumulation, examining the interplay between\nuser participation and centralized monetization strategies. Key findings\nindicate that while blockchain technology has the potential to significantly\nreduce infrastructure costs for financial services, the current Web3 landscape\nis marked by a substantial reliance on cloud providers for hosting,\nscalability, and performance.",
        "translated": "Web3的出现开创了一个分散式数字经济的新时代，承诺从中央集权向分布式、对等互动的转变。然而，这种分散式生态系统的底层基础设施往往依赖于集中式云供应商，从而创造出一种矛盾的价值和权力集中。本文研究了 Web3生态系统中价值增值和提取的机制，重点研究了集中式云的作用和收益。通过对公开材料的分析，我们阐明了云服务在所谓的分散环境中的财务影响。我们进一步探讨了个人的价值创造和积累的观点，考察了用户参与和集中货币化策略之间的相互作用。关键的研究结果表明，虽然区块链技术有可能显著降低金融服务的基础设施成本，但是目前的 Web3景观的特点是在托管、可伸缩性和性能方面大量依赖于云提供商。"
    },
    {
        "title": "Fundamental Limitations within the Selected Cryptographic Scenarios and\n  Supra-Quantum Theories",
        "url": "http://arxiv.org/abs/2311.08211v1",
        "pub_date": "2023-11-14",
        "summary": "The following submission constitutes a guide and an introduction to a\ncollection of articles submitted as a Ph.D. dissertation at the University of\nGda\\'nsk. In the dissertation, we study the fundamental limitations within the\nselected quantum and supra-quantum cryptographic scenarios in the form of upper\nbounds on the achievable key rates. We investigate various security paradigms,\nbipartite and multipartite settings, as well as single-shot and asymptotic\nregimes. Our studies, however, extend beyond the derivations of the upper\nbounds on the secret key rates in the mentioned scenarios. In particular, we\npropose a novel type of rerouting attack on the quantum Internet for which we\nfind a countermeasure and benchmark its efficiency. Furthermore, we propose\nseveral upper bounds on the performance of quantum (key) repeaters settings. We\nderive a lower bound on the secret key agreement capacity of a quantum network,\nwhich we tighten in an important case of a bidirectional quantum network. The\nsquashed nonlocality derived here as an upper bound on the secret key rate is a\nnovel non-faithful measure of nonlocality. Furthermore, the notion of the\nnon-signaling complete extension arising from the complete extension postulate\nas a counterpart of purification of a quantum state allows us to study\nanalogies between non-signaling and quantum key distribution scenarios.",
        "translated": "以下提交的文章是格达恩斯克大学博士论文提交的文章集的指南和介绍。在本文中，我们以可实现密钥速率的上界形式研究了所选量子和超量子密码场景的基本局限性。我们研究各种安全范式，二部和多部设置，以及单发和渐近的制度。然而，我们的研究超越了上述场景中密钥速率上限的推导。特别地，我们提出了一种新型的针对量子互联网的重路由攻击，并针对这种攻击找到了一种对策，并对其效率进行了测试。此外，我们还提出了量子(密钥)中继器设置性能的几个上限。在双向量子网络的一个重要情况下，我们得到了量子网络密钥协商容量的一个下界。这里推导出的压缩非局部性作为密钥速率的上界，是非局部性的一种新的非忠实度量。此外，非信号完全扩展的概念产生于完全扩展假设，作为量子态纯化的对应物，使我们能够研究非信号和量子密钥分配场景之间的类比。"
    },
    {
        "title": "Increasing the Efficiency of Cryptoasset Investigations by Connecting\n  the Cases",
        "url": "http://arxiv.org/abs/2311.08205v1",
        "pub_date": "2023-11-14",
        "summary": "Law enforcement agencies are confronted with a rapidly growing number of\ncryptoasset-related cases, often redundantly investigating the same cases\nwithout mutual knowledge or shared insights. In this paper, we explore the\nhypothesis that recognizing and acting upon connections between these cases can\nsignificantly streamline investigative processes. Through an analysis of a\ndataset comprising 34 cyberfraud and 1793 sextortion spam cases, we discovered\nthat 41% of the cyberfraud and 96.9% of the sextortion spam incidents can be\ninterconnected. We introduce a straightforward yet effective tool, which is\nintegrated into a broader cryptoasset forensics workflow and allows\ninvestigators to highlight and share case connections. Our research\nunequivocally demonstrates that recognizing case connections can lead to\nremarkable efficiencies, especially when extended across crime areas,\ninternational borders, and jurisdictions.",
        "translated": "执法机构面临着数量迅速增加的加密资产相关案件，往往在没有相互了解或共同见解的情况下重复调查同一案件。在本文中，我们探讨的假设，认识和行动之间的联系，这些案件可以显着简化调查过程。我们分析了34宗网上骗案和1793宗性勒索滥发讯息个案的资料，发现41% 的网上骗案和96.9% 的性勒索滥发讯息是可以互相联系的。我们引入了一个简单而有效的工具，它被集成到一个更广泛的加密资产取证工作流程中，并允许调查人员突出显示和共享案件关联。我们的研究毫不含糊地表明，识别案件之间的联系能够带来显著的效率，特别是在跨犯罪领域、跨国界和跨辖区的情况下。"
    },
    {
        "title": "SeDe: Balancing Blockchain Privacy and Regulatory Compliance by\n  Selective De-Anonymization",
        "url": "http://arxiv.org/abs/2311.08167v1",
        "pub_date": "2023-11-14",
        "summary": "Privacy is one of the essential pillars for the widespread adoption of\nblockchains, but public blockchains are transparent by nature. Modern analytics\ntechniques can easily subdue the pseudonymity feature of a blockchain user.\nSome applications have been able to provide practical privacy protections using\nprivacy-preserving cryptography techniques. However, malicious actors have\nabused them illicitly, discouraging honest actors from using privacy-preserving\napplications as \"mixing\" user interactions and funds with anonymous bad actors,\ncausing compliance and regulatory concerns.\n  In this paper, we propose a framework that balances privacy-preserving\nfeatures by establishing a regulatory and compliant framework called Selective\nDe-Anonymization (SeDe). The adoption of this framework allows\nprivacy-preserving applications on blockchains to de-anonymize illicit\ntransactions by recursive traversal of subgraphs of linked transactions. Our\ntechnique achieves this without leaving de-anonymization decisions or control\nin the hands of a single entity but distributing it among multiple entities\nwhile holding them accountable for their respective actions. To instantiate,\nour framework uses threshold encryption schemes and Zero-Knowledge Proofs\n(ZKPs).",
        "translated": "隐私是区块链广泛采用的重要支柱之一，但公共区块链本质上是透明的。现代分析技术可以很容易地克服区块链用户的假名特征。一些应用程序已经能够使用保护隐私的密码技术提供实用的隐私保护。然而，恶意参与者非法滥用它们，阻止诚实的参与者使用保护隐私的应用程序，将用户交互和资金与匿名的恶意参与者“混在一起”，从而引起合规和监管方面的担忧。在本文中，我们提出了一个平衡隐私保护特征的框架，建立了一个规范和兼容的框架称为选择性去匿名化(SeDe)。该框架的采用允许区块链上保护隐私的应用程序通过递归遍历链接交易的子图来消除非法交易的匿名性。我们的技术实现了这一点，同时不会将去匿名决策或控制权交给单个实体，而是将其分配给多个实体，同时让它们对各自的行为负责。例如，我们的框架使用阈值加密方案和零知识证明(Zero-Knowledge Proof，ZKP)。"
    },
    {
        "title": "On the Masking-Friendly Designs for Post-Quantum Cryptography",
        "url": "http://arxiv.org/abs/2311.08040v1",
        "pub_date": "2023-11-14",
        "summary": "Masking is a well-known and provably secure countermeasure against\nside-channel attacks. However, due to additional redundant computations,\nintegrating masking schemes is expensive in terms of performance. The\nperformance overhead of integrating masking countermeasures is heavily\ninfluenced by the design choices of a cryptographic algorithm and is often not\nconsidered during the design phase.\n  In this work, we deliberate on the effect of design choices on integrating\nmasking techniques into lattice-based cryptography. We select Scabbard, a suite\nof three lattice-based post-quantum key-encapsulation mechanisms (KEM), namely\nFlorete, Espada, and Sable. We provide arbitrary-order masked implementations\nof all the constituent KEMs of the Scabbard suite by exploiting their specific\ndesign elements. We show that the masked implementations of Florete, Espada,\nand Sable outperform the masked implementations of Kyber in terms of speed for\nany order masking. Masked Florete exhibits a $73\\%$, $71\\%$, and $70\\%$\nperformance improvement over masked Kyber corresponding to the first-, second-,\nand third-order. Similarly, Espada exhibits $56\\%$, $59\\%$, and $60\\%$ and\nSable exhibits $75\\%$, $74\\%$, and $73\\%$ enhanced performance for first-,\nsecond-, and third-order masking compared to Kyber respectively. Our results\nshow that the design decisions have a significant impact on the efficiency of\nintegrating masking countermeasures into lattice-based cryptography.",
        "translated": "掩蔽技术是一种众所周知的、被证明是安全的对抗侧信道攻击的对策。然而，由于额外的冗余计算，集成掩码方案在性能方面是昂贵的。集成掩蔽对抗的性能开销受密码算法设计选择的影响很大，在设计阶段往往没有考虑。在这项工作中，我们讨论了设计选择对于将掩蔽技术集成到基于格的密码学中的影响。我们选择 Scabbard，一套基于格子的后量子密钥封装机制(KEM) ，即 Florete，Espada 和 Sable。通过利用特定的设计元素，我们提供了 Scabbard 套件的所有组成 KEM 的任意顺序掩蔽实现。我们展示了 Florete、 Espada 和 Sable 的隐藏实现在任何订单隐藏的速度方面都优于 Kyber 的隐藏实现。蒙面 Florete 展示了73% ，71% ，70% 的性能改善，相对于蒙面 Kyber 相应的一阶，二阶和三阶。同样的，Espada 展示了 $56% $，$59% $和 $60% $，Sable 展示了 $75% $，$74% $和 $73% $一阶，二阶和三阶掩模增强性能相比 Kyber 分别。我们的研究结果表明，设计决策对于将掩蔽对策集成到基于格的密码体制中的效率有着重要的影响。"
    },
    {
        "title": "Linking QKD testbeds across Europe",
        "url": "http://arxiv.org/abs/2311.08038v1",
        "pub_date": "2023-11-14",
        "summary": "Quantum-key-distribution (QKD) networks are gaining importance and it has\nbecome necessary to analyze the most appropriate methods for their\nlong-distance interconnection. In this paper, four different methods of\ninterconnecting remote QKD networks are proposed. The methods are used to link\nthree different QKD testbeds in Europe, located in Berlin, Madrid, and Poznan.\nAlthough long-distance QKD links are only emulated, the used methods can serve\nas a blueprint for a secure interconnection of distant QKD networks in the\nfuture. Specifically, the presented approaches combine, in a transparent way,\ndifferent fiber and satellite physical media, as well as common standards of\nkey-delivery interfaces. The testbed interconnections are designed to increase\nthe security by utilizing multipath techniques and multiple hybridizations of\nQKD and post quantum cryptography (PQC) algorithms.",
        "translated": "量子密钥分配(QKD)网络越来越受到重视，分析其长距离互连的最佳方法成为必要。本文提出了四种不同的远程 QKD 网络互连方法。这些方法被用来连接位于柏林、马德里和波兹南的欧洲三个不同的量子密码测试床。虽然长距离的量子密钥分配链路只是模拟，但是所使用的方法可以作为未来远距离量子密钥分配网络安全互连的蓝图。具体地说，所提出的方法以透明的方式结合了不同的光纤和卫星物理介质，以及密钥交付接口的共同标准。测试平台互连的设计，旨在利用多路径技术和多重混合的量子密钥分发和后量子密码学(PQC)算法，以提高安全性。"
    },
    {
        "title": "A practical key-recovery attack on LWE-based key-encapsulation mechanism\n  schemes using Rowhammer",
        "url": "http://arxiv.org/abs/2311.08027v1",
        "pub_date": "2023-11-14",
        "summary": "Physical attacks are serious threats to cryptosystems deployed in the real\nworld. In this work, we propose a microarchitectural end-to-end attack\nmethodology on generic lattice-based post-quantum key encapsulation mechanisms\nto recover the long-term secret key. Our attack targets a critical component of\na Fujisaki-Okamoto transform that is used in the construction of almost all\nlattice-based key encapsulation mechanisms. We demonstrate our attack model on\npractical schemes such as Kyber and Saber by using Rowhammer. We show that our\nattack is highly practical and imposes little preconditions on the attacker to\nsucceed. As an additional contribution, we propose an improved version of the\nplaintext checking oracle, which is used by almost all physical attack\nstrategies on lattice-based key-encapsulation mechanisms. Our improvement\nreduces the number of queries to the plaintext checking oracle by as much as\n$39\\%$ for Saber and approximately $23\\%$ for Kyber768. This can be of\nindependent interest and can also be used to reduce the complexity of other\nattacks.",
        "translated": "物理攻击是对部署在现实世界中的密码系统的严重威胁。在这项工作中，我们提出了一个微结构端到端的攻击方法对通用的格子为基础的后量子密钥封装机制，以恢复长期的密钥。我们的攻击目标是 Fujisaki-Okamoto 变换的一个关键组件，该变换用于构建几乎所有基于格子的关键封装机制。我们使用 RowHammer 对 Kyber 和 Saber 等实际方案进行了攻击模型的演示。我们展示了我们的攻击是非常实用的，并且几乎不对攻击者施加任何成功的先决条件。作为一个额外的贡献，我们提出了一个改进版本的明文检查预言，这是几乎所有的物理攻击策略使用的格为基础的密钥封装机制。我们的改进减少了对明文检查甲骨文的查询数量，Saber 减少了39% ，Kyber768减少了大约23% 。这可以引起独立的兴趣，也可以用来降低其他攻击的复杂性。"
    },
    {
        "title": "ExpM+NF: Differentially Private Machine Learning that Surpasses DPSGD",
        "url": "http://arxiv.org/abs/2311.09200v1",
        "pub_date": "2023-11-15",
        "summary": "In this pioneering work we formulate ExpM+NF, a method for training machine\nlearning (ML) on private data with pre-specified differentially privacy\nguarantee $\\varepsilon&gt;0, \\delta=0$, by using the Exponential Mechanism (ExpM)\nand an auxiliary Normalizing Flow (NF). We articulate theoretical benefits of\nExpM+NF over Differentially Private Stochastic Gradient Descent (DPSGD), the\nstate-of-the-art (SOTA) and de facto method for differentially private ML, and\nwe empirically test ExpM+NF against DPSGD using the SOTA implementation (Opacus\nwith PRV accounting) in multiple classification tasks on the Adult Dataset\n(census data) and MIMIC-III Dataset (electronic healthcare records) using\nLogistic Regression and GRU-D, a deep learning recurrent neural network with\n~20K-100K parameters. In all experiments, ExpM+NF achieves greater than 93% of\nthe non-private training accuracy (AUC) for $\\varepsilon \\in [1\\mathrm{e}{-3},\n1]$, exhibiting greater accuracy (higher AUC) and privacy (lower $\\varepsilon$\nwith $\\delta=0$) than DPSGD. Differentially private ML generally considers\n$\\varepsilon \\in [1,10]$ to maintain reasonable accuracy; hence, ExpM+NF's\nability to provide strong accuracy for orders of magnitude better privacy\n(smaller $\\varepsilon$) substantially pushes what is currently possible in\ndifferentially private ML. Training time results are presented showing ExpM+NF\nis comparable to (slightly faster) than DPSGD. Code for these experiments will\nbe provided after review. Limitations and future directions are provided.",
        "translated": "在这项开创性的工作中，我们制定了 ExpM + NF，这是一种通过使用指数机制(ExpM)和辅助归一化流(NF)对私有数据进行机器学习(ML)训练的方法，具有预先指定的差异隐私保证 $varepsilon > 0，delta = 0 $。我们阐明了 expM + NF 相对于差异私人随机梯度下降(DPSGD)的理论益处，差异私人机器学习的最新技术(SOTA)和事实上的方法，并且我们使用 SOTA 实施(使用 PRV 计算的 Opacus)在成人数据集(人口普查数据)和 MIMIC-III 数据集(电子医疗记录)上的多重分类任务中使用 Logit模型和 GRU-D (一种具有? 20K-100K 参数的深度学习递归神经网络)对 DPSGD 进行实证测试。在所有实验中，ExpM + NF 在[1数学{ e }{-3} ，1] $中达到 $varepsilon 的非私有训练准确度(AUC)的93% 以上，显示出更高的准确性(更高的 AUC)和隐私(更低的 $varepsilon $与 $delta = 0 $)比 DPSGD。差别私有机器学习通常认为 $varepsilon 在[1,10] $中保持合理的准确性，因此，expM + NF 为数量级更好的隐私提供强大准确性的能力(更小的 $varepsilon $)大大推动了目前差别私有机器学习的可能性。训练时间的结果表明，EXM + NF 与 DPSGD 相当(稍快)。这些实验的代码将在审查后提供。提供了局限性和未来方向。"
    },
    {
        "title": "Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts",
        "url": "http://arxiv.org/abs/2311.09127v1",
        "pub_date": "2023-11-15",
        "summary": "Existing work on jailbreak Multimodal Large Language Models (MLLMs) has\nfocused primarily on adversarial examples in model inputs, with less attention\nto vulnerabilities in model APIs. To fill the research gap, we carry out the\nfollowing work: 1) We discover a system prompt leakage vulnerability in GPT-4V.\nThrough carefully designed dialogue, we successfully steal the internal system\nprompts of GPT-4V. This finding indicates potential exploitable security risks\nin MLLMs; 2)Based on the acquired system prompts, we propose a novel MLLM\njailbreaking attack method termed SASP (Self-Adversarial Attack via System\nPrompt). By employing GPT-4 as a red teaming tool against itself, we aim to\nsearch for potential jailbreak prompts leveraging stolen system prompts.\nFurthermore, in pursuit of better performance, we also add human modification\nbased on GPT-4's analysis, which further improves the attack success rate to\n98.7\\%; 3) We evaluated the effect of modifying system prompts to defend\nagainst jailbreaking attacks. Results show that appropriately designed system\nprompts can significantly reduce jailbreak success rates. Overall, our work\nprovides new insights into enhancing MLLM security, demonstrating the important\nrole of system prompts in jailbreaking, which could be leveraged to greatly\nfacilitate jailbreak success rates while also holding the potential for\ndefending against jailbreaks.",
        "translated": "现有的破解多模态大语言模型(MLLM)的工作主要集中在模型输入中的对抗性例子，较少关注模型 API 中的漏洞。为了填补这方面的研究空白，我们进行了以下工作: 1)发现了 GPT-4V 系统的一个即时泄漏漏洞。通过精心设计的对话，我们成功地窃取了 GPT-4V 的内部系统提示。2)基于获得的系统提示，本文提出了一种新的 MLLM 越狱攻击方法—— SASP (Self-Adversaliary Evolution via System Prompt)。通过使用 GPT-4作为红色团队工具来对抗自身，我们的目标是利用被盗系统提示寻找潜在的越狱提示。此外，为了追求更好的性能，我们还增加了基于 GPT-4分析的人工修改，使攻击成功率进一步提高到98.7% ; 3)我们评估了修改系统提示来防御越狱攻击的效果。结果表明，合理设计系统提示可以显著降低越狱成功率。总的来说，我们的工作为加强 MLLM 安全性提供了新的见解，展示了系统提示在越狱中的重要作用，可以利用系统提示大大提高越狱成功率，同时保持防御越狱的潜力。"
    },
    {
        "title": "Assessing the Robustness of Intelligence-Driven Reinforcement Learning",
        "url": "http://arxiv.org/abs/2311.09027v1",
        "pub_date": "2023-11-15",
        "summary": "Robustness to noise is of utmost importance in reinforcement learning\nsystems, particularly in military contexts where high stakes and uncertain\nenvironments prevail. Noise and uncertainty are inherent features of military\noperations, arising from factors such as incomplete information, adversarial\nactions, or unpredictable battlefield conditions. In RL, noise can critically\nimpact decision-making, mission success, and the safety of personnel. Reward\nmachines offer a powerful tool to express complex reward structures in RL\ntasks, enabling the design of tailored reinforcement signals that align with\nmission objectives. This paper considers the problem of the robustness of\nintelligence-driven reinforcement learning based on reward machines. The\npreliminary results presented suggest the need for further research in\nevidential reasoning and learning to harden current state-of-the-art\nreinforcement learning approaches before being mission-critical-ready.",
        "translated": "对噪声的鲁棒性在强化学习系统中至关重要，特别是在高风险和不确定环境盛行的军事环境中。噪音和不确定性是军事行动的固有特征，产生于诸如信息不完整、敌对行动或不可预测的战场条件等因素。在 RL 中，噪声可以严重影响决策、任务成功和人员安全。奖励机器提供了一个强大的工具，以表达复杂的奖励结构在 RL 任务，使设计量身定制的加强信号，以符合任务的目标。本文研究了基于奖励机器的智能驱动强化学习的鲁棒性问题。初步结果表明，需要进一步研究证据推理和学习，以巩固目前的最先进的强化学习方法，然后才能为关键任务做好准备。"
    },
    {
        "title": "Adversarial Attacks to Reward Machine-based Reinforcement Learning",
        "url": "http://arxiv.org/abs/2311.09014v1",
        "pub_date": "2023-11-15",
        "summary": "In recent years, Reward Machines (RMs) have stood out as a simple yet\neffective automata-based formalism for exposing and exploiting task structure\nin reinforcement learning settings. Despite their relevance, little to no\nattention has been directed to the study of their security implications and\nrobustness to adversarial scenarios, likely due to their recent appearance in\nthe literature. With my thesis, I aim to provide the first analysis of the\nsecurity of RM-based reinforcement learning techniques, with the hope of\nmotivating further research in the field, and I propose and evaluate a novel\nclass of attacks on RM-based techniques: blinding attacks.",
        "translated": "近年来，奖励机器(Reward Machines，RM)作为一种简单而有效的基于自动机的形式主义已经脱颖而出，用于揭示和利用强化学习环境中的任务结构。尽管它们具有相关性，但很少或根本没有注意研究它们的安全影响和对抗性情景的稳健性，这可能是由于它们最近出现在文献中。在我的论文中，我将首次分析基于 RM 的强化学习攻击技术的安全性，希望能够推动该领域的进一步研究，并提出和评估一类针对基于 RM 的技术的新型攻击: 致盲攻击。"
    },
    {
        "title": "Homomorphic Polynomial Public Key Cryptography for Quantum-secure\n  Digital Signature",
        "url": "http://arxiv.org/abs/2311.08967v1",
        "pub_date": "2023-11-15",
        "summary": "In their 2022 study, Kuang et al. introduced Multivariable Polynomial Public\nKey (MPPK) cryptography, leveraging the inversion relationship between\nmultiplication and division for quantum-safe public key systems. They extended\nMPPK into Homomorphic Polynomial Public Key (HPPK), employing homomorphic\nencryption for large hidden ring operations. Originally designed for key\nencapsulation (KEM), HPPK's security relies on homomorphic encryption of public\npolynomials. This paper expands HPPK KEM to a digital signature scheme, facing\nchallenges due to the distinct nature of verification compared to decryption.\nTo adapt HPPK KEM to digital signatures, the authors introduce an extension of\nthe Barrett reduction algorithm, transforming modular multiplications into\ndivisions in the verification equation over a prime field. The extended\nalgorithm non-linearly embeds the signature into public polynomial\ncoefficients, addressing vulnerabilities in earlier MPPK DS schemes. Security\nanalysis demonstrates exponential complexity for private key recovery and\nforged signature attacks, considering ring bit length twice that of the prime\nfield size.",
        "translated": "在他们2022年的研究中，Kuang 等人引入了多变量多项式公钥(MPPK)加密技术，利用量子安全公钥系统中乘法和除法之间的逆关系。他们将 MPPK 扩展为同态多项式公开密钥(hPPK) ，使用同态加密进行大型隐藏环操作。HPPK 最初是为密钥封装(KEM)设计的，其安全性依赖于公共多项式的同态加密。本文将 HPPK KEM 扩展为一个数字签名方案，由于其验证性质与解密性质的不同而面临挑战。为了使 HPPK KEM 适用于数字签名，作者引入了 Barrett 约简算法的一个扩展，将模乘法转化为素数域上验证方程中的除法。该扩展算法将签名非线性嵌入到公共多项式系数中，解决了早期 MPPK DS 方案中的漏洞。安全性分析表明，考虑环长是素数字域长度的两倍，私钥恢复和伪造签名攻击的复杂度呈指数级增长。"
    },
    {
        "title": "Combining Shamir &amp; Additive Secret Sharing to Improve Efficiency of SMC\n  Primitives Against Malicious Adversaries",
        "url": "http://arxiv.org/abs/2311.08934v1",
        "pub_date": "2023-11-15",
        "summary": "Secure multi-party computation provides a wide array of protocols for\nmutually distrustful parties be able to securely evaluate functions of private\ninputs. Within recent years, many such protocols have been proposed\nrepresenting a plethora of strategies to securely and efficiently handle such\ncomputation. These protocols have become increasingly efficient, but their\nperformance still is impractical in many settings. We propose new approaches to\nsome of these problems which are either more efficient than previous works\nwithin the same security models or offer better security guarantees with\ncomparable efficiency. The goals of this research are to improve efficiency and\nsecurity of secure multi-party protocols and explore the application of such\napproaches to novel threat scenarios. Some of the novel optimizations employed\nare dynamically switching domains of shared secrets, asymmetric computations,\nand advantageous functional transformations, among others. Specifically, this\nwork presents a novel combination of Shamir and Additive secret sharing to be\nused in parallel which allows for the transformation of efficient protocols\nsecure against passive adversaries to be secure against active adversaries.\nFrom this set of primitives we propose the construction of a comparison\nprotocol which can be implemented under that approach with a complexity which\nis more efficient than other recent works for common domains of interest.\nFinally, we present a system which addresses a critical security threat for the\nprotection and obfuscation of information which may be of high consequence.",
        "translated": "安全多方计算为互不信任的各方提供了广泛的协议，使他们能够安全地评估私人输入的功能。近年来，许多这样的协议被提出来，代表了安全有效地处理这种计算的大量策略。这些协议已经变得越来越有效，但是它们的性能在许多设置中仍然是不切实际的。我们提出了一些新的方法来解决其中的一些问题，这些方法要么在同样的安全模式下比以前的工作更有效率，要么以可比的效率提供更好的安全保障。本研究的目标是提高安全多方协议的效率和安全性，并探讨这些方法在新的威胁场景中的应用。采用的一些新优化是动态切换共享秘密、不对称计算和有利的函数转换等领域。具体来说，这项工作提出了一个新的沙米尔和加法秘密共享的并行使用的新组合，它允许转换有效的协议安全对被动的对手，以安全对主动的对手。从这组原语出发，我们提出了一个比较协议的构造，该协议可以在这种方法下实现，并且比其他最近的工作更加复杂，适用于感兴趣的公共领域。最后，我们提出了一个系统，解决了一个关键的安全威胁，以保护和混淆的信息，这可能是高度重要的。"
    },
    {
        "title": "Progressive Feedback-Enhanced Transformer for Image Forgery Localization",
        "url": "http://arxiv.org/abs/2311.08910v1",
        "pub_date": "2023-11-15",
        "summary": "Blind detection of the forged regions in digital images is an effective\nauthentication means to counter the malicious use of local image editing\ntechniques. Existing encoder-decoder forensic networks overlook the fact that\ndetecting complex and subtle tampered regions typically requires more feedback\ninformation. In this paper, we propose a Progressive FeedbACk-enhanced\nTransformer (ProFact) network to achieve coarse-to-fine image forgery\nlocalization. Specifically, the coarse localization map generated by an initial\nbranch network is adaptively fed back to the early transformer encoder layers\nfor enhancing the representation of positive features while suppressing\ninterference factors. The cascaded transformer network, combined with a\ncontextual spatial pyramid module, is designed to refine discriminative\nforensic features for improving the forgery localization accuracy and\nreliability. Furthermore, we present an effective strategy to automatically\ngenerate large-scale forged image samples close to real-world forensic\nscenarios, especially in realistic and coherent processing. Leveraging on such\nsamples, a progressive and cost-effective two-stage training protocol is\napplied to the ProFact network. The extensive experimental results on nine\npublic forensic datasets show that our proposed localizer greatly outperforms\nthe state-of-the-art on the generalization ability and robustness of image\nforgery localization. Code will be publicly available at\nhttps://github.com/multimediaFor/ProFact.",
        "translated": "对数字图像中伪造区域的盲检测是对抗恶意使用局部图像编辑技术的一种有效的认证手段。现有的编码器-解码器取证网络忽略了这样一个事实，即检测复杂和微妙的篡改区域通常需要更多的反馈信息。本文提出了一种渐进反馈增强变压器(ProFact)网络来实现图像的粗细伪造定位。具体来说，初始分支网络生成的粗定位图自适应地反馈到早期变压器编码层，以增强正特征的表示，同时抑制干扰因素。为了提高伪造定位的准确性和可靠性，设计了级联变压器网络，并结合上下文空间金字塔模块对鉴别特征进行了细化。此外，我们提出了一个有效的策略，以自动生成大规模的伪造图像样本接近真实世界的法医场景，特别是在现实和一致的处理。利用这些样本，一个渐进和成本效益的两阶段训练协议被应用到 ProFact 网络。在9个公共法医数据集上的大量实验结果表明，本文提出的定位方法在图像伪造定位的泛化能力和鲁棒性方面大大优于现有的定位方法。代码将在 https://github.com/multimediafor/profact 公开。"
    },
    {
        "title": "Formal Verification of Zero-Knowledge Circuits",
        "url": "http://arxiv.org/abs/2311.08858v1",
        "pub_date": "2023-11-15",
        "summary": "Zero-knowledge circuits are sets of equality constraints over arithmetic\nexpressions interpreted in a prime field; they are used to encode computations\nin cryptographic zero-knowledge proofs. We make the following contributions to\nthe problem of ensuring that a circuit correctly encodes a computation: a\nformal framework for circuit correctness; an ACL2 library for prime fields; an\nACL2 model of the existing R1CS (Rank-1 Constraint Systems) formalism to\nrepresent circuits, along with ACL2 and Axe tools to verify circuits of this\nform; a novel PFCS (Prime Field Constraint Systems) formalism to represent\nhierarchically structured circuits, along with an ACL2 model of it and ACL2\ntools to verify circuits of this form in a compositional and scalable way;\nverification of circuits, ranging from simple to complex; and discovery of bugs\nand optimizations in existing zero-knowledge systems.",
        "translated": "零知识电路是素数域上解释的算术表达式上的等式约束集，用于加密零知识证明中的计算编码。我们在确保电路正确编码计算的问题上做出了以下贡献: 电路正确性的形式框架; 素数域的 ACL2库; 现有 R1CS (Rank-1 Constraint Systems)形式主义的 ACL2模型来表示电路，以及 ACL2和 Axe 工具来验证这种形式的电路; 一种新的 PFCS (Prime Field Constraint Systems)形式主义来表示层次结构化的电路，以及它的 ACL2模型和 ACL2工具来验证这种形式的电路的合成和可扩展的方式; 电路的验证，从简单到复杂; 以及在现有的零知识系统中发现错误和优化。"
    },
    {
        "title": "Comments on \"Dynamic Consensus Committee-Based for Secure Data Sharing\n  With Authorized Multi-Receiver Searchable Encryption\"",
        "url": "http://arxiv.org/abs/2311.08813v1",
        "pub_date": "2023-11-15",
        "summary": "Recently, Yang et al. introduced an efficient searchable encryption scheme\ntitled \"Dynamic Consensus Committee-Based for Secure Data Sharing With\nAuthorized Multi-Receiver Searchable Encryption (DCC-SE),\" published in IEEE\nTransactions on Information Forensics and Security (DOI:\n10.1109/TIFS.2023.3305183). According to the authors, DCC-SE meets various\nsecurity requirements, especially the keyword trapdoor indistinguishability\nagainst chosen keyword attacks (KT-IND-CKA). In this letter, however, we reveal\na significant vulnerability of DCC-SE: any users involved in the system can\nexecute attacks against KT-IND-CKA security. This flaw potentially results in\nthe unintended disclosure of sensitive keyword information related to the\ndocuments. We present a detailed cryptanalysis on DCC-SE. In addition, to\naddress this vulnerability, we discuss the root cause and identify a flaw in\nthe security proof of DCC-SE. Subsequently, we provide a solution that\neffectively addresses this concern without significantly increasing\ncomputational overhead.",
        "translated": "最近，杨等人介绍了一种高效的可搜索加密方案，名为“基于动态共识委员会的安全数据共享授权多接收方可搜索加密(DCC-SE)”，发表在 IEEE 信息取证和安全会刊(DOI: 10.1109/TIFS.2023.3305183)。根据作者的研究，DCC-SE 可以满足各种安全需求，特别是针对所选关键字攻击的关键字陷阱门不可区分性(KT-IND-CKA)。然而，在这封信中，我们揭示了 DCC-SE 的一个重大漏洞: 任何参与该系统的用户都可以对 KT-IND-CKA 安全执行攻击。这一缺陷可能导致意外披露与文件有关的敏感关键字信息。本文对 DCC-SE 进行了详细的密码分析。此外，为了解决这一问题，我们还讨论了问题的根本原因，并找出了 DCC-SE 安全证明中的一个缺陷。随后，我们提供了一个解决方案，有效地解决这个问题，而不会显著增加计算开销。"
    },
    {
        "title": "NLP-Based Techniques for Cyber Threat Intelligence",
        "url": "http://arxiv.org/abs/2311.08807v1",
        "pub_date": "2023-11-15",
        "summary": "In the digital era, threat actors employ sophisticated techniques for which,\noften, digital traces in the form of textual data are available. Cyber Threat\nIntelligence~(CTI) is related to all the solutions inherent to data collection,\nprocessing, and analysis useful to understand a threat actor's targets and\nattack behavior. Currently, CTI is assuming an always more crucial role in\nidentifying and mitigating threats and enabling proactive defense strategies.\nIn this context, NLP, an artificial intelligence branch, has emerged as a\npowerful tool for enhancing threat intelligence capabilities. This survey paper\nprovides a comprehensive overview of NLP-based techniques applied in the\ncontext of threat intelligence. It begins by describing the foundational\ndefinitions and principles of CTI as a major tool for safeguarding digital\nassets. It then undertakes a thorough examination of NLP-based techniques for\nCTI data crawling from Web sources, CTI data analysis, Relation Extraction from\ncybersecurity data, CTI sharing and collaboration, and security threats of CTI.\nFinally, the challenges and limitations of NLP in threat intelligence are\nexhaustively examined, including data quality issues and ethical\nconsiderations. This survey draws a complete framework and serves as a valuable\nresource for security professionals and researchers seeking to understand the\nstate-of-the-art NLP-based threat intelligence techniques and their potential\nimpact on cybersecurity.",
        "translated": "在数字时代，威胁行为者使用复杂的技术，通常以文本数据的形式提供数字痕迹。网络威胁情报 ~ (CTI)涉及数据收集、处理和分析所固有的所有解决方案，这些解决方案有助于理解威胁行为者的目标和攻击行为。目前，CTI 在识别和减轻威胁以及实施积极防御战略方面发挥着更为关键的作用。在这种背景下，NLP 作为人工智能的一个分支，已经成为增强威胁情报能力的有力工具。本文综述了基于自然语言处理(NLP)技术在威胁情报中的应用。首先介绍了 CTI 作为保护数字资产的主要工具的基本定义和原则。然后对基于自然语言处理(NLP)的 CTI 数据从网络源爬取、 CTI 数据分析、网络安全数据关联提取、 CTI 共享与协作以及 CTI 面临的安全威胁等技术进行了深入研究。最后，详尽分析了自然语言处理在威胁情报中的挑战和局限性，包括数据质量问题和伦理考虑。这项调查绘制了一个完整的框架，并作为一个宝贵的资源，安全专业人员和研究人员寻求了解国家的最先进的 NLP 为基础的威胁情报技术及其对网络安全的潜在影响。"
    },
    {
        "title": "Graph models for Cybersecurity -- A Survey",
        "url": "http://arxiv.org/abs/2311.10050v1",
        "pub_date": "2023-11-16",
        "summary": "Graph models are helpful means of analyzing computer networks as well as\ncomplex system architectures for security. In this paper we evaluate the\ncurrent state of research for representing and analysing cyber-attack using\ngraph models, i.e. attack graph (AG) formalisms. We propose a taxonomy on\nattack graph formalisms, based on 70 models, which we analysed with respect to\ntheir \\textit{graph semantic}, involved agents and analysis features.\nAdditionally, we adress which formalisms allow for automatic attack graph\ngeneration from raw or processes data inputs. Our taxonomy is especially\ndesigned to help users and applied researchers identify a suitable AG model for\ntheir needs. A summary of the individual AG formalisms is provided as\nsupplementary material.",
        "translated": "图模型是分析计算机网络和复杂系统安全结构的有用手段。本文评价了利用图模型，即攻击图(AG)形式表示和分析网络攻击的研究现状。我们提出了一个攻击图形式的分类法，基于70个模型，我们分析了它们的文本{图语义} ，涉及到的代理和分析特征。此外，我们还讨论了哪些形式允许从原始数据输入或处理数据输入自动生成攻击图。我们的分类是专门设计来帮助用户和应用研究人员确定一个合适的 AG 模型，以满足他们的需要。作为补充材料，提供了各自的 AG 形式的摘要。"
    },
    {
        "title": "Software Dependability Measurement at the Age Of 36",
        "url": "http://arxiv.org/abs/2311.10039v1",
        "pub_date": "2023-11-16",
        "summary": "Thirty-six years after the first edition of IEEE standard 982.1, Measures of\nthe Software Aspects of Dependability, the third edition focuses on the\nmeasurement of in-service software dependability. This article explains how\nthis new point of view evolved and shaped the third edition's guidance for\nsoftware dependability measurement.",
        "translated": "在 IEEE 标准982.1第一版《软件可靠性测量》发布36年后，第三版的重点是在线软件可靠性测量。本文解释了这种新观点是如何发展和形成第三版的软件可靠性度量指南的。"
    },
    {
        "title": "Towards more Practical Threat Models in Artificial Intelligence Security",
        "url": "http://arxiv.org/abs/2311.09994v1",
        "pub_date": "2023-11-16",
        "summary": "Recent works have identified a gap between research and practice in\nartificial intelligence security: threats studied in academia do not always\nreflect the practical use and security risks of AI. For example, while models\nare often studied in isolation, they form part of larger ML pipelines in\npractice. Recent works also brought forward that adversarial manipulations\nintroduced by academic attacks are impractical. We take a first step towards\ndescribing the full extent of this disparity. To this end, we revisit the\nthreat models of the six most studied attacks in AI security research and match\nthem to AI usage in practice via a survey with \\textbf{271} industrial\npractitioners. On the one hand, we find that all existing threat models are\nindeed applicable. On the other hand, there are significant mismatches:\nresearch is often too generous with the attacker, assuming access to\ninformation not frequently available in real-world settings. Our paper is thus\na call for action to study more practical threat models in artificial\nintelligence security.",
        "translated": "最近的工作发现了人工智能安全研究和实践之间的差距: 学术界研究的威胁并不总是反映人工智能的实际使用和安全风险。例如，虽然模型通常是孤立地研究的，但在实践中它们构成了更大的 ML 管道的一部分。最近的研究也提出，学术攻击引入的对抗性操纵是不切实际的。我们向描述这种差距的全面程度迈出了第一步。为此，我们重新审视了人工智能安全研究中研究最多的六种攻击的威胁模型，并通过对 textbf {271}行业从业人员的调查，将它们与人工智能在实践中的使用进行了匹配。一方面，我们发现所有现有的威胁模型确实适用。另一方面，存在明显的不匹配: 研究往往对攻击者过于慷慨，假设访问的信息在现实世界的设置中并不经常可用。因此，本文呼吁采取行动，研究人工智能安全中更加实用的威胁模型。"
    },
    {
        "title": "Market Research on IIoT Standard Compliance Monitoring Providers and\n  deriving Attributes for IIoT Compliance Monitoring",
        "url": "http://arxiv.org/abs/2311.09991v1",
        "pub_date": "2023-11-16",
        "summary": "Adapting security architectures to common standards like IEC 62443 or ISO\n27000 in the Industrial Internet of Things (IIoT) involves complex processes\nand compliance reports. Automatic monitoring of compliance status would enhance\nthis process. Despite limited research, practical applications exist. This\npaper conducts a market study on providers implementing IEC 62443 in IIoT,\naiming to formulate a catalog of monitorable attributes aligned with the\nstandard. The study reveals challenges, such as a lack of formal separation in\nsecurity architectures, limiting visibility. Despite these challenges,\npractical implementations share commonalities, providing insights into viable\nmonitoring properties. The research serves as a crucial entry point into\ndeveloping a comprehensive catalog of monitorable attributes for IEC 62443\nstandards in IIoT.\n  Aligned with the IEC 62443 SR catalog of document 3-3, monitorable attributes\nare derived based on current research about IIoT security and Expert Knowledge.\nThe provided tables serve as an exemplary extract, not exhaustive, defining\nthree types of attributes based on their origin of creation.",
        "translated": "在工业物联网(IIoT)中，使安全架构适应 IEC 62443或 ISO 27000等通用标准涉及复杂的过程和遵从性报告。遵守情况的自动监测将加强这一过程。尽管研究有限，实际应用仍然存在。本文对 IloT 中实施 IEC 62443标准的供应商进行了市场研究，旨在制定符合该标准的可监测属性目录。这项研究揭示了一些挑战，比如安全架构中缺乏正式的分离，限制了可见性。尽管存在这些挑战，但是实际的实现共享共同点，为可行的监视属性提供见解。这项研究是开发 IloT 中 IEC 62443标准可监测属性综合目录的一个重要切入点。根据 IloT 安全性和专家知识的研究现状，结合 IEC 62443 SR 目录文件3-3，推导出可监测属性。所提供的表是一个范例提取，并不是详尽无遗的，它根据创建的起源定义了三种类型的属性。"
    },
    {
        "title": "Hijacking Large Language Models via Adversarial In-Context Learning",
        "url": "http://arxiv.org/abs/2311.09948v1",
        "pub_date": "2023-11-16",
        "summary": "In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs\nfor specific tasks by utilizing labeled examples as demonstrations in the\nprecondition prompts. Despite its promising performance, ICL suffers from\ninstability with the choice and arrangement of examples. Additionally, crafted\nadversarial attacks pose a notable threat to the robustness of ICL. However,\nexisting attacks are either easy to detect, rely on external models, or lack\nspecificity towards ICL. To address these issues, this work introduces a novel\ntransferable attack for ICL, aiming to hijack LLMs to generate the targeted\nresponse. The proposed LLM hijacking attack leverages a gradient-based prompt\nsearch method to learn and append imperceptible adversarial suffixes to the\nin-context demonstrations. Extensive experimental results on various tasks and\ndatasets demonstrate the effectiveness of our LLM hijacking attack, resulting\nin a distracted attention towards adversarial tokens, consequently leading to\nthe targeted unwanted outputs.",
        "translated": "在上下文中学习(In-context learning，ICL)已经成为一种强大的范式，它利用标签示例作为前置提示中的演示，为特定任务利用 LLM。尽管 ICL 具有良好的性能，但由于示例的选择和安排，它仍然存在不稳定性。此外，精心设计的对手攻击对 ICL 的健壮性构成了显著的威胁。然而，现有的攻击要么易于检测，要么依赖于外部模型，要么缺乏针对 ICL 的特异性。为了解决这些问题，本文提出了一种新的针对 ICL 的可转移攻击，目的是劫持 LLM 以产生有针对性的响应。提出的 LLM 劫持攻击利用基于梯度的提示搜索方法来学习和附加不可察觉的敌对后缀的上下文演示。在各种任务和数据集上的大量实验结果证明了我们的 LLM 劫持攻击的有效性，导致对敌对令牌的注意力分散，从而导致有针对性的不必要的输出。"
    },
    {
        "title": "The Software Genome Project: Venture to the Genomic Pathways of Open\n  Source Software and Its Applications",
        "url": "http://arxiv.org/abs/2311.09881v1",
        "pub_date": "2023-11-16",
        "summary": "With the boom in modern software development, open-source software has become\nan integral part of various industries, driving progress in computer science.\nHowever, the immense complexity and diversity of the open-source ecosystem also\npose a series of challenges, including issues of quality, security, management,\nmaintenance, compliance, and sustainability. Existing open-source governance\napproaches, while excelling in community building and collaboration, still face\nshortcomings in decentralized management, security, and maintenance. To address\nthese challenges, inspired by the Human Genome Project, we treat the software\nsource code as software DNA and propose the \\textbf{Software Genome Project},\nwhich is geared towards the secure monitoring and exploitation of open-source\nsoftware. By identifying and labeling integrated and classified code features\nat a fine-grained level, and effectively identifying safeguards for functional\nimplementations and non-functional requirements at different levels of\ngranularity, Software Genome Project builds a complete set of software genome\nmaps to help developers and managers gain a deeper understanding of software\ncomplexity and diversity. By dissecting and summarizing functional and\nundesirable genes, Software Genome Project helps facilitate targeted software\nremediation and optimization, provides valuable insight and understanding of\nthe entire software ecosystem, and supports critical development tasks such as\ntechnology selection and open source governance. This project is expected to\ndrive the evolution of software development towards more efficient, reliable,\nand sustainable software solutions.",
        "translated": "随着现代软件开发的蓬勃发展，开源软件已成为各行各业不可或缺的一部分，推动着计算机科学的进步。然而，开源生态系统的巨大复杂性和多样性也带来了一系列挑战，包括质量、安全、管理、维护、合规和可持续性等问题。现有的开源治理方法虽然在社区构建和协作方面表现出色，但在分散管理、安全和维护方面仍然存在缺陷。为了应对这些挑战，受到人类基因组计划的启发，我们将软件源代码视为软件 DNA，并提出 textbf { Software Genome Project } ，它面向对开源软件的安全监控和利用。通过在细粒度级别上识别和标记集成和分类代码特征，并在不同粒度级别上有效识别功能实现和非功能需求的保障措施，Software Genome Project 构建了一套完整的软件基因组图谱，以帮助开发人员和管理人员更深入地理解软件的复杂性和多样性。通过解剖和总结功能基因和不良基因，软件基因组项目有助于促进有针对性的软件修复和优化，提供有价值的洞察力和对整个软件生态系统的理解，并支持关键的开发任务，如技术选择和开源治理。该项目有望推动软件开发向更高效、可靠和可持续的软件解决方案发展。"
    },
    {
        "title": "Performance Trade-offs of Watermarking Large Language Models",
        "url": "http://arxiv.org/abs/2311.09816v1",
        "pub_date": "2023-11-16",
        "summary": "Amidst growing concerns of large language models (LLMs) being misused for\ngenerating misinformation or completing homework assignments, watermarking has\nemerged as an effective solution for distinguishing human-written and\nLLM-generated text. A prominent watermarking strategy is to embed a signal into\ngenerated text by upsampling a (pseudorandomly-chosen) subset of tokens at\nevery generation step. Although this signal is imperceptible to a human reader,\nit is detectable through statistical testing. However, implanting such signals\nalters the model's output distribution and can have unintended effects when\nwatermarked LLMs are used for downstream applications. In this work, we\nevaluate the performance of watermarked LLMs on a diverse suite of tasks,\nincluding text classification, textual entailment, reasoning, question\nanswering, translation, summarization, and language modeling. We find that\nwatermarking has negligible impact on the performance of tasks posed as k-class\nclassification problems in the average case. However, the accuracy can plummet\nto that of a random classifier for some scenarios (that occur with\nnon-negligible probability). Tasks that are cast as multiple-choice questions\nand short-form generation are surprisingly unaffected by watermarking. For\nlong-form generation tasks, including summarization and translation, we see a\ndrop of 15-20% in the performance due to watermarking. Our findings highlight\nthe trade-offs that users should be cognizant of when using watermarked models,\nand point to cases where future research could improve existing trade-offs.",
        "translated": "随着人们对大语言模型(LLM)被误用于生成错误信息或完成作业的担忧日益加剧，水印技术已经成为区分人写文本和 LLM 生成文本的有效解决方案。一个突出的水印策略是通过在每个生成步骤上采样一个(伪随机选择的)令牌子集，将信号嵌入到生成的文本中。尽管这种信号对于人类读者来说是无法察觉的，但是它是可以通过统计检验检测出来的。然而，植入这样的信号改变了模型的输出分布，并且当水印 LLM 用于下游应用时可能会产生意想不到的效果。在这项工作中，我们评估了水印 LLM 在一系列不同任务中的表现，包括文本分类、文字蕴涵、推理、问题回答、翻译、摘要和语言建模。我们发现，在一般情况下，水印对 k 类分类问题中任务的性能影响可以忽略不计。然而，在某些情况下(发生的概率不可忽略) ，精度可能会急剧下降到随机分类器的精度。作为多项选择题和短格式生成的任务令人惊讶地不受水印的影响。对于长形式的生成任务，包括摘要和翻译，由于水印，我们看到性能下降了15-20% 。我们的发现强调了用户在使用水印模型时应该意识到的权衡，并指出了未来的研究可以改善现有权衡的情况。"
    },
    {
        "title": "Breaking Boundaries: Balancing Performance and Robustness in Deep\n  Wireless Traffic Forecasting",
        "url": "http://arxiv.org/abs/2311.09790v1",
        "pub_date": "2023-11-16",
        "summary": "Balancing the trade-off between accuracy and robustness is a long-standing\nchallenge in time series forecasting. While most of existing robust algorithms\nhave achieved certain suboptimal performance on clean data, sustaining the same\nperformance level in the presence of data perturbations remains extremely hard.\n% In this paper, we study a wide array of perturbation scenarios and propose\nnovel defense mechanisms against adversarial attacks using real-world telecom\ndata. We compare our strategy against two existing adversarial training\nalgorithms under a range of maximal allowed perturbations, defined using\n$\\ell_{\\infty}$-norm, $\\in [0.1,0.4]$. % Our findings reveal that our hybrid\nstrategy, which is composed of a classifier to detect adversarial examples, a\ndenoiser to eliminate noise from the perturbed data samples, and a standard\nforecaster, achieves the best performance on both clean and perturbed data. %\nOur optimal model can retain up to $92.02\\%$ the performance of the original\nforecasting model in terms of Mean Squared Error (MSE) on clean data, while\nbeing more robust than the standard adversarially trained models on perturbed\ndata. Its MSE is 2.71$\\times$ and 2.51$\\times$ lower than those of comparing\nmethods on normal and perturbed data, respectively. In addition, the components\nof our models can be trained in parallel, resulting in better computational\nefficiency. % Our results indicate that we can optimally balance the trade-off\nbetween the performance and robustness of forecasting models by improving the\nclassifier and denoiser, even in the presence of sophisticated and destructive\npoisoning attacks.",
        "translated": "在准确性和稳健性之间取得平衡是时间序列预测中长期存在的挑战。虽然大多数现有的鲁棒算法在干净数据上已经达到了一定的次优性能，但是在存在数据扰动的情况下维持相同的性能水平仍然是非常困难的。% 在本文中，我们研究了一系列的扰动场景，并提出了新的防御机制对抗对手攻击使用真实世界的电信数据。我们将我们的策略与现有的两种对抗性训练算法进行比较，这两种算法在一系列最大允许扰动下使用 $ell _ { infty } $- 范数，$in [0.1,0.4] $定义。% 我们的研究结果显示，我们的混合策略，其中包括一个分类器检测对手的例子，一个去噪器消除噪声从干扰的数据样本，和一个标准的预测器，达到最佳的性能干净和干扰的数据。% 我们的最佳模型可以保留高达92.02% $原始预测模型在干净数据的均方差方面的表现，同时比标准的对抗训练模型在受干扰数据方面的表现更为稳健。它的 MSE 分别是2.71美元乘以 $和2.51美元乘以 $低于正常数据和扰动数据的比较方法。此外，我们的模型的组件可以并行训练，导致更好的计算效率。% 我们的结果表明，我们可以通过改进分类器和去噪器，在预测模型的性能和稳健性之间取得最佳平衡，即使在复杂和破坏性的中毒攻击的情况下。"
    },
    {
        "title": "Improving the Generation Quality of Watermarked Large Language Models\n  via Word Importance Scoring",
        "url": "http://arxiv.org/abs/2311.09668v1",
        "pub_date": "2023-11-16",
        "summary": "The strong general capabilities of Large Language Models (LLMs) bring\npotential ethical risks if they are unrestrictedly accessible to malicious\nusers. Token-level watermarking inserts watermarks in the generated texts by\naltering the token probability distributions with a private random number\ngenerator seeded by its prefix tokens. However, this watermarking algorithm\nalters the logits during generation, which can lead to a downgraded text\nquality if it chooses to promote tokens that are less relevant given the input.\nIn this work, we propose to improve the quality of texts generated by a\nwatermarked language model by Watermarking with Importance Scoring (WIS). At\neach generation step, we estimate the importance of the token to generate, and\nprevent it from being impacted by watermarking if it is important for the\nsemantic correctness of the output. We further propose three methods to predict\nimportance scoring, including a perturbation-based method and two model-based\nmethods. Empirical experiments show that our method can generate texts with\nbetter quality with comparable level of detection rate.",
        "translated": "大型语言模型(LLM)强大的通用功能会带来潜在的道德风险，如果恶意用户可以不受限制地访问它们的话。令牌级水印通过改变其前缀令牌所引入的私有随机数生成器的令牌概率分布，在生成的文本中插入水印。然而，这种水印算法在生成过程中改变了 logit，如果它选择提升与输入相关性较低的标记，则可能导致文本质量下降。在这项工作中，我们提出了改善质量的水印语言模型生成的水印与重要性评分(WIS)。在每一个生成步骤中，我们都会估计生成的令牌的重要性，并且如果它对输出的语义正确性很重要的话，我们会防止它受到水印的影响。我们进一步提出了三种预测重要性评分的方法，包括一种基于扰动的方法和两种基于模型的方法。实验结果表明，该方法能够生成质量较好、检测率相当的文本。"
    },
    {
        "title": "On the Exploitability of Reinforcement Learning with Human Feedback for\n  Large Language Models",
        "url": "http://arxiv.org/abs/2311.09641v1",
        "pub_date": "2023-11-16",
        "summary": "Reinforcement Learning with Human Feedback (RLHF) is a methodology designed\nto align Large Language Models (LLMs) with human preferences, playing an\nimportant role in LLMs alignment. Despite its advantages, RLHF relies on human\nannotators to rank the text, which can introduce potential security\nvulnerabilities if any adversarial annotator (i.e., attackers) manipulates the\nranking score by up-ranking any malicious text to steer the LLM adversarially.\nTo assess the red-teaming of RLHF against human preference data poisoning, we\npropose RankPoison, a poisoning attack method on candidates' selection of\npreference rank flipping to reach certain malicious behaviors (e.g., generating\nlonger sequences, which can increase the computational cost). With poisoned\ndataset generated by RankPoison, we can perform poisoning attacks on LLMs to\ngenerate longer tokens without hurting the original safety alignment\nperformance. Moreover, applying RankPoison, we also successfully implement a\nbackdoor attack where LLMs can generate longer answers under questions with the\ntrigger word. Our findings highlight critical security challenges in RLHF,\nunderscoring the necessity for more robust alignment methods for LLMs.",
        "translated": "人类反馈强化学习是一种将大型语言模型(LLM)与人类偏好结合起来的方法论，在 LLM 结合中发挥着重要作用。尽管 RLHF 有其优势，但它依赖于人工注释器对文本进行排序，如果任何对手注释器(即攻击者)通过提升任何恶意文本的排名来对抗性地操纵 LLM 来操纵排名分数，这可能会引入潜在的安全漏洞。为了评估 RLHF 针对人类偏好数据中毒的红队，我们提出了 Rank剧毒，一种针对候选人选择偏好级别翻转以达到某些恶意行为(例如，生成更长的序列，这可能会增加计算成本)的中毒攻击方法。使用 Rank剧毒生成的中毒数据集，我们可以对 LLM 执行中毒攻击以生成更长的令牌，而不会损害原始的安全对齐性能。此外，应用 Rank剧毒，我们还成功地实现了一个后门攻击，LLM 可以在带有触发词的问题下生成更长的答案。我们的研究结果突出了 RLHF 中的关键安全挑战，强调了对 LLM 使用更强大的对准方法的必要性。"
    },
    {
        "title": "An efficient quantum parallel repetition theorem and applications",
        "url": "http://arxiv.org/abs/2311.10681v1",
        "pub_date": "2023-11-17",
        "summary": "We prove a tight parallel repetition theorem for $3$-message\ncomputationally-secure quantum interactive protocols between an efficient\nchallenger and an efficient adversary. We also prove under plausible\nassumptions that the security of $4$-message computationally secure protocols\ndoes not generally decrease under parallel repetition. These mirror the\nclassical results of Bellare, Impagliazzo, and Naor [BIN97]. Finally, we prove\nthat all quantum argument systems can be generically compiled to an equivalent\n$3$-message argument system, mirroring the transformation for quantum proof\nsystems [KW00, KKMV07].\n  As immediate applications, we show how to derive hardness amplification\ntheorems for quantum bit commitment schemes (answering a question of Yan\n[Yan22]), EFI pairs (answering a question of Brakerski, Canetti, and Qian\n[BCQ23]), public-key quantum money schemes (answering a question of Aaronson\nand Christiano [AC13]), and quantum zero-knowledge argument systems. We also\nderive an XOR lemma [Yao82] for quantum predicates as a corollary.",
        "translated": "我们证明了一个严格的并行重复定理的 $3 $消息计算安全的量子交互协议之间的有效的挑战者和有效的对手。在合理的假设下，我们还证明了 $4 $消息计算安全协议的安全性在并行重复下通常不会降低。这反映了 Bellare、 Impagliazzo 和 Naor 的经典结果。最后，我们证明了所有的量子论证系统都可以通用地编译成一个等价的 $3 $- 消息论证系统，反映了量子证明系统的转换[ KW00，KKMV07]。作为即时应用，我们展示了如何推导出量子比特承诺方案的硬度放大定理(回答 Yan [ Yan22]的问题) ，电喷对(回答 Brakerski，卡内蒂和钱的问题[ bcQ23]) ，公钥量子货币方案(回答 Aaronson 和 Christiano [ AC13]的问题) ，以及量子零知识论证系统。作为推论，我们还推导出量子谓词的一个 XOR 引理[姚82]。"
    },
    {
        "title": "QKD Entity Source Authentication: Defense-in-Depth for Post Quantum\n  Cryptography",
        "url": "http://arxiv.org/abs/2311.10636v1",
        "pub_date": "2023-11-17",
        "summary": "Quantum key distribution (QKD) was conceived by Charles Bennett and Gilles\nBrassard in December of 1984. In the ensuing 39 years QKD systems have been\ndeployed around the world to provide secure encryption for terrestrial as well\nas satellite communication. In 2016 the National Institute of Standards and\nTechnology (NIST) began a program to standardize a series of quantum resistant\nalgorithms to replace our current encryption standards thereby protecting\nagainst future quantum computers breaking public key cryptography. This program\nis known as post quantum cryptography or PQC. One of the tenets of\ncybersecurity is to use an approach that simultaneously provides multiple\nprotections known as defense-in-depth. This approach seeks to avoid single\npoints of failure. The goal of this paper is to examine the suitability of a\nhybrid QKD / PQC defense-in-depth strategy. A focus of the paper will be to\nexamine the sufficiency of initial QKD hardware authentication (entity source\nauthentication) which is necessary to guard against man-in-the-middle attacks.",
        "translated": "量子密钥分配(QKD)是由 Charles Bennett 和 Gilles Brassard 于1984年12月提出的。在随后的39年中，QKD 系统已经在世界各地部署，为地面和卫星通信提供安全的加密。2016年，美国国家标准与技术研究所(NIST)开始了一项计划，以标准化一系列量子抵抗算法，取代我们目前的加密标准，从而防止未来量子计算机破坏公钥加密。这个程序被称为邮政量子密码学或 PQC。网络安全的原则之一是使用一种同时提供多重保护的方法，即深度防御。这种方法试图避免单点故障。本文的目的是检验 QKD/PQC 混合深度防御策略的适用性。本文的重点是研究初始 QKD 硬件认证(实体源认证)的充分性，这是防范中间人攻击所必需的。"
    },
    {
        "title": "A Tale of Unrealized Hope: Hardware Performance Counter Against Cache\n  Attacks",
        "url": "http://arxiv.org/abs/2311.10542v1",
        "pub_date": "2023-11-17",
        "summary": "This paper investigates an emerging cache side channel attack defense\napproach involving the use of hardware performance counters (HPCs). These\ncounters monitor microarchitectural events and analyze statistical deviations\nto differentiate between malicious and benign software. With numerous proposals\nand promising reported results, we seek to investigate whether published\nHPC-based detection methods are evaluated in a proper setting and under the\nright assumptions, such that their quality can be ensured for real-word\ndeployment against cache side-channel attacks. To achieve this goal, this paper\npresents a comprehensive evaluation and scrutiny of existing literature on the\nsubject matter in a form of a survey, accompanied by experimental evidences to\nsupport our evaluation.",
        "translated": "本文研究了一种新兴的缓存端信道攻击防御方法，该方法使用了硬件性能计数器(HPC)。这些计数器监视微架构事件并分析统计偏差，以区分恶意软件和良性软件。通过大量的建议和有希望的报告结果，我们试图调查是否在适当的设置和正确的假设下评估已发布的基于 HPC 的检测方法，以确保它们的质量可以针对缓存侧信道攻击的实际字部署。为了实现这一目标，本文以调查的形式对现有文献进行了全面的评价和审查，并附有实验证据来支持我们的评价。"
    },
    {
        "title": "A Large-Scale Study on the Prevalence and Usage of TEE-based Features on\n  Android",
        "url": "http://arxiv.org/abs/2311.10511v1",
        "pub_date": "2023-11-17",
        "summary": "In the realm of mobile security, where OS-based protections have proven\ninsufficient against robust attackers, Trusted Execution Environments (TEEs)\nhave emerged as a hardware-based security technology. Despite the industry's\npersistence in advancing TEE technology, the impact on end users and developers\nremains largely unexplored. This study addresses this gap by conducting a\nlarge-scale analysis of TEE utilization in Android applications, focusing on\nthe key areas of cryptography, digital rights management, biometric\nauthentication, and secure dialogs.\n  To facilitate our extensive analysis, we introduce Mobsec Analytika, a\nframework tailored for large-scale app examinations, which we make available to\nthe research community. Through the analysis of 170,550 popular Android apps,\nour analysis illuminates the implementation of TEE-related features and their\ncontextual usage.\n  Our findings reveal that TEE features are predominantly utilized indirectly\nthrough third-party libraries, with only 6.7% of apps directly invoking the\nAPIs. Moreover, the study reveals the underutilization of the recent TEE-based\nUI feature Protected Confirmation.",
        "translated": "在移动安全领域，基于操作系统的保护已被证明不足以抵御健壮的攻击者，可信执行环境(Trusted Execution Environment，TEE)已经成为一种基于硬件的安全技术。尽管业界坚持推进 TEE 技术，但对终端用户和开发人员的影响仍然很大程度上未被探索。这项研究通过对安卓应用程序中的 TEE 使用情况进行大规模分析，重点关注密码学、数字版权管理、生物计量学和安全对话等关键领域，从而弥补了这一差距。为了便于我们进行广泛的分析，我们引入了 Mobsec Analytika，这是一个为大规模应用程序考试量身定制的框架，我们将其提供给研究社区。通过对170,550个流行的 Android 应用程序的分析，我们的分析说明了 TEE 相关功能的实现及其上下文使用。我们的研究结果表明，TEE 特性主要通过第三方库间接利用，只有6.7% 的应用程序直接调用 API。此外，该研究还揭示了最近基于 TEE 的 UI 特性 ProtectedConsure 未得到充分利用。"
    },
    {
        "title": "From Principle to Practice: Vertical Data Minimization for Machine\n  Learning",
        "url": "http://arxiv.org/abs/2311.10500v1",
        "pub_date": "2023-11-17",
        "summary": "Aiming to train and deploy predictive models, organizations collect large\namounts of detailed client data, risking the exposure of private information in\nthe event of a breach. To mitigate this, policymakers increasingly demand\ncompliance with the data minimization (DM) principle, restricting data\ncollection to only that data which is relevant and necessary for the task.\nDespite regulatory pressure, the problem of deploying machine learning models\nthat obey DM has so far received little attention. In this work, we address\nthis challenge in a comprehensive manner. We propose a novel vertical DM (vDM)\nworkflow based on data generalization, which by design ensures that no\nfull-resolution client data is collected during training and deployment of\nmodels, benefiting client privacy by reducing the attack surface in case of a\nbreach. We formalize and study the corresponding problem of finding\ngeneralizations that both maximize data utility and minimize empirical privacy\nrisk, which we quantify by introducing a diverse set of policy-aligned\nadversarial scenarios. Finally, we propose a range of baseline vDM algorithms,\nas well as Privacy-aware Tree (PAT), an especially effective vDM algorithm that\noutperforms all baselines across several settings. We plan to release our code\nas a publicly available library, helping advance the standardization of DM for\nmachine learning. Overall, we believe our work can help lay the foundation for\nfurther exploration and adoption of DM principles in real-world applications.",
        "translated": "为了训练和部署预测模型，组织收集了大量详细的客户数据，在发生违规事件时有暴露私人信息的风险。为了缓解这种情况，决策者越来越多地要求遵守数据最小化(DM)原则，将数据收集限制在与任务相关和必要的数据上。尽管有监管压力，但是部署服从 DM 的机器学习模型的问题到目前为止还没有得到足够的重视。在这项工作中，我们全面应对这一挑战。提出了一种基于数据泛化的垂直 DM (vDM)工作流，该工作流通过设计保证在模型的训练和部署过程中不收集全分辨率的客户端数据，在发生攻击时减少攻击面，从而有利于客户端的隐私保护。我们正式化和研究相应的问题，找到最大限度地提高数据效用和最小化经验隐私风险的一般化，我们量化通过引入一组不同的政策一致的对抗情景。最后，我们提出了一系列基线 vDM 算法，以及隐私感知树(Privacy-aware Tree，PAT) ，这是一种特别有效的 vDM 算法，在多个设置中都优于所有基线。我们计划将我们的代码作为一个公开可用的库发布，帮助推进机器学习的 DM 标准化。总的来说，我们相信我们的工作可以为进一步探索和在实际应用中采用 DM 原则奠定基础。"
    },
    {
        "title": "A Novel VAPT Algorithm: Enhancing Web Application Security Trough OWASP\n  top 10 Optimization",
        "url": "http://arxiv.org/abs/2311.10450v1",
        "pub_date": "2023-11-17",
        "summary": "This research study is built upon cybersecurity audits and investigates the\noptimization of an Open Web Application Security Project (OWASP) Top 10\nalgorithm for Web Applications (WA) security audits using Vulnerability\nAssessment and Penetration Testing (VAPT) processes. The study places\nparticular emphasis on enhancing the VAPT process by optimizing the OWASP\nalgorithm. To achieve this, the research utilizes desk documents to gain\nknowledge of WA cybersecurity audits and their associated tools. It also delves\ninto archives to explore VAPT processes and identify techniques, methods, and\ntools for VAPT automation. Furthermore, the research proposes a prototype\noptimization that streamlines the two steps of VAPT using the OWASP Top 10\nalgorithm through an experimental procedure. The results are obtained within a\nvirtual environment, which employs black box testing methods as the primary\nmeans of data acquisition and analysis. In this experimental setting, the OWASP\nalgorithm demonstrates an impressive level of precision, achieving a precision\nrate exceeding 90%. It effectively covers all researched vulnerabilities, thus\njustifying its optimization. This research contributes significantly to the\nenhancement of the OWASP algorithm and benefits the offensive security\ncommunity. It plays a crucial role in ensuring compliance processes for\nprofessionals and analysts in the security and software development fields.",
        "translated": "本研究建立在网络安全审计的基础上，研究开放网络应用安全项目(OWASP)的前10名算法的最优化网络应用(WA)安全审计使用漏洞评估和渗透测试(VAPT)过程。研究重点是通过优化 OWASP 算法来提高 VAPT 过程。为了实现这一目标，本研究利用案头文件来了解 WA 网络安全审计及其相关工具。它还深入研究档案，以探索 VAPT 过程，并确定 VAPT 自动化的技术、方法和工具。此外，研究提出了一个原型优化，简化了 VAPT 的两个步骤，使用 OWASP Top 10算法通过一个实验过程。这些结果是在一个虚拟环境中获得的，该虚拟环境使用黑盒测试方法作为数据采集和分析的主要手段。在这个实验环境中，OWASP 算法展示了令人印象深刻的精度水平，达到了超过90% 的精度率。它有效地覆盖了所有研究的漏洞，从而证明了其优化的合理性。本研究对 OWASP 算法的改进有重要意义，有利于进攻安全社区。它在确保安全和软件开发领域的专业人员和分析人员遵守流程方面发挥着至关重要的作用。"
    },
    {
        "title": "DeepClean: Machine Unlearning on the Cheap by Resetting Privacy\n  Sensitive Weights using the Fisher Diagonal",
        "url": "http://arxiv.org/abs/2311.10448v1",
        "pub_date": "2023-11-17",
        "summary": "Machine learning models trained on sensitive or private data can\ninadvertently memorize and leak that information. Machine unlearning seeks to\nretroactively remove such details from model weights to protect privacy. We\ncontribute a lightweight unlearning algorithm that leverages the Fisher\nInformation Matrix (FIM) for selective forgetting. Prior work in this area\nrequires full retraining or large matrix inversions, which are computationally\nexpensive. Our key insight is that the diagonal elements of the FIM, which\nmeasure the sensitivity of log-likelihood to changes in weights, contain\nsufficient information for effective forgetting. Specifically, we compute the\nFIM diagonal over two subsets -- the data to retain and forget -- for all\ntrainable weights. This diagonal representation approximates the complete FIM\nwhile dramatically reducing computation. We then use it to selectively update\nweights to maximize forgetting of the sensitive subset while minimizing impact\non the retained subset. Experiments show that our algorithm can successfully\nforget any randomly selected subsets of training data across neural network\narchitectures. By leveraging the FIM diagonal, our approach provides an\ninterpretable, lightweight, and efficient solution for machine unlearning with\npractical privacy benefits.",
        "translated": "接受敏感或私有数据培训的机器学习模型可能会在不经意间记忆并泄露这些信息。机器去除试图追溯性地从模型权重中去除这些细节，以保护隐私。我们贡献了一个轻量级的忘却算法，它利用费雪资讯矩阵(fIM)来选择性遗忘。先前在这个领域的工作需要完全再培训或大型矩阵反演，这是计算昂贵。我们的主要见解是，FIM 的对角线元素，衡量对数似然对权重变化的敏感性，包含有效遗忘的足够信息。具体来说，我们计算两个子集上的 FIM 对角线——要保留和遗忘的数据——对于所有可训练的权重。这种对角线表示近似于完整的 FIM，同时大大减少了计算量。然后，我们使用它选择性地更新权重，以最大限度地遗忘敏感子集，同时最小化对保留子集的影响。实验结果表明，该算法能够成功地忘记神经网络结构中随机选择的训练数据子集。通过利用 FIM 对角线，我们的方法提供了一个可解释的、轻量级的、高效的机器忘却解决方案，并带来了实际的隐私好处。"
    },
    {
        "title": "UA-Radar: Exploring the Impact of User Agents on the Web",
        "url": "http://arxiv.org/abs/2311.10420v1",
        "pub_date": "2023-11-17",
        "summary": "In the early days of the web, giving the same web page to different browsers\ncould provide very different results. As the rendering engine behind each\nbrowser would differ, some elements of a page could break or be positioned in\nthe wrong location. At that time, the User Agent (UA) string was introduced for\ncontent negotiation. By knowing the browser used to connect to the server, a\ndeveloper could provide a web page that was tailored for that specific browser\nto remove any usability problems. Over the past three decades, the UA string\nremained exposed by browsers, but its current usefulness is being debated.\nBrowsers now adopt the exact same standards and use the same languages to\ndisplay the same content to users, bringing the question if the content of the\nUA string is still relevant today, or if it is a relic of the past. Moreover,\nthe diversity of means to browse the web has become so large that the UA string\nis one of the top contributors to tracking users in the field of browser\nfingerprinting, bringing a sense of urgency to deprecate it. In this paper, our\ngoal is to understand the impact of the UA on the web and if this legacy string\nis still actively used to adapt the content served to users. We introduce\nUA-Radar, a web page similarity measurement tool that compares in-depth two web\npages from the code to their actual rendering, and highlights the similarities\nit finds. We crawled 270, 048 web pages from 11, 252 domains using 3 different\nbrowsers and 2 different UA strings to observe that 100% of the web pages were\nsimilar before any JavaScript was executed, demonstrating the absence of\ndifferential serving. Our experiments also show that only a very small number\nof websites are affected by the lack of UA information, which can be fixed in\nmost cases by updating code to become browser-agnostic. Our study brings some\nproof that it may be time to turn the page on the UA string and retire it from\ncurrent web browsers.",
        "translated": "在网络的早期，给不同的浏览器提供相同的网页可以提供非常不同的结果。由于每个浏览器背后的呈现引擎不同，页面的某些元素可能会中断或位于错误的位置。当时，为内容协商引入了 User Agent (UA)字符串。通过了解用于连接到服务器的浏览器，开发人员可以提供一个专门为该特定浏览器定制的网页，以消除任何可用性问题。在过去的三十年中，浏览器仍然暴露了 UA 字符串，但是它当前的用途正在被讨论。如今，浏览器采用完全相同的标准，使用相同的语言向用户显示相同的内容，这就带来了一个问题: UA 字符串的内容是否仍然适用于今天，或者它是否已成为过去的遗物。此外，浏览网页的方式变得如此多样化，以至于 UA 字符串成为跟踪浏览器指纹领域用户的主要贡献者之一，这带来了一种摒弃它的紧迫感。在本文中，我们的目标是了解 UA 对 Web 的影响，以及这个遗留字符串是否仍然被主动地用于适应向用户提供的内容。我们介绍 UA-Radar，这是一个网页相似性度量工具，它将两个网页从代码到实际渲染进行深入的比较，并强调它所发现的相似性。我们使用3个不同的浏览器和2个不同的 UA 字符串从11,252个域中抓取了270,048个网页，观察到在执行任何 JavaScript 之前，100% 的网页是相似的，这表明没有差异服务。我们的实验还表明，只有极少数的网站会受到 UA 信息缺乏的影响，这在大多数情况下可以通过更新代码变得与浏览器无关来解决。我们的研究带来了一些证据表明，现在可能是时候把页面上的 UA 字符串和退休它从目前的网络浏览器。"
    },
    {
        "title": "A Survey on Off-chain Networks: Frameworks, Technologies, Solutions and\n  Challenges",
        "url": "http://arxiv.org/abs/2311.10298v1",
        "pub_date": "2023-11-17",
        "summary": "Blockchain has received increasing attention in academia and industry.\nHowever, the increasing transaction volumes and limited on-chain storage\nunderscore scalability as a key challenge hindering the widespread adoption of\nblockchain. Fortunately, off-chain networks that enable transactions outside\nthe blockchain show promising potential to mitigate the scalability challenge.\nOff-chain solutions that address blockchain scalability hurdles, such as\npayment channel networks, facilitate secure and fast off-chain transactions,\nthus relieving the main chain's strain. In this article, we provide a\ncomprehensive review of key technologies, solutions, and challenges of\noff-chain networks. First, we introduce the background of off-chain networks\nencompassing design motivation, framework, overview, and application scenarios.\nWe then review the key issues and technologies associated with off-chain\nnetworks. Subsequently, we summarize the mainstream solutions for the\ncorresponding key issues. Finally, we discuss some research challenges and open\nissues in this area.",
        "translated": "区块链已经越来越受到学术界和工业界的重视。然而，不断增长的事务量和有限的链上存储突出了可伸缩性，这是阻碍区块链广泛采用的一个关键挑战。幸运的是，支持区块链之外事务的脱链网络显示出缓解可伸缩性挑战的潜力。解决区块链可扩展性障碍的非链解决方案，如支付渠道网络，促进安全和快速的非链交易，从而缓解主链的压力。在本文中，我们提供了一个关键技术，解决方案和脱链网络的挑战综述。首先，我们介绍了脱链网络的背景，包括设计动机、框架、概述和应用场景。然后，我们回顾了与脱链网络相关的关键问题和技术。随后，针对相应的关键问题总结了主流解决方案。最后，我们讨论了这一领域的一些研究挑战和尚未解决的问题。"
    },
    {
        "title": "Secure Instruction and Data-Level Information Flow Tracking Model for\n  RISC-V",
        "url": "http://arxiv.org/abs/2311.10283v1",
        "pub_date": "2023-11-17",
        "summary": "Rising device use and third-party IP integration in semiconductors raise\nsecurity concerns. Unauthorized access, fault injection, and privacy invasion\nare potential threats from untrusted actors. Different security techniques have\nbeen proposed to provide resilience to secure devices from potential\nvulnerabilities; however, no one technique can be applied as an overarching\nsolution. We propose an integrated Information Flow Tracking (IFT) technique to\nenable runtime security to protect system integrity by tracking the flow of\ndata from untrusted communication channels. Existing hardware-based IFT schemes\nare either fine-, which are resource-intensive, or coarse-grained models, which\nhave minimal precision logic, providing either control flow or data-flow\nintegrity. No current security model provides multi-granularity due to the\ndifficulty in balancing both the flexibility and hardware overheads at the same\ntime. This study proposes a multi-level granularity IFT model that integrates a\nhardware-based IFT technique with a gate-level-based IFT (GLIFT) technique,\nalong with flexibility, for better precision and assessments. Translation from\nthe instruction level to the data level is based on module instantiation with\nsecurity-critical data for accurate information flow behaviors without any\nfalse conservative flows. A simulation-based IFT model is demonstrated, which\ntranslates the architecture-specific extensions into a compiler-specific\nsimulation model with toolchain extensions for Reduced Instruction Set\nArchitecture (RISC-V) to verify the security extensions. This approach provides\nbetter precision logic by enhancing the tagged mechanism with 1-bit tags and\nimplementing an optimized shadow logic that eliminates the area overhead by\ntracking the data for only security-critical modules.",
        "translated": "半导体设备使用的增加和第三方 IP 集成引起了安全方面的担忧。未经授权的访问、错误注入和隐私侵犯是来自不可信参与者的潜在威胁。已经提出了不同的安全技术来提供对潜在漏洞的安全设备的弹性; 然而，没有一种技术可以作为总体解决方案应用。提出了一种集成的信息流跟踪(IFT)技术，通过跟踪来自不可信通信信道的数据流，使得运行时安全能够保护系统的完整性。现有的基于硬件的 IFT 方案要么是资源密集型的精细模型，要么是具有最小精度逻辑的粗粒度模型，提供控制流或数据流完整性。当前的安全模型由于难以同时兼顾灵活性和硬件开销，没有提供多粒度的安全模型。该研究提出了一种多级粒度 IFT 模型，该模型将基于硬件的 IFT 技术与基于门级的 IFT (GLIFT)技术结合在一起，同时具有灵活性，可以获得更好的精度和评估。从指令级到数据级的转换是基于安全关键数据的模块实例化，以便在没有任何错误保守流的情况下实现准确的信息流行为。基于模拟的 IFT 模型被演示，它将特定于体系结构的扩展转换为特定于编译器的模拟模型，并使用降低指令集架构的工具链扩展(RISC-V)来验证安全性扩展。该方法通过使用1位标记增强标记机制，并实现优化的影子逻辑，通过仅跟踪安全关键模块的数据来消除区域开销，从而提供更好的精度逻辑。"
    },
    {
        "title": "Public-key pseudoentanglement and the hardness of learning ground state\n  entanglement structure",
        "url": "http://arxiv.org/abs/2311.12017v1",
        "pub_date": "2023-11-20",
        "summary": "Given a local Hamiltonian, how difficult is it to determine the entanglement\nstructure of its ground state? We show that this problem is computationally\nintractable even if one is only trying to decide if the ground state is\nvolume-law vs near area-law entangled. We prove this by constructing strong\nforms of pseudoentanglement in a public-key setting, where the circuits used to\nprepare the states are public knowledge. In particular, we construct two\nfamilies of quantum circuits which produce volume-law vs near area-law\nentangled states, but nonetheless the classical descriptions of the circuits\nare indistinguishable under the Learning with Errors (LWE) assumption.\nIndistinguishability of the circuits then allows us to translate our\nconstruction to Hamiltonians. Our work opens new directions in Hamiltonian\ncomplexity, for example whether it is difficult to learn certain phases of\nmatter.",
        "translated": "给定一个局域哈密顿量，确定其基态的纠缠结构有多难？我们表明，这个问题是计算难以处理，即使人们只是试图确定基态是体积定律与近面积定律纠缠。我们通过在公钥设置中构造强形式的伪纠缠来证明这一点，其中用于准备状态的电路是公共知识。特别地，我们构造了两类量子电路，它们产生体积定律和近面积定律的纠缠态，但是在误差学习(LWE)假设下，电路的经典描述是难以区分的。电路的不可区分性使我们能够将构造转化为哈密顿量。我们的工作开辟了哈密顿复杂性的新方向，例如是否很难学习物质的某些阶段。"
    },
    {
        "title": "A Novel Secure NFC-based Approach for BMS Monitoring and Diagnostic\n  Readout",
        "url": "http://arxiv.org/abs/2311.12006v1",
        "pub_date": "2023-11-20",
        "summary": "In modern systems that rely on the use of Battery Management Systems (BMS),\nlongevity and the re-use of battery packs have always been important topics of\ndiscussion. These battery packs would be stored inside warehouses where they\nwould need to be properly monitored and configured before their re-integration\ninto the new systems. Traditional use of wired connections can be very\ncumbersome, and sometimes even impossible, due to the outer layers and\npackaging. To circumvent these issues, we propose an extension to the\nconventional BMS design that incorporates the use of Near Field Communication\n(NFC) for the purpose of wireless battery pack status readout. Additionally, to\nensure that these packs are only managed by authenticated devices and that the\ndata that is communicated with is protected against outside eavesdropping and\ntampering, we present a solution in the form of a lightweight security layer on\ntop of the NFC protocol. To show the feasibility of our design, an accompanying\nprototype has been implemented and evaluated.",
        "translated": "在依赖于电池管理系统(BMS)的现代系统中，电池组的寿命和重复使用一直是讨论的重要话题。这些电池组将储存在仓库中，在重新集成到新系统之前，需要对它们进行适当的监测和配置。传统的有线连接的使用可能非常麻烦，有时甚至不可能，由于外层和包装。为了规避这些问题，我们提出了一个扩展的传统 BMS 设计，包括使用近场通信(NFC)的目的是无线电池组状态读出。此外，为了确保这些包只由经过身份验证的设备管理，并且与之通信的数据受到保护，以防止外部窃听和篡改，我们在 NFC 协议的基础上提出了一种轻量级安全层形式的解决方案。为了说明我们的设计的可行性，一个附带的原型已经实现和评估。"
    },
    {
        "title": "BrainWash: A Poisoning Attack to Forget in Continual Learning",
        "url": "http://arxiv.org/abs/2311.11995v1",
        "pub_date": "2023-11-20",
        "summary": "Continual learning has gained substantial attention within the deep learning\ncommunity, offering promising solutions to the challenging problem of\nsequential learning. Yet, a largely unexplored facet of this paradigm is its\nsusceptibility to adversarial attacks, especially with the aim of inducing\nforgetting. In this paper, we introduce \"BrainWash,\" a novel data poisoning\nmethod tailored to impose forgetting on a continual learner. By adding the\nBrainWash noise to a variety of baselines, we demonstrate how a trained\ncontinual learner can be induced to forget its previously learned tasks\ncatastrophically, even when using these continual learning baselines. An\nimportant feature of our approach is that the attacker requires no access to\nprevious tasks' data and is armed merely with the model's current parameters\nand the data belonging to the most recent task. Our extensive experiments\nhighlight the efficacy of BrainWash, showcasing degradation in performance\nacross various regularization-based continual learning methods.",
        "translated": "连续学习已经引起了深度学习领域的广泛关注，为连续学习这一具有挑战性的问题提供了有希望的解决方案。然而，这个范式的一个很大程度上未被探索的方面是它对敌对攻击的易感性，特别是为了诱导遗忘。在这篇文章中，我们介绍了“洗脑”，一种新的数据中毒方法，专门强加遗忘的持续学习者。通过将 BrainWash 噪声添加到各种基线中，我们展示了一个训练有素的持续学习者是如何被诱导以灾难性的方式忘记它以前学过的任务的，即使是在使用这些持续学习基线的时候。我们的方法的一个重要特性是，攻击者不需要访问以前任务的数据，仅仅使用模型的当前参数和属于最近任务的数据。我们广泛的实验突出了 BrainWash 的功效，展示了各种基于规则的持续学习方法的性能下降。"
    },
    {
        "title": "A Modular Approach to Unclonable Cryptography",
        "url": "http://arxiv.org/abs/2311.11890v1",
        "pub_date": "2023-11-20",
        "summary": "We explore a new pathway to designing unclonable cryptographic primitives. We\npropose a new notion called unclonable puncturable obfuscation (UPO) and study\nits implications for unclonable cryptography. Using UPO, we present modular\n(and arguably, simple) constructions of many primitives in unclonable\ncryptography, including public-key quantum money, quantum copy-protection for\nmany classes of functionalities, unclonable encryption, and single-decryption\nencryption. Notably, we obtain the following new results assuming the existence\nof UPO: We show that any cryptographic functionality can be copy-protected as\nlong as this functionality satisfies a notion of security, which we term as\npuncturable security. Prior feasibility results focused on copy-protecting\nspecific cryptographic functionalities. We show that copy-protection exists for\nany class of evasive functions as long as the associated distribution satisfies\na preimage-sampleability condition. Prior works demonstrated copy-protection\nfor point functions, which follows as a special case of our result. We show\nthat unclonable encryption exists in the plain model. Prior works demonstrated\nfeasibility results in the quantum random oracle model. We put forward a\ncandidate construction of UPO and prove two notions of security, each based on\nthe existence of (post-quantum) sub-exponentially secure indistinguishability\nobfuscation and one-way functions, the quantum hardness of learning with\nerrors, and a new conjecture called simultaneous inner product conjecture.",
        "translated": "我们探索了一条设计不可克隆密码原语的新途径。提出了不可克隆可穿透混淆(UPO)的概念，并研究了它对不可克隆密码学的影响。使用 UPO，我们提出了不可克隆密码学中许多原语的模块化(可以说是简单的)结构，包括公钥量子货币，多种功能的量子复制保护，不可克隆加密和单解密加密。值得注意的是，我们假设 UPO 的存在，得到了以下新的结果: 我们证明了任何加密功能都可以被拷贝保护，只要该功能满足安全性的概念，我们称之为可穿透安全性。先前的可行性结果侧重于特定的复制保护加密功能。我们证明了只要关联分布满足预像可采样性条件，任何类型的规避函数都存在拷贝保护。先前的工作演示了点函数的拷贝保护，这是我们的结果的一个特例。我们证明了普通模型中存在不可克隆加密。前人的工作证明了量子随机预言模型的可行性结果。我们提出了 UPO 的一个候选构造，并证明了两个安全性概念，每个概念都基于(后量子)亚指数安全不可区分模糊和单向函数的存在性，带误差学习的量子硬度，以及一个称为同时内积猜想的新猜想。"
    },
    {
        "title": "Look into the Mirror: Evolving Self-Dual Bent Boolean Functions",
        "url": "http://arxiv.org/abs/2311.11884v1",
        "pub_date": "2023-11-20",
        "summary": "Bent Boolean functions are important objects in cryptography and coding\ntheory, and there are several general approaches for constructing such\nfunctions. Metaheuristics proved to be a strong choice as they can provide many\nbent functions, even when the size of the Boolean function is large (e.g., more\nthan 20 inputs). While bent Boolean functions represent only a small part of\nall Boolean functions, there are several subclasses of bent functions providing\nspecific properties and challenges. One of the most interesting subclasses\ncomprises (anti-)self-dual bent Boolean functions. This paper provides a\ndetailed experimentation with evolutionary algorithms with the goal of evolving\n(anti-)self-dual bent Boolean functions. We experiment with two encodings and\ntwo fitness functions to directly evolve self-dual bent Boolean functions. Our\nexperiments consider Boolean functions with sizes of up to 16 inputs, and we\nsuccessfully construct self-dual bent functions for each dimension. Moreover,\nwhen comparing with the evolution of bent Boolean functions, we notice that the\ndifficulty for evolutionary algorithms is rather similar. Finally, we also\ntried evolving secondary constructions for self-dual bent functions, but this\ndirection provided no successful results.",
        "translated": "Bent 布尔函数是密码学和编码理论中的重要研究对象，构造 Bent 布尔函数的方法有很多种。Metaheurtics 被证明是一个强有力的选择，因为它们可以提供许多弯曲的函数，即使布尔函数很大(例如，超过20个输入)。虽然曲线布尔函数只代表所有布尔函数的一小部分，但是曲线函数的几个子类提供了特定的属性和挑战。最有趣的子类之一包含(反)自对偶弯曲布尔函数。本文以进化(反)自对偶布尔函数为目标，对进化算法进行了详细的实验研究。我们用两个编码和两个适应度函数来直接进化自对偶弯曲布尔函数。我们的实验考虑的布尔函数的大小多达16个输入，我们成功地构造了每个维度的自对偶弯曲函数。此外，当与曲线布尔函数的进化相比较时，我们注意到进化算法的困难是相似的。最后，我们也尝试发展自对偶弯曲函数的二次构造，但是这个方向没有提供成功的结果。"
    },
    {
        "title": "A New Angle: On Evolving Rotation Symmetric Boolean Functions",
        "url": "http://arxiv.org/abs/2311.11881v1",
        "pub_date": "2023-11-20",
        "summary": "Rotation symmetric Boolean functions represent an interesting class of\nBoolean functions as they are relatively rare compared to general Boolean\nfunctions. At the same time, the functions in this class can have excellent\nproperties, making them interesting for various practical applications. The\nusage of metaheuristics to construct rotation symmetric Boolean functions is a\ndirection that has been explored for almost twenty years. Despite that, there\nare very few results considering evolutionary computation methods. This paper\nuses several evolutionary algorithms to evolve rotation symmetric Boolean\nfunctions with different properties. Despite using generic metaheuristics, we\nobtain results that are competitive with prior work relying on customized\nheuristics. Surprisingly, we find that bitstring and floating point encodings\nwork better than the tree encoding. Moreover, evolving highly nonlinear general\nBoolean functions is easier than rotation symmetric ones.",
        "translated": "旋转对称布尔函数代表了一类有趣的布尔函数，因为与一般布尔函数相比，旋转对称布尔函数相对较少。同时，这个类中的函数可以具有优秀的性质，使它们在各种实际应用中变得有趣。利用元启发法构造旋转对称布尔函数是近二十年来研究的一个方向。尽管如此，考虑进化计算方法的结果还是很少。本文利用多种进化算法进化出具有不同性质的旋转对称布尔函数。尽管使用通用元启发法，我们获得的结果是有竞争力的以前的工作依赖于定制启发法。令人惊讶的是，我们发现位字符串和浮点编码比树编码工作得更好。此外，演化高度非线性的一般布尔函数比旋转对称函数更容易。"
    },
    {
        "title": "Secure Data Transmission over Insecure Radio Channel in Wireless of\n  Things (WoT) Network",
        "url": "http://arxiv.org/abs/2311.11864v1",
        "pub_date": "2023-11-20",
        "summary": "Potential capacity of processors is enhancing rapidly which leads to the\nincrease of computational ability of the adversary. As a result, the required\nkey size for conventional encryption techniques is growing everyday for complex\nunbreakable security communication systems. The Public Key Cryptography (PKC)\ntechniques which use larger keys cannot be fitted in tiny resource constrained\nWireless of Things (WoT) devices. Some Symmetric Key Cryptosystems (SKC) use\nsmaller keys, which can be fitted in the tiny devices. But in large networks\nwhere the number of nodes is in the order of 103, the memory constraint does\nnot allow the system to do so. The existing secure data communication in\ninsecure medium uses various conventional encryption methods like Public Key\nCryptography (PKC) and Symmetric Key Cryptosystems (SKC). Generally, modern\nencryption methods need huge processing power, memory and time. Also in some\ncases, Key Pre-distribution System (KPS) is used among different communicating\ndevices. With the growing need for larger key size in the conventional secure\ncommunication system, the existing resources in the communicating devices\nsuffer from resource starvation. Hence, the need of a novel mechanism for\nsecure communication is inevitable. But the existing secure communication\nmechanisms like PKC, SKC or KPS do not ensure elimination of resource\nstarvation issue in tiny devices during communication. In these existing\nconventional mechanisms, the plain text is generally converted into cipher text\nwith greater size than the plain text at the device level, which leads to\nresource starvation. At the time of transmission, the cipher text at the device\nend requires more bandwidth than the plain text which puts bandwidth overhead\non the broadcast channel (BC).",
        "translated": "处理器的潜在容量正在迅速增加，这导致了对手计算能力的增加。因此，对于复杂的不可破解的安全通信系统，传统加密技术所需的密钥大小每天都在增加。使用较大密钥的公钥密码体制(PKC)技术不能适用于资源有限的无线物联网(WOT)设备。一些对称密钥加密系统(SKC)使用较小的密钥，可以安装在微型设备中。但是在节点数量大约为103的大型网络中，内存约束不允许系统这样做。现有的不安全介质中的安全数据通信采用各种传统的加密方法，如公钥密码体制(PKC)和对称密钥密码体制(SKC)。现代加密方法通常需要大量的处理能力、内存和时间。在某些情况下，密钥预分发系统(KPS)也用于不同的通信设备之间。随着传统保安通讯系统对较大密码匙的需求日益增加，通讯装置的现有资源受到资源衰竭的影响。因此，必然需要一种新的安全通信机制。但现有的保安通讯机制，例如 PKC、 SKC 或 KPS，并不能确保在通讯过程中消除微型设备的资源衰竭问题。在这些现有的传统机制中，在设备级别，纯文本一般会转换为比纯文本大的密码文本，从而导致资源衰竭。在传输时，设备端的加密文本比纯文本需要更多的带宽，纯文本在广播信道(BC)上产生带宽开销。"
    },
    {
        "title": "Zero redundancy distributed learning with differential privacy",
        "url": "http://arxiv.org/abs/2311.11822v1",
        "pub_date": "2023-11-20",
        "summary": "Deep learning using large models have achieved great success in a wide range\nof domains. However, training these models on billions of parameters is very\nchallenging in terms of the training speed, memory cost, and communication\nefficiency, especially under the privacy-preserving regime with differential\nprivacy (DP). On the one hand, DP optimization has comparable efficiency to the\nstandard non-private optimization on a single GPU, but on multiple GPUs,\nexisting DP distributed learning (such as pipeline parallel) has suffered from\nsignificantly worse efficiency. On the other hand, the Zero Redundancy\nOptimizer (ZeRO) is a state-of-the-art solution to the standard distributed\nlearning, exhibiting excellent training efficiency on large models, but to work\ncompatibly with DP is technically complicated. In this work, we develop a new\nsystematic solution, DP-ZeRO, (I) to scale up the trainable DP model size, e.g.\nto GPT-100B, (II) to obtain the same computation and communication efficiency\nas the standard ZeRO, and (III) to enable mixed-precision DP training. Our\nDP-ZeRO, like the standard ZeRO, has the potential to train models with\narbitrary size and is evaluated on the world's largest DP models in terms of\nthe number of trainable parameters.",
        "translated": "使用大型模型的深度学习在许多领域取得了巨大的成功。然而，对这些模型进行数十亿个参数的训练在训练速度、内存成本和通信效率方面是非常具有挑战性的，特别是在保护隐私的差分隐私(DP)机制下。一方面，DP 优化在单个 GPU 上的效率与标准的非私有优化相当，但在多个 GPU 上，现有 DP 分布式学习(如流水线并行)的效率明显较低。另一方面，零冗余优化器(ZeRO Redundancy Optimizer，ZRO)是标准分布式学习的最先进的解决方案，在大型模型上表现出优异的训练效率，但与 DP 兼容工作在技术上是复杂的。在这项工作中，我们开发了一个新的系统解决方案，DP-ZRO，(I)扩大可训练 DP 模型的大小，如 GPT-100B，(II)获得相同的计算和通信效率的标准 ZRO，(III)使混合精度 DP 训练。我们的 DP-ZRO，像标准的 ZRO 一样，有潜力训练任意大小的模型，并且在世界上最大的 DP 模型上根据可训练参数的数量进行评估。"
    },
    {
        "title": "Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI\n  Systems",
        "url": "http://arxiv.org/abs/2311.11796v1",
        "pub_date": "2023-11-20",
        "summary": "Artificial Intelligence (AI) systems such as autonomous vehicles, facial\nrecognition, and speech recognition systems are increasingly integrated into\nour daily lives. However, despite their utility, these AI systems are\nvulnerable to a wide range of attacks such as adversarial, backdoor, data\npoisoning, membership inference, model inversion, and model stealing attacks.\nIn particular, numerous attacks are designed to target a particular model or\nsystem, yet their effects can spread to additional targets, referred to as\ntransferable attacks. Although considerable efforts have been directed toward\ndeveloping transferable attacks, a holistic understanding of the advancements\nin transferable attacks remains elusive. In this paper, we comprehensively\nexplore learning-based attacks from the perspective of transferability,\nparticularly within the context of cyber-physical security. We delve into\ndifferent domains -- the image, text, graph, audio, and video domains -- to\nhighlight the ubiquitous and pervasive nature of transferable attacks. This\npaper categorizes and reviews the architecture of existing attacks from various\nviewpoints: data, process, model, and system. We further examine the\nimplications of transferable attacks in practical scenarios such as autonomous\ndriving, speech recognition, and large language models (LLMs). Additionally, we\noutline the potential research directions to encourage efforts in exploring the\nlandscape of transferable attacks. This survey offers a holistic understanding\nof the prevailing transferable attacks and their impacts across different\ndomains.",
        "translated": "人工智能(AI)系统，如自动驾驶汽车、人脸识别和语音识别系统，正日益融入我们的日常生活。然而，尽管这些人工智能系统具有实用性，但是它们很容易受到各种攻击，如对手攻击、后门攻击、数据中毒、成员推断、模型反转和模型窃取攻击。特别是，许多攻击被设计为针对特定的模型或系统，但是它们的效果可以扩散到其他目标，称为可转移攻击。尽管在开发可转移攻击方面已经做出了相当大的努力，但是对可转移攻击进展的整体理解仍然是难以实现的。本文从可转移性的角度，特别是在网络物理安全的背景下，全面探讨了基于学习的攻击。我们深入研究不同的领域——图像、文本、图形、音频和视频领域——以强调可转移攻击无处不在和普遍存在的本质。本文从数据、过程、模型和系统等不同角度对现有攻击的体系结构进行了分类和回顾。我们进一步研究可转移攻击在实际场景中的含义，如自主驾驶、语音识别和大型语言模型(LLM)。此外，我们概述了潜在的研究方向，以鼓励努力探索可转移攻击的景观。这项调查全面了解了当前流行的可转移攻击及其在不同领域的影响。"
    },
    {
        "title": "Private and Secure Post-Quantum Verifiable Random Function with NIZK\n  Proof and Ring-LWE Encryption in Blockchain",
        "url": "http://arxiv.org/abs/2311.11734v1",
        "pub_date": "2023-11-20",
        "summary": "We present a secure and private blockchain-based Verifiable Random Function\n(VRF) scheme addressing some limitations of classical VRF constructions. Given\nthe imminent quantum computing adversarial scenario, conventional cryptographic\nmethods face vulnerabilities. To enhance our VRF's secure randomness, we adopt\npost-quantum Ring-LWE encryption for synthesizing pseudo-random sequences.\nConsidering computational costs and resultant on-chain gas costs, we suggest a\nbifurcated architecture for VRF design, optimizing interactions between\non-chain and off-chain. Our approach employs a secure ring signature supported\nby NIZK proof and a delegated key generation method, inspired by the\nChaum-Pedersen equality proof and the Fiat-Shamir Heuristic. Our VRF scheme\nintegrates multi-party computation (MPC) with blockchain-based decentralized\nidentifiers (DID), ensuring both security and randomness. We elucidate the\nsecurity and privacy aspects of our VRF scheme, analyzing temporal and spatial\ncomplexities. We also approximate the entropy of the VRF scheme and detail its\nimplementation in a Solidity contract. Also, we delineate a method for\nvalidating the VRF's proof, matching for the contexts requiring both randomness\nand verification. Conclusively, using the NIST SP800-22 of the statistical\nrandomness test suite, our results exhibit a 98.86% pass rate over 11 test\ncases, with an average p-value of 0.5459 from 176 total tests.",
        "translated": "我们提出了一个安全和私有的基于区块链的可验证随机函数(VRF)方案，解决了经典 VRF 结构的一些局限性。考虑到即将到来的量子计算对抗场景，传统的加密方法面临着漏洞。为了增强 VRF 的安全随机性，我们采用后量子环 LWE 加密来合成伪随机序列。考虑到计算成本和由此产生的链上气体成本，我们提出了一个分支架构的 VRF 设计，优化之间的交互作用上链和下链。我们的方法采用了一个安全的环签名支持的 NIZK 证明和委托的密钥生成方法，灵感来自 Chaum-Pedersen 平等证明和菲亚特-沙米尔启发式。我们的 VRF 方案集成了多方计算(MPC)和基于区块链的分散标识符(DIS) ，保证了安全性和随机性。我们阐述了我们的 VRF 方案的安全性和隐私性，分析了时间和空间的复杂性。我们还对 VRF 方案的熵进行了近似，并详细讨论了它在 Solidy 契约中的实现。此外，我们描述了一种验证 VRF 证明的方法，匹配需要随机性和验证的上下文。总之，使用统计随机性测试套件的 NIST SP800-22，我们的结果显示在11个测试案例中通过率为98.86% ，从176个总测试中平均 p 值为0.5459。"
    },
    {
        "title": "High-resolution Image-based Malware Classification using Multiple\n  Instance Learning",
        "url": "http://arxiv.org/abs/2311.12760v1",
        "pub_date": "2023-11-21",
        "summary": "This paper proposes a novel method of classifying malware into families using\nhigh-resolution greyscale images and multiple instance learning to overcome\nadversarial binary enlargement. Current methods of visualisation-based malware\nclassification largely rely on lossy transformations of inputs such as resizing\nto handle the large, variable-sized images. Through empirical analysis and\nexperimentation, it is shown that these approaches cause crucial information\nloss that can be exploited. The proposed solution divides the images into\npatches and uses embedding-based multiple instance learning with a\nconvolutional neural network and an attention aggregation function for\nclassification. The implementation is evaluated on the Microsoft Malware\nClassification dataset and achieves accuracies of up to $96.6\\%$ on\nadversarially enlarged samples compared to the baseline of $22.8\\%$. The Python\ncode is available online at https://github.com/timppeters/MIL-Malware-Images .",
        "translated": "提出了一种利用高分辨率灰度图像和多实例学习将恶意软件分门别类的新方法。目前基于可视化的恶意软件分类方法主要依赖于输入的有损转换，例如调整大小以处理大型、可变大小的图像。通过实证分析和实验表明，这些方法造成了关键的信息损失，可以利用。提出的解决方案将图像划分为补丁，并使用基于嵌入的多实例学习、卷积神经网络和注意力聚合功能进行分类。该实现在微软恶意软件分类数据集上进行评估，在对抗性放大的样本上达到高达96.6% 的准确率，而基准值为22.8% 。Python 代码可以在网上 https://github.com/timppeters/mil-malware-images 下载。"
    },
    {
        "title": "Attacking Motion Planners Using Adversarial Perception Errors",
        "url": "http://arxiv.org/abs/2311.12722v1",
        "pub_date": "2023-11-21",
        "summary": "Autonomous driving (AD) systems are often built and tested in a modular\nfashion, where the performance of different modules is measured using\ntask-specific metrics. These metrics should be chosen so as to capture the\ndownstream impact of each module and the performance of the system as a whole.\nFor example, high perception quality should enable prediction and planning to\nbe performed safely. Even though this is true in general, we show here that it\nis possible to construct planner inputs that score very highly on various\nperception quality metrics but still lead to planning failures. In an analogy\nto adversarial attacks on image classifiers, we call such inputs\n\\textbf{adversarial perception errors} and show they can be systematically\nconstructed using a simple boundary-attack algorithm. We demonstrate the\neffectiveness of this algorithm by finding attacks for two different black-box\nplanners in several urban and highway driving scenarios using the CARLA\nsimulator. Finally, we analyse the properties of these attacks and show that\nthey are isolated in the input space of the planner, and discuss their\nimplications for AD system deployment and testing.",
        "translated": "自主驾驶(AD)系统通常是以模块化的方式构建和测试的，其中不同模块的性能是使用特定于任务的度量来衡量的。应该选择这些指标，以便捕获每个模块的下游影响和整个系统的性能。例如，高感知质量应该使预测和计划能够安全地执行。即使这在一般情况下是正确的，我们在这里展示的是，有可能构建在各种感知质量指标上得分非常高但仍然导致计划失败的计划输入。类似于对图像分类器的对抗性攻击，我们把这样的输入称为 textbf {对抗性感知错误} ，并证明它们可以用一个简单的边界攻击算法系统地构造。我们通过使用 CARLA 模拟器在几个城市和高速公路驾驶场景中发现针对两个不同黑盒规划者的攻击，证明了该算法的有效性。最后，我们分析了这些攻击的特性，证明了它们在规划器的输入空间中是孤立的，并讨论了它们对 AD 系统部署和测试的影响。"
    },
    {
        "title": "Attacks of fairness in Federated Learning",
        "url": "http://arxiv.org/abs/2311.12715v1",
        "pub_date": "2023-11-21",
        "summary": "Federated Learning is an important emerging distributed training paradigm\nthat keeps data private on clients. It is now well understood that by\ncontrolling only a small subset of FL clients, it is possible to introduce a\nbackdoor to a federated learning model, in the presence of certain attributes.\nIn this paper, we present a new type of attack that compromises the fairness of\nthe trained model. Fairness is understood to be the attribute-level performance\ndistribution of a trained model. It is particularly salient in domains where,\nfor example, skewed accuracy discrimination between subpopulations could have\ndisastrous consequences. We find that by employing a threat model similar to\nthat of a backdoor attack, an attacker is able to influence the aggregated\nmodel to have an unfair performance distribution between any given set of\nattributes. Furthermore, we find that this attack is possible by controlling\nonly a single client. While combating naturally induced unfairness in FL has\npreviously been discussed in depth, its artificially induced kind has been\nneglected. We show that defending against attacks on fairness should be a\ncritical consideration in any situation where unfairness in a trained model\ncould benefit a user who participated in its training.",
        "translated": "联合学习是一种重要的新兴分布式培训模式，它使客户端的数据保密。现在大家都知道，通过只控制 FL 客户机的一小部分，就有可能在存在某些属性的情况下引入联邦学习模型的后门。在本文中，我们提出了一种新的攻击类型，它损害了训练模型的公平性。公平被理解为训练模型的属性级绩效分布。这在一些领域尤其突出，例如，在亚种群之间存在偏差的准确性歧视，可能造成灾难性的后果。我们发现，通过使用类似于后门攻击的威胁模型，攻击者能够影响聚合模型，在任何给定的属性集之间产生不公平的性能分布。此外，我们发现这种攻击可能只控制一个单一的客户端。在对外语教学中自然诱发的不公平现象进行深入探讨的同时，人为诱发的不公平现象却被忽视了。我们表明，在任何情况下，如果训练模型中的不公平可能使参与训练的用户受益，那么防御对公平性的攻击应该是一个关键的考虑因素。"
    },
    {
        "title": "Decrypting Nonlinearity: Koopman Interpretation and Analysis of\n  Cryptosystems",
        "url": "http://arxiv.org/abs/2311.12714v1",
        "pub_date": "2023-11-21",
        "summary": "Public-key cryptosystems rely on computationally difficult problems for\nsecurity, traditionally analyzed using number theory methods. In this paper, we\nintroduce a novel perspective on cryptosystems by viewing the Diffie-Hellman\nkey exchange and the Rivest-Shamir-Adleman cryptosystem as nonlinear dynamical\nsystems. By applying Koopman theory, we transform these dynamical systems into\nhigher-dimensional spaces and analytically derive equivalent purely linear\nsystems. This formulation allows us to reconstruct the secret integers of the\ncryptosystems through straightforward manipulations, leveraging the tools\navailable for linear systems analysis. Additionally, we establish an upper\nbound on the minimum lifting dimension required to achieve perfect accuracy.\nOur results on the required lifting dimension are in line with the\nintractability of brute-force attacks. To showcase the potential of our\napproach, we establish connections between our findings and existing results on\nalgorithmic complexity. Furthermore, we extend this methodology to a\ndata-driven context, where the Koopman representation is learned from data\nsamples of the cryptosystems.",
        "translated": "公钥密码体制依赖于计算难度大的安全问题，传统上使用数论方法进行分析。本文从非线性动力系统的角度出发，介绍了一种新的密码体制观点，即迪菲－赫尔曼密钥交换密码体制和 Rivest- 沙米尔-阿德尔曼密码体制。通过应用库普曼理论，我们将这些动力系统转化为高维空间，并解析地推导出等价的纯线性系统。这个公式允许我们通过简单的操作重建密码系统的秘密整数，利用线性系统分析可用的工具。此外，我们建立了一个上界的最小提升维数要求，以实现完美的精度。我们关于所需提升维度的结果与蛮力攻击的难处理性一致。为了展示我们的方法的潜力，我们在我们的发现和算法复杂性的现有结果之间建立联系。此外，我们将这种方法推广到一个数据驱动的环境，其中库普曼表示是从密码系统的数据样本中学习的。"
    },
    {
        "title": "Short Voting Codes For Practical Code Voting",
        "url": "http://arxiv.org/abs/2311.12710v1",
        "pub_date": "2023-11-21",
        "summary": "To preserve voter secrecy on untrusted voter devices we propose to use short\nvoting codes. This ensures voting codes remain practical even if the voter is\nable to select multiple voting choices. We embed the mechanism in a protocol\nthat avoids complex cryptography in both the setup and the voting phase and\nrelies only on standard cryptographic primitives. Trusting the setup, and one\nout of multiple server components, the protocol provides vote secrecy,\ncast-as-intended, recorded-as-cast, tallied-as-recorded, eligibility and\nuniversal verifiability.",
        "translated": "为了保护不可信选民设备上的选民保密性，我们建议使用简短的投票代码。这确保了即使选民能够选择多个投票选择，投票代码仍然是可行的。我们将该机制嵌入到一个协议中，该协议在设置和投票阶段都避免了复杂的加密，并且只依赖于标准的加密原语。该协议信任设置，并且是多个服务器组件中的一个，它提供投票保密性、预期投票、预期投票、预期投票、预期投票、预期投票、预期投票和预期投票。"
    },
    {
        "title": "D-GATE: Decentralized Geolocation and Time Enforcement for Usage Control",
        "url": "http://arxiv.org/abs/2311.12647v1",
        "pub_date": "2023-11-21",
        "summary": "In the context of cloud environments, data providers entrust their data to\ndata consumers in order to allow further computing on their own IT\ninfrastructure. Usage control measures allow the data provider to restrict the\nusage of its data even on the data consumer's system. Two of these restrictions\ncan be the geographic location and time limitations. Current solutions that\ncould be used to enforce such constraints can be easily manipulated. These\ninclude solutions based on the system time, organizational agreements,\nGPS-based techniques or simple delay measurements to derive the distance to\nknown reference servers. With D-GATE, we propose a reliable solution that uses\ntrusted execution environments and relies on a decentralized mesh of reference\nnodes, so-called GeoClients. Here, participants periodically measure the lowest\nnetwork delay to each other to geolocate themselves. For data providers, it is\nthus possible to technically attest usage control with time and geolocation\nconstraints without depending on centralized reference systems.",
        "translated": "在云环境中，数据提供商将其数据委托给数据使用者，以便在自己的 IT 基础设施上进行进一步的计算。使用控制措施允许数据提供者甚至在数据使用者的系统上限制其数据的使用。其中两个限制可能是地理位置和时间限制。可以用来强制执行这种约束的现有解决方案可以很容易地加以操纵。这些解决方案包括基于系统时间、组织协议、基于全球定位系统的技术或简单的延迟测量，以求出到已知参考服务器的距离。使用 D-GATE，我们提出了一个可靠的解决方案，它使用可信的执行环境，并依赖于一个分散的参考节点网格，即所谓的 GeoClients。在这里，参与者定期测量最低的网络延迟到对方地理定位自己。因此，对于数据提供者来说，可以在不依赖中央参考系统的情况下，通过时间和地理位置限制从技术上证明使用控制。"
    },
    {
        "title": "Hyena: Optimizing Homomorphically Encrypted Convolution for Private CNN\n  Inference",
        "url": "http://arxiv.org/abs/2311.12519v1",
        "pub_date": "2023-11-21",
        "summary": "Processing convolution layers remains a huge bottleneck for private deep\nconvolutional neural network (CNN) inference for large datasets. To solve this\nissue, this paper presents a novel homomorphic convolution algorithm that\nprovides speedup, communication cost, and storage saving. We first note that\npadded convolution provides the advantage of model storage saving, but it does\nnot support channel packing, thereby increasing the amount of computation and\ncommunication. We address this limitation by proposing a novel plaintext\nmultiplication algorithm using the Walsh-Hadamard matrix. Furthermore, we\npropose the optimization techniques to significantly reduce the latency of the\nproposed convolution by selecting the optimal encryption parameters and\napplying lazy reduction. It achieves 1.6-3.8x speedup and reduces the weight\nstorage by 2000-8000x compared to the conventional convolution. When the\nproposed convolution is employed for CNNs like VGG-16, ResNet-20, and\nMobileNetV1 on ImageNet, it reduces the end-to-end latency by 1.3-2.6x, the\nmemory usage by 2.1-7.9x and communication cost by 1.7-2.0x compared to\nconventional method.",
        "translated": "处理卷积层仍然是大型数据集私有深度卷积神经网络(CNN)推理的一个巨大瓶颈。为了解决这个问题，本文提出了一种新的同态卷积算法，它提供了加速、通信开销和存储节省。我们首先注意到填充卷积提供了节省模型存储空间的优点，但是它不支持通道填充，从而增加了计算量和通信量。我们通过使用 Walsh-Hadamard 矩阵提出一种新的明文乘法算法来解决这个限制。此外，我们提出了优化技术，通过选择最佳的加密参数和应用延迟约简来显著降低所提出卷积的延迟。它实现了1.6 -3.8倍的加速比，并减少了2000-8000倍的权重存储比传统的卷积。当在 ImageNet 上使用 VGG-16、 ResNet-20和 MobileNetV1这样的 CNN 时，与传统方法相比，它减少了1.3-2.6倍的端到端延迟、2.1.7.9倍的内存使用和1.7-2.0倍的通信成本。"
    },
    {
        "title": "Heuristics for Detecting CoinJoin Transactions on the Bitcoin Blockchain",
        "url": "http://arxiv.org/abs/2311.12491v1",
        "pub_date": "2023-11-21",
        "summary": "This research delves into the intricacies of Bitcoin, a decentralized\npeer-to-peer network, and its associated blockchain, which records all\ntransactions since its inception. While this ensures integrity and\ntransparency, the transparent nature of Bitcoin potentially compromises users'\nprivacy rights. To address this concern, users have adopted CoinJoin, a method\nthat amalgamates multiple transaction intents into a single, larger transaction\nto bolster transactional privacy. This process complicates individual\ntransaction tracing and disrupts many established blockchain analysis\nheuristics. Despite its significance, limited research has been conducted on\nidentifying CoinJoin transactions. Particularly noteworthy are varied CoinJoin\nimplementations such as JoinMarket, Wasabi, and Whirlpool, each presenting\ndistinct challenges due to their unique transaction structures. This study\ndelves deeply into the open-source implementations of these protocols, aiming\nto develop refined heuristics for identifying their transactions on the\nblockchain. Our exhaustive analysis covers transactions up to block 760,000,\noffering a comprehensive insight into CoinJoin transactions and their\nimplications for Bitcoin blockchain analysis.",
        "translated": "这项研究深入探讨了比特币的复杂性，一个分散的点对点网络，及其相关的区块链，其中记录所有交易自成立以来。虽然这确保了完整性和透明度，但比特币的透明性可能会损害用户的隐私权。为了解决这个问题，用户已经采用了 CoinJoin，这种方法将多个事务意图合并到一个单一的、更大的事务中，以支持事务隐私。这个过程使单个事务跟踪复杂化，并且破坏了许多已建立的区块链分析启发式方法。尽管它的重要性，已经进行了有限的研究，以确定硬币连接交易。特别值得注意的是各种 CoinJoin 实现，例如 JoinMarket、 Wasabi 和 Whirlpool，由于它们独特的事务结构，每个实现都提出了不同的挑战。这项研究深入探讨了这些协议的开源实现，旨在开发精细的启发式方法来识别它们在区块链上的事务。我们的详尽分析涵盖了高达760,000块的交易，提供了一个全面的洞察 CoinJoin 交易及其对比特币区块链分析的影响。"
    },
    {
        "title": "How Far Have We Gone in Vulnerability Detection Using Large Language\n  Models",
        "url": "http://arxiv.org/abs/2311.12420v1",
        "pub_date": "2023-11-21",
        "summary": "As software becomes increasingly complex and prone to vulnerabilities,\nautomated vulnerability detection is critically important, yet challenging.\nGiven the significant successes of Large Language Models (LLMs) in various\ntasks, there is growing anticipation of their efficacy in vulnerability\ndetection. However, a quantitative understanding of their potential in\nvulnerability detection is still missing. To bridge this gap, we introduce a\ncomprehensive vulnerability benchmark VulBench. This benchmark aggregates\nhigh-quality data from a wide range of CTF (Capture-the-Flag) challenges and\nreal-world applications, with annotations for each vulnerable function\ndetailing the vulnerability type and its root cause. Through our experiments\nencompassing 16 LLMs and 6 state-of-the-art (SOTA) deep learning-based models\nand static analyzers, we find that several LLMs outperform traditional deep\nlearning approaches in vulnerability detection, revealing an untapped potential\nin LLMs. This work contributes to the understanding and utilization of LLMs for\nenhanced software security.",
        "translated": "随着软件变得越来越复杂和容易出现漏洞，自动漏洞检测是至关重要的，但也是具有挑战性的。鉴于大型语言模型(LLM)在各种任务中取得的显著成功，人们越来越期待它们在漏洞检测中的有效性。然而，对于它们在漏洞检测方面的潜力的定量理解仍然缺乏。为了弥补这一差距，我们引入了一个全面的脆弱性基准 VulBench。该基准汇集了来自各种 CTF (Capture-the-Flag)挑战和现实世界应用程序的高质量数据，并为每个易受攻击的功能提供注释，详细说明了易受攻击的类型及其根本原因。通过我们包括16个 LLM 和6个最先进的(SOTA)基于深度学习的模型和静态分析器的实验，我们发现几个 LLM 在漏洞检测方面优于传统的深度学习方法，揭示了 LLM 中未开发的潜力。这项工作有助于理解和利用 LLM 来增强软件安全性。"
    },
    {
        "title": "Malicious URL Detection via Pretrained Language Model Guided Multi-Level\n  Feature Attention Network",
        "url": "http://arxiv.org/abs/2311.12372v1",
        "pub_date": "2023-11-21",
        "summary": "The widespread use of the Internet has revolutionized information retrieval\nmethods. However, this transformation has also given rise to a significant\ncybersecurity challenge: the rapid proliferation of malicious URLs, which serve\nas entry points for a wide range of cyber threats. In this study, we present an\nefficient pre-training model-based framework for malicious URL detection.\nLeveraging the subword and character-aware pre-trained model, CharBERT, as our\nfoundation, we further develop three key modules: hierarchical feature\nextraction, layer-aware attention, and spatial pyramid pooling. The\nhierarchical feature extraction module follows the pyramid feature learning\nprinciple, extracting multi-level URL embeddings from the different Transformer\nlayers of CharBERT. Subsequently, the layer-aware attention module autonomously\nlearns connections among features at various hierarchical levels and allocates\nvarying weight coefficients to each level of features. Finally, the spatial\npyramid pooling module performs multiscale downsampling on the weighted\nmulti-level feature pyramid, achieving the capture of local features as well as\nthe aggregation of global features. The proposed method has been extensively\nvalidated on multiple public datasets, demonstrating a significant improvement\nover prior works, with the maximum accuracy gap reaching 8.43% compared to the\nprevious state-of-the-art method. Additionally, we have assessed the model's\ngeneralization and robustness in scenarios such as cross-dataset evaluation and\nadversarial attacks. Finally, we conducted real-world case studies on the\nactive phishing URLs.",
        "translated": "互联网的广泛使用彻底改变了信息检索的方法。然而，这种转变也带来了一个重大的网络安全挑战: 恶意网址的迅速扩散，这些网址是一系列网络威胁的切入点。在这项研究中，我们提出了一个有效的基于预训练模型的恶意 URL 检测框架。利用子词和字符感知预训练模型 CharBERT 作为基础，进一步开发了三个关键模块: 层次特征提取、层次感知注意和空间金字塔合并。分层特征提取模块遵循金字塔特征学习原理，从 CharBERT 的不同变压器层提取多级 URL 嵌入。随后，层次感知注意模块自主学习各层次特征之间的联系，并为每个层次的特征分配不同的权重系数。最后，空间金字塔池模块对加权多层特征金字塔进行多尺度下采样，实现局部特征的捕获和全局特征的聚合。该方法已在多个公共数据集上得到了广泛验证，与以往的工作相比有了显著的改进，最大精度差距达到了8.43% 。此外，我们还评估了该模型在跨数据集评估和对手攻击等场景中的泛化性和鲁棒性。最后，我们对活跃的网络钓鱼 URL 进行了实际案例研究。"
    },
    {
        "title": "Summary Reports Optimization in the Privacy Sandbox Attribution\n  Reporting API",
        "url": "http://arxiv.org/abs/2311.13586v1",
        "pub_date": "2023-11-22",
        "summary": "The Privacy Sandbox Attribution Reporting API has been recently deployed by\nGoogle Chrome to support the basic advertising functionality of attribution\nreporting (aka conversion measurement) after deprecation of third-party\ncookies. The API implements a collection of privacy-enhancing guardrails\nincluding contribution bounding and noise injection. It also offers flexibility\nfor the analyst to allocate the contribution budget.\n  In this work, we present methods for optimizing the allocation of the\ncontribution budget for summary reports from the Attribution Reporting API. We\nevaluate them on real-world datasets as well as on a synthetic data model that\nwe find to accurately capture real-world conversion data. Our results\ndemonstrate that optimizing the parameters that can be set by the analyst can\nsignificantly improve the utility achieved by querying the API while satisfying\nthe same privacy bounds.",
        "translated": "隐私沙盒归属报告 API 最近被 Google Chrome 部署，以支持归属报告(又名转换测量)的基本广告功能。空气污染指数实施一系列保障私隐的措施，包括限制供款及噪音注入。它还为分析人员分配贡献预算提供了灵活性。在这项工作中，我们提出了一些方法，用于优化来自归属报告 API 的摘要报告的贡献预算分配。我们评估他们在真实世界的数据集，以及在一个合成的数据模型，我们发现准确地捕获真实世界的转换数据。我们的结果表明，优化分析师可以设置的参数可以显著提高通过查询 API 实现的效用，同时满足相同的隐私界限。"
    },
    {
        "title": "Enigma: Privacy-Preserving Execution of QAOA on Untrusted Quantum\n  Computers",
        "url": "http://arxiv.org/abs/2311.13546v1",
        "pub_date": "2023-11-22",
        "summary": "Quantum computers can solve problems that are beyond the capabilities of\nconventional computers. As quantum computers are expensive and hard to\nmaintain, the typical model for performing quantum computation is to send the\ncircuit to a quantum cloud provider. This leads to privacy concerns for\ncommercial entities as an untrusted server can learn protected information from\nthe provided circuit. Current proposals for Secure Quantum Computing (SQC)\neither rely on emerging technologies (such as quantum networks) or incur\nprohibitive overheads (for Quantum Homomorphic Encryption). The goal of our\npaper is to enable low-cost privacy-preserving quantum computation that can be\nused with current systems.\n  We propose Enigma, a suite of privacy-preserving schemes specifically\ndesigned for the Quantum Approximate Optimization Algorithm (QAOA). Unlike\nprevious SQC techniques that obfuscate quantum circuits, Enigma transforms the\ninput problem of QAOA, such that the resulting circuit and the outcomes are\nunintelligible to the server. We introduce three variants of Enigma. Enigma-I\nprotects the coefficients of QAOA using random phase flipping and fudging of\nvalues. Enigma-II protects the nodes of the graph by introducing decoy qubits,\nwhich are indistinguishable from primary ones. Enigma-III protects the edge\ninformation of the graph by modifying the graph such that each node has an\nidentical number of connections. For all variants of Enigma, we demonstrate\nthat we can still obtain the solution for the original problem. We evaluate\nEnigma using IBM quantum devices and show that the privacy improvements of\nEnigma come at only a small reduction in fidelity (1%-13%).",
        "translated": "量子计算机可以解决传统计算机无法解决的问题。由于量子计算机昂贵且难以维护，执行量子计算的典型模式是将电路发送给量子云供应商。这导致商业实体的隐私问题，因为不受信任的服务器可以从提供的电路中学习受保护的信息。目前关于安全量子计算(SQC)的建议要么依赖于新兴技术(如量子网络) ，要么就会产生过高的开销(对于量子同态加密而言)。我们的论文的目标是使低成本的保密量子计算能够用于当前的系统。我们提出了 Enigma，一套专门为量子近似优化算法(QAOA)设计的隐私保护方案。与以前的 SQC 技术混淆量子电路不同，Enigma 转换了 QAOA 的输入问题，使得产生的电路和结果对服务器来说是难以理解的。我们介绍三种不同的英格玛。Enigma-I 保护 QAOA 系数的方法是使用随机相位翻转和值的伪造。Enigma-II 通过引入诱饵量子比特来保护图中的节点，而诱饵量子比特与主量子比特是无法区分的。Enigma-III 通过修改图形使每个节点具有相同数量的连接来保护图形的边信息。对于 Enigma 的所有变体，我们证明我们仍然可以获得原始问题的解决方案。我们使用 IBM 量子设备对 Enigma 进行了评估，结果表明，Enigma 的隐私改善仅仅是保真度的小幅下降(1% -13%)。"
    },
    {
        "title": "Explaining high-dimensional text classifiers",
        "url": "http://arxiv.org/abs/2311.13454v1",
        "pub_date": "2023-11-22",
        "summary": "Explainability has become a valuable tool in the last few years, helping\nhumans better understand AI-guided decisions. However, the classic\nexplainability tools are sometimes quite limited when considering\nhigh-dimensional inputs and neural network classifiers. We present a new\nexplainability method using theoretically proven high-dimensional properties in\nneural network classifiers. We present two usages of it: 1) On the classical\nsentiment analysis task for the IMDB reviews dataset, and 2) our\nMalware-Detection task for our PowerShell scripts dataset.",
        "translated": "在过去的几年里，可解释性已经成为一种有价值的工具，帮助人类更好地理解人工智能引导的决策。然而，当考虑高维输入和神经网络分类器时，经典的可解释性工具有时非常有限。利用神经网络分类器的理论证明的高维特性，提出了一种新的可解释性方法。我们介绍了它的两种用法: 1)用于 IMDB 评论数据集的经典情绪分析任务，2)用于 PowerShell 脚本数据集的 Malware-Detection 任务。"
    },
    {
        "title": "Differentially Private Non-Convex Optimization under the KL Condition\n  with Optimal Rates",
        "url": "http://arxiv.org/abs/2311.13447v1",
        "pub_date": "2023-11-22",
        "summary": "We study private empirical risk minimization (ERM) problem for losses\nsatisfying the $(\\gamma,\\kappa)$-Kurdyka-{\\L}ojasiewicz (KL) condition. The\nPolyak-{\\L}ojasiewicz (PL) condition is a special case of this condition when\n$\\kappa=2$. Specifically, we study this problem under the constraint of $\\rho$\nzero-concentrated differential privacy (zCDP). When $\\kappa\\in[1,2]$ and the\nloss function is Lipschitz and smooth over a sufficiently large region, we\nprovide a new algorithm based on variance reduced gradient descent that\nachieves the rate\n$\\tilde{O}\\big(\\big(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}}\\big)^\\kappa\\big)$ on the\nexcess empirical risk, where $n$ is the dataset size and $d$ is the dimension.\nWe further show that this rate is nearly optimal. When $\\kappa \\geq 2$ and the\nloss is instead Lipschitz and weakly convex, we show it is possible to achieve\nthe rate $\\tilde{O}\\big(\\big(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}}\\big)^\\kappa\\big)$\nwith a private implementation of the proximal point method. When the KL\nparameters are unknown, we provide a novel modification and analysis of the\nnoisy gradient descent algorithm and show that this algorithm achieves a rate\nof\n$\\tilde{O}\\big(\\big(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}}\\big)^{\\frac{2\\kappa}{4-\\kappa}}\\big)$\nadaptively, which is nearly optimal when $\\kappa = 2$. We further show that,\nwithout assuming the KL condition, the same gradient descent algorithm can\nachieve fast convergence to a stationary point when the gradient stays\nsufficiently large during the run of the algorithm. Specifically, we show that\nthis algorithm can approximate stationary points of Lipschitz, smooth (and\npossibly nonconvex) objectives with rate as fast as\n$\\tilde{O}\\big(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}}\\big)$ and never worse than\n$\\tilde{O}\\big(\\big(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}}\\big)^{1/2}\\big)$. The latter\nrate matches the best known rate for methods that do not rely on variance\nreduction.",
        "translated": "研究了满足 $(γ，kappa) $- Kurdyka-{ L } ojasiewicz (KL)条件的损失私人经验风险最小化(ERM)问题。当 $kappa = 2 $时，Polyak-{ L } ojasiewicz (PL)条件是此条件的一个特例。具体来说，我们是在欧元零集中差分隐私(zCDP)的约束下研究这个问题的。当文献[1,2] $中的 $kappa 为 Lipschitz 且损失函数在一个足够大区域上光滑时，我们提出了一种基于方差缩减梯度下降法的新算法，该算法在超额经验风险下达到 $tilde { O } big (big (frac { sqrt { d }}{ n sqrt { rho }} big) ^ kappa big) ，其中 $n $为数据集大小，$d $为维数。我们进一步证明了这个速率几乎是最优的。当 $kappa geq 2 $的损失为 Lipschitz 和弱凸时，我们证明了利用近似点方法的私有实现可以实现速率 $Tlde { O } big (big (frac { sqrt { d }}{ n sqrt { rho }} big) ^ kappa big) $。在 KL 参数未知的情况下，我们对带噪梯度下降法算法进行了修改和分析，结果表明该算法自适应地达到了 $tilde { O } big (big (frac { sqrt { d }}{ n sqrt { rho }} big) ^ { frac {2 kappa }{4-kappa }) $的速率，在 $kappa = 2 $时接近最优。我们进一步证明，在不假设 KL 条件的情况下，当梯度在算法运行期间保持不变时，同样的梯度下降法算法可以快速收敛到一个驻点足够大。具体地说，我们证明了该算法可以逼近 Lipschitz 平稳点，平滑(可能是非凸)目标，其速度与 $tilde { O } big (frac { sqrt { d }}{ n sqrt { rho }} big) $一样快，而且不会比 $tilde { O } big (big (frac { sqrt { d }{ n sqrt { rho } big) ^ {1/2} big) $差。后者的速率与不依赖于方差减少的方法的最佳已知速率相匹配。"
    },
    {
        "title": "Transfer Attacks and Defenses for Large Language Models on Coding Tasks",
        "url": "http://arxiv.org/abs/2311.13445v1",
        "pub_date": "2023-11-22",
        "summary": "Modern large language models (LLMs), such as ChatGPT, have demonstrated\nimpressive capabilities for coding tasks including writing and reasoning about\ncode. They improve upon previous neural network models of code, such as\ncode2seq or seq2seq, that already demonstrated competitive results when\nperforming tasks such as code summarization and identifying code\nvulnerabilities. However, these previous code models were shown vulnerable to\nadversarial examples, i.e. small syntactic perturbations that do not change the\nprogram's semantics, such as the inclusion of \"dead code\" through false\nconditions or the addition of inconsequential print statements, designed to\n\"fool\" the models. LLMs can also be vulnerable to the same adversarial\nperturbations but a detailed study on this concern has been lacking so far. In\nthis paper we aim to investigate the effect of adversarial perturbations on\ncoding tasks with LLMs. In particular, we study the transferability of\nadversarial examples, generated through white-box attacks on smaller code\nmodels, to LLMs. Furthermore, to make the LLMs more robust against such\nadversaries without incurring the cost of retraining, we propose prompt-based\ndefenses that involve modifying the prompt to include additional information\nsuch as examples of adversarially perturbed code and explicit instructions for\nreversing adversarial perturbations. Our experiments show that adversarial\nexamples obtained with a smaller code model are indeed transferable, weakening\nthe LLMs' performance. The proposed defenses show promise in improving the\nmodel's resilience, paving the way to more robust defensive solutions for LLMs\nin code-related applications.",
        "translated": "现代大型语言模型(LLM) ，例如 ChatGPT，已经展示了令人印象深刻的编码任务能力，包括编写和推理代码。它们改进了先前的代码神经网络模型，例如 code2seq 或 seq2seq，这些模型在执行代码总结和识别代码漏洞等任务时已经显示出具有竞争力的结果。然而，这些先前的代码模型被证明易受敌对的例子，即小的语法扰动，不改变程序的语义，如包含“死代码”通过错误的条件或添加无关紧要的打印语句，旨在“欺骗”模型。LLM 也可能受到同样的对抗性扰动的影响，但迄今为止还没有对这一问题进行详细的研究。本文旨在研究对抗扰动对 LLM 编码任务的影响。特别是，我们研究了通过对较小代码模型的白盒攻击生成的对抗性示例对 LLM 的可转移性。此外，为了使 LLM 在不引起再培训成本的情况下对这些对手更加强大，我们建议基于提示的防御，包括修改提示以包括额外的信息，如对抗性扰动代码的例子和逆转对抗性扰动的明确指示。我们的实验表明，用一个较小的代码模型获得的对抗性例子的确是可转移的，削弱了 LLM 的性能。提出的防御方案显示了提高模型弹性的希望，为代码相关应用程序中 LLM 的更健壮的防御解决方案铺平了道路。"
    },
    {
        "title": "Gradual Verification for Smart Contracts",
        "url": "http://arxiv.org/abs/2311.13351v1",
        "pub_date": "2023-11-22",
        "summary": "Blockchains facilitate secure resource transactions through smart contracts,\nyet these digital agreements are prone to vulnerabilities, particularly when\ninteracting with external contracts, leading to substantial monetary losses.\nTraditional verification techniques fall short in providing comprehensive\nsecurity assurances, especially against re-entrancy attacks, due to the\nunavailable implementations of external contracts. This paper introduces an\nincremental approach: gradual verification. We combine static and dynamic\nverification techniques to enhance security, guarantee soundness and\nflexibility, and optimize resource usage in smart contract interactions. By\nimplementing a prototype for gradually verifying Algorand smart contracts via\nthe pyTEAL language, we demonstrate the effectiveness of our approach,\ncontributing to the safe and efficient execution of smart contracts.",
        "translated": "区块链通过智能合同促进安全的资源交易，然而这些数字协议容易出现漏洞，特别是在与外部合同相互作用时，导致巨大的金钱损失。由于无法实现外部合同，传统的验证技术无法提供全面的安全保证，尤其是针对重入攻击的安全保证。本文介绍了一种渐进式方法: 渐进式验证。我们结合了静态和动态验证技术，以增强安全性，保证健全性和灵活性，并在智能合同交互中优化资源使用。通过实现一个通过 pyTEAL 语言逐步验证 Algorand 智能合同的原型，我们证明了我们的方法的有效性，有助于安全和高效地执行智能合同。"
    },
    {
        "title": "Automated generation of attack trees with optimal shape and labelling",
        "url": "http://arxiv.org/abs/2311.13331v1",
        "pub_date": "2023-11-22",
        "summary": "The problem this article addresses is, given a formal specification of a\nsystem, how to produce an attack tree that correctly and clearly describes the\nways the system can be attacked. Correctness means that the attacks displayed\nby the attack tree are indeed attacks in the system; clarity means that the\ntree is efficient in communicating the attack scenario. To pursue clarity, we\nintroduce an attack-tree generation algorithm that minimises the tree size and\nthe information length of its labels without sacrificing correctness. We\nachieve this by establishing a connection between the problem of factorising\nalgebraic expressions and the problem of minimising the tree size. Notably, our\ngeneration algorithm can handle complex attacks that execute actions in\nparallel and sequentially. For completeness, we introduce a system model that\nintegrates well with our generation approach, and validate the resulting\nframework via a running example.",
        "translated": "本文要解决的问题是，给定一个系统的形式规范，如何生成一个攻击树，正确而清晰地描述系统可能受到攻击的方式。正确性意味着攻击树显示的攻击确实是系统中的攻击; 清晰性意味着该树能够有效地通信攻击场景。为了清晰起见，我们引入了一种攻击树生成算法，在不牺牲正确性的前提下，最小化攻击树的大小和标签的信息长度。我们通过在代数表达式的因式分解问题和最小化树大小问题之间建立联系来实现这一点。值得注意的是，我们的生成算法可以处理并行和顺序执行操作的复杂攻击。为了完整起见，我们引入了一个与我们的生成方法很好地集成的系统模型，并通过一个运行示例验证了结果框架。"
    },
    {
        "title": "Hard Label Black Box Node Injection Attack on Graph Neural Networks",
        "url": "http://arxiv.org/abs/2311.13244v1",
        "pub_date": "2023-11-22",
        "summary": "While graph neural networks have achieved state-of-the-art performances in\nmany real-world tasks including graph classification and node classification,\nrecent works have demonstrated they are also extremely vulnerable to\nadversarial attacks. Most previous works have focused on attacking node\nclassification networks under impractical white-box scenarios. In this work, we\nwill propose a non-targeted Hard Label Black Box Node Injection Attack on Graph\nNeural Networks, which to the best of our knowledge, is the first of its kind.\nUnder this setting, more real world tasks can be studied because our attack\nassumes no prior knowledge about (1): the model architecture of the GNN we are\nattacking; (2): the model's gradients; (3): the output logits of the target GNN\nmodel. Our attack is based on an existing edge perturbation attack, from which\nwe restrict the optimization process to formulate a node injection attack. In\nthe work, we will evaluate the performance of the attack using three datasets,\nCOIL-DEL, IMDB-BINARY, and NCI1.",
        "translated": "尽管图神经网络在许多真实世界的任务(包括图分类和节点分类)中取得了最先进的性能，但最近的研究表明它们也极易受到敌对攻击。以往的研究主要集中在攻击不切实际的白盒场景下的节点分类网络。在这项工作中，我们将提出一个非目标的硬标签黑盒节点注入攻击图神经网络，据我们所知，这是第一次。在这种情况下，我们可以研究更多的现实世界的任务，因为我们的攻击假设没有事先知道(1) : 我们正在攻击的 GNN 的模型架构; (2) : 模型的梯度; (3) : 目标 GNN 模型的输出 logits。我们的攻击是基于现有的边缘扰动攻击，从而限制了优化过程来制定节点注入攻击。在工作中，我们将使用三个数据集(COIL-DEL、 IMDB-BINARY 和 NCI1)来评估攻击的性能。"
    },
    {
        "title": "A Survey of Adversarial CAPTCHAs on its History, Classification and\n  Generation",
        "url": "http://arxiv.org/abs/2311.13233v1",
        "pub_date": "2023-11-22",
        "summary": "Completely Automated Public Turing test to tell Computers and Humans Apart,\nshort for CAPTCHA, is an essential and relatively easy way to defend against\nmalicious attacks implemented by bots. The security and usability trade-off\nlimits the use of massive geometric transformations to interfere deep model\nrecognition and deep models even outperformed humans in complex CAPTCHAs. The\ndiscovery of adversarial examples provides an ideal solution to the security\nand usability trade-off by integrating adversarial examples and CAPTCHAs to\ngenerate adversarial CAPTCHAs that can fool the deep models. In this paper, we\nextend the definition of adversarial CAPTCHAs and propose a classification\nmethod for adversarial CAPTCHAs. Then we systematically review some commonly\nused methods to generate adversarial examples and methods that are successfully\nused to generate adversarial CAPTCHAs. Also, we analyze some defense methods\nthat can be used to defend adversarial CAPTCHAs, indicating potential threats\nto adversarial CAPTCHAs. Finally, we discuss some possible future research\ndirections for adversarial CAPTCHAs at the end of this paper.",
        "translated": "完全自动化的公共图灵测试告诉计算机和人类分开，简称 CAPTCHA，是一个重要的和相对容易的方式来防御恶意攻击的机器人实施。安全性和可用性的权衡限制了使用大量的几何变换来干扰深度模型识别，甚至在复杂的 CAPTCHA 中深度模型的表现超过了人类。对抗性例子的发现为安全性和可用性的权衡提供了一个理想的解决方案，通过集成对抗性例子和 CAPTCHA 来生成对抗性 CAPTCHA，可以欺骗深度模型。本文扩展了对抗性 CAPTCHA 的定义，提出了一种对抗性 CAPTCHA 的分类方法。然后系统地回顾了一些常用的生成对抗性实例的方法，以及一些成功生成对抗性 CAPTCHA 的方法。此外，我们还分析了一些防御方法，可以用来防御对手 CAPTCHA，指出潜在的威胁对手 CAPTCHA。最后，我们讨论了对抗性 CAPTCHA 未来可能的研究方向。"
    },
    {
        "title": "SecureCut: Federated Gradient Boosting Decision Trees with Efficient\n  Machine Unlearning",
        "url": "http://arxiv.org/abs/2311.13174v1",
        "pub_date": "2023-11-22",
        "summary": "In response to legislation mandating companies to honor the \\textit{right to\nbe forgotten} by erasing user data, it has become imperative to enable data\nremoval in Vertical Federated Learning (VFL) where multiple parties provide\nprivate features for model training. In VFL, data removal, i.e.,\n\\textit{machine unlearning}, often requires removing specific features across\nall samples under privacy guarentee in federated learning. To address this\nchallenge, we propose \\methname, a novel Gradient Boosting Decision Tree (GBDT)\nframework that effectively enables both \\textit{instance unlearning} and\n\\textit{feature unlearning} without the need for retraining from scratch.\nLeveraging a robust GBDT structure, we enable effective data deletion while\nreducing degradation of model performance. Extensive experimental results on\npopular datasets demonstrate that our method achieves superior model utility\nand forgetfulness compared to \\textit{state-of-the-art} methods. To our best\nknowledge, this is the first work that investigates machine unlearning in VFL\nscenarios.",
        "translated": "为了响应法律要求公司通过删除用户数据来尊重文本{被遗忘的权利} ，在垂直联邦学习(VFL)中实现数据删除已经变得势在必行，因为多方为模型培训提供了私有特性。在 VFL 中，数据删除，即文本{机器学习} ，通常需要在联邦学习中隐私保护下删除所有样本的特定特征。为了应对这个挑战，我们提出 methname，一个新的梯度提升决策树(GBDT)框架，它能够有效地支持 texttit {实例忘记}和 texttit {特征忘记} ，而不需要从头开始重新训练。利用一个健壮的 GBDT 结构，我们能够有效地删除数据，同时减少模型性能的退化。在流行数据集上的大量实验结果表明，与文本{最先进的}方法相比，我们的方法具有更好的模型实用性和遗忘性。据我们所知，这是第一个在 VFL 场景中研究机器去学习的工作。"
    },
    {
        "title": "Differentially Private SGD Without Clipping Bias: An Error-Feedback\n  Approach",
        "url": "http://arxiv.org/abs/2311.14632v1",
        "pub_date": "2023-11-24",
        "summary": "Differentially Private Stochastic Gradient Descent with gradient clipping\n(DPSGD-GC) is a powerful tool for training deep learning models using sensitive\ndata, providing both a solid theoretical privacy guarantee and high efficiency.\nHowever, using DPSGD-GC to ensure Differential Privacy (DP) comes at the cost\nof model performance degradation due to DP noise injection and gradient\nclipping. Existing research has extensively analyzed the theoretical\nconvergence of DPSGD-GC, and has shown that it only converges when using large\nclipping thresholds that are dependent on problem-specific parameters.\nUnfortunately, these parameters are often unknown in practice, making it hard\nto choose the optimal clipping threshold. Therefore, in practice, DPSGD-GC\nsuffers from degraded performance due to the {\\it constant} bias introduced by\nthe clipping.\n  In our work, we propose a new error-feedback (EF) DP algorithm as an\nalternative to DPSGD-GC, which not only offers a diminishing utility bound\nwithout inducing a constant clipping bias, but more importantly, it allows for\nan arbitrary choice of clipping threshold that is independent of the problem.\nWe establish an algorithm-specific DP analysis for our proposed algorithm,\nproviding privacy guarantees based on R{\\'e}nyi DP. Additionally, we\ndemonstrate that under mild conditions, our algorithm can achieve nearly the\nsame utility bound as DPSGD without gradient clipping. Our empirical results on\nCifar-10/100 and E2E datasets, show that the proposed algorithm achieves higher\naccuracies than DPSGD while maintaining the same level of DP guarantee.",
        "translated": "带有梯度裁剪的差分私有随机梯度下降(dpsgd-GC)是使用敏感数据训练深度学习模型的强大工具，提供了坚实的理论隐私保障和高效率。然而，使用 dpsgd-GC 来确保差分隐私(DP)的代价是模型性能由于 DP 噪声注入和梯度剪切而降低。现有的研究已经广泛地分析了 DPSGD-GC 的理论收敛性，并表明它只有在使用依赖于特定问题参数的大剪切阈值时才会收敛。遗憾的是，这些参数在实际应用中往往是未知的，因此很难选择最佳剪切阈值。因此，在实践中，DPSGD-GC 由于剪切引入的{ it 常数}偏置而遭受性能下降。在我们的工作中，我们提出了一个新的错误反馈(EF) DP 算法作为 DPSGD-GC 的替代，它不仅提供了一个减少的效用界限而不会引起恒定的剪切偏差，更重要的是，它允许任意选择的剪切阈值是独立于问题。我们为提出的算法建立了一个特定于算法的 DP 分析，提供了基于 R {‘ e } nyi DP 的隐私保护。此外，我们证明，在温和的条件下，我们的算法可以实现几乎相同的效用界的 DPSGD 没有梯度裁剪。我们对 Cifar-10/100和 E2E 数据集的实验结果表明，该算法在保持 DP 保证水平的情况下，比 DPSGD 具有更高的精度。"
    },
    {
        "title": "MABFuzz: Multi-Armed Bandit Algorithms for Fuzzing Processors",
        "url": "http://arxiv.org/abs/2311.14594v1",
        "pub_date": "2023-11-24",
        "summary": "As the complexities of processors keep increasing, the task of effectively\nverifying their integrity and security becomes ever more daunting. The\nintricate web of instructions, microarchitectural features, and\ninterdependencies woven into modern processors pose a formidable challenge for\neven the most diligent verification and security engineers. To tackle this\ngrowing concern, recently, researchers have developed fuzzing techniques\nexplicitly tailored for hardware processors. However, a prevailing issue with\nthese hardware fuzzers is their heavy reliance on static strategies to make\ndecisions in their algorithms. To address this problem, we develop a novel\ndynamic and adaptive decision-making framework, MABFuzz, that uses multi-armed\nbandit (MAB) algorithms to fuzz processors. MABFuzz is agnostic to, and hence,\napplicable to, any existing hardware fuzzer. In the process of designing\nMABFuzz, we encounter challenges related to the compatibility of MAB algorithms\nwith fuzzers and maximizing their efficacy for fuzzing. We overcome these\nchallenges by modifying the fuzzing process and tailoring MAB algorithms to\naccommodate special requirements for hardware fuzzing.\n  We integrate three widely used MAB algorithms in a state-of-the-art hardware\nfuzzer and evaluate them on three popular RISC-V-based processors. Experimental\nresults demonstrate the ability of MABFuzz to cover a broader spectrum of\nprocessors' intricate landscapes and doing so with remarkable efficiency. In\nparticular, MABFuzz achieves up to 308x speedup in detecting vulnerabilities\nand up to 5x speedup in achieving coverage compared to a state-of-the-art\ntechnique.",
        "translated": "随着处理器的复杂性不断增加，有效地验证其完整性和安全性的任务变得更加艰巨。编织进现代处理器的指令、微架构特征和相互依赖的复杂网络，即使是最勤奋的验证和安全工程师也面临着巨大的挑战。为了解决这个日益增长的问题，最近，研究人员开发了专门为硬件处理器量身定制的模糊技术。然而，这些硬件模糊器的一个普遍问题是它们严重依赖静态策略来在算法中做出决策。为了解决这个问题，我们开发了一个新的动态和自适应决策框架，MABfuzz，它使用多臂老虎机(MAB)算法来模糊处理器。MABFuzz 不可知，因此适用于任何现有的硬件 Fuzzer。在设计 MABFuzz 的过程中，我们遇到了与 MAB 算法与模糊算法的兼容性和最大化模糊效率有关的挑战。我们通过修改模糊处理过程和裁剪 MAB 算法来适应硬件模糊处理的特殊要求来克服这些挑战。我们将三种广泛使用的 MAB 算法集成在一个最先进的硬件模糊器中，并在三种流行的基于 RISC-V 的处理器上对它们进行评估。实验结果表明，MABFuzz 能够覆盖更广泛的处理器复杂环境，并且具有显著的效率。特别是，与最先进的技术相比，MABFuzz 在检测漏洞方面的速度提高了308倍，在实现覆盖方面的速度提高了5倍。"
    },
    {
        "title": "FRAD: Front-Running Attacks Detection on Ethereum using Ternary\n  Classification Model",
        "url": "http://arxiv.org/abs/2311.14514v1",
        "pub_date": "2023-11-24",
        "summary": "With the evolution of blockchain technology, the issue of transaction\nsecurity, particularly on platforms like Ethereum, has become increasingly\ncritical. Front-running attacks, a unique form of security threat, pose\nsignificant challenges to the integrity of blockchain transactions. In these\nattack scenarios, malicious actors monitor other users' transaction activities,\nthen strategically submit their own transactions with higher fees. This ensures\ntheir transactions are executed before the monitored transactions are included\nin the block. The primary objective of this paper is to delve into a\ncomprehensive classification of transactions associated with front-running\nattacks, which aims to equip developers with specific strategies to counter\neach type of attack. To achieve this, we introduce a novel detection method\nnamed FRAD (Front-Running Attacks Detection on Ethereum using Ternary\nClassification Model). This method is specifically tailored for transactions\nwithin decentralized applications (DApps) on Ethereum, enabling accurate\nclassification of front-running attacks involving transaction displacement,\ninsertion, and suppression. Our experimental validation reveals that the\nMultilayer Perceptron (MLP) classifier offers the best performance in detecting\nfront-running attacks, achieving an impressive accuracy rate of 84.59% and\nF1-score of 84.60%.",
        "translated": "随着区块链技术的发展，交易安全问题，尤其是在 Ethereum 这样的平台上，变得越来越重要。前向攻击是一种独特的安全威胁形式，对区块链交易的完整性提出了重大挑战。在这些攻击场景中，恶意角色监视其他用户的事务活动，然后策略性地以更高的费用提交自己的事务。这样可以确保它们的事务在被监视的事务包括在块中之前执行。本文的主要目的是深入研究与前端运行攻击相关的事务的全面分类，其目的是为开发人员提供针对每种类型攻击的具体策略。为了实现这一目标，我们提出了一种新的检测方法 FRAD (基于三元分类模型的以太网前向攻击检测)。这种方法是专门为以太网上的分散应用程序(DApps)中的事务量身定制的，能够准确分类涉及事务位移、插入和抑制的前端运行攻击。我们的实验验证表明，多层感知机(MLP)分类器在检测前向攻击方面提供了最佳的性能，达到了令人印象深刻的84.59% 的准确率和84.60% 的 F1评分。"
    },
    {
        "title": "Malware Analysis on AI Technique",
        "url": "http://arxiv.org/abs/2311.14501v1",
        "pub_date": "2023-11-24",
        "summary": "In today's world, we are performing our maximum work through the Internet,\ni.e., online payment, data transfer, etc., per day. More than thousands of\nusers are connecting. So, it's essential to provide security to the user. It is\nnecessary to detect and prevent malicious object from gaining persistence and\ncausing destruction within the organization. Therefore, Malware analysis is\nneeded in order to secure the system. This necessitates the use of effective\nand efficient approaches for detecting OS malware. Due to the cheap cost of\ntechnology, artificial intelligence has also become less difficult to implement\nin projects to analyse malware. The categorization and analysis of malware on\nOS using various AI-based analysis techniques are covered in detail in this\npaper.",
        "translated": "在今天的世界，我们通过互联网完成我们的最大工作，即，在线支付，数据传输等，每天。数以千计的用户正在连接。因此，为用户提供安全性非常重要。有必要检测和防止恶意对象在组织内获得持久性并造成破坏。因此，需要对恶意软件进行分析，以确保系统的安全。这就需要使用有效和高效的方法来检测操作系统恶意软件。由于技术成本低廉，人工智能在分析恶意软件的项目中的实施也变得不那么困难。本文详细介绍了利用各种基于人工智能的分析技术对操作系统中的恶意软件进行分类和分析。"
    },
    {
        "title": "Comment on \"Improved RSA Technique with Efficient Data Integrity\n  Verification for Outsourcing Database in Cloud\"",
        "url": "http://arxiv.org/abs/2311.14499v1",
        "pub_date": "2023-11-24",
        "summary": "In 2022, Neela and Kavitha proposed an improved RSA encryption algorithm\n(IREA) for cloud environment. In this paper, we review and comment on the\ncorrectness of the IREA technique. We prove that the private key generation in\nthe proposed IREA is mathematical incorrect. That is, decryption of the cipher\ngenerated by the public key is not possible with the private key. Further, we\ndiscuss the possible modifications in IREA to make it correct decryption.",
        "translated": "2022年，Neela 和 Kavitha 针对云环境提出了一种改进的 RSA 加密算法(IREA)。本文对 IREA 技术的正确性进行了评述。我们证明了所提出的 IREA 中的私钥生成方法在数学上是不正确的。也就是说，使用私钥无法对公钥生成的密码进行解密。此外，我们讨论了在 IREA 可能的修改，使其正确解密。"
    },
    {
        "title": "RTPS Attack Dataset Description",
        "url": "http://arxiv.org/abs/2311.14496v1",
        "pub_date": "2023-11-24",
        "summary": "This paper explains all about our RTPS datasets. We collect attack and normal\npacket data by injecting attack data in an Unmanned Ground Vehicle (UGV) which\nis normal state. To collect this dataset, We assembled a test bed consisting of\nUGV, controller, PC, and router. We conducted two types of Attacks \"Command\nInjection\" and \"ARP Spoofing\" on the testbed. The data collection time is 180,\n300, 600, and 1200, the scenario has 30 each on collection time. 240 total. We\nexpect this dataset will contribute to the development of technologies such as\nanomaly detection to address security threat issues in ROS2 networks and UGVs.",
        "translated": "本文解释了所有关于我们的 RTPS 数据集。我们以正常状态的无人地面载具(UGV)注入攻击数据，以收集攻击和正常数据包的数据。为了收集这个数据集，我们组装了一个由 UGV、控制器、 PC 和路由器组成的测试平台。我们在测试平台上进行了两种类型的攻击“命令注入”和“ ARP 欺骗”。数据收集时间分别为180、300、600和1200，场景中的收集时间分别为30。总共240个。我们预计这个数据集将有助于技术的发展，例如异常检测解决 ROS2网络和 UGVs 中的安全威胁问题。"
    },
    {
        "title": "Universal Jailbreak Backdoors from Poisoned Human Feedback",
        "url": "http://arxiv.org/abs/2311.14455v1",
        "pub_date": "2023-11-24",
        "summary": "Reinforcement Learning from Human Feedback (RLHF) is used to align large\nlanguage models to produce helpful and harmless responses. Yet, prior work\nshowed these models can be jailbroken by finding adversarial prompts that\nrevert the model to its unaligned behavior. In this paper, we consider a new\nthreat where an attacker poisons the RLHF training data to embed a \"jailbreak\nbackdoor\" into the model. The backdoor embeds a trigger word into the model\nthat acts like a universal \"sudo command\": adding the trigger word to any\nprompt enables harmful responses without the need to search for an adversarial\nprompt. Universal jailbreak backdoors are much more powerful than previously\nstudied backdoors on language models, and we find they are significantly harder\nto plant using common backdoor attack techniques. We investigate the design\ndecisions in RLHF that contribute to its purported robustness, and release a\nbenchmark of poisoned models to stimulate future research on universal\njailbreak backdoors.",
        "translated": "人类反馈的强化学习是用来调整大型语言模型，以产生有益和无害的反应。然而，之前的研究表明，这些模型可以通过寻找对抗性的提示来破解，这些提示可以使模型恢复到不一致的行为。在本文中，我们考虑了一个新的威胁，其中攻击者毒害的 RLHF 训练数据嵌入一个“越狱后门”到模型。后门将一个触发词嵌入到模型中，就像一个通用的“ sudo 命令”: 将触发词添加到任何提示中都可以实现有害的反应，而无需搜索对手的提示。通用越狱后门在语言模型上比以前研究过的后门要强大得多，而且我们发现使用通用后门攻击技术植入后门要困难得多。我们调查 RLHF 的设计决策，有助于其所谓的健壮性，并发布一个中毒模型的基准，以刺激未来的研究通用越狱后门。"
    },
    {
        "title": "Segment (Almost) Nothing: Prompt-Agnostic Adversarial Attacks on\n  Segmentation Models",
        "url": "http://arxiv.org/abs/2311.14450v1",
        "pub_date": "2023-11-24",
        "summary": "General purpose segmentation models are able to generate (semantic)\nsegmentation masks from a variety of prompts, including visual (points, boxed,\netc.) and textual (object names) ones. In particular, input images are\npre-processed by an image encoder to obtain embedding vectors which are later\nused for mask predictions. Existing adversarial attacks target the end-to-end\ntasks, i.e. aim at altering the segmentation mask predicted for a specific\nimage-prompt pair. However, this requires running an individual attack for each\nnew prompt for the same image. We propose instead to generate prompt-agnostic\nadversarial attacks by maximizing the $\\ell_2$-distance, in the latent space,\nbetween the embedding of the original and perturbed images. Since the encoding\nprocess only depends on the image, distorted image representations will cause\nperturbations in the segmentation masks for a variety of prompts. We show that\neven imperceptible $\\ell_\\infty$-bounded perturbations of radius\n$\\epsilon=1/255$ are often sufficient to drastically modify the masks predicted\nwith point, box and text prompts by recently proposed foundation models for\nsegmentation. Moreover, we explore the possibility of creating universal, i.e.\nnon image-specific, attacks which can be readily applied to any input without\nfurther computational cost.",
        "translated": "通用分割模型能够从各种提示中生成(语义)分割掩码，包括可视的(点，方框等)和文本的(对象名称)。特别是，输入图像由图像编码器进行预处理，以获得嵌入向量，这些向量后来用于掩码预测。现有的对抗性攻击针对端到端的任务，也就是说，目的是改变为特定的图像提示对预测的分割掩码。但是，这需要为同一图像的每个新提示运行单独的攻击。相反，我们提出通过最大化原始图像和扰动图像嵌入之间的潜在空间中的 $ell _ 2 $- 距离来产生提示不可知的对抗攻击。由于编码过程只依赖于图像，畸变的图像表示会对各种提示信息的分割掩码造成干扰。我们表明，即使是半径为 $epsilon = 1/255 $的不可察觉的 $ell _ infty $- 有界扰动，通常也足以根据最近提出的分割基础模型对点、框和文本提示预测的掩码进行彻底修改。此外，我们还探索了创建通用攻击(即非图像特定攻击)的可能性，这种攻击可以很容易地应用于任何输入，而不需要进一步的计算成本。"
    },
    {
        "title": "AI-based Attack Graph Generation",
        "url": "http://arxiv.org/abs/2311.14342v1",
        "pub_date": "2023-11-24",
        "summary": "With the advancement of IoT technology, many electronic devices are\ninterconnected through networks, communicating with each other and performing\nspecific roles. However, as numerous devices join networks, the threat of\ncyberattacks also escalates. Preventing and detecting cyber threats are\ncrucial, and one method of preventing such threats involves using attack\ngraphs. Attack graphs are widely used to assess security threats within\nnetworks. However, a drawback emerges as the network scales, as generating\nattack graphs becomes time-consuming. To overcome this limitation, artificial\nintelligence models can be employed. By utilizing AI models, attack graphs can\nbe created within a short period, approximating optimal outcomes. AI models\ndesigned for attack graph generation consist of encoders and decoders, trained\nusing reinforcement learning algorithms. After training the AI models, we\nconfirmed the model's learning effectiveness by observing changes in loss and\nreward values. Additionally, we compared attack graphs generated by the AI\nmodel with those created through conventional methods.",
        "translated": "随着物联网技术的发展，许多电子设备通过网络相互连接，相互通信，发挥特定的作用。然而，随着大量设备加入网络，网络攻击的威胁也在升级。预防和检测网络威胁是至关重要的，预防这类威胁的一种方法是使用攻击图表。攻击图被广泛用于评估网络中的安全威胁。但是，随着网络扩展，生成攻击图变得非常耗时，这就出现了一个缺点。为了克服这个限制，可以采用人工智能模型。利用人工智能模型，可以在短时间内建立攻击图，逼近最优结果。为生成攻击图而设计的人工智能模型由编码器和解码器组成，使用强化学习算法进行训练。通过对 AI 模型的训练，我们通过观察损失值和奖励值的变化来验证模型的学习效果。此外，我们还比较了人工智能模型生成的攻击图和传统方法生成的攻击图。"
    },
    {
        "title": "C-ITS Environment Modeling and Attack Modeling",
        "url": "http://arxiv.org/abs/2311.14327v1",
        "pub_date": "2023-11-24",
        "summary": "As technology advances, cities are evolving into smart cities, with the\nability to process large amounts of data and the increasing complexity and\ndiversification of various elements within urban areas. Among the core systems\nof a smart city is the Cooperative-Intelligent Transport Systems (C-ITS). C-ITS\nis a system where vehicles provide real-time information to drivers about\nsurrounding traffic conditions, sudden stops, falling objects, and other\naccident risks through roadside base stations. It consists of road\ninfrastructure, C-ITS centers, and vehicle terminals. However, as smart cities\nintegrate many elements through networks and electronic control, they are\nsusceptible to cybersecurity issues. In the case of cybersecurity problems in\nC-ITS, there is a significant risk of safety issues arising. This technical\ndocument aims to model the C-ITS environment and the services it provides, with\nthe purpose of identifying the attack surface where security incidents could\noccur in a smart city environment. Subsequently, based on the identified attack\nsurface, the document aims to construct attack scenarios and their respective\nstages. The document provides a description of the concept of C-ITS, followed\nby the description of the C-ITS environment model, service model, and attack\nscenario model defined by us.",
        "translated": "随着技术的进步，城市正在发展成为智能城市，有能力处理大量数据，城市地区内各种要素的复杂性和多样性日益增加。智能城市的核心系统之一是协同智能交通系统(C-ITS)。ITS 是一个系统，车辆通过路边基站向驾驶员提供有关周围交通状况、突然停车、坠落物体和其他事故风险的实时信息。它由道路基础设施、 C-ITS 中心和车辆终端组成。然而，由于智能城市通过网络和电子控制整合了许多元素，它们容易受到网络安全问题的影响。在 C-ITS 的网络安全问题的情况下，有一个重大的安全问题产生的风险。本技术文件旨在模拟 C-ITS 环境及其提供的服务，以确定在智能城市环境中可能发生安全事件的攻击表面。随后，基于已识别的攻击面，文档旨在构建攻击场景及其各个阶段。本文首先介绍了 C-ITS 的概念，然后介绍了我们定义的 C-ITS 环境模型、服务模型和攻击场景模型。"
    },
    {
        "title": "Local Differentially Private Heavy Hitter Detection in Data Streams with\n  Bounded Memory",
        "url": "http://arxiv.org/abs/2311.16062v1",
        "pub_date": "2023-11-27",
        "summary": "Top-$k$ frequent items detection is a fundamental task in data stream mining.\nMany promising solutions are proposed to improve memory efficiency while still\nmaintaining high accuracy for detecting the Top-$k$ items. Despite the memory\nefficiency concern, the users could suffer from privacy loss if participating\nin the task without proper protection, since their contributed local data\nstreams may continually leak sensitive individual information. However, most\nexisting works solely focus on addressing either the memory-efficiency problem\nor the privacy concerns but seldom jointly, which cannot achieve a satisfactory\ntradeoff between memory efficiency, privacy protection, and detection accuracy.\n  In this paper, we present a novel framework HG-LDP to achieve accurate\nTop-$k$ item detection at bounded memory expense, while providing rigorous\nlocal differential privacy (LDP) protection. Specifically, we identify two key\nchallenges naturally arising in the task, which reveal that directly applying\nexisting LDP techniques will lead to an inferior ``accuracy-privacy-memory\nefficiency'' tradeoff. Therefore, we instantiate three advanced schemes under\nthe framework by designing novel LDP randomization methods, which address the\nhurdles caused by the large size of the item domain and by the limited space of\nthe memory. We conduct comprehensive experiments on both synthetic and\nreal-world datasets to show that the proposed advanced schemes achieve a\nsuperior ``accuracy-privacy-memory efficiency'' tradeoff, saving $2300\\times$\nmemory over baseline methods when the item domain size is $41,270$. Our code is\nopen-sourced via the link.",
        "translated": "频繁项检测是数据流挖掘中的一项基本任务。提出了许多有前途的解决方案，以提高内存效率，同时仍然保持高精度的检测顶部 $k $项目。尽管存在内存效率问题，但如果用户在没有适当保护的情况下参与任务，他们可能会遭受隐私损失，因为他们贡献的本地数据流可能会不断泄露敏感的个人信息。然而，大多数现有的研究工作仅仅集中于解决存储效率问题或隐私问题，而很少联合进行，这不能在存储效率、隐私保护和检测准确性之间达到一个令人满意的平衡。在这篇文章中，我们提出了一个新的框架 HG-lDP 来实现精确的 Top-k $项检测，同时提供严格的局部差分隐私(lDP)保护。具体来说，我们确定了任务中自然产生的两个关键挑战，它们揭示了直接应用现有的 LDP 技术将导致较差的“准确-隐私-内存效率”折衷。因此，我们通过设计新的 LDP 随机化方法，在该框架下实例化了三种先进的方案，解决了项目域大和存储空间有限的问题。我们在合成数据集和真实数据集上进行了全面的实验，结果表明，当项目域大小为41,270美元时，提出的高级方案实现了更好的“精度-隐私-内存效率”折衷，比基线方法节省了2300美元的内存。我们的代码是通过链接开源的。"
    },
    {
        "title": "RIDE: Real-time Intrusion Detection via Explainable Machine Learning\n  Implemented in a Memristor Hardware Architecture",
        "url": "http://arxiv.org/abs/2311.16018v1",
        "pub_date": "2023-11-27",
        "summary": "Deep Learning (DL) based methods have shown great promise in network\nintrusion detection by identifying malicious network traffic behavior patterns\nwith high accuracy, but their applications to real-time, packet-level\ndetections in high-speed communication networks are challenging due to the high\ncomputation time and resource requirements of Deep Neural Networks (DNNs), as\nwell as lack of explainability. To this end, we propose a packet-level network\nintrusion detection solution that makes novel use of Recurrent Autoencoders to\nintegrate an arbitrary-length sequence of packets into a more compact joint\nfeature embedding, which is fed into a DNN-based classifier. To enable\nexplainability and support real-time detections at micro-second speed, we\nfurther develop a Software-Hardware Co-Design approach to efficiently realize\nthe proposed solution by converting the learned detection policies into\ndecision trees and implementing them using an emerging architecture based on\nmemristor devices. By jointly optimizing associated software and hardware\nconstraints, we show that our approach leads to an extremely efficient,\nreal-time solution with high detection accuracy at the packet level. Evaluation\nresults on real-world datasets (e.g., UNSW and CIC-IDS datasets) demonstrate\nnearly three-nines detection accuracy with a substantial speedup of nearly four\norders of magnitude.",
        "translated": "基于深度学习(Deep Learning，DL)的方法通过高精度识别恶意网络流量行为模式，在网络入侵检测方面显示出巨大的前景，但是由于深度神经网络(Deep neurNetworks，DNN)的高计算时间和资源需求，以及缺乏解释性，它们在高速通信网络中的实时、包级检测应用面临着挑战。为此，我们提出了一种分组级网络入侵检测方案，该方案利用循环自动编码器将任意长度的分组序列集成到一个更紧凑的联合特征嵌入中，并将其输入到基于 DNN 的分类器中。为了实现可解释性和支持微秒级的实时检测，我们进一步开发了一种软硬件协同设计方法，通过将学习到的检测策略转换为决策树，并使用基于记忆电阻器件的新兴架构来实现它们，从而有效地实现所提出的解决方案。通过联合优化相关的软件和硬件约束，我们表明，我们的方法导致了一个非常有效的，实时的解决方案，在数据包级别的高检测精度。对真实世界数据集(例如新南威尔士大学和 CIC-IDS 数据集)的评估结果表明，检测准确率接近3-9，大幅提高了近4个数量级。"
    },
    {
        "title": "Using Decentralized Aggregation for Federated Learning with Differential\n  Privacy",
        "url": "http://arxiv.org/abs/2311.16008v1",
        "pub_date": "2023-11-27",
        "summary": "Nowadays, the ubiquitous usage of mobile devices and networks have raised\nconcerns about the loss of control over personal data and research advance\ntowards the trade-off between privacy and utility in scenarios that combine\nexchange communications, big databases and distributed and collaborative (P2P)\nMachine Learning techniques. On the other hand, although Federated Learning\n(FL) provides some level of privacy by retaining the data at the local node,\nwhich executes a local training to enrich a global model, this scenario is\nstill susceptible to privacy breaches as membership inference attacks. To\nprovide a stronger level of privacy, this research deploys an experimental\nenvironment for FL with Differential Privacy (DP) using benchmark datasets. The\nobtained results show that the election of parameters and techniques of DP is\ncentral in the aforementioned trade-off between privacy and utility by means of\na classification example.",
        "translated": "如今，移动设备和网络的普遍使用引起了人们对个人数据失去控制的担忧，并且在交换通信、大型数据库和分布式协作(P2P)机器学习技术相结合的场景中，研究进展趋向于在隐私和实用性之间进行权衡。另一方面，尽管联邦学习(Federated Learning，FL)通过在本地节点保留数据(执行本地训练以丰富全局模型)来提供某种程度的隐私，但这种情况仍然容易受到成员推断攻击的隐私破坏。为了提供更强的隐私水平，这项研究部署了一个实验环境与差分隐私(DP)使用基准数据集。通过一个分类实例表明，DP 参数和技术的选择是上述隐私权和效用权衡的核心。"
    },
    {
        "title": "Microarchitectural Security of AWS Firecracker VMM for Serverless Cloud\n  Platforms",
        "url": "http://arxiv.org/abs/2311.15999v1",
        "pub_date": "2023-11-27",
        "summary": "Firecracker is a virtual machine manager (VMM) built by Amazon Web Services\n(AWS) for serverless cloud platforms, services that run code for end users on a\nper-task basis, automatically managing server infrastructure. Firecracker\nprovides fast and lightweight VMs and promises a combination of the speed of\ncontainers, typically used to isolate small tasks, and the security of VMs,\nwhich tend to provide greater isolation at the cost of performance. This\ncombination of security and efficiency, AWS claims, makes it not only possible\nbut safe to run thousands of user tasks from different users on the same\nhardware, with the host system frequently switching between active tasks.\nThough AWS states that microarchitectural attacks are included in their threat\nmodel, this class of attacks directly relies on shared hardware, just as the\nscalability of serverless computing relies on sharing hardware between\nunprecedented numbers of users. In this work, we investigate how secure\nFirecracker is against microarchitectural attacks. First, we review\nFirecracker's stated isolation model and recommended best practices for\ndeployment, identify potential threat models for serverless platforms, and\nanalyze potential weak points. Then, we use microarchitectural attack\nproof-of-concepts to test the isolation provided by Firecracker and find that\nit offers little protection against Spectre or MDS attacks. We discover two\nparticularly concerning cases: 1) a Medusa variant that threatens Firecracker\nVMs but not processes running outside them, and is not mitigated by defenses\nrecommended by AWS, and 2) a Spectre-PHT variant that remains exploitable even\nif recommended countermeasures are in place and SMT is disabled in the system.\nIn summary, we show that AWS overstates the security inherent to the\nFirecracker VMM and provides incomplete guidance for properly securing cloud\nsystems that use Firecracker.",
        "translated": "Fireacker 是 Amazon Web Services (AWS)为无服务器云平台构建的虚拟机管理器(VMM) ，这些服务根据每个任务为终端用户运行代码，自动管理服务器基础设施。Fireacker 提供了快速轻量级的虚拟机，并承诺将容器的速度(通常用于隔离小型任务)与虚拟机的安全性(通常以性能为代价提供更高的隔离)结合起来。AWS 声称，这种安全性和效率的结合使得在同一硬件上运行来自不同用户的数千个用户任务成为可能，而且是安全的，因为主机系统经常在活动任务之间切换。尽管 AWS 声明微架构攻击包含在其威胁模型中，但这类攻击直接依赖于共享硬件，就像无服务器计算的可伸缩性依赖于空前数量的用户之间共享硬件一样。在这项工作中，我们研究如何安全的鞭炮是对微架构的攻击。首先，我们回顾了 Fireacker 的隔离模型，并推荐了部署的最佳实践，识别了无服务器平台的潜在威胁模型，并分析了潜在的弱点。然后，我们使用微架构攻击概念验证来测试 Fireacker 提供的隔离，发现它对 Spectre 或 MDS 攻击提供的保护很少。我们发现了两个特别值得关注的案例: 1)一个美杜莎变体威胁鞭炮 VM，但不是在它们之外运行的进程，并且不受 AWS 推荐的防御措施的减轻，2)一个 Spectre-PHT 变体，即使推荐的对策已经到位，SMT 在系统中被禁用，仍然可以利用。总之，我们展示了 AWS 夸大了 Fireacker VMM 固有的安全性，并且为正确保护使用 Fireacker 的云系统提供了不完整的指导。"
    },
    {
        "title": "When Graph Convolution Meets Double Attention: Online Privacy Disclosure\n  Detection with Multi-Label Text Classification",
        "url": "http://arxiv.org/abs/2311.15917v1",
        "pub_date": "2023-11-27",
        "summary": "With the rise of Web 2.0 platforms such as online social media, people's\nprivate information, such as their location, occupation and even family\ninformation, is often inadvertently disclosed through online discussions.\nTherefore, it is important to detect such unwanted privacy disclosures to help\nalert people affected and the online platform. In this paper, privacy\ndisclosure detection is modeled as a multi-label text classification (MLTC)\nproblem, and a new privacy disclosure detection model is proposed to construct\nan MLTC classifier for detecting online privacy disclosures. This classifier\ntakes an online post as the input and outputs multiple labels, each reflecting\na possible privacy disclosure. The proposed presentation method combines three\ndifferent sources of information, the input text itself, the label-to-text\ncorrelation and the label-to-label correlation. A double-attention mechanism is\nused to combine the first two sources of information, and a graph convolutional\nnetwork (GCN) is employed to extract the third source of information that is\nthen used to help fuse features extracted from the first two sources of\ninformation. Our extensive experimental results, obtained on a public dataset\nof privacy-disclosing posts on Twitter, demonstrated that our proposed privacy\ndisclosure detection method significantly and consistently outperformed other\nstate-of-the-art methods in terms of all key performance indicators.",
        "translated": "随着在线社交媒体等 Web 2.0平台的兴起，人们的个人信息，如他们的位置、职业甚至家庭信息，常常在网上讨论中被不经意地泄露出来。因此，重要的是要发现这种不必要的隐私泄露，以帮助提醒受影响的人和在线平台。将隐私泄露检测建模为一个多标签文本分类(MLTC)问题，提出了一种新的隐私泄露检测模型，构造了一个 MLTC 分类器用于检测在线隐私泄露。这个分类器将一个在线帖子作为输入并输出多个标签，每个标签都反映了可能的隐私泄露。提出的表示方法结合了三种不同的信息来源: 输入文本本身、标签到文本的相关性和标签到标签的相关性。该方法采用双注意机制对前两个信息源进行融合，并利用图卷积网络(GCN)提取第三个信息源，然后利用这个信息源帮助融合从前两个信息源提取的特征。我们广泛的实验结果，获得了一个公开的数据集隐私披露的 Twitter 帖子，表明我们提出的隐私披露检测方法显着和一贯优于其他国家的最先进的方法在所有关键性能指标。"
    },
    {
        "title": "Towards Adaptive RF Fingerprint-based Authentication of IIoT devices",
        "url": "http://arxiv.org/abs/2311.15888v1",
        "pub_date": "2023-11-27",
        "summary": "As IoT technologies mature, they are increasingly finding their way into more\nsensitive domains, such as Medical and Industrial IoT, in which safety and\ncyber-security are of great importance. While the number of deployed IoT\ndevices continues to increase exponentially, they still present severe\ncyber-security vulnerabilities. Effective authentication is paramount to\nsupport trustworthy IIoT communications, however, current solutions focus on\nupper-layer identity verification or key-based cryptography which are often\ninadequate to the heterogeneous IIoT environment. In this work, we present a\nfirst step towards achieving powerful and flexible IIoT device authentication,\nby leveraging AI adaptive Radio Frequency Fingerprinting technique selection\nand tuning, at the PHY layer for highly accurate device authentication over\nchallenging RF environments.",
        "translated": "随着物联网技术的成熟，它们越来越多地进入更敏感的领域，如医疗和工业物联网，其中安全和网络安全是非常重要的。虽然部署的物联网设备数量继续呈指数增长，但它们仍然存在严重的网络安全漏洞。有效的身份验证对于支持可信的 IIoT 通信是至关重要的，然而，目前的解决方案主要集中在上层身份验证或基于密钥的加密技术，这对于异构的 IIoT 环境来说往往是不够的。在这项工作中，我们提出了实现强大和灵活的 IIoT 设备认证的第一步，利用人工智能自适应射频指纹技术的选择和调整，在物理层在具有挑战性的射频环境中进行高度精确的设备认证。"
    },
    {
        "title": "Learning with Errors over Group Rings Constructed by Semi-direct Product",
        "url": "http://arxiv.org/abs/2311.15868v1",
        "pub_date": "2023-11-27",
        "summary": "The Learning with Errors (LWE) problem has been widely utilized as a\nfoundation for numerous cryptographic tools over the years. In this study, we\nfocus on an algebraic variant of the LWE problem called Group ring LWE\n(GR-LWE). We select group rings (or their direct summands) that underlie\nspecific families of finite groups constructed by taking the semi-direct\nproduct of two cyclic groups. Unlike the Ring-LWE problem described in\n\\cite{lyubashevsky2010ideal}, the multiplication operation in the group rings\nconsidered here is non-commutative. As an extension of Ring-LWE, it maintains\ncomputational hardness and can be potentially applied in many cryptographic\nscenarios. In this paper, we present two polynomial-time quantum reductions.\nFirstly, we provide a quantum reduction from the worst-case shortest\nindependent vectors problem (SIVP) in ideal lattices with polynomial\napproximate factor to the search version of GR-LWE. This reduction requires\nthat the underlying group ring possesses certain mild properties; Secondly, we\npresent another quantum reduction for two types of group rings, where the\nworst-case SIVP problem is directly reduced to the (average-case) decision\nGR-LWE problem. The pseudorandomness of GR-LWE samples guaranteed by this\nreduction can be consequently leveraged to construct semantically secure\npublic-key cryptosystems.",
        "translated": "多年来，错误学习(LWE)问题已被广泛用作众多密码学工具的基础。在这项研究中，我们集中在一个代数变体的 LWE 问题称为群环 LWE (GR-LWE)。我们选择群环(或它们的直和)作为有限群的特定族的基础，这些特定族是由两个循环群的半直积构成的。与引文{ lyubashevsky2010理想}中描述的环-LWE 问题不同，这里所考虑的群环中的乘法运算是非交换的。作为 Ring-LWE 的一个扩展，它保持了计算的硬度，可以应用于许多加密场景。在本文中，我们提出了两个多项式时间量子约简。首先，我们提供了一个量子约简从最坏情况最短独立向量问题(SIVP)在理想格与多项式近似因子的搜索版本的 GR-LWE。这种约简要求下层群环具有一定的温和性质; 其次，我们对两类群环提出了另一种量子约简，其中最坏情况下的 SIVP 问题直接约简为(平均情况下)决策 GR-LWE 问题。因此，我们可以利用这个简化所保证的 GR-LWE 样本的伪随机数，来建构语义上安全的公开密码匙系统。"
    },
    {
        "title": "IPv6 Bitcoin-Certified Addresses",
        "url": "http://arxiv.org/abs/2311.15842v1",
        "pub_date": "2023-11-27",
        "summary": "A pivotal feature of IPv6 is its plug-and-play capability that enables hosts\nto integrate seamlessly into networks. In the absence of a trusted authority or\nsecurity infrastructure, the challenge for hosts is generating their own\naddress and verifying ownership of others. Cryptographically Generated\nAddresses (CGA) solves this problem by binding IPv6 addresses to hosts' public\nkeys to prove address ownership. CGA generation involves solving a\ncryptographic puzzle similar to Bitcoin's Proof-of-Work (PoW) to deter address\nspoofing. Unfortunately, solving the puzzle often causes undesirable address\ngeneration delays, which has hindered the adoption of CGA. In this paper, we\npresent Bitcoin-Certified Addresses (BCA), a new technique to bind IPv6\naddresses to hosts' public keys. BCA reduces the computational cost of\ngenerating addresses by using the PoW computed by Bitcoin nodes to secure the\nbinding. Compared to CGA, BCA provides better protection against spoofing\nattacks and improves the privacy of hosts. Due to the decentralized nature of\nthe Bitcoin network, BCA avoids reliance on a trusted authority, similar to\nCGA. BCA shows how the PoW computed by Bitcoin nodes can be reused, which saves\ncosts for hosts and makes Bitcoin mining more efficient.",
        "translated": "IPv6的一个关键特性是其即插即用功能，使主机能够无缝集成到网络中。在缺乏可信的权威或安全基础设施的情况下，主机面临的挑战是生成自己的地址并验证其他主机的所有权。加密生成地址(CGA)通过将 IPv6地址绑定到主机的公钥来证明地址所有权，从而解决了这个问题。CGA 生成涉及到解决一个类似于比特币的工作证明(PoW)的加密难题，以防止地址欺骗。遗憾的是，解决这个问题常常会导致不希望的地址生成延迟，这阻碍了 CGA 的采用。本文提出了一种将 IPv6地址绑定到主机公钥的新技术——比特币认证地址(BCA)。BCA 通过使用比特币节点计算的 PoW 来保证绑定的安全性，从而降低了生成地址的计算成本。与 CGA 相比，BCA 提供了更好的防欺骗攻击保护，提高了主机的隐私性。由于比特币网络的分散性，BCA 避免依赖于类似于 CGA 的可信授权。BCA 展示了如何重用比特币节点计算的 PoW，从而为主机节省成本，提高比特币挖掘的效率。"
    },
    {
        "title": "Ontologising Trustworthy in the Telecommunications Domain",
        "url": "http://arxiv.org/abs/2311.15839v1",
        "pub_date": "2023-11-27",
        "summary": "Based upon trusted and confidential computing platforms, telecommunications\nsystems must provide guaranteed security for the processes and data running\natop them. This in turn requires us to provide trustworthy systems. The term\ntrustworthy is poorly defined with corresponding misunderstanding and\nmisapplication. We present a definition of this term, as well as others,\ndemonstrate its application against certain telecommunications use cases and\naddress how the learnings from ontologising these structures contribute to\nstandardisation and the necessity for FAIR ontologies across telecommunications\nstandards and hosting organisations.",
        "translated": "基于可信和保密的计算平台，电信系统必须为在其上运行的进程和数据提供有保证的安全性。这反过来又要求我们提供可信赖的系统。“值得信赖”一词的定义不清楚，存在相应的误解和误用。我们提出了这个术语的定义，以及其他一些定义，展示了它在某些电信用例中的应用，并讨论了从本体化这些结构中获得的经验如何有助于标准化，以及跨电信标准和托管组织的 FAIR 本体的必要性。"
    },
    {
        "title": "Attacking at non-harmonic frequencies in screaming-channel attacks",
        "url": "http://arxiv.org/abs/2311.15832v1",
        "pub_date": "2023-11-27",
        "summary": "Screaming-channel attacks enable Electromagnetic (EM) Side-Channel Attacks\n(SCAs) at larger distances due to higher EM leakage energies than traditional\nSCAs, relaxing the requirement of close access to the victim. This attack can\nbe mounted on devices integrating Radio Frequency (RF) modules on the same die\nas digital circuits, where the RF can unintentionally capture, modulate,\namplify, and transmit the leakage along with legitimate signals. Leakage\nresults from digital switching activity, so the hypothesis of previous works\nwas that this leakage would appear at multiples of the digital clock frequency,\ni.e., harmonics. This work demonstrates that compromising signals appear not\nonly at the harmonics and that leakage at non-harmonics can be exploited for\nsuccessful attacks. Indeed, the transformations undergone by the leaked signal\nare complex due to propagation effects through the substrate and power and\nground planes, so the leakage also appears at other frequencies. We first\npropose two methodologies to locate frequencies that contain leakage and\ndemonstrate that it appears at non-harmonic frequencies. Then, our experimental\nresults show that screaming-channel attacks at non-harmonic frequencies can be\nas successful as at harmonics when retrieving a 16-byte AES key. As the RF\nspectrum is polluted by interfering signals, we run experiments and show\nsuccessful attacks in a more realistic, noisy environment where harmonic\nfrequencies are contaminated by multi-path fading and interference. These\nattacks at non-harmonic frequencies increase the attack surface by providing\nattackers with an increased number of potential frequencies where attacks can\nsucceed.",
        "translated": "尖叫信道攻击比传统的电磁信道攻击具有更高的电磁泄漏能量，使得电磁侧信道攻击具有更大的距离，从而放松了近距离接近受害者的要求。这种攻击可以安装在与数字电路集成无线电频率(RF)模块的设备上，在这种设备上，RF 可以无意中捕获、调制、放大和传输泄漏以及合法信号。泄漏是数字开关活动造成的，因此前人的假设是，这种泄漏会出现在数字时钟频率的倍数，即谐波。这项工作表明，泄漏信号不仅出现在谐波和非谐波泄漏可以利用成功的攻击。实际上，由于泄漏信号在基片、功率和接地平面上的传播效应，泄漏信号经历的变换是复杂的，因此泄漏信号也出现在其他频率上。我们首先提出两种方法来定位含有泄漏的频率，并证明它出现在非谐波频率。然后，我们的实验结果表明，尖叫信道攻击在非谐波频率可以一样成功的谐波时，检索一个16字节的 AES 密钥。由于射频频谱受到干扰信号的污染，我们在一个更真实、噪声更大的环境中进行了实验，实验结果表明，在多径衰落和干扰污染的谐波频率下，攻击是成功的。这些非谐波频率的攻击通过为攻击者提供更多的潜在频率来增加攻击的成功率。"
    },
    {
        "title": "Scalable Extraction of Training Data from (Production) Language Models",
        "url": "http://arxiv.org/abs/2311.17035v1",
        "pub_date": "2023-11-28",
        "summary": "This paper studies extractable memorization: training data that an adversary\ncan efficiently extract by querying a machine learning model without prior\nknowledge of the training dataset. We show an adversary can extract gigabytes\nof training data from open-source language models like Pythia or GPT-Neo,\nsemi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing\ntechniques from the literature suffice to attack unaligned models; in order to\nattack the aligned ChatGPT, we develop a new divergence attack that causes the\nmodel to diverge from its chatbot-style generations and emit training data at a\nrate 150x higher than when behaving properly. Our methods show practical\nattacks can recover far more data than previously thought, and reveal that\ncurrent alignment techniques do not eliminate memorization.",
        "translated": "本文研究了可提取记忆: 对手可以通过查询机器学习模型有效地提取训练数据，而不需要知道训练数据集的先验知识。我们展示了对手可以从开源语言模型(如 Pythia 或 GPT-Neo)、半开放模型(如 LLaMA 或 Falcon)和封闭模型(如 ChatGPT)中提取千兆字节的训练数据。文献中的现有技术足以攻击未对齐的模型; 为了攻击对齐的 ChatGPT，我们开发了一种新的发散攻击，这种攻击会导致模型偏离其聊天机器人风格的代数，并以比正常运行时高出150倍的速率发出训练数据。我们的方法表明，实际的攻击可以恢复比以前认为的更多的数据，并揭示了目前的对齐技术并没有消除记忆。"
    },
    {
        "title": "When the Few Outweigh the Many: Illicit Content Recognition with\n  Few-Shot Learning",
        "url": "http://arxiv.org/abs/2311.17026v1",
        "pub_date": "2023-11-28",
        "summary": "The anonymity and untraceability benefits of the Dark web account for the\nexponentially-increased potential of its popularity while creating a suitable\nwomb for many illicit activities, to date. Hence, in collaboration with\ncybersecurity and law enforcement agencies, research has provided approaches\nfor recognizing and classifying illicit activities with most exploiting textual\ndark web markets' content recognition; few such approaches use images that\noriginated from dark web content. This paper investigates this alternative\ntechnique for recognizing illegal activities from images. In particular, we\ninvestigate label-agnostic learning techniques like One-Shot and Few-Shot\nlearning featuring the use Siamese neural networks, a state-of-the-art approach\nin the field. Our solution manages to handle small-scale datasets with\npromising accuracy. In particular, Siamese neural networks reach 90.9% on\n20-Shot experiments over a 10-class dataset; this leads us to conclude that\nsuch models are a promising and cheaper alternative to the definition of\nautomated law-enforcing machinery over the dark web.",
        "translated": "到目前为止，暗网的匿名性和不可追踪性的好处使其受欢迎的潜力呈指数级增长，同时为许多非法活动创造了一个合适的子宫。因此，通过与网络安全和执法机构合作，研究已经提供了识别和分类非法活动的方法，其中大多数利用文本暗网市场的内容识别; 很少有这样的方法使用来自暗网内容的图像。本文研究了这种从图像中识别非法活动的替代技术。特别是，我们研究标签不可知学习技术，如一次性和 Few-Shot 学习使用暹罗神经网络，在该领域的最先进的方法。我们的解决方案设法处理小规模的数据集与承诺的准确性。特别是，暹罗神经网络在10类数据集的20-Shot 实验中达到了90.9% ; 这使我们得出结论，这样的模型是一个有前途的和更便宜的替代定义的自动执法机器在暗网上。"
    },
    {
        "title": "Message Recovery Attack in NTRU through VFK Lattices",
        "url": "http://arxiv.org/abs/2311.17022v1",
        "pub_date": "2023-11-28",
        "summary": "In the present paper, we implement a message recovery attack to all variants\nof the NTRU cryptosystem. Our approach involves a reduction from the\nNTRU-lattice to a Voronoi First Kind lattice, enabling the application of a\npolynomial CVP exact algorithm crucial for executing the Message Recovery. The\nefficacy of our attack relies on a specific oracle that permits us to\napproximate an unknown quantity. Furthermore, we outline the mathematical\nconditions under which the attack is successful. Finally, we delve into a\nwell-established polynomial algorithm for CVP on VFK lattices and its\nimplementation, shedding light on its efficacy in our attack. Subsequently, we\npresent comprehensive experimental results on the NTRU-HPS and the NTRU-Prime\nvariants of the NIST submissions and propose a method that could indicate the\nresistance of the NTRU cryptosystem to our attack.",
        "translated": "在本文中，我们实现了一个针对所有 NTRU 密码体制变种的消息恢复攻击。我们的方法包括从 NTRU 格到 Voronoi 第一类格的简化，使得应用多项式 CVP 精确算法对于执行消息恢复至关重要。我们攻击的效果依赖于一个特定的神谕，它允许我们接近一个未知的数量。此外，我们概述了攻击成功的数学条件。最后，我们深入研究了 VFK 格上 CVP 的多项式算法及其实现，阐明了其在攻击中的有效性。随后，我们对 NIST 提交的 NTRU-HPS 和 NTRU-Prime 变体进行了全面的实验结果，并提出了一种可以表明 NTRU 密码系统抵抗我们攻击的方法。"
    },
    {
        "title": "Counter-terrorism in cyber-physical spaces: Best practices and\n  technologies from the state of the art",
        "url": "http://arxiv.org/abs/2311.17012v1",
        "pub_date": "2023-11-28",
        "summary": "Context: The demand for protection and security of physical spaces and urban\nareas increased with the escalation of terroristic attacks in recent years. We\nenvision with the proposed cyber-physical systems and spaces, a city that would\nindeed become a smarter urbanistic object, proactively providing alerts and\nbeing protective against any threat. Objectives: This survey intend to provide\na systematic multivocal literature survey comprised of an updated,\ncomprehensive and timely overview of state of the art in counter-terrorism\ncyber-physical systems, hence aimed at the protection of cyber-physical spaces.\nHence, provide guidelines to law enforcement agencies and practitioners\nproviding a description of technologies and best practices for the protection\nof public spaces. Methods: We analyzed 112 papers collected from different\nonline sources, both from the academic field and from websites and blogs\nranging from 2004 till mid-2022. Results: a) There is no one single\nbullet-proof solution available for the protection of public spaces. b) From\nour analysis we found three major active fields for the protection of public\nspaces: Information Technologies, Architectural approaches, Organizational\nfield. c) While the academic suggest best practices and methodologies for the\nprotection of urban areas, the market did not provide any type of\nimplementation of such suggested approaches, which shows a lack of\nfertilization between academia and industry. Conclusion: The overall analysis\nhas led us to state that there is no one single solution available, conversely,\nmultiple methods and techniques can be put in place to guarantee safety and\nsecurity in public spaces. The techniques range from architectural design to\nrethink the design of public spaces keeping security into account in\ncontinuity, to emerging technologies such as AI and predictive surveillance.",
        "translated": "背景: 近年来，随着恐怖主义袭击的升级，对有形空间和城市地区的保护和安全的需求增加。我们设想的网络物理系统和空间，一个城市将确实成为一个更聪明的城市化对象，主动提供警报和防范任何威胁。目标: 本调查旨在提供一个系统的多方文献调查，包括对反恐网络物理系统的最新、全面和及时的现状进行概述，因此旨在保护网络物理空间。因此，向执法机构和从业人员提供指导方针，说明保护公共空间的技术和最佳做法。方法: 对2004年至2022年中期从学术领域、网站、博客等不同来源收集的112篇论文进行分析。结果: a)没有一个单一的防弹解决方案可用于保护公共空间。从我们的分析，我们发现三个主要的活跃领域保护公共空间: 信息技术，建筑方法，组织领域。(c)虽然学术界提出了保护城市地区的最佳做法和方法，但市场没有提供任何类型的实施这些建议的方法，这表明学术界和工业界之间缺乏肥料。结论: 总体分析表明，没有单一的解决方案，相反，可以采取多种方法和技术来保证公共场所的安全和安保。这些技术包括从建筑设计到重新考虑公共空间的设计以保持安全的连续性，再到人工智能和预测性监视等新兴技术。"
    },
    {
        "title": "FP-Fed: Privacy-Preserving Federated Detection of Browser Fingerprinting",
        "url": "http://arxiv.org/abs/2311.16940v1",
        "pub_date": "2023-11-28",
        "summary": "Browser fingerprinting often provides an attractive alternative to\nthird-party cookies for tracking users across the web. In fact, the increasing\nrestrictions on third-party cookies placed by common web browsers and recent\nregulations like the GDPR may accelerate the transition. To counter browser\nfingerprinting, previous work proposed several techniques to detect its\nprevalence and severity. However, these rely on 1) centralized web crawls\nand/or 2) computationally intensive operations to extract and process signals\n(e.g., information-flow and static analysis). To address these limitations, we\npresent FP-Fed, the first distributed system for browser fingerprinting\ndetection. Using FP-Fed, users can collaboratively train on-device models based\non their real browsing patterns, without sharing their training data with a\ncentral entity, by relying on Differentially Private Federated Learning\n(DP-FL). To demonstrate its feasibility and effectiveness, we evaluate FP-Fed's\nperformance on a set of 18.3k popular websites with different privacy levels,\nnumbers of participants, and features extracted from the scripts. Our\nexperiments show that FP-Fed achieves reasonably high detection performance and\ncan perform both training and inference efficiently, on-device, by only relying\non runtime signals extracted from the execution trace, without requiring any\nresource-intensive operation.",
        "translated": "浏览器指纹识别通常是第三方 cookie 的一个有吸引力的替代品，用于在网络上跟踪用户。事实上，普通网络浏览器对第三方 cookie 的限制越来越多，最近 GDPR 等法规可能会加速这种转变。为了对抗浏览器指纹识别，以前的工作提出了几种技术来检测其普遍性和严重性。然而，这些依赖于1)集中的网络抓取和/或2)计算密集型操作来提取和处理信号(例如，信息流和静态分析)。为了解决这些限制，我们提出 FP-Fed，第一个用于浏览器指纹检测的分布式系统。使用 FP-Fed，用户可以通过差异私有联邦学习(DP-FL) ，基于他们真实的浏览模式协同训练设备上的模型，而无需与一个中心实体共享他们的训练数据。为了证明其可行性和有效性，我们评估了 FP-Fed 在一组18.3 k 流行网站上的表现，这些网站具有不同的隐私水平、参与者数量和从脚本中提取的特征。我们的实验表明，FP-Fed 能够取得相当高的检测性能，并且能够在设备上有效地进行训练和推理，仅仅依靠从执行跟踪中提取的运行时信号，而不需要任何资源密集型操作。"
    },
    {
        "title": "A Direct Lazy Sampling Proof Technique in Probabilistic Relational Hoare\n  Logic",
        "url": "http://arxiv.org/abs/2311.16844v1",
        "pub_date": "2023-11-28",
        "summary": "Programs using random values can either make all choices in advance (eagerly)\nor sample as needed (lazily). In formal proofs, we focus on\nindistinguishability between two lazy programs, a common requirement in the\nrandom oracle model (ROM). While rearranging sampling instructions often solves\nthis, it gets complex when sampling is spread across procedures. The\ntraditional approach, introduced by Bellare and Rogaway in 2004, converts\nprograms to eager sampling, but requires assuming finite memory, a polynomial\nbound, and artificial resampling functions. We introduce a novel approach in\nprobabilistic Relational Hoare Logic (pRHL) that directly proves\nindistinguishability, eliminating the need for conversions and the mentioned\nassumptions. We also implement this approach in the EasyCrypt theorem prover,\nshowing that it can be a convenient alternative to the traditional method.",
        "translated": "使用随机值的程序可以提前(热切地)做出所有选择，也可以根据需要(懒惰地)进行采样。在形式证明中，我们关注两个惰性程序之间的不可区分性，这是随机预言模型(ROM)中的一个常见要求。虽然重新排列抽样指令通常可以解决这个问题，但是当抽样分布在不同的过程中时，问题就变得复杂了。Bellare 和 Rogaway 在2004年引入的传统方法将程序转换为即时采样，但需要假设有限内存、多项式界和人工重采样函数。我们在概率关系霍尔逻辑(pRHL)中引入了一种新的方法，直接证明了不可区分性，消除了转换的需要和上述假设。我们在 EasyCrypt 定理证明中也实现了这种方法，表明它可以成为传统方法的一种方便的替代方法。"
    },
    {
        "title": "Blockchain-based Zero Trust on the Edge",
        "url": "http://arxiv.org/abs/2311.16744v1",
        "pub_date": "2023-11-28",
        "summary": "Internet of Things (IoT) devices pose significant security challenges due to\ntheir heterogeneity (i.e., hardware and software) and vulnerability to\nextensive attack surfaces. Today's conventional perimeter-based systems use\ncredential-based authentication (e.g., username/password, certificates, etc.)\nto decide whether an actor can access a network. However, the verification\nprocess occurs only at the system's perimeter because most IoT devices lack\nrobust security measures due to their limited hardware and software\ncapabilities, making them highly vulnerable. Therefore, this paper proposes a\nnovel approach based on Zero Trust Architecture (ZTA) extended with blockchain\nto further enhance security. The blockchain component serves as an immutable\ndatabase for storing users' requests and is used to verify trustworthiness by\nanalyzing and identifying potentially malicious user activities. We discuss the\nframework, processes of the approach, and the experiments carried out on a\ntestbed to validate its feasibility and applicability in the smart city\ncontext. Lastly, the evaluation focuses on non-functional properties such as\nperformance, scalability, and complexity.",
        "translated": "物联网(IoT)设备由于其异构性(即硬件和软件)以及易受广泛攻击的弱点，构成了重大的安全挑战。当今传统的基于外围的系统使用基于凭证的身份验证(例如，用户名/密码、证书等)来决定参与者是否可以访问网络。然而，验证过程只发生在系统的外围，因为大多数物联网设备缺乏强大的安全措施，由于其有限的硬件和软件能力，使他们高度脆弱。为此，本文提出了一种基于区块链扩展的零信任体系结构(ZTA)来进一步提高安全性的新方法。区块链组件作为存储用户请求的不可变数据库，用于通过分析和识别潜在的恶意用户活动来验证可信度。我们讨论了该方法的框架、过程和在试验台上进行的实验，以验证其在智能城市环境中的可行性和适用性。最后，评估的重点是非功能性属性，比如性能、可伸缩性和复杂性。"
    },
    {
        "title": "A Unified Hardware-based Threat Detector for AI Accelerators",
        "url": "http://arxiv.org/abs/2311.16684v1",
        "pub_date": "2023-11-28",
        "summary": "The proliferation of AI technology gives rise to a variety of security\nthreats, which significantly compromise the confidentiality and integrity of AI\nmodels and applications. Existing software-based solutions mainly target one\nspecific attack, and require the implementation into the models, rendering them\nless practical. We design UniGuard, a novel unified and non-intrusive detection\nmethodology to safeguard FPGA-based AI accelerators. The core idea of UniGuard\nis to harness power side-channel information generated during model inference\nto spot any anomaly. We employ a Time-to-Digital Converter to capture power\nfluctuations and train a supervised machine learning model to identify various\ntypes of threats. Evaluations demonstrate that UniGuard can achieve 94.0%\nattack detection accuracy, with high generalization over unknown or adaptive\nattacks and robustness against varied configurations (e.g., sensor frequency\nand location).",
        "translated": "人工智能技术的普及带来了各种各样的安全威胁，严重损害了人工智能模型和应用程序的保密性和完整性。现有的基于软件的解决方案主要针对一种特定的攻击，并且需要在模型中实现，这使得它们不太实用。我们设计了 UniGuard，一种新颖的统一非侵入式检测方法来保护基于 FPGA 的人工智能加速器。UniGuard 的核心思想是利用模型推理过程中产生的电力侧信道信息来发现任何异常。我们使用一个时间到数字转换器来捕捉功率波动，并训练一个监督式学习模型来识别各种类型的威胁。评估表明，UniGuard 可以达到94.0% 的攻击检测准确率，具有对未知或自适应攻击的高泛化能力和对不同配置(例如，传感器频率和位置)的鲁棒性。"
    },
    {
        "title": "Unclonable Cryptography in the Plain Model",
        "url": "http://arxiv.org/abs/2311.16663v1",
        "pub_date": "2023-11-28",
        "summary": "By leveraging the no-cloning principle of quantum mechanics, unclonable\ncryptography enables us to achieve novel cryptographic protocols that are\notherwise impossible classically. Two most notable examples of unclonable\ncryptography are quantum copy-protection and unclonable encryption. Despite\nreceiving a lot of attention in recent years, two important open questions\nstill remain: copy-protection for point functions in the plain model, which is\nusually considered as feasibility demonstration, and unclonable encryption with\nunclonable indistinguishability security in the plain model.\n  In this work, by relying on previous works of Coladangelo, Liu, Liu, and\nZhandry (Crypto'21) and Culf and Vidick (Quantum'22), we establish a new\nmonogamy-of-entanglement property for subspace coset states, which allows us to\nobtain the following new results:\n  - We show that copy-protection of point functions exists in the plain model,\nwith different challenge distributions (including arguably the most natural\nones).\n  - We show, for the first time, that unclonable encryption with unclonable\nindistinguishability security exists in the plain model.",
        "translated": "通过利用量子力学的不可克隆原则，不可克隆加密技术使我们能够实现传统上不可能实现的新型加密协议。不可克隆加密的两个最著名的例子是量子拷贝保护和不可克隆加密。尽管近年来受到了广泛的关注，但仍然存在两个重要的未解决的问题: 普通模型中点函数的拷贝保护(通常被认为是可行性论证)和普通模型中不可克隆加密和不可克隆不可区分安全。在这项工作中，我们依据 Coladangelo，Liu，Liu，and Zhandry (Crypto’21)和 Culf and Vidick (Quantum’22)的著作，建立了一个新的子空间陪集态的一夫一妻纠缠性质，这使我们得到以下新的结果:-我们证明了点函数的复制保护存在于平原模型中，具有不同的挑战分布(包括可以说是最自然的)。我们第一次证明了普通模型中存在不可克隆的加密和不可区分的安全性。"
    },
    {
        "title": "Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method\n  Perspective",
        "url": "http://arxiv.org/abs/2311.16646v1",
        "pub_date": "2023-11-28",
        "summary": "Dataset distillation offers a potential means to enhance data efficiency in\ndeep learning. Recent studies have shown its ability to counteract backdoor\nrisks present in original training samples. In this study, we delve into the\ntheoretical aspects of backdoor attacks and dataset distillation based on\nkernel methods. We introduce two new theory-driven trigger pattern generation\nmethods specialized for dataset distillation. Following a comprehensive set of\nanalyses and experiments, we show that our optimization-based trigger design\nframework informs effective backdoor attacks on dataset distillation. Notably,\ndatasets poisoned by our designed trigger prove resilient against conventional\nbackdoor attack detection and mitigation methods. Our empirical results\nvalidate that the triggers developed using our approaches are proficient at\nexecuting resilient backdoor attacks.",
        "translated": "数据集提取为提高深度学习中的数据效率提供了一种潜在的手段。最近的研究表明，它有能力抵消原始训练样本中存在的后门风险。在本研究中，我们深入研究了基于核方法的后门攻击和数据集提取的理论方面。我们介绍了两种新的理论驱动的触发模式生成方法，专门用于数据集精馏。通过一系列全面的分析和实验，我们发现我们的基于优化的触发器设计框架为数据集提取提供了有效的后门攻击。值得注意的是，我们设计的触发器毒害的数据集证明了对传统的后门攻击检测和缓解方法的弹性。我们的实验结果验证了使用我们的方法开发的触发器能够很好地执行弹性后门攻击。"
    },
    {
        "title": "The Symmetric alpha-Stable Privacy Mechanism",
        "url": "http://arxiv.org/abs/2311.17789v1",
        "pub_date": "2023-11-29",
        "summary": "With the rapid growth of digital platforms, there is increasing apprehension\nabout how personal data is being collected, stored, and used by various\nentities. These concerns range from data breaches and cyber-attacks to\npotential misuse of personal information for targeted advertising and\nsurveillance. As a result, differential privacy (DP) has emerged as a prominent\ntool for quantifying a system's level of protection. The Gaussian mechanism is\ncommonly used because the Gaussian density is closed under convolution, a\ncommon method utilized when aggregating datasets. However, the Gaussian\nmechanism only satisfies approximate differential privacy. In this work, we\npresent novel analysis of the Symmetric alpha-Stable (SaS) mechanism. We prove\nthat the mechanism is purely differentially private while remaining closed\nunder convolution. From our analysis, we believe the SaS Mechanism is an\nappealing choice for privacy focused applications.",
        "translated": "随着数字平台的快速发展，人们越来越担心个人数据是如何被各种实体收集、存储和使用的。这些担忧包括数据泄露和网络攻击，以及有针对性的广告和监控可能滥用个人信息。因此，差分隐私(DP)已经成为量化系统保护水平的重要工具。高斯机制是常用的，因为高斯密度是封闭的卷积下，一种常见的方法用于聚集数据集。然而，高斯机制只能满足近似差分隐私。在这项工作中，我们提出了新的对称 α 稳定(SaS)机制的分析。我们证明了该机制是纯微分私有的，但在卷积情况下仍然是闭的。根据我们的分析，我们相信 SaS 机制对于专注于隐私的应用程序来说是一个很有吸引力的选择。"
    },
    {
        "title": "Addressing Membership Inference Attack in Federated Learning with Model\n  Compression",
        "url": "http://arxiv.org/abs/2311.17750v1",
        "pub_date": "2023-11-29",
        "summary": "Federated Learning (FL) has been proposed as a privacy-preserving solution\nfor machine learning. However, recent works have shown that Federated Learning\ncan leak private client data through membership attacks. In this paper, we show\nthat the effectiveness of these attacks on the clients negatively correlates\nwith the size of the client datasets and model complexity. Based on this\nfinding, we propose model-agnostic Federated Learning as a privacy-enhancing\nsolution because it enables the use of models of varying complexity in the\nclients. To this end, we present $\\texttt{MaPP-FL}$, a novel privacy-aware FL\napproach that leverages model compression on the clients while keeping a full\nmodel on the server. We compare the performance of $\\texttt{MaPP-FL}$ against\nstate-of-the-art model-agnostic FL methods on the CIFAR-10, CIFAR-100, and\nFEMNIST vision datasets. Our experiments show the effectiveness of\n$\\texttt{MaPP-FL}$ in preserving the clients' and the server's privacy while\nachieving competitive classification accuracies.",
        "translated": "联邦学习(FL)作为一种保护隐私的机器学习解决方案已经被提出。然而，最近的研究表明，联邦学习可以通过成员资格攻击泄露私人客户端数据。在本文中，我们证明了这些对客户端攻击的有效性与客户端数据集的大小和模型复杂度呈负相关。基于这一发现，我们提出了模型无关联邦学习作为一种隐私增强解决方案，因为它允许在客户端使用不同复杂度的模型。为此，我们提出了 $texttt { MaPP-FL } $，这是一种新颖的隐私感知 FL 方法，它在客户机上利用模型压缩，同时在服务器上保持完整的模型。我们比较了 $texttt { MaPP-FL } $与 CIFAR-10，CIFAR-100和 FEMNIST 视觉数据集上最先进的模型不可知 FL 方法的性能。我们的实验显示了 $texttt { MaPP-FL } $在保护客户端和服务器的隐私同时达到竞争性分类精度的有效性。"
    },
    {
        "title": "RACED: Routing in Payment Channel Networks Using Distributed Hash Tables",
        "url": "http://arxiv.org/abs/2311.17668v1",
        "pub_date": "2023-11-29",
        "summary": "The Bitcoin scalability problem has led to the development of off-chain\nfinancial mechanisms such as payment channel networks (PCNs) which help users\nprocess transactions of varying amounts, including micro-payment transactions,\nwithout writing each transaction to the blockchain. Since PCNs only allow\npath-based transactions, effective, secure routing protocols that find a path\nbetween a sender and receiver are fundamental to PCN operations. In this paper,\nwe propose RACED, a routing protocol that leverages the idea of Distributed\nHash Tables (DHTs) to route transactions in PCNs in a fast and secure way. Our\nexperiments on real-world transaction datasets show that RACED gives an average\ntransaction success ratio of 98.74%, an average pathfinding time of 31.242\nseconds, which is $1.65*10^3$, $1.8*10^3$, and $4*10^2$ times faster than three\nother recent routing protocols that offer comparable security/privacy\nproperties. We rigorously analyze and prove the security of RACED in the\nUniversal Composability framework.",
        "translated": "比特币的可扩展性问题导致了支付渠道网络(PCN)等非链式金融机制的发展，这些机制帮助用户处理不同金额的交易，包括小额支付交易，而无需将每笔交易写入区块链。由于 PCN 只允许基于路径的事务，因此在发送方和接收方之间找到路径的有效、安全的路由协议是 PCN 操作的基础。在本文中，我们提出了 RACED，这是一个利用分布式哈希表(dHT)思想的路由协议，以一种快速和安全的方式在 PCN 中路由事务。我们在真实世界事务数据集上的实验表明，RACED 给出的平均事务成功率为98.74% ，平均寻路时间为31.242秒，即 $1.65 * 10 ^ 3 $，$1.8 * 10 ^ 3 $和 $4 * 10 ^ 2 $比其他三种最近提供可比安全/隐私属性的路由协议快一倍。我们在通用可组合性框架下严谨地分析和证明了 RACED 的安全性。"
    },
    {
        "title": "Q-learning Based Optimal False Data Injection Attack on Probabilistic\n  Boolean Control Networks",
        "url": "http://arxiv.org/abs/2311.17631v1",
        "pub_date": "2023-11-29",
        "summary": "In this paper, we present a reinforcement learning (RL) method for solving\noptimal false data injection attack problems in probabilistic Boolean control\nnetworks (PBCNs) where the attacker lacks knowledge of the system model.\nSpecifically, we employ a Q-learning (QL) algorithm to address this problem. We\nthen propose an improved QL algorithm that not only enhances learning\nefficiency but also obtains optimal attack strategies for large-scale PBCNs\nthat the standard QL algorithm cannot handle. Finally, we verify the\neffectiveness of our proposed approach by considering two attacked PBCNs,\nincluding a 10-node network and a 28-node network.",
        "translated": "在这篇文章中，我们提出了一个强化学习(RL)方法来解决概率布尔控制网络(PBCNs)中的最优虚假数据注入攻击问题，当攻击者缺乏系统模型的知识时。具体来说，我们使用了 Q- 学习(QL)算法来解决这个问题。然后，我们提出了一种改进的 QL 算法，不仅提高了学习效率，而且获得了标准 QL 算法无法处理的大规模 PBCNs 的最优攻击策略。最后，通过考虑10节点网络和28节点网络这两个受攻击的 PBCNs，验证了该方法的有效性。"
    },
    {
        "title": "sec-certs: Examining the security certification practice for better\n  vulnerability mitigation",
        "url": "http://arxiv.org/abs/2311.17603v1",
        "pub_date": "2023-11-29",
        "summary": "Products certified under security certification frameworks such as Common\nCriteria undergo significant scrutiny during the costly certification process.\nYet, critical vulnerabilities, including private key recovery (ROCA, Minerva,\nTPM-Fail...), get discovered in certified products with high assurance levels.\nFurthermore, assessing which certified products are impacted by such\nvulnerabilities is complicated due to the large amount of unstructured\ncertification-related data and unclear relationships between the certificates.\nTo address these problems, we conducted a large-scale automated analysis of\nCommon Criteria and FIPS 140 certificates. We trained unsupervised models to\nlearn which vulnerabilities from NIST's National Vulnerability Database impact\nexisting certified products and how certified products reference each other.\nOur tooling automates the analysis of tens of thousands of\ncertification-related documents, extracting machine-readable features where\nmanual analysis is unattainable. Further, we identify the security requirements\nthat are associated with products being affected by fewer and less severe\nvulnerabilities (on average). This indicates which aspects of certification\ncorrelate with higher security. We demonstrate how our tool can be used for\nbetter vulnerability mitigation on four case studies of known, high-profile\nvulnerabilities. All tools and continuously updated results are available at\nhttps://seccerts.org.",
        "translated": "在费用高昂的认证过程中，通用标准等安全认证框架认证的产品要接受严格的审查。然而，关键的漏洞，包括私钥恢复(ROCA，密涅瓦，tPM-Fail...) ，会在高保证水平的认证产品中被发现。此外，由于大量与认证有关的非结构化数据以及证书之间的关系不明确，评估哪些认证产品受到此类漏洞的影响是复杂的。为了解决这些问题，我们对 Common Criteria 和 FIPS 140证书进行了大规模的自动化分析。我们对无监督模型进行了培训，以了解 NIST 的国家脆弱性数据库中哪些漏洞会影响现有的认证产品，以及认证产品如何相互参照。我们的工具可以自动分析成千上万的认证相关文档，在无法进行手工分析的地方提取机器可读的特性。此外，我们确定与受到越来越少和越来越严重的漏洞(平均)影响的产品相关的安全需求。这表明认证的哪些方面与更高的安全性相关。我们展示了如何使用我们的工具来更好地缓解已知的、引人注目的漏洞的四个案例研究。所有工具和不断更新的结果可在 https://seccerts.org 获得。"
    },
    {
        "title": "Emergent Outcomes of the veToken Model",
        "url": "http://arxiv.org/abs/2311.17589v1",
        "pub_date": "2023-11-29",
        "summary": "Decentralised organisations use blockchains as a basis for governance: they\nuse on-chain transactions to allocate voting weight, publish proposals, cast\nvotes, and enact the results.\n  However, blockchain-based governance structures have challenges, mostly\nnotably, the need to align the short-term outlook of pseudononymous voters with\nthe long-term growth and success of the decentralised organisation. The\nVote-Escrowed Token (veToken) model attempts to resolve this tension by\nrequiring voters to escrow or lock tokens of value for an extended period in\nexchange for voting weight.\n  In this paper, we describe the veToken model and analyse its emergent\noutcomes. We show that voting behaviour follows bribes set by higher-level\nprotocols, and that the cost per vote varies depending on how it is acquired.\nWe describe the implementation of the veToken model by Curve Finance, a popular\nautomated market maker for stablecoins, and the ecosystem of protocols that has\narisen on top of this implementation. We show that voting markets such as\nVotium largely determine the outcome of fortnightly votes held by Convex\nFinance, and we show that Frax Finance, a stablecoin issuer, plays a central\nrole in the ecosystem even though they directly lock relatively few tokens with\nCurve. Instead, they indirectly lock tokens through yield aggregators such as\nConvex Finance and purchase voting weight through voting markets such as\nVotium.\n  Although the veToken model in isolation is straight-forward and easily\nexplained, it leads to many complex and emergent outcomes. Decentralised\norganisations should consider these outcomes before adopting the model.",
        "translated": "分散的组织使用区块链作为治理的基础: 他们使用在链上的交易来分配投票权重，发布提案，投票，并制定结果。然而，基于区块链的治理结构面临挑战，主要是需要将匿名选民的短期前景与分权组织的长期增长和成功结合起来。投票代管令牌(veToken)模型试图通过要求投票人代管或锁定价值令牌以换取投票权来解决这种紧张关系。在本文中，我们描述了 veToken 模型并分析了它的紧急结果。我们表明，投票行为遵循更高层次协议设定的贿赂，每次投票的成本取决于获得投票的方式。我们描述了 curleFinance 对 veToken 模型的实现，这是一个流行的稳定币自动化做市商，以及在此实现之上出现的协议生态系统。我们展示了投票市场，比如 Vodium，在很大程度上决定了 Convex Finance 每两周一次的投票结果。我们还展示了 Frax Finance，一个稳定币发行商，在生态系统中扮演着核心角色，尽管他们直接锁定了相对较少的曲线代币。相反，它们通过 Convex Finance 等收益率聚合器间接锁定代币，并通过 Vodium 等投票市场购买投票权重。尽管单独的 veToken 模型很简单，也很容易解释，但是它会导致许多复杂和紧急的结果。分散的组织在采用模型之前应该考虑这些结果。"
    },
    {
        "title": "Data Driven Approaches to Cybersecurity Governance for Board\n  Decision-Making -- A Systematic Review",
        "url": "http://arxiv.org/abs/2311.17578v1",
        "pub_date": "2023-11-29",
        "summary": "Cybersecurity governance influences the quality of strategic decision-making\nto ensure cyber risks are managed effectively. Board of Directors are the\ndecisions-makers held accountable for managing this risk; however, they lack\nadequate and efficient information necessary for making such decisions. In\naddition to the myriad of challenges they face, they are often insufficiently\nversed in the technology or cybersecurity terminology or not provided with the\ncorrect tools to support them to make sound decisions to govern cybersecurity\neffectively. A different approach is needed to ensure BoDs are clear on the\napproach the business is taking to build a cyber resilient organization. This\nsystematic literature review investigates the existing risk measurement\ninstruments, cybersecurity metrics, and associated models for supporting BoDs.\nWe identified seven conceptual themes through literature analysis that form the\nbasis of this study's main contribution. The findings showed that, although\nsophisticated cybersecurity tools exist and are developing, there is limited\ninformation for Board of Directors to support them in terms of metrics and\nmodels to govern cybersecurity in a language they understand. The review also\nprovides some recommendations on theories and models that can be further\ninvestigated to provide support to Board of Directors.",
        "translated": "网络安全治理影响战略决策的质量，以确保网络风险得到有效管理。董事会是负责管理这种风险的决策者，然而，他们缺乏作出这种决策所必需的充分和有效的信息。除了面临无数挑战之外，他们往往对技术或网络安全术语不够熟悉，或者没有正确的工具支持他们作出正确的决定，有效治理网络安全。需要采取一种不同的方法来确保董事会清楚地了解企业建立网络弹性组织所采取的方法。本系统的文献综述调查了现有的风险测量工具，网络安全指标，以及相关的模型来支持董事会。我们通过文献分析确定了七个概念主题，这些主题构成了本研究主要贡献的基础。调查结果显示，尽管存在并正在开发先进的网络安全工具，但董事会用于支持它们的信息有限，无法用它们理解的语言制定治理网络安全的指标和模型。本文还对可以进一步研究的理论和模型提出了一些建议，以便为董事会提供支持。"
    },
    {
        "title": "MMA-Diffusion: MultiModal Attack on Diffusion Models",
        "url": "http://arxiv.org/abs/2311.17516v1",
        "pub_date": "2023-11-29",
        "summary": "In recent years, Text-to-Image (T2I) models have seen remarkable\nadvancements, gaining widespread adoption. However, this progress has\ninadvertently opened avenues for potential misuse, particularly in generating\ninappropriate or Not-Safe-For-Work (NSFW) content. Our work introduces\nMMA-Diffusion, a framework that presents a significant and realistic threat to\nthe security of T2I models by effectively circumventing current defensive\nmeasures in both open-source models and commercial online services. Unlike\nprevious approaches, MMA-Diffusion leverages both textual and visual modalities\nto bypass safeguards like prompt filters and post-hoc safety checkers, thus\nexposing and highlighting the vulnerabilities in existing defense mechanisms.",
        "translated": "近年来，文本到图像(T2I)模型取得了显著的进步，得到了广泛的采用。然而，这一进展无意中为潜在的滥用开辟了道路，特别是在生成不适当或不安全的工作(NSFW)内容方面。我们的工作介绍了 MMA 扩散，这个框架通过有效地规避开源模型和商业在线服务的现有防御措施，对 T2I 模型的安全构成了重大而现实的威胁。与以前的方法不同，MMA-Disversation 利用文本和视觉模式来绕过提示过滤器和事后安全检查等安全保障措施，从而暴露和突出现有防御机制中的漏洞。"
    },
    {
        "title": "A Multiparty Commutative Hashing Protocol based on the Discrete\n  Logarithm Problem",
        "url": "http://arxiv.org/abs/2311.17498v1",
        "pub_date": "2023-11-29",
        "summary": "Let $\\mathcal{X}$ and $\\mathcal{Y}$ be two sets and suppose that a set of\nparticipants $P=\\{P_1,P_2,\\dots,P_n\\}$ would like to calculate the keyed hash\nvalue of some message $m\\in\\mathcal{X}$ known to a single participant in $P$\ncalled the data owner. Also, suppose that each participant $P_i$ knows a secret\nvalue $x_i\\in\\mathcal{X}$. In this paper, we will propose a protocol that\nenables the participants in this setup to calculate the value\n$y=H(m,x_1,x_2,\\dots ,x_n)$ of a hash function\n$H:\\mathcal{X}^{n+1}\\rightarrow\\mathcal{Y}$ such that the function $H$ is a\none-way function, participants in $P\\backslash\\{P_i\\}$ cannot obtain $x_i$,\nparticipants other than the data owner cannot obtain $m$, and the hash value\n$y=H(m,x_1,x_2,\\dots ,x_n)$ remains the same regardless the order of the secret\n$x_i$ values.",
        "translated": "设 $mathal { X } $和 $mathal { Y } $为两个集合，假设一组参与者 $P = { P _ 1，P _ 2，圆点，P _ n } $希望计算数学中某个消息 $m 的键哈希值，该消息 $m 为 $P $中单个参与者所知，称为数据所有者。另外，假设每个参与者 $P _ i $知道数学{ X } $中的一个秘密值 $x _ i。在本文中，我们将提出一个协议，使参与者能够计算散列函数 $H 的值 $y = H (m，x _ 1，x _ 2，圆点，x _ n) $: mathcal { X } ^ { n + 1} right-tarrow mathical { Y } $，使得函数 $h $是一个单向函数,$P 反斜杠{ P _ i } $中的参与者不能获得 $x _ i $，除了数据所有者之外的参与者不能获得 $m $，并且散列值 $y = H (m，x _ 1，x _ 2，点，x _ n) $保持不变，不管秘密 $x _ i $值的顺序如何。"
    },
    {
        "title": "Eden: An Ultra Fast, Provably Secure, and Fully Decentralized Blockchain\n  Interoperability Protocol",
        "url": "http://arxiv.org/abs/2311.17454v1",
        "pub_date": "2023-11-29",
        "summary": "As the blockchain ecosystem continues to evolve and expand, the need for\nseamless interoperability between disparate blockchain networks has become\nincreasingly paramount. Interoperability not only enhances the functionality\nand reach of individual blockchains but also fosters a collaborative\nenvironment that can unlock new possibilities for decentralized applications.\nIn this paper, we present Eden, an elastic decentralized envoy network that\nleverage zero-knowledge MapReduce framework to facilitates ultra-fast and\nsecure cross-chain communication while maintaining complete decentralization.\nWe detail the Eden's design choices, its comprehensive security model, and the\ninnovative mechanisms it incorporates to ensure elasticity and resilience, even\nunder challenging network conditions.",
        "translated": "随着区块链生态系统的不断发展和扩展，对不同区块链网络之间无缝互操作性的需求变得越来越重要。互操作性不仅增强了单个区块链的功能和覆盖范围，而且还促进了协作环境，可以为分散应用程序开启新的可能性。在本文中，我们介绍了伊甸园，一个弹性分散特使网络，利用零知识 MapReduce 框架，以促进超快速和安全的跨链通信，同时保持完整的地方分权。我们详细介绍了伊甸园的设计选择，其全面的安全模式，以及它采用的创新机制，以确保弹性和弹性，即使在具有挑战性的网络条件下。"
    },
    {
        "title": "Crypto Wash Trading: Direct vs. Indirect Estimation",
        "url": "http://arxiv.org/abs/2311.18717v1",
        "pub_date": "2023-11-30",
        "summary": "Recent studies using indirect statistical methods estimate that around 70% of\ntraded value on centralized crypto exchanges like Binance, can be characterized\nas wash trading. This paper turns to NFT markets, where transaction\ntransparency, including analysis of roundtrip trades and common wallet\nactivities, allows for more accurate direct estimation methods to be applied.\nWe find roughly 30% of NFT volume and between 45-95% of traded value, involve\nwash trading. More importantly, our approach enables a critical evaluation of\ncommon indirect estimation methods used in the literature. We find major\ndifferences in their effectiveness; some failing entirely. Roundedness filters,\nlike those used in Cong et al. (2023), emerge as the most accurate. In fact,\nthe two approaches can be closely aligned via hyper-parameter optimization if\ndirect data is available.",
        "translated": "最近使用间接统计方法的研究估计，在像 Binance 这样的中央加密货币交易所，大约70% 的交易价值可以被描述为洗钱交易。本文转向 NFT 市场，在这里，交易透明度，包括往返交易和普通钱包活动的分析，允许应用更准确的直接估计方法。我们发现大约30% 的 NFT 成交量和45-95% 的交易价值，涉及洗涤交易。更重要的是，我们的方法可以对文献中常用的间接估计方法进行评估。我们发现它们的有效性有很大的不同，有些完全失败了。像 Cong 等人(2023)中使用的那样，圆度过滤器是最准确的。事实上，如果有直接的数据，这两种方法可以通过超参数优化紧密结合。"
    },
    {
        "title": "Scalable and Lightweight Post-Quantum Authentication for Internet of\n  Things",
        "url": "http://arxiv.org/abs/2311.18674v1",
        "pub_date": "2023-11-30",
        "summary": "Internet of Things (IoT) applications are composed of massive quantities of\nresource-limited devices that collect sensitive data with long-term operational\nand security requirements. With the threat of emerging quantum computers,\nPost-Quantum Cryptography (PQC) is a critical requirement for IoTs. In\nparticular, digital signatures offer scalable authentication with\nnon-repudiation and are an essential tool for IoTs. However, as seen in NIST\nPQC standardization, post-quantum signatures are extremely costly for\nresource-limited IoTs. Hence, there is a significant need for quantum-safe\nsignatures that respect the processing, memory, and bandwidth limitations of\nIoTs. In this paper, we created a new lightweight quantum-safe digital\nsignature referred to as INFinity-HORS (INF-HORS), which is (to the best of our\nknowledge) the first signer-optimal hash-based signature with (polynomially)\nunbounded signing capability. INF-HORS enables a verifier to non-interactively\nconstruct one-time public keys from a master public key via encrypted function\nevaluations. This strategy avoids the performance bottleneck of hash-based\nstandards (e.g., SPHINCS+) by eliminating hyper-tree structures. It also does\nnot require a trusted party or non-colliding servers to distribute public keys.\nOur performance analysis confirms that INF-HORS is magnitudes of times more\nsigner computation efficient than selected NIST PQC schemes (e.g., SPHINCS+,\nDilithium, Falcon) with a small memory footprint.",
        "translated": "物联网(IoT)应用程序由大量资源有限的设备组成，这些设备收集具有长期操作和安全需求的敏感数据。随着新兴量子计算机的出现，后量子密码学(PQC)成为物联网的关键需求。特别是，数字签名提供了具有不可否认性的可扩展认证，是物联网的重要工具。然而，正如在 NIST PQC 标准化中看到的那样，后量子签名对于资源有限的物联网来说是极其昂贵的。因此，我们迫切需要量子安全的签名，以尊重物联网的处理、内存和带宽限制。本文创建了一种新的轻量级量子安全数字签名 INFinity-HORS (INF-HORS) ，它是(据我们所知)第一个具有(多项式)无界签名能力的签名者最优哈希签名。INF-HORS 允许验证器通过加密的函数计算，从主公钥非交互地构造一次性公钥。该策略通过消除超树结构，避免了基于散列的标准(如 SPHINCS +)的性能瓶颈。它也不需要可信方或非碰撞服务器来分发公钥。我们的性能分析证实，INF-HORS 比选择的 NIST PQC 方案(例如 SPHINCS + ，Dilithium，Falcon)具有更高的签名计算效率，而且占用的内存较小。"
    },
    {
        "title": "FFT: Towards Harmlessness Evaluation and Analysis for LLMs with\n  Factuality, Fairness, Toxicity",
        "url": "http://arxiv.org/abs/2311.18580v1",
        "pub_date": "2023-11-30",
        "summary": "The widespread of generative artificial intelligence has heightened concerns\nabout the potential harms posed by AI-generated texts, primarily stemming from\nfactoid, unfair, and toxic content. Previous researchers have invested much\neffort in assessing the harmlessness of generative language models. However,\nexisting benchmarks are struggling in the era of large language models (LLMs),\ndue to the stronger language generation and instruction following capabilities,\nas well as wider applications. In this paper, we propose FFT, a new benchmark\nwith 2116 elaborated-designed instances, for LLM harmlessness evaluation with\nfactuality, fairness, and toxicity. To investigate the potential harms of LLMs,\nwe evaluate 9 representative LLMs covering various parameter scales, training\nstages, and creators. Experiments show that the harmlessness of LLMs is still\nunder-satisfactory, and extensive analysis derives some insightful findings\nthat could inspire future research for harmless LLM research.",
        "translated": "生成性人工智能的广泛应用加剧了人们对人工智能产生的文本所带来的潜在危害的担忧，这些文本主要来源于虚假的、不公平的和有毒的内容。以前的研究者们在评估生成语言模型的无害性方面投入了大量的精力。然而，现有的基准测试在大型语言模型(LLM)时代正处于挣扎之中，这是由于更强的语言生成和指令跟踪能力以及更广泛的应用。在本文中，我们提出了 FFT，一个新的基准，2116精心设计的实例，用于 LLM 无害性评价的真实性，公平性和毒性。为了研究 LLM 的潜在危害，我们评估了9个具有代表性的 LLM，涵盖了不同的参数量表、训练阶段和创造者。实验结果表明，LLM 的无害性仍然不能令人满意，广泛的分析得出了一些有见地的结论，可以为今后无害 LLM 的研究提供启发。"
    },
    {
        "title": "Unconditionally Secure Commitments with Quantum Auxiliary Inputs",
        "url": "http://arxiv.org/abs/2311.18566v1",
        "pub_date": "2023-11-30",
        "summary": "We show the following unconditional results on quantum commitments in two\nrelated yet different models:\n  1. We revisit the notion of quantum auxiliary-input commitments introduced by\nChailloux, Kerenidis, and Rosgen (Comput. Complex. 2016) where both the\ncommitter and receiver take the same quantum state, which is determined by the\nsecurity parameter, as quantum auxiliary inputs. We show that\ncomputationally-hiding and statistically-binding quantum auxiliary-input\ncommitments exist unconditionally, i.e., without relying on any unproven\nassumption, while Chailloux et al. assumed a complexity-theoretic assumption,\n${\\bf QIP}\\not\\subseteq{\\bf QMA}$. On the other hand, we observe that achieving\nboth statistical hiding and statistical binding at the same time is impossible\neven in the quantum auxiliary-input setting. To the best of our knowledge, this\nis the first example of unconditionally proving computational security of any\nform of (classical or quantum) commitments for which statistical security is\nimpossible. As intermediate steps toward our construction, we introduce and\nunconditionally construct post-quantum sparse pseudorandom distributions and\nquantum auxiliary-input EFI pairs which may be of independent interest.\n  2. We introduce a new model which we call the common reference quantum state\n(CRQS) model where both the committer and receiver take the same quantum state\nthat is randomly sampled by an efficient setup algorithm. We unconditionally\nprove that there exist statistically hiding and statistically binding\ncommitments in the CRQS model, circumventing the impossibility in the plain\nmodel.\n  We also discuss their applications to zero-knowledge proofs, oblivious\ntransfers, and multi-party computations.",
        "translated": "我们在两个相关但不同的模型中展示了量子承诺的以下无条件结果: 1。我们重新审视由 Chailloux，Kerenidis 和 Rosgen (Comput)引入的量子辅助输入承诺的概念。复杂。2016年) ，提交者和接收者采取相同的量子状态，这是由安全参数，作为量子辅助输入。我们表明，计算隐藏和统计结合的量子辅助输入承诺是无条件存在的，即不依赖于任何未经证实的假设，而 Chailloux 等人假设了复杂性理论假设，${ bf QIP }而不是子集{ bf QMA } $。另一方面，我们观察到，即使在量子辅助输入环境中，同时实现统计隐藏和统计绑定也是不可能的。据我们所知，这是第一个无条件证明任何形式(经典或量子)承诺的计算安全性的例子，而统计安全性是不可能的。作为构造的中间步骤，我们引入并无条件构造了后量子稀疏伪随机分布和量子辅助输入 EFI 对，它们可能具有独立的意义。2.我们引入了一个新的模型，我们称之为公共参考量子态(CRQS)模型，其中提交者和接收者都采用相同的量子态，并通过一个有效的设置算法进行随机采样。我们无条件地证明了在 CRQS 模型中存在统计隐藏和统计绑定的承诺，避免了在普通模型中的不可能性。我们还讨论了它们在零知识证明、遗忘转移和多方计算中的应用。"
    },
    {
        "title": "Decentralized Deepfake Detection Network using Blockchain Technology",
        "url": "http://arxiv.org/abs/2311.18545v1",
        "pub_date": "2023-11-30",
        "summary": "Deepfake technology is a major threat to the integrity of digital media. This\npaper presents a comprehensive framework for a blockchain-based decentralized\nsystem designed to tackle the escalating challenge of digital content\nintegrity. The proposed system integrates advanced deep learning algorithms\nwith the immutable and transparent nature of blockchain technology to create a\ntrustless environment where authenticity can be verified without relying on a\nsingle centralized authority. Furthermore, the system utilizes smart contracts\nfor dynamic algorithm management and token-based incentives further enhances\nthe system's effectiveness and adaptability. The decentralized architecture of\nthe system democratizes the process of verifying digital content and introduces\na novel approach to combat deepfakes. The collaborative and adjustable nature\nof this system sets a new benchmark for digital media integrity, offering a\nmore robust digital media environment.",
        "translated": "深度伪造技术是对数字媒体完整性的主要威胁。本文提出了一个基于区块链的分散系统的综合框架，旨在解决不断升级的数字内容完整性的挑战。该系统将先进的深度学习算法与区块链技术的不可变和透明特性相结合，创造了一个不可信赖的环境，在这个环境中，可以不依赖单一的中央权威机构来验证真实性。此外，系统利用智能契约进行动态算法管理和基于令牌的激励，进一步提高了系统的有效性和适应性。该系统的分散式体系结构使数字内容的验证过程民主化，并引入了一种打击深度假冒的新方法。该系统的协作性和可调性为数字媒体的完整性提供了一个新的基准，提供了一个更加健壮的数字媒体环境。"
    },
    {
        "title": "Bridging Both Worlds in Semantics and Time: Domain Knowledge Based\n  Analysis and Correlation of Industrial Process",
        "url": "http://arxiv.org/abs/2311.18539v1",
        "pub_date": "2023-11-30",
        "summary": "Modern industrial control systems (ICS) attacks infect supervisory control\nand data acquisition (SCADA) hosts to stealthily alter industrial processes,\ncausing damage. To detect attacks with low false alarms, recent work detects\nattacks in both SCADA and process data. Unfortunately, this led to the same\nproblem - disjointed (false) alerts, due to the semantic and time gap in SCADA\nand process behavior, i.e., SCADA execution does not map to process dynamics\nnor evolve at similar time scales. We propose BRIDGE to analyze and correlate\nSCADA and industrial process attacks using domain knowledge to bridge their\nunique semantic and time evolution. This enables operators to tie malicious\nSCADA operations to their adverse process effects, which reduces false alarms\nand improves attack understanding. BRIDGE (i) identifies process constraints\nviolations in SCADA by measuring actuation dependencies in SCADA\nprocess-control, and (ii) detects malicious SCADA effects in processes via a\nphysics-informed neural network that embeds generic knowledge of inertial\nprocess dynamics. BRIDGE then dynamically aligns both analysis (i and ii) in a\ntime-window that adjusts their time evolution based on process inertial delays.\nWe applied BRIDGE to 11 diverse real-world industrial processes, and adaptive\nattacks inspired by past events. BRIDGE correlated 98.3% of attacks with 0.8%\nfalse positives (FP), compared to 78.3% detection accuracy and 13.7% FP of\nrecent work.",
        "translated": "现代工业控制系统(ICS)攻击感染数据采集与监控系统(SCADA)主机，悄悄改变工业过程，造成损害。为了检测具有低误报率的攻击，最近的工作在 SCADA 和处理数据中都检测到了攻击。不幸的是，由于 SCADA 和流程行为中的语义和时间差异，这导致了相同的问题脱节(错误)警报，也就是说，SCADA 的执行不映射到流程动态，也不在相似的时间尺度上发展。利用领域知识对 SCADA 和工业过程攻击进行分析和关联，建立其独特的语义和时间演化的桥梁。这使得操作员能够将恶意 SCADA 操作与其不利的进程效应联系起来，从而减少虚假警报并提高对攻击的理解。BRIDGE (i)通过测量 SCADA 过程控制中的驱动依赖性来识别 SCADA 中的过程约束违规，并且(ii)通过嵌入惯性过程动力学通用知识的物理知情神经网络检测过程中的恶意 SCADA 效应。然后，BRIDGE 动态地将两个分析(i 和 ii)在一个时间窗内对齐，该时间窗根据过程惯性延迟调整它们的时间演化。我们将桥应用于11个不同的现实世界工业过程，以及受过去事件启发的适应性攻击。BRIDGE 将98.3% 的攻击与0.8% 的假阳性(FP)相关，相比之下，最近的工作中检测准确率为78.3% ，假阳性率为13.7% 。"
    },
    {
        "title": "Detecting Anomalous Network Communication Patterns Using Graph\n  Convolutional Networks",
        "url": "http://arxiv.org/abs/2311.18525v1",
        "pub_date": "2023-11-30",
        "summary": "To protect an organizations' endpoints from sophisticated cyberattacks,\nadvanced detection methods are required. In this research, we present\nGCNetOmaly: a graph convolutional network (GCN)-based variational autoencoder\n(VAE) anomaly detector trained on data that include connection events among\ninternal and external machines. As input, the proposed GCN-based VAE model\nreceives two matrices: (i) the normalized adjacency matrix, which represents\nthe connections among the machines, and (ii) the feature matrix, which includes\nvarious features (demographic, statistical, process-related, and Node2vec\nstructural features) that are used to profile the individual nodes/machines.\nAfter training the model on data collected for a predefined time window, the\nmodel is applied on the same data; the reconstruction score obtained by the\nmodel for a given machine then serves as the machine's anomaly score.\nGCNetOmaly was evaluated on real, large-scale data logged by Carbon Black EDR\nfrom a large financial organization's automated teller machines (ATMs) as well\nas communication with Active Directory (AD) servers in two setups: unsupervised\nand supervised. The results of our evaluation demonstrate GCNetOmaly's\neffectiveness in detecting anomalous behavior of machines on unsupervised data.",
        "translated": "为了保护组织的端点不受复杂的网络攻击，需要先进的检测方法。在这项研究中，我们提出了 GCNetOmaly: 一个基于图卷积网络(GCN)的变分自动编码器(VAE)异常检测器训练的数据，其中包括内部和外部机器之间的连接事件。作为输入，提出的基于 GCN 的 VAE 模型接收两个矩阵: (i)表示机器之间连接的归一化邻接矩阵和(ii)特征矩阵，其包括用于分析各个节点/机器的各种特征(人口统计学，统计学，过程相关和 Node2vec 结构特征)。将模型训练到预定义时间窗口的数据上，将模型应用到相同的数据上，得到给定机器的重建得分作为机器的异常得分。通过 Carbon Black EDR 从一家大型金融机构的自动取款机(ATM)中记录的真实的大规模数据，以及与 Active Directory (AD)服务器在两种设置下的通信，GCNetOmaly 进行了评估: 无监督和监督。我们的评估结果证明了 GCNetOmaly 在检测机器在无监督数据上的异常行为方面的有效性。"
    },
    {
        "title": "Data-Agnostic Model Poisoning against Federated Learning: A Graph\n  Autoencoder Approach",
        "url": "http://arxiv.org/abs/2311.18498v1",
        "pub_date": "2023-11-30",
        "summary": "This paper proposes a novel, data-agnostic, model poisoning attack on\nFederated Learning (FL), by designing a new adversarial graph autoencoder\n(GAE)-based framework. The attack requires no knowledge of FL training data and\nachieves both effectiveness and undetectability. By listening to the benign\nlocal models and the global model, the attacker extracts the graph structural\ncorrelations among the benign local models and the training data features\nsubstantiating the models. The attacker then adversarially regenerates the\ngraph structural correlations while maximizing the FL training loss, and\nsubsequently generates malicious local models using the adversarial graph\nstructure and the training data features of the benign ones. A new algorithm is\ndesigned to iteratively train the malicious local models using GAE and\nsub-gradient descent. The convergence of FL under attack is rigorously proved,\nwith a considerably large optimality gap. Experiments show that the FL accuracy\ndrops gradually under the proposed attack and existing defense mechanisms fail\nto detect it. The attack can give rise to an infection across all benign\ndevices, making it a serious threat to FL.",
        "translated": "本文通过设计一种新的基于对抗图形自动编码器(GAE)的框架，提出了一种新的、与数据无关的、模型中毒的联邦学习(FL)攻击方法。该攻击不需要 FL 训练数据的知识，并且实现了有效性和不可探测性。攻击者通过听取良性局部模型和全局模型，提取良性局部模型之间的图结构相关性，并利用训练数据特征对模型进行验证。然后攻击者在最大化 FL 训练损失的同时对抗地重新生成图结构相关性，并利用对抗图结构和良性图结构的训练数据特征生成恶意的局部模型。设计了一种新的算法，利用 GAE 和次梯度下降迭代训练恶意局部模型。严格证明了 FL 在攻击下的收敛性，并给出了相当大的最优性间隔。实验结果表明，在提出的攻击和现有的防御机制无法检测到的情况下，FL 的准确性逐渐下降。这种攻击可以引起所有良性设备的感染，使其成为 FL 的严重威胁。"
    },
    {
        "title": "Enhancing the security of image transmission in Quantum era: A\n  Chaos-Assisted QKD Approach using entanglement",
        "url": "http://arxiv.org/abs/2311.18471v1",
        "pub_date": "2023-11-30",
        "summary": "The emergence of quantum computing has introduced unprecedented security\nchallenges to conventional cryptographic systems, particularly in the domain of\noptical communications. This research addresses these challenges by\ninnovatively combining quantum key distribution (QKD), specifically the E91\nprotocol, with logistic chaotic maps to establish a secure image transmission\nscheme. Our approach utilizes the unpredictability of chaotic systems alongside\nthe robust security mechanisms inherent in quantum entanglement. The scheme is\nfurther fortified with an eavesdropping detection mechanism based on CHSH\ninequality, thereby enhancing its resilience against unauthorized access.\nThrough quantitative simulations, we demonstrate the effectiveness of this\nscheme in encrypting images, achieving high entropy and sensitivity to the\noriginal images. The results indicate a significant improvement in encryption\nand decryption efficiency, showcasing the potential of the scheme as a viable\nsolution against the vulnerabilities posed by quantum computing advancements.\nOur research offers a novel perspective in secure optical communications,\nblending the principles of chaos theory with QKD to create a more robust\ncryptographic framework.",
        "translated": "量子计算的出现给传统密码系统带来了前所未有的安全挑战，尤其是在光通信领域。本研究创新性地将量子密钥分配(QKD) ，特别是 E91协议与逻辑混沌映射相结合，建立一个安全的图像传输方案，以解决这些问题。我们的方法利用了混沌系统的不可预测性，以及量子纠缠固有的强大安全机制。该计划进一步加强了一个基于 CHSH 不平等的窃听检测机制，从而增强了其对未授权访问的弹性。通过定量仿真，验证了该方案对图像进行加密的有效性，实现了对原始图像的高熵和灵敏度。结果表明，该方案在加密和解密效率方面有了显著的提高，显示了该方案作为一种针对量子计算进步所带来的脆弱性的可行解决方案的潜力。我们的研究提供了一个新的视角在安全光通信，融合了混沌理论和量子密钥分发的原则，创建一个更加健壮的密码框架。"
    },
    {
        "title": "The Role of Visual Features in Text-Based CAPTCHAs: An fNIRS Study for\n  Usable Security",
        "url": "http://arxiv.org/abs/2311.18436v1",
        "pub_date": "2023-11-30",
        "summary": "To mitigate dictionary attacks or similar undesirable automated attacks to\ninformation systems, developers mostly prefer using CAPTCHA challenges as Human\nInteractive Proofs (HIPs) to distinguish between human users and scripts.\nAppropriate use of CAPTCHA requires a setup that balances between robustness\nand usability during the design of a challenge. The previous research reveals\nthat most usability studies have used accuracy and response time as measurement\ncriteria for quantitative analysis. The present study aims at applying optical\nneuroimaging techniques for the analysis of CAPTCHA design. The functional\nNear-Infrared Spectroscopy technique was used to explore the hemodynamic\nresponses in the prefrontal cortex elicited by CAPTCHA stimulus of varying\ntypes. )e findings suggest that regions in the left and right dorsolateral and\nright dorsomedial prefrontal cortex respond to the degrees of line occlusion,\nrotation, and wave distortions present in a CAPTCHA. The systematic addition of\nthe visual effects introduced nonlinear effects on the behavioral and\nprefrontal oxygenation measures, indicative of the emergence of Gestalt effects\nthat might have influenced the perception of the overall CAPTCHA figure.",
        "translated": "为了减轻字典攻击或类似的信息系统不良自动化攻击，开发人员大多倾向于使用 CAPTCHA 挑战作为人类交互式证明(HIP)来区分人类用户和脚本。正确使用 CAPTCHA 需要在挑战设计过程中平衡稳健性和可用性的设置。以往的研究表明，大多数可用性研究都使用准确性和响应时间作为定量分析的衡量标准。本研究旨在将光学神经影像技术应用于 CAPTCHA 设计分析。采用功能性近红外光谱技术技术研究了不同类型的验证码刺激引起的脑前额叶外皮血流动力学反应。)E 研究结果表明，左、右背外侧和右背内侧脑前额叶外皮的区域对验证码中线阻塞、旋转和波形扭曲的程度有反应。视觉效果的系统添加引入了对行为和前额叶氧合测量的非线性效应，表明格式塔效应的出现可能影响了整个 CAPTCHA 图形的感知。"
    },
    {
        "title": "Reduction from sparse LPN to LPN, Dual Attack 3.0",
        "url": "http://arxiv.org/abs/2312.00747v1",
        "pub_date": "2023-12-01",
        "summary": "The security of code-based cryptography relies primarily on the hardness of\ndecoding generic linear codes. Until very recently, all the best algorithms for\nsolving the decoding problem were information set decoders (ISD). However,\nrecently a new algorithm called RLPN-decoding which relies on a completely\ndifferent approach was introduced and it has been shown that RLPN outperforms\nsignificantly ISD decoders for a rather large range of rates. This RLPN decoder\nrelies on two ingredients, first reducing decoding to some underlying LPN\nproblem, and then computing efficiently many parity-checks of small weight when\nrestricted to some positions. We revisit RLPN-decoding by noticing that, in\nthis algorithm, decoding is in fact reduced to a sparse-LPN problem, namely\nwith a secret whose Hamming weight is small. Our new approach consists this\ntime in making an additional reduction from sparse-LPN to plain-LPN with a\ncoding approach inspired by coded-BKW. It outperforms significantly the ISD's\nand RLPN for code rates smaller than 0.42. This algorithm can be viewed as the\ncode-based cryptography cousin of recent dual attacks in lattice-based\ncryptography. We depart completely from the traditional analysis of this kind\nof algorithm which uses a certain number of independence assumptions that have\nbeen strongly questioned recently in the latter domain. We give instead a\nformula for the LPNs noise relying on duality which allows to analyze the\nbehavior of the algorithm by relying only on the analysis of a certain weight\ndistribution. By using only a minimal assumption whose validity has been\nverified experimentally we are able to justify the correctness of our\nalgorithm. This key tool, namely the duality formula, can be readily adapted to\nthe lattice setting and is shown to give a simple explanation for some\nphenomena observed on dual attacks in lattices in [DP23].",
        "translated": "基于代码的密码学的安全性主要取决于解码一般线性码的硬度。直到最近，所有解决解码问题的最佳算法都是信息集解码器(ISD)。然而，最近一种新的算法称为 RLPN-译码，它依赖于一种完全不同的方法被引入，并且已经表明，RLPN 在相当大的速率范围内优于 ISD 译码器。这种 RLPN 译码器依赖于两个因素，首先将译码减少到一些潜在的 LPN 问题，然后在限制到某些位置时有效地计算许多小权重的奇偶校验。我们重新讨论了 RLPN 译码，注意到在这种算法中，译码实际上被简化为一个稀疏 LPN 问题，即一个汉明权值较小的秘密。我们的新方法包括这个时间在做一个额外的减少从稀疏-LPN 到平原-LPN 的编码方法的启发编码-BKW。当码率小于0.42时，它的性能明显优于 ISD 和 RLPN。这种算法可以被看作是基于代码的密码学的近亲的双重攻击的格为基础的密码学。我们完全背离了传统的分析这种算法使用一定数量的独立性假设，最近在后一领域受到强烈质疑。我们给出了一个基于对偶性的 LPNs 噪声公式，该公式仅依靠对一定权重分布的分析就可以分析算法的性能。通过只使用一个最小假设，其有效性已得到实验验证，我们能够证明我们的算法的正确性。这个关键的工具，即对偶公式，可以很容易地适应格的设置，并显示给出了一个简单的解释，一些现象观察到的对偶攻击格在[ DP23]。"
    },
    {
        "title": "Crystal: Enhancing Blockchain Mining Transparency with Quorum\n  Certificate",
        "url": "http://arxiv.org/abs/2312.00741v1",
        "pub_date": "2023-12-01",
        "summary": "Researchers have discovered a series of theoretical attacks against Bitcoin's\nNakamoto consensus; the most damaging ones are selfish mining, double-spending,\nand consistency delay attacks. These attacks have one common cause: block\nwithholding. This paper proposes Crystal, which leverages quorum certificates\nto resist block withholding misbehavior. Crystal continuously elects committees\nfrom miners and requires each block to have a quorum certificate, i.e., a set\nof signatures issued by members of its committee. Consequently, an attacker has\nto publish its blocks to obtain quorum certificates, rendering block\nwithholding impossible. To build Crystal, we design a novel two-round committee\nelection in a Sybil-resistant, unpredictable and non-interactive way, and a\nreward mechanism to incentivize miners to follow the protocol. Our analysis and\nevaluations show that Crystal can significantly mitigate selfish mining and\ndouble-spending attacks. For example, in Bitcoin, an attacker with 30% of the\ntotal computation power will succeed in double-spending attacks with a\nprobability of 15.6% to break the 6-confirmation rule; however, in Crystal, the\nsuccess probability for the same attacker falls to 0.62%. We provide formal\nend-to-end safety proofs for Crystal, ensuring no unknown attacks will be\nintroduced. To the best of our knowledge, Crystal is the first protocol that\nprevents selfish mining and double-spending attacks while providing safety\nproof.",
        "translated": "研究人员发现了一系列针对比特币中本聪共识的理论攻击，其中最具破坏性的是自私挖矿、双重消费和一致性延迟攻击。这些攻击有一个共同的原因: 阻塞扣留。本文提出了 Crystal，它利用仲裁证书来抵制块保留错误行为。克里斯特尔不断从矿工中选举委员会，并要求每个区块拥有法定人数证书，即委员会成员签发的一套签名。因此，攻击者必须发布它的块来获得仲裁证书，这使得阻塞不可能被阻止。为了构建 Crystal，我们设计了一个新颖的两轮委员会选举，它采用了一种防止 Sybil、不可预测和非互动的方式，并设计了一个奖励机制来激励矿工遵循协议。我们的分析和评估表明，水晶可以显著减轻自私的采矿和双重支出的攻击。例如，在比特币中，计算能力占总计算能力30% 的攻击者将成功进行两次消耗攻击，成功率为15.6% ，以打破6确认规则; 然而，在 Crystal 中，同一攻击者的成功率下降到0.62% 。我们为 Crystal 提供正式的端到端安全证明，确保不会引入未知攻击。据我们所知，水晶是第一个协议，防止自私采矿和双重支出的攻击，同时提供安全保证。"
    },
    {
        "title": "Zipr: A High-Impact, Robust, Open-source, Multi-platform, Static Binary\n  Rewriter",
        "url": "http://arxiv.org/abs/2312.00714v1",
        "pub_date": "2023-12-01",
        "summary": "Zipr is a tool for static binary rewriting, first published in 2016. Zipr was\nengineered to support arbitrary program modification with an emphasis on low\noverhead, robustness, and flexibility to perform security enhancements and\ninstrumentation. Originally targeted to Linux x86-32 binaries, Zipr now\nsupports 32- and 64-bit binaries for X86, ARM, and MIPS architectures, as well\nas preliminary support for Windows programs.\n  These features have helped Zipr make a dramatic impact on research. It was\nfirst used in the DARPA Cyber Grand Challenge to take second place overall,\nwith the best security score of any participant, Zipr has now been used in a\nvariety of research areas by both the original authors as well as third\nparties. Zipr has also led to publications in artificial diversity, program\ninstrumentation, program repair, fuzzing, autonomous vehicle security, research\ncomputing security, as well as directly contributing to two student\ndissertations. The open-source repository has accepted accepted patches from\nseveral external authors, demonstrating the impact of Zipr beyond the original\nauthors.",
        "translated": "Zipr 是一个用于静态二进制重写的工具，于2016年首次发布。Zipr 被设计成支持任意的程序修改，强调低开销、健壮性和灵活性，以执行安全性增强和检测。Zipr 最初的目标是 Linux X86-32二进制文件，现在支持 X86、 ARM 和 MIPS 架构的32位和64位二进制文件，以及对 Windows 程序的初步支持。这些特性帮助 Zipr 对研究产生了巨大的影响。Zipr 最初是在美国国防部高级研究计划局(DARPA)的网络大挑战赛中使用的，总体而言排名第二，是所有参赛者中安全得分最高的。 Zipr 现在已经被原作者和第三方用于各种研究领域。Zipr 还在人工多样性、程序检测、程序修复、模糊化、无人机安全、研究计算安全等领域发表了多篇文章，并直接为两篇学生论文做出了贡献。开源知识库已经接受了几个外部作者的补丁，证明了 Zipr 的影响超越了原作者。"
    },
    {
        "title": "A Holistic Approach for Trustworthy Distributed Systems with WebAssembly\n  and TEEs",
        "url": "http://arxiv.org/abs/2312.00702v1",
        "pub_date": "2023-12-01",
        "summary": "Publish/subscribe systems play a key role in enabling communication between\nnumerous devices in distributed and large-scale architectures. While widely\nadopted, securing such systems often trades portability for additional\nintegrity and attestation guarantees. Trusted Execution Environments (TEEs)\noffer a potential solution with enclaves to enhance security and trust.\nHowever, application development for TEEs is complex, and many existing\nsolutions are tied to specific TEE architectures, limiting adaptability.\nCurrent communication protocols also inadequately manage attestation proofs or\nexpose essential attestation information. This paper introduces a novel\napproach using WebAssembly to address these issues, a key enabling technology\nnowadays capturing academia and industry attention. We present the design of a\nportable and fully attested publish/subscribe middleware system as a holistic\napproach for trustworthy and distributed communication between various systems.\nBased on this proposal, we have implemented and evaluated in-depth a\nfully-fledged publish/subscribe broker running within Intel SGX, compiled in\nWebAssembly, and built on top of industry-battled frameworks and standards,\ni.e., MQTT and TLS protocols. Our extended TLS protocol preserves the privacy\nof attestation information, among other benefits. Our experimental results\nshowcase most overheads, revealing a 1.55x decrease in message throughput when\nusing a trusted broker. We open-source the contributions of this work to the\nresearch community to facilitate experimental reproducibility.",
        "translated": "发布/订阅系统在支持分布式和大规模体系结构中的多个设备之间的通信方面发挥着关键作用。虽然被广泛采用，但保护这类系统往往以可移植性换取额外的完整性和认证保证。可信执行环境(Trusted Execution Environment，TEE)提供了一个潜在的解决方案，可以使用飞地来增强安全性和信任。然而，TEE 的应用程序开发是复杂的，并且许多现有的解决方案都与特定的 TEE 体系结构相关联，从而限制了适应性。当前的通信协议也没有充分地管理证明证明或公开必要的证明信息。本文介绍了一种使用 WebAssembly 来解决这些问题的新方法，这是当今吸引学术界和工业界注意的一种关键的使能技术。我们提出了一种可移植的、经过充分验证的发布/订阅中间件系统的设计，作为在不同系统之间进行可信赖和分布式通信的整体方法。基于这个提议，我们已经实现并深入评估了一个在 Intel SGX 内运行的成熟的发布/订阅代理，该代理在 WebAssembly 中编译，并构建在行业内争夺激烈的框架和标准之上，即 MQTT 和 TLS 协议。我们的扩展 TLS 协议保护了认证信息的隐私，以及其他好处。我们的实验结果展示了大多数开销，显示使用可信代理时消息吞吐量减少了1.55倍。我们开源的贡献，这项工作的研究社区，以促进实验的可重复性。"
    },
    {
        "title": "Classification of cyber attacks on IoT and ubiquitous computing devices",
        "url": "http://arxiv.org/abs/2312.00686v1",
        "pub_date": "2023-12-01",
        "summary": "As the Internet of Things (IoT) has become truly ubiquitous, so has the\nsurrounding threat landscape. However, while the security of classical\ncomputing systems has significantly matured in the last decades, IoT\ncybersecurity is still typically low or fully neglected. This paper provides a\nclassification of IoT malware. Major targets and used exploits for attacks are\nidentified and referred to the specific malware. The lack of standard\ndefinitions of IoT devices and, therefore, security goals has been identified\nduring this research as a profound barrier in advancing IoT cybersecurity.\nFurthermore, standardized reporting of IoT malware by trustworthy sources is\nrequired in the field. The majority of current IoT attacks continue to be of\ncomparably low effort and level of sophistication and could be mitigated by\nexisting technical measures.",
        "translated": "随着物联网(IoT)真正变得无处不在，周围的威胁也变得无处不在。然而，尽管经典计算系统的安全性在过去几十年中已经显著成熟，但物联网的网络安全性仍然通常较低或完全被忽视。本文提供了物联网恶意软件的分类。主要目标和用于攻击的漏洞被确定，并提到特定的恶意软件。缺乏物联网设备的标准定义，因此，在这项研究中，安全目标已被确定为推进物联网网络安全的一个深刻障碍。此外，该领域还需要通过可靠来源对物联网恶意软件进行标准化报告。目前大多数物联网攻击仍然是相对较低的工作量和复杂程度，可以通过现有的技术措施加以缓解。"
    },
    {
        "title": "Applicability of Blockchain Technology in Avionics Systems",
        "url": "http://arxiv.org/abs/2312.00681v1",
        "pub_date": "2023-12-01",
        "summary": "Blockchain technology, within its fast widespread and superiority\ndemonstrated by recent studies, can be also used as an informatic tool for\nsolving various aviation problems. Aviation electronics (avionics) systems\nstand out as the application area of informatics methods in solving aviation\nproblems or providing different capabilities to aircrafts. Avionics systems are\nelectronic systems used in air and space vehicles for many purposes such as\nsurveillance, navigation and communication. In this study, the applicability of\nblockchain technology as a new approach in the development of avionics systems\nis discussed, and in this regard, a method inspired by the previously\nimplemented applications in electronic flight systems is proposed to help\nevaluate the applicability of this technology in new avionics system designs.\nThe potential of blockchain for solving the problems especially in basic\nservices, communication, navigation and flight management systems; the problem\nstructures for which application of this technology would be a reliable\nsolution; and the superiority and inferiority of its use in avionic systems are\nexplained. A guiding paper is proposed for aviation engineers/experts to make a\ndecision on applying blockchain into avionics systems.",
        "translated": "区块链技术以其快速的广泛性和近年来研究所证明的优越性，也可以作为解决各种航空问题的信息工具。航空电子(航空电子)系统作为信息学方法在解决航空问题或为飞机提供不同能力方面的应用领域脱颖而出。航空电子系统是用于航空和航天器的电子系统，用于监视、导航和通信等多种目的。本文讨论了区块链技术作为一种新的航空电子系统开发方法的适用性，并在此基础上提出了一种受以往电子飞行系统应用启发的方法，以帮助评估区块链技术在新的航空电子系统设计中的适用性。区块链解决问题的潜力，特别是在基本服务、通信、导航和飞行管理系统的问题; 这种技术的应用将是一个可靠的解决办法的问题结构; 以及它在航空电子系统中使用的优势和劣势的解释。为航空工程师/专家在航空电子系统中应用区块链做出决策提供了指导性文件。"
    },
    {
        "title": "Hashmarks: Privacy-Preserving Benchmarks for High-Stakes AI Evaluation",
        "url": "http://arxiv.org/abs/2312.00645v1",
        "pub_date": "2023-12-01",
        "summary": "There is a growing need to gain insight into language model capabilities that\nrelate to sensitive topics, such as bioterrorism or cyberwarfare. However,\ntraditional open source benchmarks are not fit for the task, due to the\nassociated practice of publishing the correct answers in human-readable form.\nAt the same time, enforcing mandatory closed-quarters evaluations might stifle\ndevelopment and erode trust. In this context, we propose hashmarking, a\nprotocol for evaluating language models in the open without having to disclose\nthe correct answers. In its simplest form, a hashmark is a benchmark whose\nreference solutions have been cryptographically hashed prior to publication.\nFollowing an overview of the proposed evaluation protocol, we go on to assess\nits resilience against traditional attack vectors (e.g. rainbow table attacks),\nas well as against failure modes unique to increasingly capable generative\nmodels.",
        "translated": "越来越需要深入了解与生物恐怖主义或网络战等敏感话题有关的语言模型能力。然而，传统的开源基准测试并不适合这项任务，因为相关的实践是以人类可读的形式发布正确的答案。与此同时，实施强制性的封闭式评估可能会扼杀发展，侵蚀信任。在这种背景下，我们提出了散列标记(hashmark) ，这是一种公开评估语言模型而无需披露正确答案的协议。在最简单的形式中，hashmark 是一种基准，其引用解决方案在发布之前已经加密散列。在对所提议的评估协议进行了概述之后，我们继续评估其对传统攻击载体(例如彩虹表攻击)的弹性，以及对越来越强大的生成模型所特有的失效模式的弹性。"
    },
    {
        "title": "Using Honeybuckets to Characterize Cloud Storage Scanning in the Wild",
        "url": "http://arxiv.org/abs/2312.00580v1",
        "pub_date": "2023-12-01",
        "summary": "In this work, we analyze to what extent actors target poorly-secured cloud\nstorage buckets for attack. We deployed hundreds of AWS S3 honeybuckets with\ndifferent names and content to lure and measure different scanning strategies.\nActors exhibited clear preferences for scanning buckets that appeared to belong\nto organizations, especially commercial entities in the technology sector with\na vulnerability disclosure program. Actors continuously engaged with the\ncontent of buckets by downloading, uploading, and deleting files. Most\nalarmingly, we recorded multiple instances in which malicious actors\ndownloaded, read, and understood a document from our honeybucket, leading them\nto attempt to gain unauthorized server access.",
        "translated": "在这项工作中，我们分析参与者在多大程度上针对安全性较差的云存储桶进行攻击。我们部署了数百个具有不同名称和内容的 AWS S3蜜桶来诱惑和测量不同的扫描策略。参与者显示出对扫描桶的明显偏好，这些桶似乎属于组织，特别是有漏洞披露程序的技术部门的商业实体。参与者通过下载、上传和删除文件不断处理 bucket 的内容。最令人担忧的是，我们记录了多个实例，其中恶意角色从我们的蜜桶下载、读取和理解文档，导致他们试图获得未经授权的服务器访问。"
    },
    {
        "title": "Hiding in text/plain sight: Security defences of Tor Onion Services",
        "url": "http://arxiv.org/abs/2312.00545v1",
        "pub_date": "2023-12-01",
        "summary": "Tor Onion Services are a way to host websites and other internet services\nanonymously. Onion Services are often used to bypass internet censorship and\nprovide information services to users in oppressive regimes. This paper\npresents an analysis of the security defences deployed on these Onion Services.\nOnion Services tend to have better security policy than sites on the clear web.\nHowever they lag behind in the deployment of HTTPS, a key defence to ensuring\nthe security of users of such services.",
        "translated": "洋葱服务是一种匿名托管网站和其他互联网服务的方式。洋葱服务常常被用来绕过互联网审查，向处于压迫政权的用户提供信息服务。本文分析了在这些 Onion 服务上部署的安全防御。Onion Services 的安全策略往往比清晰 Web 上的站点更好。然而，它们在 HTTPS 的部署方面落后了，而 HTTPS 是确保此类服务用户安全的关键防御措施。"
    },
    {
        "title": "The Impact of Privacy and Security Attitudes and Concerns of Travellers\n  on Their Willingness to Use Mobility-as-a-Service Systems",
        "url": "http://arxiv.org/abs/2312.00519v1",
        "pub_date": "2023-12-01",
        "summary": "This paper reports results from an online survey on the impact of travellers'\nprivacy and security attitudes and concerns on their willingness to use\nmobility-as-a-service (MaaS) systems. This study is part of a larger project\nthat aims at investigating barriers to potential MaaS uptake. The online survey\nwas designed to cover data privacy and security attitudes and concerns as well\nas a variety of socio-psychological and socio-demographic variables associated\nwith travellers' intentions to use MaaS systems. The study involved $n=320$ UK\nparticipants recruited via the Prolific survey platform. Overall, correlation\nanalysis and a multiple regression model indicated that, neither attitudes nor\nconcerns of participants over the privacy and security of personal data would\nsignificantly impact their decisions to use MaaS systems, which was an\nunexpected result, however, their trust in (commercial and governmental)\nwebsites would. Another surprising result is that, having been a victim of\nimproper invasion of privacy did not appear to affect individuals' intentions\nto use MaaS systems, whereas frequency with which one heard about misuse of\npersonal data did. Implications of the results and future directions are also\ndiscussed, e.g., MaaS providers are encouraged to work on improving the\ntrustworthiness of their corporate image.",
        "translated": "本文报告了一项关于旅行者隐私和安全态度的影响以及对他们使用移动即服务(MaaS)系统意愿的关注的在线调查的结果。这项研究是一个更大的项目的一部分，旨在调查潜在的 MaaS 吸收障碍。这项在线调查旨在涵盖数据隐私、安全态度和担忧，以及与旅行者使用 MaaS 系统意图相关的各种社会心理和社会人口变量。这项研究涉及320美元的英国参与者招募通过多产调查平台。总的来说，相关性分析和多元回归模型表明，参与者对个人数据隐私和安全的态度和担忧都不会显着影响他们使用 MaaS 系统的决定，这是一个意想不到的结果，然而，他们对(商业和政府)网站的信任会。另一个令人惊讶的结果是，成为不正当侵犯隐私的受害者似乎并没有影响个人使用 MaaS 系统的意图，而人们听到滥用个人数据的频率却有所影响。结果的含义和未来的方向也进行了讨论，例如，MaaS 提供商被鼓励致力于提高其公司形象的可信度。"
    },
    {
        "title": "Hot PATE: Private Aggregation of Distributions for Diverse Task",
        "url": "http://arxiv.org/abs/2312.02132v1",
        "pub_date": "2023-12-04",
        "summary": "The Private Aggregation of Teacher Ensembles (PATE)\nframework~\\cite{PapernotAEGT:ICLR2017} is a versatile approach to\nprivacy-preserving machine learning. In PATE, teacher models are trained on\ndistinct portions of sensitive data, and their predictions are privately\naggregated to label new training examples for a student model.\n  Until now, PATE has primarily been explored with classification-like tasks,\nwhere each example possesses a ground-truth label, and knowledge is transferred\nto the student by labeling public examples. Generative AI models, however,\nexcel in open ended \\emph{diverse} tasks with multiple valid responses and\nscenarios that may not align with traditional labeled examples. Furthermore,\nthe knowledge of models is often encapsulated in the response distribution\nitself and may be transferred from teachers to student in a more fluid way. We\npropose \\emph{hot PATE}, tailored for the diverse setting. In hot PATE, each\nteacher model produces a response distribution and the aggregation method must\npreserve both privacy and diversity of responses. We demonstrate, analytically\nand empirically, that hot PATE achieves privacy-utility tradeoffs that are\ncomparable to, and in diverse settings, significantly surpass, the baseline\n``cold'' PATE.",
        "translated": "教师群体的私人聚合(PATE)框架引用{ PapernotAEGT: ICLR2017}是一种通用的保护隐私的机器学习方法。在 PATE，教师模型是根据敏感数据的不同部分进行培训的，他们的预测被私下汇总，以便为一个学生模型贴上新的培训例子的标签。到目前为止，PATE 主要是通过类似于分类的任务来探索的，其中每个例子都有一个基本事实的标签，知识通过标记公共例子传递给学生。然而，生成式人工智能模型擅长于具有多个有效响应和场景(可能与传统的标签示例不一致)的开放式工作模式{不同}任务。此外，模型的知识往往封装在响应分布本身，并可能从教师转移到学生在一个更流动的方式。我们建议的方式{热 PATE } ，为多样化的设置。在热 PATE 中，每个教师模型产生一个响应分布，聚合方法必须同时保持响应的隐私性和多样性。我们分析和经验证明，热 PATE 实现了隐私效用的权衡，与基线“冷”PATE 相当，并且在不同的设置下，明显超过。"
    },
    {
        "title": "Tree of Attacks: Jailbreaking Black-Box LLMs Automatically",
        "url": "http://arxiv.org/abs/2312.02119v1",
        "pub_date": "2023-12-04",
        "summary": "While Large Language Models (LLMs) display versatile functionality, they\ncontinue to generate harmful, biased, and toxic content, as demonstrated by the\nprevalence of human-designed jailbreaks. In this work, we present Tree of\nAttacks with Pruning (TAP), an automated method for generating jailbreaks that\nonly requires black-box access to the target LLM. TAP utilizes an LLM to\niteratively refine candidate (attack) prompts using tree-of-thoughts reasoning\nuntil one of the generated prompts jailbreaks the target. Crucially, before\nsending prompts to the target, TAP assesses them and prunes the ones unlikely\nto result in jailbreaks. Using tree-of-thought reasoning allows TAP to navigate\na large search space of prompts and pruning reduces the total number of queries\nsent to the target. In empirical evaluations, we observe that TAP generates\nprompts that jailbreak state-of-the-art LLMs (including GPT4 and GPT4-Turbo)\nfor more than 80% of the prompts using only a small number of queries. This\nsignificantly improves upon the previous state-of-the-art black-box method for\ngenerating jailbreaks.",
        "translated": "虽然大语言模型(LLM)显示了多种功能，但它们继续产生有害的、有偏见的和有毒的内容，人类设计的越狱的普遍性就证明了这一点。在这项工作中，我们提出了使用修剪的攻击树(TAP) ，一种自动生成越狱的方法，只需要黑盒访问目标 LLM。TAP 利用 LLM 使用思想树推理迭代地细化候选(攻击)提示，直到其中一个生成的提示越狱目标。至关重要的是，在向目标发送提示之前，TAP 会对它们进行评估，并删除那些不太可能导致越狱的提示。使用思维树推理允许 TAP 在大量的提示搜索空间中导航，并且可以减少发送到目标的查询总数。在实证评估中，我们观察到 TAP 仅使用少量查询就为80% 以上的提示生成越狱最先进的 LLM (包括 GPT4和 GPT4-Turbo)。这显著改进了以前用于生成越狱的最先进的黑盒方法。"
    },
    {
        "title": "Distributed Optimization with Feasible Set Privacy",
        "url": "http://arxiv.org/abs/2312.02112v1",
        "pub_date": "2023-12-04",
        "summary": "We consider the setup of a constrained optimization problem with two agents\n$E_1$ and $E_2$ who jointly wish to learn the optimal solution set while\nkeeping their feasible sets $\\mathcal{P}_1$ and $\\mathcal{P}_2$ private from\neach other. The objective function $f$ is globally known and each feasible set\nis a collection of points from a global alphabet. We adopt a sequential\nsymmetric private information retrieval (SPIR) framework where one of the\nagents (say $E_1$) privately checks in $\\mathcal{P}_2$, the presence of\ncandidate solutions of the problem constrained to $\\mathcal{P}_1$ only, while\nlearning no further information on $\\mathcal{P}_2$ than the solution alone.\nFurther, we extract an information theoretically private threshold PSI (ThPSI)\nprotocol from our scheme and characterize its download cost. We show that,\ncompared to privately acquiring the feasible set $\\mathcal{P}_1\\cap\n\\mathcal{P}_2$ using an SPIR-based private set intersection (PSI) protocol, and\nfinding the optimum, our scheme is better as it incurs less information leakage\nand less download cost than the former. Over all possible uniform mappings of\n$f$ to a fixed range of values, our scheme outperforms the former with a high\nprobability.",
        "translated": "我们考虑一个约束最佳化问题的建立，其中有两个代理 $e _ 1 $和 $e _ 2 $，它们共同希望学习最优解集，同时保持它们的可行集 $mathal { P } _ 1 $和 $mathal { P } _ 2 $之间的私有性。目标函数 $f $是全局已知的，并且每个可行集是来自全局字母表的点的集合。我们采用一个序贯对称私有信息检索(SPIR)框架，其中一个代理(比如 $E _ 1 $)私下检查 $mathcal { P } _ 2 $，问题的候选解的存在性仅限于 $mathal { P } _ 1 $，而不学习关于 $mathcal { P } _ 2 $的进一步信息。进一步，我们从我们的方案中提取了一个理论上的私有门限 PSI (ThPSI)协议，并描述了它的下载成本。我们发现，与使用基于 SPIR 的私有集交集(PSI)协议私有获取可行集 $mathcal { P } _ 1 cap mathcal { P } _ 2 $相比，我们的方案更好，因为它比前者的信息泄露更小，下载成本更低。在所有可能的 $f $到固定值范围的一致映射中，我们的方案以很高的概率优于前者。"
    },
    {
        "title": "Mitigating Data Injection Attacks on Federated Learning",
        "url": "http://arxiv.org/abs/2312.02102v1",
        "pub_date": "2023-12-04",
        "summary": "Federated learning is a technique that allows multiple entities to\ncollaboratively train models using their data without compromising data\nprivacy. However, despite its advantages, federated learning can be susceptible\nto false data injection attacks. In these scenarios, a malicious entity with\ncontrol over specific agents in the network can manipulate the learning\nprocess, leading to a suboptimal model. Consequently, addressing these data\ninjection attacks presents a significant research challenge in federated\nlearning systems. In this paper, we propose a novel technique to detect and\nmitigate data injection attacks on federated learning systems. Our mitigation\nmethod is a local scheme, performed during a single instance of training by the\ncoordinating node, allowing the mitigation during the convergence of the\nalgorithm. Whenever an agent is suspected to be an attacker, its data will be\nignored for a certain period, this decision will often be re-evaluated. We\nprove that with probability 1, after a finite time, all attackers will be\nignored while the probability of ignoring a trustful agent becomes 0, provided\nthat there is a majority of truthful agents. Simulations show that when the\ncoordinating node detects and isolates all the attackers, the model recovers\nand converges to the truthful model.",
        "translated": "联合学习是一种技术，允许多个实体协作训练模型使用他们的数据，而不损害数据隐私。然而，尽管联邦学习具有很多优点，但是它很容易受到虚假数据注入攻击。在这些场景中，控制网络中特定代理的恶意实体可以操纵学习过程，从而导致次优模型。因此，解决这些数据注入攻击是联邦学习系统中的一个重大研究挑战。本文提出了一种新的联邦学习系统数据注入攻击检测和缓解技术。我们的缓解方法是一个局部方案，由协调节点在单个训练实例中执行，允许在算法收敛期间进行缓解。每当一个代理被怀疑是攻击者，它的数据将被忽略一段时间，这个决定将经常被重新评估。证明了在有限时间内，当可信代理占大多数时，忽略可信代理的概率为0时，所有攻击者将被忽略。仿真结果表明，当协调节点检测并隔离所有攻击者时，模型恢复并收敛到真实模型。"
    },
    {
        "title": "Federated Learning is Better with Non-Homomorphic Encryption",
        "url": "http://arxiv.org/abs/2312.02074v1",
        "pub_date": "2023-12-04",
        "summary": "Traditional AI methodologies necessitate centralized data collection, which\nbecomes impractical when facing problems with network communication, data\nprivacy, or storage capacity. Federated Learning (FL) offers a paradigm that\nempowers distributed AI model training without collecting raw data. There are\ndifferent choices for providing privacy during FL training. One of the popular\nmethodologies is employing Homomorphic Encryption (HE) - a breakthrough in\nprivacy-preserving computation from Cryptography. However, these methods have a\nprice in the form of extra computation and memory footprint. To resolve these\nissues, we propose an innovative framework that synergizes permutation-based\ncompressors with Classical Cryptography, even though employing Classical\nCryptography was assumed to be impossible in the past in the context of FL. Our\nframework offers a way to replace HE with cheaper Classical Cryptography\nprimitives which provides security for the training process. It fosters\nasynchronous communication and provides flexible deployment options in various\ncommunication topologies.",
        "translated": "传统的人工智能方法需要集中式的数据采集，当遇到网络通信、数据隐私或存储容量等问题时，集中式的数据采集就变得不切实际。联邦学习(FL)提供了一个范式，授权分布式人工智能模型的训练，而不收集原始数据。有不同的选择，以提供隐私期间的外语培训。其中一个流行的方法就是使用同态加密(HE)——这是密码学在保护隐私计算方面的一个突破。然而，这些方法的代价是额外的计算和内存占用。为了解决这些问题，我们提出了一个创新的框架，使基于排列的压缩器与经典密码学协同工作，尽管在过去的 FL 中使用经典密码学被认为是不可能的。我们的框架提供了一种方法来取代高等教育与廉价的古典密码学原语，提供了培训过程的安全性。它促进异步通信，并在各种通信拓扑中提供灵活的部署选项。"
    },
    {
        "title": "Kirchhoff Meets Johnson: In Pursuit of Unconditionally Secure\n  Communication",
        "url": "http://arxiv.org/abs/2312.02042v1",
        "pub_date": "2023-12-04",
        "summary": "Noise: an enemy to be dealt with and a major factor limiting communication\nsystem performance. However, what if there is gold in that garbage? In\nconventional engineering, our focus is primarily on eliminating, suppressing,\ncombating, or even ignoring noise and its detrimental impacts. Conversely,\ncould we exploit it similarly to biology, which utilizes noise-alike carrier\nsignals to convey information? In this context, the utilization of noise, or\nnoise-alike signals in general, has been put forward as a means to realize\nunconditionally secure communication systems in the future. In this tutorial\narticle, we begin by tracing the origins of thermal noise-based communication\nand highlighting one of its significant applications for ensuring\nunconditionally secure networks: the Kirchhoff-law-Johnson-noise (KLJN) secure\nkey exchange scheme. We then delve into the inherent challenges tied to secure\ncommunication and discuss the imperative need for physics-based key\ndistribution schemes in pursuit of unconditional security. Concurrently, we\nprovide a concise overview of quantum key distribution (QKD) schemes and draw\ncomparisons with their KLJN-based counterparts. Finally, extending beyond wired\ncommunication loops, we explore the transmission of noise signals over-the-air\nand evaluate their potential for stealth and secure wireless communication\nsystems.",
        "translated": "噪声: 通信系统性能的主要限制因素。但是，如果垃圾里有金子呢？在传统工程中，我们的重点主要是消除，抑制，打击，甚至忽略噪音及其有害影响。相反，我们能否利用它类似于利用类似噪声的载波信号来传递信息的生物学？在这种背景下，利用噪声，或一般的噪声相似信号，已被提出作为一种手段，以实现无条件安全的通信系统的未来。在这篇教程文章中，我们首先追踪基于热噪声的通信的起源，并强调它在确保无条件安全网络方面的一个重要应用: Kirchhoff-law-Johnson- 噪声(KLJN)安全密钥交换方案。然后，我们深入研究了与安全通信相关的内在挑战，并讨论了在追求无条件安全的过程中迫切需要基于物理的密钥分配方案。同时，我们提供了量子密钥分配(QKD)方案的简要概述，并与它们的基于 KLJN 的对应方案进行了比较。最后，超越有线通信循环，我们探索噪声信号在空中的传输，并评估其潜在的隐形和安全的无线通信系统。"
    },
    {
        "title": "A Survey on Large Language Model (LLM) Security and Privacy: The Good,\n  the Bad, and the Ugly",
        "url": "http://arxiv.org/abs/2312.02003v1",
        "pub_date": "2023-12-04",
        "summary": "Large Language Models (LLMs), such as GPT-3 and BERT, have revolutionized\nnatural language understanding and generation. They possess deep language\ncomprehension, human-like text generation capabilities, contextual awareness,\nand robust problem-solving skills, making them invaluable in various domains\n(e.g., search engines, customer support, translation). In the meantime, LLMs\nhave also gained traction in the security community, revealing security\nvulnerabilities and showcasing their potential in security-related tasks. This\npaper explores the intersection of LLMs with security and privacy.\nSpecifically, we investigate how LLMs positively impact security and privacy,\npotential risks and threats associated with their use, and inherent\nvulnerabilities within LLMs. Through a comprehensive literature review, the\npaper categorizes findings into \"The Good\" (beneficial LLM applications), \"The\nBad\" (offensive applications), and \"The Ugly\" (vulnerabilities and their\ndefenses). We have some interesting findings. For example, LLMs have proven to\nenhance code and data security, outperforming traditional methods. However,\nthey can also be harnessed for various attacks (particularly user-level\nattacks) due to their human-like reasoning abilities. We have identified areas\nthat require further research efforts. For example, research on model and\nparameter extraction attacks is limited and often theoretical, hindered by LLM\nparameter scale and confidentiality. Safe instruction tuning, a recent\ndevelopment, requires more exploration. We hope that our work can shed light on\nthe LLMs' potential to both bolster and jeopardize cybersecurity.",
        "translated": "大型语言模型(LLM) ，如 GPT-3和 BERT，彻底改变了自然语言的理解和生成。他们具有深刻的语言理解能力，类似人类的文本生成能力，上下文意识和强大的问题解决技能，使他们在各个领域(如搜索引擎，客户支持，翻译)无价。与此同时，LLM 也在安全领域获得了吸引力，揭示了安全漏洞，并在与安全相关的任务中展示了它们的潜力。本文探讨了 LLM 与安全性和隐私性的交集。具体来说，我们研究 LLM 如何积极影响安全和隐私、与其使用相关的潜在风险和威胁，以及 LLM 内部固有的脆弱性。通过全面的文献综述，本文将研究结果分为“好的”(有益的 LLM 应用)、“坏的”(攻击性应用)和“丑陋的”(漏洞及其防御)。我们有些有趣的发现。例如，LLM 已被证明可以增强代码和数据安全性，性能优于传统方法。然而，由于它们具有类似于人类的推理能力，因此也可以利用它们进行各种攻击(特别是用户级攻击)。我们已经确定了需要进一步研究的领域。例如，模型和参数提取攻击的研究是有限的，往往是理论上的，受到 LLM 参数规模和机密性的阻碍。安全的指令调优是最近的发展，需要更多的探索。我们希望我们的工作能够揭示 LLM 在加强和危害网络安全方面的潜力。"
    },
    {
        "title": "DFTWS for blockchain: Deterministic, Fair and Transparent Winner\n  Selection",
        "url": "http://arxiv.org/abs/2312.01951v1",
        "pub_date": "2023-12-04",
        "summary": "This publication describes the block winner selection process that will be\nused in a novel Proof-of-Useful-Work blockchain for High Energy Physics that\nthe authors are currently working on. Instead of spamming hashing operations to\nmine blocks, miners will be running Monte Carlo simulations to support a\nreal-world HEP experiment with useful data. The block problems will be defined\nby a Root Authority which is represented by a HEP experiment like CBM. The\nfocus in this publication is a mechanism that allows the Root Authority to\nselect a winner from a list of nodes that solved a block problem. The mechanism\nis designed so that winner selection is deterministic, fair and transparent.\nThis mechanism allows every node to verify the fairness of the winner selection\nprocess without giving the nodes a tool to be able to improve their own winning\nchances.",
        "translated": "这份出版物描述了区块获胜者的选择过程，这个过程将被用于作者目前正在研究的高能物理的有用工作区块链的新证明。矿工们将运行蒙特卡洛模拟，以支持一个真实世界的 HEP 实验，而不是向矿区发送垃圾邮件散列操作。块问题将由一个根权威来定义，这个根权威由一个像 CBM 这样的 HEP 实验来表示。此发布的重点是允许根权威从解决块问题的节点列表中选择获胜者的机制。该机制的设计使得获胜者的选择是确定性的、公平的和透明的。这种机制允许每个节点验证赢家选择过程的公平性，而不需要为节点提供一个工具来提高它们自己的赢家机会。"
    },
    {
        "title": "Intrusion Detection System with Machine Learning and Multiple Datasets",
        "url": "http://arxiv.org/abs/2312.01941v1",
        "pub_date": "2023-12-04",
        "summary": "As Artificial Intelligence (AI) technologies continue to gain traction in the\nmodern-day world, they ultimately pose an immediate threat to current\ncybersecurity systems via exploitative methods. Prompt engineering is a\nrelatively new field that explores various prompt designs that can hijack large\nlanguage models (LLMs). If used by an unethical attacker, it can enable an AI\nsystem to offer malicious insights and code to them. In this paper, an enhanced\nintrusion detection system (IDS) that utilizes machine learning (ML) and\nhyperparameter tuning is explored, which can improve a model's performance in\nterms of accuracy and efficacy. Ultimately, this improved system can be used to\ncombat the attacks made by unethical hackers. A standard IDS is solely\nconfigured with pre-configured rules and patterns; however, with the\nutilization of machine learning, implicit and different patterns can be\ngenerated through the models' hyperparameter settings and parameters. In\naddition, the IDS will be equipped with multiple datasets so that the accuracy\nof the models improves. We evaluate the performance of multiple ML models and\ntheir respective hyperparameter settings through various metrics to compare\ntheir results to other models and past research work. The results of the\nproposed multi-dataset integration method yielded an accuracy score of 99.9%\nwhen equipped with the XGBoost and random forest classifiers and\nRandomizedSearchCV hyperparameter technique.",
        "translated": "随着人工智能(AI)技术在当今世界继续获得推动力，它们最终通过剥削手段对当前的网络安全系统构成直接威胁。即时工程是一个相对较新的领域，它探索各种可以劫持大型语言模型(LLM)的即时设计。如果被不道德的攻击者使用，它可以使人工智能系统提供恶意的洞察力和代码给他们。本文探讨一种利用机器学习和超参数调整的增强型入侵预防系统(IDS) ，它可以提高模型的准确性和有效性。最终，这个改进的系统可以用来对抗不道德的黑客的攻击。标准的 IDS 只配置了预配置的规则和模式; 然而，利用机器学习，可以通过模型的超参数设置和参数生成隐式和不同的模式。此外，入侵检测系统将配备多个数据集，以提高模型的准确性。我们通过各种指标来评估多个机器学习模型及其各自的超参数设置的性能，以便将其结果与其他模型和过去的研究工作进行比较。结合 XGBoost 和随机森林分类器以及随机搜索 CV 超参数技术，提出的多数据集集成方法的准确率达到了99.9% 。"
    },
    {
        "title": "The CURE To Vulnerabilities in RPKI Validation",
        "url": "http://arxiv.org/abs/2312.01872v1",
        "pub_date": "2023-12-04",
        "summary": "Over recent years, the Resource Public Key Infrastructure (RPKI) has seen\nincreasing adoption, with now 37.8% of the major networks filtering bogus BGP\nroutes. Systems interact with the RPKI over Relying Party (RP) implementations\nthat fetch RPKI objects and feed BGP routers with the validated\nprefix-ownership data. Consequently, any vulnerabilities or flaws within the RP\nsoftware can substantially threaten the stability and security of Internet\nrouting. We uncover severe flaws in all popular RP implementations, making them\nsusceptible to path traversal attacks, remotely triggered crashes, and inherent\ninconsistencies, violating RPKI standards. We report a total of 18\nvulnerabilities that canbe exploited to downgrade RPKI validation in border\nrouters or, worse, enable poisoning of the validation process, resulting in\nmalicious prefixes being wrongfully validated and legitimate RPKI-covered\nprefixes failing validation. Furthermore, our research discloses\ninconsistencies in the validation process, with two popular implementations\nleaving 8149 prefixes unprotected from hijacks, 6405 of which belong to Amazon.\nWhile these findings are significant in their own right, our principal\ncontribution lies in developing CURE, the first-of-its-kind system to\nsystematically detect bugs, vulnerabilities, and RFC compliance issues in RP\nimplementations via automated test generation. CURE is a powerful RPKI\npublication point emulator that enables easy and efficient fuzzing of complex\nRP validation pipelines. It is designed with a set of novel techniques,\nutilizing differential and stateful fuzzing. We generated over 600 million test\ncases and tested all popular RPs on them. Following our disclosure, the vendors\nalready assigned CVEs to the vulnerabilities we found.",
        "translated": "近年来，资源公开密钥基础设施(RPKI)得到了越来越多的采用，目前有37.8% 的主要网络过滤伪造的 BGP 路由。系统与 RPKI over Relying Party (RP)实现交互，这些实现获取 RPKI 对象并向 BGP 路由器提供经过验证的前缀所有权数据。因此，RP 软件中的任何漏洞或缺陷都会严重威胁到 Internet 路由的稳定性和安全性。我们发现了所有流行的 RP 实现中的严重缺陷，使得它们容易受到路径遍历攻击、远程触发的崩溃以及内在的不一致性，从而违反了 RPKI 标准。我们报告了总共18个漏洞，这些漏洞可以被用来降低边界路由器中的 RPKI 验证的等级，或者更糟糕的是，使验证过程中毒，导致恶意前缀被错误地验证，合法的 RPKI 覆盖的前缀失败验证。此外，我们的研究揭示了验证过程中的不一致性，有两个流行的实现使得8149个前缀没有受到劫持保护，其中6405个属于 Amazon。虽然这些发现本身意义重大，但我们的主要贡献在于开发 CURE，这是首个通过自动测试生成系统地检测 RP 实现中的 bug、漏洞和 RFC 遵从性问题的系统。CURE 是一个功能强大的 RPKI 发布点模拟器，可以轻松有效地对复杂 RP 验证管道进行模糊化。它采用了一套新颖的技术，利用微分和状态模糊。我们生成了超过6亿个测试用例，并在它们上面测试了所有流行的 RP。根据我们的披露，供应商已经为我们发现的漏洞分配了 CVE。"
    },
    {
        "title": "Cyber Insurance for Cyber Resilience",
        "url": "http://arxiv.org/abs/2312.02921v1",
        "pub_date": "2023-12-05",
        "summary": "Cyber insurance is a complementary mechanism to further reduce the financial\nimpact on the systems after their effort in defending against cyber attacks and\nimplementing resilience mechanism to maintain the system-level operator even\nthough the attacker is already in the system. This chapter presents a review of\nthe quantitative cyber insurance design framework that takes into account the\nincentives as well as the perceptual aspects of multiple parties. The design\nframework builds on the correlation between state-of-the-art attacker vectors\nand defense mechanisms. In particular, we propose the notion of residual risks\nto characterize the goal of cyber insurance design. By elaborating the\ninsurer's observations necessary for the modeling of the cyber insurance\ncontract, we make comparison between the design strategies of the insurer under\nscenarios with different monitoring rules. These distinct but practical\nscenarios give rise to the concept of the intensity of the moral hazard issue.\nUsing the modern techniques in quantifying the risk preferences of individuals,\nwe link the economic impacts of perception manipulation with moral hazard. With\nthe joint design of cyber insurance design and risk perceptions, cyber\nresilience can be enhanced under mild assumptions on the monitoring of\ninsurees' actions. Finally, we discuss possible extensions on the cyber\ninsurance design framework to more sophisticated settings and the regulations\nto strengthen the cyber insurance markets.",
        "translated": "网上保险是一个互补机制，以进一步减低系统在抵御网上攻击和实施抗御机制以维持系统级操作员(即使攻击者已在系统内)后所受到的财务影响。本章介绍了一个定量的网络保险设计框架，考虑到激励和多方感性方面的审查。该设计框架基于最先进的攻击向量和防御机制之间的相关性。特别是，我们提出了残余风险的概念，以表征网络保险设计的目标。通过阐述网络保险合同建模所需的保险人观察，比较了不同监控规则情景下保险人的设计策略。这些截然不同但切合实际的情况引出了道德风险问题强度的概念。利用现代技术来量化个体的风险偏好，我们将感知操纵的经济影响与道德风险联系起来。透过联合设计网上保险的设计和风险认知，我们可以在适当的假设下监察保险人的行为，从而提高网上抗御风险的能力。最后，我们讨论了网络保险设计框架的可能扩展，以更复杂的设置和法规，以加强网络保险市场。"
    },
    {
        "title": "Zero Trust for Cyber Resilience",
        "url": "http://arxiv.org/abs/2312.02882v1",
        "pub_date": "2023-12-05",
        "summary": "The increased connectivity and potential insider threats make traditional\nnetwork defense vulnerable. Instead of assuming that everything behind the\nsecurity perimeter is safe, the zero-trust security model verifies every\nincoming request before granting access. This chapter draws attention to the\ncyber resilience within the zero-trust model. We introduce the evolution from\ntraditional perimeter-based security to zero trust and discuss their\ndifference. Two key elements of the zero-trust engine are trust evaluation (TE)\nand policy engine (PE). We introduce the design of the two components and\ndiscuss how their interplay would contribute to cyber resilience. Dynamic game\ntheory and learning are applied as quantitative approaches to achieve automated\nzero-trust cyber resilience. Several case studies and implementations are\nintroduced to illustrate the benefits of such a security model.",
        "translated": "日益增长的连通性和潜在的内部威胁使得传统的网络防御容易受到攻击。零信任安全模型不假设安全边界之后的所有内容都是安全的，而是在授予访问权限之前验证每个传入请求。本章提请注意零信任模型中的网络弹性。介绍了传统的基于周界的安全模型向零信任模型的演化过程，并讨论了它们之间的区别。零信任引擎的两个关键元素是信任评估(TE)和策略引擎(PE)。我们将介绍这两个组件的设计，并讨论它们的相互作用如何有助于提高网络应变能力。将动态博弈理论和学习作为定量方法，实现自动零信任网络弹性。介绍了几个案例研究和实现，以说明这种安全模型的好处。"
    },
    {
        "title": "Can a Tabula Recta provide security in the XXI century?",
        "url": "http://arxiv.org/abs/2312.02869v1",
        "pub_date": "2023-12-05",
        "summary": "In the not so unlikely scenario of total compromise of computers accessible\nto a group of users, they might be tempted to resort to human-computable\npaper-and-pencil cryptographic methods aided by a classic Tabula Recta, which\nhelps to perform addition and subtraction directly with letters. But do these\nclassic algorithms, or some new ones using the same simple tools, have any\nchance against computer-aided cryptanalysis? In this paper I discuss how some\nhuman-computable algorithms can indeed afford sufficient security in this\nsituation, drawing conclusions from computer-based statistical analysis. Three\nkinds of algorithms are discussed: those that concentrate entropy from shared\ntext sources, stream ciphers based on arithmetic of non-binary spaces, and\nhash-like algorithms that may be used to generate a password from a challenge\ntext.",
        "translated": "在一群用户可以使用的计算机完全妥协的情况下，他们可能会倾向于采用人工计算的纸笔加密方法，辅以经典的 Tabula Recta，这有助于直接用字母执行加减运算。但是，这些经典算法，或者使用同样简单工具的一些新算法，有机会对抗计算机辅助密码分析吗？在本文中，我讨论了一些人工计算算法如何在这种情况下确实能够提供足够的安全性，从基于计算机的统计分析中得出结论。讨论了三种算法: 从共享文本源集中熵的算法、基于非二进制空间算法的流密码算法和可用于从挑战文本生成密码的类哈希算法。"
    },
    {
        "title": "A Review of Password-less User Authentication Schemes",
        "url": "http://arxiv.org/abs/2312.02845v1",
        "pub_date": "2023-12-05",
        "summary": "Since the demise of the password was predicted in 2004, different attempts in\nindustry and academia have been made to create an alternative for the use of\npasswords in authentication, without compromising on security and user\nexperience. This review examines password-less authentication schemes that have\nbeen proposed since after the death knell was placed on passwords in 2004. We\nstart with a brief discussion of the requirements of authentication systems and\nthen identify various password-less authentication proposals to date. We then\nevaluate the truly password-less and practical schemes using a framework that\nexamines authentication credentials based on their impact on user experience,\noverall security, and ease of deployment. The findings of this review observe a\ndifficulty in balancing security with a user experience compared to that of\npasswords in new password-less schemes, providing the opportunity for new\napplied research to leverage existing knowledge and combine technologies and\ntechniques in innovative ways that can address this imbalance.",
        "translated": "自从2004年预测密码将不复存在以来，工业界和学术界作出了不同的努力，在不损害安全和用户体验的情况下，为在认证中使用密码创造一种替代办法。本文回顾了自2004年密码丧钟敲响之后提出的无密码认证方案。我们首先简要讨论认证系统的需求，然后确定迄今为止的各种无密码认证方案。然后，我们使用一个框架来评估真正无密码和实用的方案，该框架根据身份验证凭据对用户体验、总体安全性和部署方便性的影响来检查身份验证凭据。本次审查的结果表明，与新的无密码方案中的密码体验相比，在平衡安全与用户体验方面存在困难，这为新的应用研究提供了机会，以利用现有知识，并以创新方式将技术和技术结合起来，解决这种不平衡。"
    },
    {
        "title": "Scaling Laws for Adversarial Attacks on Language Model Activations",
        "url": "http://arxiv.org/abs/2312.02780v1",
        "pub_date": "2023-12-05",
        "summary": "We explore a class of adversarial attacks targeting the activations of\nlanguage models. By manipulating a relatively small subset of model\nactivations, $a$, we demonstrate the ability to control the exact prediction of\na significant number (in some cases up to 1000) of subsequent tokens $t$. We\nempirically verify a scaling law where the maximum number of target tokens\n$t_\\mathrm{max}$ predicted depends linearly on the number of tokens $a$ whose\nactivations the attacker controls as $t_\\mathrm{max} = \\kappa a$. We find that\nthe number of bits of control in the input space needed to control a single bit\nin the output space (what we call attack resistance $\\chi$) is remarkably\nconstant between $\\approx 16$ and $\\approx 25$ over 2 orders of magnitude of\nmodel sizes for different language models. Compared to attacks on tokens,\nattacks on activations are predictably much stronger, however, we identify a\nsurprising regularity where one bit of input steered either via activations or\nvia tokens is able to exert control over a similar amount of output bits. This\ngives support for the hypothesis that adversarial attacks are a consequence of\ndimensionality mismatch between the input and output spaces. A practical\nimplication of the ease of attacking language model activations instead of\ntokens is for multi-modal and selected retrieval models, where additional data\nsources are added as activations directly, sidestepping the tokenized input.\nThis opens up a new, broad attack surface. By using language models as a\ncontrollable test-bed to study adversarial attacks, we were able to experiment\nwith input-output dimensions that are inaccessible in computer vision,\nespecially where the output dimension dominates.",
        "translated": "我们探讨了一类针对语言模型激活的对抗性攻击。通过操纵模型激活的一个相对较小的子集 $a $，我们展示了控制后续标记 $t $的大量(在某些情况下高达1000)的精确预测的能力。我们通过经验验证了一个扩展定律，其中预测的目标标记 $t _ mathrm { max } $的最大数量线性地取决于攻击者控制的激活为 $t _ mathrm { max } = kappa a $的标记 $a $的数量。我们发现输入空间中控制输出空间中的一个比特所需的控制位数(我们称之为攻击阻力 $chi $)在大约16美元和大约25美元之间是非常稳定的。对于不同的语言模型，超过2数量级的模型大小。与对令牌的攻击相比，对激活的攻击可以预见地强得多，然而，我们发现了一个令人惊讶的规律，即通过激活或通过令牌操纵的一个比特输入能够对类似数量的输出比特施加控制。这支持了对抗性攻击是输入和输出空间维度不匹配的结果的假设。易于攻击的语言模型激活而不是令牌的一个实际含义是对于多模态和选择的检索模型，其中额外的数据源直接作为激活添加，规避了令牌化输入。这打开了一个新的，广泛的攻击表面。通过使用语言模型作为一个可控的测试平台来研究对抗性攻击，我们能够实验输入-输出维度在计算机视觉中无法访问，特别是在输出维度占主导地位的情况下。"
    },
    {
        "title": "Airdrops: Giving Money Away Is Harder Than It Seems",
        "url": "http://arxiv.org/abs/2312.02752v1",
        "pub_date": "2023-12-05",
        "summary": "Airdrops are used by blockchain applications and platforms to attract an\ninitial user base, and to grow the user base over time. In the case of many\nairdrops, tokens are distributed to select users as a \"reward\" for interacting\nwith the underlying platform, with a long-term goal of creating a loyal\ncommunity that will generate genuine economic activity well after the airdrop\nhas been completed. Although airdrops are widely used by the blockchain\nindustry, a proper understanding of the factors contributing to an airdrop's\nsuccess is generally lacking. In this work, we outline the design space for\nairdrops, and specify a reasonable list of outcomes that an airdrop should\nideally result in. We then analyze on-chain data from several larger-scale\nairdrops to empirically evaluate the success of previous airdrops, with respect\nto our desiderata. In our analysis, we demonstrate that airdrop farmers\nfrequently dispose of the lion's share of airdrops proceeds via exchanges. Our\nanalysis is followed by an overview of common pitfalls that common airdrop\ndesigns lend themselves to, which are then used to suggest concrete guidelines\nfor better airdrops.",
        "translated": "区块链应用程序和平台使用空投来吸引初始用户基础，并随着时间的推移增加用户基础。在许多空投的情况下，代币被分发给选择用户，作为与底层平台互动的“奖励”，其长期目标是创建一个忠诚的社区，在空投完成后很久就能产生真正的经济活动。虽然空投被区块链行业广泛使用，但是对于空投成功的因素缺乏正确的理解。在这项工作中，我们概述了空投的设计空间，并指定了一个合理的结果清单，空投应理想的结果。然后，我们分析来自几次大规模空投的连锁数据，以实证的方式评估以往空投的成功与否。在我们的分析中，我们证明空投农民经常通过交换处置大部分空投收益。我们的分析之后是一个常见的陷阱，常见的空投设计借给自己的概述，然后用来建议具体的指导方针，以更好的空投。"
    },
    {
        "title": "User Interaction Data in Apps: Comparing Policy Claims to\n  Implementations",
        "url": "http://arxiv.org/abs/2312.02710v1",
        "pub_date": "2023-12-05",
        "summary": "As mobile app usage continues to rise, so does the generation of extensive\nuser interaction data, which includes actions such as swiping, zooming, or the\ntime spent on a screen. Apps often collect a large amount of this data and\nclaim to anonymize it, yet concerns arise regarding the adequacy of these\nmeasures. In many cases, the so-called anonymized data still has the potential\nto profile and, in some instances, re-identify individual users. This situation\nis compounded by a lack of transparency, leading to potential breaches of user\ntrust.\n  Our work investigates the gap between privacy policies and actual app\nbehavior, focusing on the collection and handling of user interaction data. We\nanalyzed the top 100 apps across diverse categories using static analysis\nmethods to evaluate the alignment between policy claims and implemented data\ncollection techniques. Our findings highlight the lack of transparency in data\ncollection and the associated risk of re-identification, raising concerns about\nuser privacy and trust. This study emphasizes the importance of clear\ncommunication and enhanced transparency in privacy practices for mobile app\ndevelopment.",
        "translated": "随着移动应用程序使用的持续增长，大量用户交互数据的产生也在增加，其中包括滑动、缩放或在屏幕上花费的时间等操作。应用程序经常收集大量这类数据并声称要对其进行匿名化，然而这些措施是否充分引起了人们的关注。在许多情况下，所谓的匿名数据仍然有可能对个人用户进行分析，并在某些情况下重新识别个人用户。这种情况由于缺乏透明度而加剧，导致潜在的用户信任受到破坏。我们的工作调查了隐私政策和实际应用程序行为之间的差距，重点是收集和处理用户交互数据。我们使用静态分析方法分析了不同类别的前100个应用程序，以评估保单索赔和实现的数据收集技术之间的一致性。我们的研究结果突出了数据收集缺乏透明度以及相关的重新识别风险，引起了对用户隐私和信任的担忧。这项研究强调了明确的沟通的重要性，并提高了移动应用程序开发的隐私实践的透明度。"
    },
    {
        "title": "(Provable) Adversarial Robustness for Group Equivariant Tasks: Graphs,\n  Point Clouds, Molecules, and More",
        "url": "http://arxiv.org/abs/2312.02708v1",
        "pub_date": "2023-12-05",
        "summary": "A machine learning model is traditionally considered robust if its prediction\nremains (almost) constant under input perturbations with small norm. However,\nreal-world tasks like molecular property prediction or point cloud segmentation\nhave inherent equivariances, such as rotation or permutation equivariance. In\nsuch tasks, even perturbations with large norm do not necessarily change an\ninput's semantic content. Furthermore, there are perturbations for which a\nmodel's prediction explicitly needs to change. For the first time, we propose a\nsound notion of adversarial robustness that accounts for task equivariance. We\nthen demonstrate that provable robustness can be achieved by (1) choosing a\nmodel that matches the task's equivariances (2) certifying traditional\nadversarial robustness. Certification methods are, however, unavailable for\nmany models, such as those with continuous equivariances. We close this gap by\ndeveloping the framework of equivariance-preserving randomized smoothing, which\nenables architecture-agnostic certification. We additionally derive the first\narchitecture-specific graph edit distance certificates, i.e. sound robustness\nguarantees for isomorphism equivariant tasks like node classification. Overall,\na sound notion of robustness is an important prerequisite for future work at\nthe intersection of robust and geometric machine learning.",
        "translated": "当机器学习模型的预测值在小范数输入扰动下保持(几乎)不变时，机器学习模型传统上被认为是鲁棒的。然而，真实世界的任务，如分子特性预测或点云分割有固有的等方差，如旋转或置换等方差。在这样的任务中，即使是大范围的扰动也不一定改变输入的语义内容。此外，还有一些扰动，模型的预测显然需要改变。第一次，我们提出了一个健全的对抗鲁棒性的概念，解释任务等方差。然后，我们证明了可证明的鲁棒性可以通过(1)选择一个模型，匹配任务的等方差(2)证明传统的对抗鲁棒性。然而，认证方法对于许多模型是不可用的，例如那些具有连续等方差的模型。我们通过开发等方差保持随机平滑的框架来弥补这一差距，该框架支持体系结构无关的认证。此外，我们还得到了第一个特定于体系结构的图编辑距离证书，即同构等变任务(如节点分类)的健壮性保证。总的来说，一个健全的鲁棒性概念是未来在鲁棒性和几何机器学习交叉点工作的重要先决条件。"
    },
    {
        "title": "Robust Backdoor Detection for Deep Learning via Topological Evolution\n  Dynamics",
        "url": "http://arxiv.org/abs/2312.02673v1",
        "pub_date": "2023-12-05",
        "summary": "A backdoor attack in deep learning inserts a hidden backdoor in the model to\ntrigger malicious behavior upon specific input patterns. Existing detection\napproaches assume a metric space (for either the original inputs or their\nlatent representations) in which normal samples and malicious samples are\nseparable. We show that this assumption has a severe limitation by introducing\na novel SSDT (Source-Specific and Dynamic-Triggers) backdoor, which obscures\nthe difference between normal samples and malicious samples.\n  To overcome this limitation, we move beyond looking for a perfect metric\nspace that would work for different deep-learning models, and instead resort to\nmore robust topological constructs. We propose TED (Topological Evolution\nDynamics) as a model-agnostic basis for robust backdoor detection. The main\nidea of TED is to view a deep-learning model as a dynamical system that evolves\ninputs to outputs. In such a dynamical system, a benign input follows a natural\nevolution trajectory similar to other benign inputs. In contrast, a malicious\nsample displays a distinct trajectory, since it starts close to benign samples\nbut eventually shifts towards the neighborhood of attacker-specified target\nsamples to activate the backdoor.\n  Extensive evaluations are conducted on vision and natural language datasets\nacross different network architectures. The results demonstrate that TED not\nonly achieves a high detection rate, but also significantly outperforms\nexisting state-of-the-art detection approaches, particularly in addressing the\nsophisticated SSDT attack. The code to reproduce the results is made public on\nGitHub.",
        "translated": "深度学习中的后门攻击在模型中插入了一个隐藏的后门，以触发针对特定输入模式的恶意行为。现有的检测方法假定一个度量空间(用于原始输入或其潜在表示) ，其中正常样本和恶意样本是可分的。我们通过引入一种新的 SSDT (源特定和动态触发器)后门，模糊了正常样本和恶意样本之间的区别，从而表明这种假设具有严重的局限性。为了克服这个限制，我们不再寻找适用于不同深度学习模型的完美度量空间，而是求助于更健壮的拓扑结构。我们提出 TED (拓扑演化动力学)作为鲁棒后门检测的模型不可知基础。TED 的主要思想是将深度学习模式视为一种动力系统，将输入转化为输出。在这种动力系统下，一个良性的输入遵循一个与其他良性输入类似的自然进化轨迹。相比之下，恶意样本显示一个独特的轨迹，因为它开始接近良性样本，但最终转移到邻近的攻击者指定的目标样本，以激活后门。对不同网络结构的视觉和自然语言数据集进行了广泛的评估。实验结果表明，TED 不仅具有较高的检测率，而且其检测性能明显优于现有的先进检测方法，尤其是在解决复杂的 SSDT 攻击方面。复制结果的代码在 GitHub 上公开。"
    },
    {
        "title": "Understanding Ethereum Mempool Security under Asymmetric DoS by Symbolic\n  Fuzzing",
        "url": "http://arxiv.org/abs/2312.02642v1",
        "pub_date": "2023-12-05",
        "summary": "In blockchains, mempool controls transaction flow before consensus, denial of\nwhose service hurts the health and security of blockchain networks. This paper\npresents MPFUZZ, the first mempool fuzzer to find asymmetric DoS bugs by\nsymbolically exploring mempool state space and optimistically estimating the\npromisingness an intermediate state is in reaching bug oracles. Compared to the\nbaseline blockchain fuzzers, MPFUZZ achieves a &gt; 100x speedup in finding known\nDETER exploits. Running MPFUZZ on six major Ethereum clients leads to the\ndiscovering of new mempool vulnerabilities, which exhibit a wide variety of\nsophisticated patterns including stealthy mempool eviction and mempool locking.\nRule-based mitigation schemes are proposed against newly discovered\nvulnerabilities.",
        "translated": "在区块链中，内存池在协商一致之前控制事务流，拒绝其服务会损害区块链网络的健康和安全。本文介绍了 MPFUZZ，它是第一个通过象征性地探索 mempool 状态空间，并乐观地估计居间态达到 bug 预言的可能性来发现非对称 DoS bug 的 mempool 模糊器。与基线区块链模糊器相比，MPFUZZ 在发现已知 DETER 漏洞方面获得了 > 100倍的加速。在六个主要的 Etherum 客户端上运行 MPFUZZ 可以发现新的内存池漏洞，这些漏洞表现出各种复杂的模式，包括隐蔽的内存池驱逐和内存池锁定。针对新发现的漏洞提出了基于规则的缓解方案。"
    },
    {
        "title": "Memory Triggers: Unveiling Memorization in Text-To-Image Generative\n  Models through Word-Level Duplication",
        "url": "http://arxiv.org/abs/2312.03692v1",
        "pub_date": "2023-12-06",
        "summary": "Diffusion-based models, such as the Stable Diffusion model, have\nrevolutionized text-to-image synthesis with their ability to produce\nhigh-quality, high-resolution images. These advancements have prompted\nsignificant progress in image generation and editing tasks. However, these\nmodels also raise concerns due to their tendency to memorize and potentially\nreplicate exact training samples, posing privacy risks and enabling adversarial\nattacks. Duplication in training datasets is recognized as a major factor\ncontributing to memorization, and various forms of memorization have been\nstudied so far. This paper focuses on two distinct and underexplored types of\nduplication that lead to replication during inference in diffusion-based\nmodels, particularly in the Stable Diffusion model. We delve into these\nlesser-studied duplication phenomena and their implications through two case\nstudies, aiming to contribute to the safer and more responsible use of\ngenerative models in various applications.",
        "translated": "基于扩散的模型，如稳定扩散模型，已经彻底改变了文本到图像合成的能力，产生高质量，高分辨率的图像。这些进步推动了图像生成和编辑任务方面的重大进展。然而，这些模型也引起了关注，因为它们倾向于记忆和潜在地复制精确的训练样本，构成隐私风险，并使敌对攻击成为可能。训练数据集中的重复被认为是促进记忆的一个主要因素，到目前为止已经研究了各种形式的记忆。本文重点研究了基于扩散的模型，特别是稳定扩散模型中两种不同的复制类型，它们在推理过程中导致了复制。我们通过两个案例研究探讨了这些研究较少的重复现象及其影响，旨在促进在各种应用中更安全和更负责任地使用生成模型。"
    },
    {
        "title": "Fed-urlBERT: Client-side Lightweight Federated Transformers for URL\n  Threat Analysis",
        "url": "http://arxiv.org/abs/2312.03636v1",
        "pub_date": "2023-12-06",
        "summary": "In evolving cyber landscapes, the detection of malicious URLs calls for\ncooperation and knowledge sharing across domains. However, collaboration is\noften hindered by concerns over privacy and business sensitivities. Federated\nlearning addresses these issues by enabling multi-clients collaboration without\ndirect data exchange. Unfortunately, if highly expressive Transformer models\nare used, clients may face intolerable computational burdens, and the exchange\nof weights could quickly deplete network bandwidth. In this paper, we propose\nFed-urlBERT, a federated URL pre-trained model designed to address both privacy\nconcerns and the need for cross-domain collaboration in cybersecurity.\nFed-urlBERT leverages split learning to divide the pre-training model into\nclient and server part, so that the client part takes up less extensive\ncomputation resources and bandwidth. Our appraoch achieves performance\ncomparable to centralized model under both independently and identically\ndistributed (IID) and two non-IID data scenarios. Significantly, our federated\nmodel shows about an 7% decrease in the FPR compared to the centralized model.\nAdditionally, we implement an adaptive local aggregation strategy that\nmitigates heterogeneity among clients, demonstrating promising performance\nimprovements. Overall, our study validates the applicability of the proposed\nTransformer federated learning for URL threat analysis, establishing a\nfoundation for real-world collaborative cybersecurity efforts. The source code\nis accessible at https://github.com/Davidup1/FedURLBERT.",
        "translated": "在不断变化的网络环境中，检测恶意网址需要跨领域的合作和知识共享。然而，由于担心隐私和业务敏感性，协作常常受到阻碍。联合学习通过在没有直接数据交换的情况下实现多客户协作来解决这些问题。不幸的是，如果使用高度表达性变压器模型，客户端可能会面临难以承受的计算负担，权重的交换可能会迅速耗尽网络带宽。在本文中，我们提出了一个联邦 URL 预训练模型 Fed-urlBERT，该模型旨在解决网络安全中的隐私问题和跨域协作的需求。Fed-urlBERT 利用拆分学习将预训练模型划分为客户端和服务器端，从而减少了客户端部分所占用的计算资源和带宽。在独立和同分布(IID)以及两个非 IID 数据场景下，我们的方法实现了与集中式模型相当的性能。值得注意的是，我们的联邦模型显示，与集中模型相比，FPR 下降了约7% 。此外，我们实现了一个自适应的局部聚合策略，缓解了客户端之间的异构性，显示了有希望的性能改进。总的来说，我们的研究验证了所提出的变压器联邦学习对 URL 威胁分析的适用性，为实际的网络安全协作工作奠定了基础。源代码可在 https://github.com/davidup1/fedurlbert 查阅。"
    },
    {
        "title": "Behavioral Authentication for Security and Safety",
        "url": "http://arxiv.org/abs/2312.03429v1",
        "pub_date": "2023-12-06",
        "summary": "The issues of both system security and safety can be dissected integrally\nfrom the perspective of behavioral \\emph{appropriateness}. That is, a system is\nsecure or safe can be judged by whether the behavior of certain agent(s) is\n\\emph{appropriate} or not. Specifically, a so-called \\emph{appropriate\nbehavior} involves the right agent performing the right actions at the right\ntime under certain conditions. Then, according to different levels of\nappropriateness and degrees of custodies, behavioral authentication can be\ngraded into three levels, i.e., the authentication of behavioral\n\\emph{Identity}, \\emph{Conformity}, and \\emph{Benignity}. In a broad sense, for\nthe security and safety issue, behavioral authentication is not only an\ninnovative and promising method due to its inherent advantages but also a\ncritical and fundamental problem due to the ubiquity of behavior generation and\nthe necessity of behavior regulation in any system. By this classification,\nthis review provides a comprehensive examination of the background and\npreliminaries of behavioral authentication. It further summarizes existing\nresearch based on their respective focus areas and characteristics. The\nchallenges confronted by current behavioral authentication methods are\nanalyzed, and potential research directions are discussed to promote the\ndiversified and integrated development of behavioral authentication.",
        "translated": "系统安全和系统安全问题可以从行为适当性的角度进行整体剖析。也就是说，一个系统是安全的还是安全的，可以通过某个代理人的行为是否适当来判断。具体来说，所谓的 emph {适当行为}涉及到在特定条件下正确的代理在正确的时间执行正确的操作。然后，根据不同的适当性和监管程度，将行为认证分为三个层次，即行为认证{身份}、行为认证{符合}和行为认证{善良}。从广义上讲，对于安全问题来说，行为认证不仅是一种创新的、有前途的方法，因为其固有的优势，而且由于行为生成的普遍性和任何系统中行为规范的必要性，行为认证也是一个关键的、基本的问题。通过这一分类，本文对行为认证的背景和初步研究进行了综述。根据各自研究的重点领域和特点，进一步总结了现有的研究成果。分析了当前行为认证方法所面临的挑战，探讨了行为认证的研究方向，以促进行为认证的多元化、整合化发展。"
    },
    {
        "title": "Synthesizing Physical Backdoor Datasets: An Automated Framework\n  Leveraging Deep Generative Models",
        "url": "http://arxiv.org/abs/2312.03419v1",
        "pub_date": "2023-12-06",
        "summary": "Backdoor attacks, representing an emerging threat to the integrity of deep\nneural networks, have garnered significant attention due to their ability to\ncompromise deep learning systems clandestinely. While numerous backdoor attacks\noccur within the digital realm, their practical implementation in real-world\nprediction systems remains limited and vulnerable to disturbances in the\nphysical world. Consequently, this limitation has given rise to the development\nof physical backdoor attacks, where trigger objects manifest as physical\nentities within the real world. However, creating the requisite dataset to\ntrain or evaluate a physical backdoor model is a daunting task, limiting the\nbackdoor researchers and practitioners from studying such physical attack\nscenarios. This paper unleashes a recipe that empowers backdoor researchers to\neffortlessly create a malicious, physical backdoor dataset based on advances in\ngenerative modeling. Particularly, this recipe involves 3 automatic modules:\nsuggesting the suitable physical triggers, generating the poisoned candidate\nsamples (either by synthesizing new samples or editing existing clean samples),\nand finally refining for the most plausible ones. As such, it effectively\nmitigates the perceived complexity associated with creating a physical backdoor\ndataset, transforming it from a daunting task into an attainable objective.\nExtensive experiment results show that datasets created by our \"recipe\" enable\nadversaries to achieve an impressive attack success rate on real physical world\ndata and exhibit similar properties compared to previous physical backdoor\nattack studies. This paper offers researchers a valuable toolkit for studies of\nphysical backdoors, all within the confines of their laboratories.",
        "translated": "后门攻击作为对深度神经网络完整性的一种新兴威胁，由于其能够秘密地破坏深度学习系统而引起了人们的广泛关注。虽然在数字领域内发生了许多后门攻击，但它们在现实世界预测系统中的实际应用仍然有限，而且容易受到现实世界中的干扰。因此，这种局限性导致了物理后门攻击的发展，在这种攻击中，触发对象表现为现实世界中的物理实体。然而，创建训练或评估物理后门模型所必需的数据集是一项艰巨的任务，限制了后门研究人员和实践者研究这种物理攻击场景。这篇论文揭示了一个秘诀，使后门研究人员能够基于生成建模的进步，毫不费力地创建一个恶意的物理后门数据集。特别是，这个配方包括3个自动模块: 建议合适的物理触发器，生成有毒的候选样本(通过合成新样本或编辑现有的干净样本) ，最后精炼出最合理的样本。因此，它有效地降低了与创建物理后门数据集相关的感知复杂性，将其从一项艰巨的任务转变为一个可实现的目标。大量的实验结果表明，由我们的“秘方”创建的数据集使得对手能够在真实物理世界的数据上取得令人印象深刻的攻击成功率，并且与以前的物理后门攻击研究表现出类似的特性。本文为研究人员提供了一个有价值的工具包的物理后门的研究，所有在他们的实验室范围内。"
    },
    {
        "title": "Securing Data Platforms: Strategic Masking Techniques for Privacy and\n  Security for B2B Enterprise Data",
        "url": "http://arxiv.org/abs/2312.03293v1",
        "pub_date": "2023-12-06",
        "summary": "In today's digital age, the imperative to protect data privacy and security\nis a paramount concern, especially for business-to-business (B2B) enterprises\nthat handle sensitive information. These enterprises are increasingly\nconstructing data platforms, which are integrated suites of technology\nsolutions architected for the efficient management, processing, storage, and\ndata analysis. It has become critical to design these data platforms with\nmechanisms that inherently support data privacy and security, particularly as\nthey encounter the added complexity of safeguarding unstructured data types\nsuch as log files and text documents. Within this context, data masking stands\nout as a vital feature of data platform architecture. It proactively conceals\nsensitive elements, ensuring data privacy while preserving the information's\nvalue for business operations and analytics. This protective measure entails a\nstrategic two-fold process: firstly, accurately pinpointing the sensitive data\nthat necessitates concealment, and secondly, applying sophisticated methods to\ndisguise that data effectively within the data platform infrastructure. This\nresearch delves into the nuances of embedding advanced data masking techniques\nwithin the very fabric of data platforms and an in-depth exploration of how\nenterprises can adopt a comprehensive approach toward effective data masking\nimplementation by exploring different identification and anonymization\ntechniques.",
        "translated": "在当今的数字时代，保护数据隐私和安全是当务之急，尤其是对于处理敏感信息的 B2B (B2B)企业而言。这些企业越来越多地构建数据平台，这些平台是为高效管理、处理、存储和数据分析而构建的技术解决方案的集成套件。设计这些数据平台的机制必须能够本质上支持数据保密性和安全性，尤其是当它们遇到保护日志文件和文本文档等非结构化数据类型的额外复杂性时，这一点就变得至关重要。在这种情况下，数据屏蔽是数据平台体系结构的一个重要特性。它主动隐藏敏感元素，确保数据隐私，同时保留信息对业务操作和分析的价值。这种保护措施需要一个战略性的双重过程: 首先，准确地确定需要隐藏的敏感数据; 其次，在数据平台基础设施内采用复杂的方法有效地隐藏这些数据。本研究深入探讨了在数据平台结构中嵌入先进的数据掩蔽技术的细微差别，并深入探讨了企业如何通过探索不同的识别和匿名技术，采用一种全面的方法来实现有效的数据掩蔽。"
    },
    {
        "title": "Low-Cost High-Power Membership Inference by Boosting Relativity",
        "url": "http://arxiv.org/abs/2312.03262v1",
        "pub_date": "2023-12-06",
        "summary": "We present a robust membership inference attack (RMIA) that amplifies the\ndistinction between population data and the training data on any target model,\nby effectively leveraging both reference models and reference data in our\nlikelihood ratio test. Our algorithm exhibits superior test power\n(true-positive rate) when compared to prior methods, even at extremely low\nfalse-positive error rates (as low as 0). Also, under computation constraints,\nwhere only a limited number of reference models (as few as 1) are available,\nour method performs exceptionally well, unlike some prior attacks that approach\nrandom guessing in such scenarios. Our method lays the groundwork for\ncost-effective and practical yet powerful and robust privacy risk analysis of\nmachine learning algorithms.",
        "translated": "通过在似然比检验中有效地利用参考模型和参考数据，我们提出了一种鲁棒的成员推理攻击(RMIA) ，它放大了人口数据和任何目标模型上的训练数据之间的区别。我们的算法表现出优越的测试能力(真正的阳性率)相比，以前的方法，即使在极低的假阳性错误率(低至0)。此外，在计算约束下，只有有限数量的参考模型(少至1)可用，我们的方法执行异常良好，不像一些先前的攻击，接近随机猜测在这种情况下。该方法为机器学习算法的性价比高、实用性强、鲁棒性强的隐私风险分析奠定了基础。"
    },
    {
        "title": "A Simple Framework to Enhance the Adversarial Robustness of Deep\n  Learning-based Intrusion Detection System",
        "url": "http://arxiv.org/abs/2312.03245v1",
        "pub_date": "2023-12-06",
        "summary": "Deep learning based intrusion detection systems (DL-based IDS) have emerged\nas one of the best choices for providing security solutions against various\nnetwork intrusion attacks. However, due to the emergence and development of\nadversarial deep learning technologies, it becomes challenging for the adoption\nof DL models into IDS. In this paper, we propose a novel IDS architecture that\ncan enhance the robustness of IDS against adversarial attacks by combining\nconventional machine learning (ML) models and Deep Learning models. The\nproposed DLL-IDS consists of three components: DL-based IDS, adversarial\nexample (AE) detector, and ML-based IDS. We first develop a novel AE detector\nbased on the local intrinsic dimensionality (LID). Then, we exploit the low\nattack transferability between DL models and ML models to find a robust ML\nmodel that can assist us in determining the maliciousness of AEs. If the input\ntraffic is detected as an AE, the ML-based IDS will predict the maliciousness\nof input traffic, otherwise the DL-based IDS will work for the prediction. The\nfusion mechanism can leverage the high prediction accuracy of DL models and low\nattack transferability between DL models and ML models to improve the\nrobustness of the whole system. In our experiments, we observe a significant\nimprovement in the prediction performance of the IDS when subjected to\nadversarial attack, achieving high accuracy with low resource consumption.",
        "translated": "基于深度学习的入侵检测系统(DL-based IDS)已经成为针对各种网络入侵攻击提供安全解决方案的最佳选择之一。然而，由于对抗性深度学习技术的出现和发展，将 DL 模型应用于入侵检测系统成为一个挑战。本文结合传统的机器学习模型和深度学习模型，提出了一种新的入侵检测系统体系结构，可以增强入侵检测系统对敌对攻击的鲁棒性。提出的 DLL 入侵检测系统由三部分组成: 基于 DL 的入侵检测系统、对抗实例(AE)检测器和基于 ML 的入侵检测系统。本文首先提出了一种基于局部内禀维数(LID)的声发射探测器。然后，利用 DL 模型和 ML 模型之间较低的攻击可转移性，寻找一个鲁棒的 ML 模型，以帮助我们判断不良事件的恶意程度。如果输入流量被检测为 AE，基于 ML 的入侵检测系统将对输入流量的恶意程度进行预测，否则基于 DL 的入侵检测系统将对其进行预测。该融合机制可以利用 DL 模型的高预测精度和 DL 模型与 ML 模型之间的低攻击可转移性，提高整个系统的鲁棒性。在我们的实验中，我们观察到入侵检测系统在面对敌对攻击时的预测性能有显著的改善，在低资源消耗的情况下达到高精度。"
    },
    {
        "title": "Who Leaked the Model? Tracking IP Infringers in Accountable Federated\n  Learning",
        "url": "http://arxiv.org/abs/2312.03205v1",
        "pub_date": "2023-12-06",
        "summary": "Federated learning (FL) emerges as an effective collaborative learning\nframework to coordinate data and computation resources from massive and\ndistributed clients in training. Such collaboration results in non-trivial\nintellectual property (IP) represented by the model parameters that should be\nprotected and shared by the whole party rather than an individual user.\nMeanwhile, the distributed nature of FL endorses a malicious client the\nconvenience to compromise IP through illegal model leakage to unauthorized\nthird parties. To block such IP leakage, it is essential to make the IP\nidentifiable in the shared model and locate the anonymous infringer who first\nleaks it. The collective challenges call for \\emph{accountable federated\nlearning}, which requires verifiable ownership of the model and is capable of\nrevealing the infringer's identity upon leakage. In this paper, we propose\nDecodable Unique Watermarking (DUW) for complying with the requirements of\naccountable FL. Specifically, before a global model is sent to a client in an\nFL round, DUW encodes a client-unique key into the model by leveraging a\nbackdoor-based watermark injection. To identify the infringer of a leaked\nmodel, DUW examines the model and checks if the triggers can be decoded as the\ncorresponding keys. Extensive empirical results show that DUW is highly\neffective and robust, achieving over $99\\%$ watermark success rate for Digits,\nCIFAR-10, and CIFAR-100 datasets under heterogeneous FL settings, and\nidentifying the IP infringer with $100\\%$ accuracy even after common watermark\nremoval attempts.",
        "translated": "联邦学习(FL)作为一种有效的合作学习框架，在培训中协调来自大规模和分布式客户端的数据和计算资源。这样的协作导致了由模型参数表示的非平凡的知识产权(IP) ，这些参数应该由整个一方而不是单个用户来保护和共享。同时，FL 的分布式特性为恶意客户端通过非法模型泄露给未经授权的第三方以泄露 IP 地址提供了便利。要阻止这种 IP 泄漏，必须使 IP 在共享模型中可识别，并找到首先泄漏 IP 的匿名侵权者。集体挑战要求使用 emph {可问责的联邦学习} ，这需要可验证的模型所有权，并且能够在泄露时揭示侵权者的身份。本文提出了一种基于可解码的唯一水印算法(DUW) ，以满足可计数 FL 的要求。具体来说，在 FL 轮向客户端发送全局模型之前，DUW 通过利用基于后门的水印注入将客户端唯一密钥编码到模型中。为了识别泄漏模型的侵权者，DUW 检查模型并检查触发器是否可以被解码为相应的密钥。大量的实验结果表明，DUW 是高度有效和健壮的，在异构 FL 设置下，数字，CIFAR-10和 CIFAR-100数据集的水印成功率超过99% ，即使在常见的水印去除尝试之后，也能以100% 的准确性识别 IP 侵权者。"
    },
    {
        "title": "Parallel Proof-of-Work with DAG-Style Voting and Targeted Reward\n  Discounting",
        "url": "http://arxiv.org/abs/2312.03111v1",
        "pub_date": "2023-12-05",
        "summary": "We present parallel proof-of-work with DAG-style voting, a novel\nproof-of-work cryptocurrency protocol that, compared to Bitcoin, provides\nbetter consistency guarantees, higher transaction throughput, lower transaction\nconfirmation latency, and higher resilience against incentive attacks. The\nsuperior consistency guarantees follow from implementing parallel\nproof-of-work, a recent consensus scheme that enforces a configurable number of\nproof-of-work votes per block. Our work is inspired by another recent protocol,\nTailstorm, which structures the individual votes as tree and mitigates\nincentive attacks by discounting the mining rewards proportionally to the depth\nof the tree. We propose to structure the votes as a directed acyclic graph\n(DAG) instead of a tree. This allows for a more targeted punishment of\noffending miners and, as we show through a reinforcement learning based attack\nsearch, makes the protocol even more resilient to incentive attacks. An\ninteresting by-product of our analysis is that parallel proof-of-work without\nreward discounting is less resilient to incentive attacks than Bitcoin in some\nrealistic network scenarios.",
        "translated": "我们提出了 DAG 式投票的并行工作证明协议，这是一种新型的工作证明加密货币协议，与比特币相比，它提供了更好的一致性保证，更高的交易吞吐量，更低的交易确认延迟，以及更高的抵御激励攻击的弹性。优越的一致性保证来自于实现并行的工作证明，最近的共识方案强制每个块具有可配置的工作证明投票数。我们的工作受到最近另一个协议 Tailstorm 的启发，它将个人投票结构化为树，并通过按树的深度折现挖掘奖励来减少激励攻击。我们建议将投票结构设计为一个有向无环图(DAG) ，而不是一棵树。正如我们通过基于强化学习的攻击搜索所展示的那样，这使得该协议能够更有针对性地惩罚冒犯矿工的行为，从而更好地抵御激励性攻击。我们分析的一个有趣的副产品是，在一些现实的网络场景中，没有奖励折扣的并行工作证明对激励性攻击的抵抗力不如比特币。"
    },
    {
        "title": "SoK: Unintended Interactions among Machine Learning Defenses and Risks",
        "url": "http://arxiv.org/abs/2312.04542v1",
        "pub_date": "2023-12-07",
        "summary": "Machine learning (ML) models cannot neglect risks to security, privacy, and\nfairness. Several defenses have been proposed to mitigate such risks. When a\ndefense is effective in mitigating one risk, it may correspond to increased or\ndecreased susceptibility to other risks. Existing research lacks an effective\nframework to recognize and explain these unintended interactions. We present\nsuch a framework, based on the conjecture that overfitting and memorization\nunderlie unintended interactions. We survey existing literature on unintended\ninteractions, accommodating them within our framework. We use our framework to\nconjecture on two previously unexplored interactions, and empirically validate\nour conjectures.",
        "translated": "机器学习(ML)模型不能忽视对安全、隐私和公平性的风险。为了减轻这种风险，已经提出了几种防御措施。当一种防御措施在减轻一种风险方面是有效的，它可能相当于增加或减少对其他风险的易感性。现有的研究缺乏一个有效的框架来识别和解释这些意外的相互作用。我们提出这样一个框架，基于猜测，过度拟合和记忆的基础上无意识的相互作用。我们调查现有的关于意外互动的文献，在我们的框架内适应它们。我们使用我们的框架来推测两个以前未被探索的交互作用，并从经验上验证我们的推测。"
    },
    {
        "title": "MuFuzz: Sequence-Aware Mutation and Seed Mask Guidance for Blockchain\n  Smart Contract Fuzzing",
        "url": "http://arxiv.org/abs/2312.04512v1",
        "pub_date": "2023-12-07",
        "summary": "As blockchain smart contracts become more widespread and carry more valuable\ndigital assets, they become an increasingly attractive target for attackers.\nOver the past few years, smart contracts have been subject to a plethora of\ndevastating attacks, resulting in billions of dollars in financial losses.\nThere has been a notable surge of research interest in identifying defects in\nsmart contracts. However, existing smart contract fuzzing tools are still\nunsatisfactory. They struggle to screen out meaningful transaction sequences\nand specify critical inputs for each transaction. As a result, they can only\ntrigger a limited range of contract states, making it difficult to unveil\ncomplicated vulnerabilities hidden in the deep state space.\n  In this paper, we shed light on smart contract fuzzing by employing a\nsequence-aware mutation and seed mask guidance strategy. In particular, we\nfirst utilize data-flow-based feedback to determine transaction orders in a\nmeaningful way and further introduce a sequence-aware mutation technique to\nexplore deeper states. Thereafter, we design a mask-guided seed mutation\nstrategy that biases the generated transaction inputs to hit target branches.\nIn addition, we develop a dynamic-adaptive energy adjustment paradigm that\nbalances the fuzzing resource allocation during a fuzzing campaign. We\nimplement our designs into a new smart contract fuzzer named MuFuzz, and\nextensively evaluate it on three benchmarks. Empirical results demonstrate that\nMuFuzz outperforms existing tools in terms of both branch coverage and bug\nfinding. Overall, MuFuzz achieves higher branch coverage than state-of-the-art\nfuzzers (up to 25%) and detects 30% more bugs than existing bug detectors.",
        "translated": "随着区块链智能合同变得越来越普遍，并携带更有价值的数字资产，它们成为攻击者越来越有吸引力的目标。在过去的几年里，聪明的合同受到过多毁灭性的攻击，导致数十亿美元的财务损失。在识别智能合同中的缺陷方面，研究兴趣显著增加。然而，现有的智能合同模糊处理工具仍然不尽人意。它们很难筛选出有意义的事务序列，并为每个事务指定关键的输入。因此，它们只能触发有限范围的契约状态，使其难以揭示隐藏在深层状态空间中的复杂脆弱性。本文采用序列感知变异和种子掩码引导策略，对智能契约模糊化进行了研究。特别是，我们首先利用基于数据流的反馈来确定有意义的交易顺序，并进一步引入序列感知变异技术来探索更深层次的状态。然后，我们设计了一个掩码引导的种子变异策略，使生成的事务输入偏向于命中目标分支。此外，我们发展了一个动态-自适应能量调整范式，平衡模糊运动期间的资源分配。我们将我们的设计实现到一个新的名为 MuFuzz 的智能合同模糊器中，并在三个基准上对其进行广泛的评估。实证结果表明，MuFuzz 在分支覆盖率和 bug 发现方面都优于现有工具。总的来说，MuFuzz 实现了比最先进的 Fuzzer (高达25%)更高的分支覆盖率，并且比现有的 bug 检测器多检测出30% 的 bug。"
    },
    {
        "title": "GaitGuard: Towards Private Gait in Mixed Reality",
        "url": "http://arxiv.org/abs/2312.04470v1",
        "pub_date": "2023-12-07",
        "summary": "Augmented/Mixed Reality (AR/MR) devices are unique from other mobile systems\nbecause of their capability to offer an immersive multi-user collaborative\nexperience. While previous studies have explored privacy and security aspects\nof multiple user interactions in AR/MR, a less-explored area is the\nvulnerability of gait privacy. Gait is considered a private state because it is\na highly individualistic and a distinctive biometric trait. Thus, preserving\ngait privacy in emerging AR/MR systems is crucial to safeguard individuals from\npotential identity tracking and unauthorized profiling.\n  This paper first introduces GaitExtract, a framework designed to\nautomatically detect gait information in humans, shedding light on the nuances\nof gait privacy in AR/MR. In this paper, we designed GaitExtract, a framework\nthat can automatically detect the outside gait information of a human and\ninvestigate the vulnerability of gait privacy in AR. In a user study with $20$\nparticipants, our findings reveal that participants were uniquely identifiable\nwith an accuracy of up to $78\\%$ using GaitExtract. Consequently, we propose\nGaitGuard, a system that safeguards gait information of people appearing in the\ncamera view of the AR/MR device.\n  Furthermore, we tested GaitGuard in an MR collaborative application,\nachieving $22$ fps while streaming mitigated frames to the collaborative\nserver. Our user-study survey indicated that users are more comfortable with\nreleasing videos of them walking when GaitGuard is applied to the frames. These\nresults underscore the efficacy and practicality of GaitGuard in mitigating\ngait privacy concerns in MR contexts.",
        "translated": "增强/混合现实(AR/MR)设备是独一无二的从其他移动系统，因为他们的能力，提供沉浸式多用户协作体验。虽然以前的研究已经探索了 AR/MR 中多用户交互的隐私和安全方面，但是步态隐私的脆弱性是一个较少探索的领域。步态被认为是一种私人状态，因为它是一种高度个人主义和独特的生物特征。因此，在新出现的 AR/MR 系统中，保护步态隐私对于保护个人免遭潜在的身份跟踪和未经授权的特征分析至关重要。本文首先介绍了自动检测人体步态信息的 GaitExtract 框架，阐述了 AR/MR 中步态隐私的细微差别。在一项针对20美元参与者的用户研究中，我们的发现显示，使用 GaitExtract，参与者具有唯一可识别性，准确率高达78% 。因此，我们提出 GaitGuard，一个保护人们出现在 AR/MR 设备的相机视图中的步态信息的系统。此外，我们在一个 MR 协作应用程序中测试了 GaitGuard，实现了 $22 $fps，同时向协作服务器传输了减轻的帧。我们的用户研究调查表明，当 GaitGuard 应用到框架上时，用户更愿意发布他们行走的视频。这些结果强调了 GaitGuard 在缓解 MR 背景下步态隐私问题方面的有效性和实用性。"
    },
    {
        "title": "On the Learnability of Watermarks for Language Models",
        "url": "http://arxiv.org/abs/2312.04469v1",
        "pub_date": "2023-12-07",
        "summary": "Watermarking of language model outputs enables statistical detection of\nmodel-generated text, which has many applications in the responsible deployment\nof language models. Existing watermarking strategies operate by altering the\ndecoder of an existing language model, and the ability for a language model to\ndirectly learn to generate the watermark would have significant implications\nfor the real-world deployment of watermarks. First, learned watermarks could be\nused to build open models that naturally generate watermarked text, allowing\nfor open models to benefit from watermarking. Second, if watermarking is used\nto determine the provenance of generated text, an adversary can hurt the\nreputation of a victim model by spoofing its watermark and generating damaging\nwatermarked text. To investigate the learnability of watermarks, we propose\nwatermark distillation, which trains a student model to behave like a teacher\nmodel that uses decoding-based watermarking. We test our approach on three\ndistinct decoding-based watermarking strategies and various hyperparameter\nsettings, finding that models can learn to generate watermarked text with high\ndetectability. We also find limitations to learnability, including the loss of\nwatermarking capabilities under fine-tuning on normal text and high sample\ncomplexity when learning low-distortion watermarks.",
        "translated": "语言模型输出的数字水印能够对模型生成的文本进行统计检测，在负责任的语言模型部署中有着广泛的应用。现有的水印策略是通过改变现有语言模型的解码器来实现的，而语言模型直接学习生成水印的能力将对水印在现实世界中的应用产生重大影响。首先，学习的水印可以用来构建自然生成水印文本的开放模型，允许开放模型从水印中受益。其次，如果使用水印来确定所生成文本的来源，那么对手可以通过欺骗受害者模型的水印并生成具有破坏性的水印文本来损害受害者模型的声誉。为了研究水印的可学习性，我们提出了水印提取方法，该方法将学生模型训练成像教师模型一样使用基于解码的水印。我们在三种不同的基于解码的水印策略和不同的超参数设置上测试了我们的方法，发现模型可以学习生成具有高可检测性的水印文本。我们还发现了易学性的局限性，包括在对正常文本进行微调时水印能力的丧失以及在学习低失真水印时样本复杂度较高。"
    },
    {
        "title": "Privacy-preserving quantum federated learning via gradient hiding",
        "url": "http://arxiv.org/abs/2312.04447v1",
        "pub_date": "2023-12-07",
        "summary": "Distributed quantum computing, particularly distributed quantum machine\nlearning, has gained substantial prominence for its capacity to harness the\ncollective power of distributed quantum resources, transcending the limitations\nof individual quantum nodes. Meanwhile, the critical concern of privacy within\ndistributed computing protocols remains a significant challenge, particularly\nin standard classical federated learning (FL) scenarios where data of\nparticipating clients is susceptible to leakage via gradient inversion attacks\nby the server. This paper presents innovative quantum protocols with quantum\ncommunication designed to address the FL problem, strengthen privacy measures,\nand optimize communication efficiency. In contrast to previous works that\nleverage expressive variational quantum circuits or differential privacy\ntechniques, we consider gradient information concealment using quantum states\nand propose two distinct FL protocols, one based on private inner-product\nestimation and the other on incremental learning. These protocols offer\nsubstantial advancements in privacy preservation with low communication\nresources, forging a path toward efficient quantum communication-assisted FL\nprotocols and contributing to the development of secure distributed quantum\nmachine learning, thus addressing critical privacy concerns in the quantum\ncomputing era.",
        "translated": "分布式量子计算，尤其是分布式量子机器学习，因其能够超越单个量子节点的局限性，利用分布式量子资源的集体能力而获得了显著的重视。与此同时，分布式计算协议中对隐私的关注仍然是一个重大挑战，特别是在标准的经典联邦学习(FL)场景中，参与客户端的数据很容易通过服务器的梯度反转攻击而泄漏。本文提出了一种创新的量子通信协议，旨在解决 FL 问题，加强隐私措施，优化通信效率。与之前利用表达式变分量子电路或差分隐私技术的工作不同，我们考虑使用量子态隐藏梯度信息，并提出了两种不同的 FL 协议，一种基于私有内积估计，另一种基于在线机机器学习。这些协议以较低的通信资源在保护隐私方面取得了重大进展，为有效的量子通信辅助 FL 协议开辟了道路，并有助于开发安全的分布式量子机器学习，从而解决了量子计算时代的关键隐私问题。"
    },
    {
        "title": "FreqFed: A Frequency Analysis-Based Approach for Mitigating Poisoning\n  Attacks in Federated Learning",
        "url": "http://arxiv.org/abs/2312.04432v1",
        "pub_date": "2023-12-07",
        "summary": "Federated learning (FL) is a collaborative learning paradigm allowing\nmultiple clients to jointly train a model without sharing their training data.\nHowever, FL is susceptible to poisoning attacks, in which the adversary injects\nmanipulated model updates into the federated model aggregation process to\ncorrupt or destroy predictions (untargeted poisoning) or implant hidden\nfunctionalities (targeted poisoning or backdoors). Existing defenses against\npoisoning attacks in FL have several limitations, such as relying on specific\nassumptions about attack types and strategies or data distributions or not\nsufficiently robust against advanced injection techniques and strategies and\nsimultaneously maintaining the utility of the aggregated model. To address the\ndeficiencies of existing defenses, we take a generic and completely different\napproach to detect poisoning (targeted and untargeted) attacks. We present\nFreqFed, a novel aggregation mechanism that transforms the model updates (i.e.,\nweights) into the frequency domain, where we can identify the core frequency\ncomponents that inherit sufficient information about weights. This allows us to\neffectively filter out malicious updates during local training on the clients,\nregardless of attack types, strategies, and clients' data distributions. We\nextensively evaluate the efficiency and effectiveness of FreqFed in different\napplication domains, including image classification, word prediction, IoT\nintrusion detection, and speech recognition. We demonstrate that FreqFed can\nmitigate poisoning attacks effectively with a negligible impact on the utility\nof the aggregated model.",
        "translated": "联合学习(FL)是一种合作学习范式，允许多个客户共同训练一个模型，而无需共享他们的训练数据。然而，FL 容易受到中毒攻击，其中对手将操纵的模型更新注入到联邦模型聚合过程中，以破坏或摧毁预测(非目标中毒)或植入隐藏功能(目标中毒或后门)。现有的针对 FL 中毒攻击的防御方法存在一些局限性，比如依赖于攻击类型和策略或数据分布的特定假设，或者针对先进的注入技术和策略不够强大，同时保持聚合模型的效用。为了解决现有防御系统的缺陷，我们采用了一种通用的、完全不同的方法来检测中毒(有针对性的和非针对性的)攻击。我们提出了一种新的聚合机制 FreqFed，它将模型更新(即权重)转换为频率域，在频率域中我们可以识别继承了关于权重的足够信息的核心频率成分。这使我们能够在对客户机进行本地培训期间有效地过滤掉恶意更新，而不管攻击类型、策略和客户机的数据分布如何。我们广泛评估了 FreqFed 在不同应用领域的效率和有效性，包括图像分类、词汇预测、物联网入侵检测和语音识别。我们证明了 FreqFed 可以有效地减轻中毒攻击，对聚合模型的实用性的影响可以忽略不计。"
    },
    {
        "title": "On the Impact of Multi-dimensional Local Differential Privacy on\n  Fairness",
        "url": "http://arxiv.org/abs/2312.04404v1",
        "pub_date": "2023-12-07",
        "summary": "Automated decision systems are increasingly used to make consequential\ndecisions in people's lives. Due to the sensitivity of the manipulated data as\nwell as the resulting decisions, several ethical concerns need to be addressed\nfor the appropriate use of such technologies, in particular, fairness and\nprivacy. Unlike previous work, which focused on centralized differential\nprivacy (DP) or local DP (LDP) for a single sensitive attribute, in this paper,\nwe examine the impact of LDP in the presence of several sensitive attributes\n(i.e., multi-dimensional data) on fairness. Detailed empirical analysis on\nsynthetic and benchmark datasets revealed very relevant observations. In\nparticular, (1) multi-dimensional LDP is an efficient approach to reduce\ndisparity, (2) the multi-dimensional approach of LDP (independent vs. combined)\nmatters only at low privacy guarantees, and (3) the outcome Y distribution has\nan important effect on which group is more sensitive to the obfuscation. Last,\nwe summarize our findings in the form of recommendations to guide practitioners\nin adopting effective privacy-preserving practices while maintaining fairness\nand utility in ML applications.",
        "translated": "在人们的生活中，自动决策系统越来越多地被用于做出重大决定。由于操纵数据的敏感性以及由此产生的决定，需要解决若干伦理问题，以适当使用这些技术，特别是公平和隐私。与之前的工作不同，本文主要针对单个敏感属性的集中式差分隐私(DP)或本地 DP (LDP) ，我们研究了在多个敏感属性(即多维数据)存在的情况下，本地 DP 对公平性的影响。对合成和基准数据集的详细实证分析揭示了非常相关的观察结果。特别是，(1)多维 LDP 是一种有效的方法，以减少差异，(2)多维 LDP (独立与组合)的方法只有在低隐私保障问题，和(3)结果 Y 分布有一个重要的影响团体更敏感的混淆。最后，我们以建议的形式总结我们的发现，以指导从业者采用有效的保护隐私的做法，同时保持机器学习应用的公平性和实用性。"
    },
    {
        "title": "NeuJeans: Private Neural Network Inference with Joint Optimization of\n  Convolution and Bootstrapping",
        "url": "http://arxiv.org/abs/2312.04356v1",
        "pub_date": "2023-12-07",
        "summary": "Fully homomorphic encryption (FHE) is a promising cryptographic primitive for\nrealizing private neural network inference (PI) services by allowing a client\nto fully offload the inference task to a cloud server while keeping the client\ndata oblivious to the server. This work proposes NeuJeans, an FHE-based\nsolution for the PI of deep convolutional neural networks (CNNs). NeuJeans\ntackles the critical problem of the enormous computational cost for the FHE\nevaluation of convolutional layers (conv2d), mainly due to the high cost of\ndata reordering and bootstrapping. We first propose an encoding method\nintroducing nested structures inside encoded vectors for FHE, which enables us\nto develop efficient conv2d algorithms with reduced data reordering costs.\nHowever, the new encoding method also introduces additional computations for\nconversion between encoding methods, which could negate its advantages. We\ndiscover that fusing conv2d with bootstrapping eliminates such computations\nwhile reducing the cost of bootstrapping. Then, we devise optimized execution\nflows for various types of conv2d and apply them to end-to-end implementation\nof CNNs. NeuJeans accelerates the performance of conv2d by up to 5.68 times\ncompared to state-of-the-art FHE-based PI work and performs the PI of a CNN at\nthe scale of ImageNet (ResNet18) within a mere few seconds",
        "translated": "完全同态加密(fHE)是一种很有前途的密码学原理，通过允许客户端将推理任务完全卸载到云服务器，同时保持客户端数据不被服务器察觉，从而实现私有神经网络推理(PI)服务。这项工作提出了 NeuJans，一个基于 FHE 的深卷积神经网络(CNN) PI 的解决方案。NeuJans 解决了卷积层 FHE 评估(conv2d)的巨大计算成本这一关键问题，主要是由于数据重排和自举的高成本。本文首先提出了一种在 FHE 编码矢量内引入嵌套结构的编码方法，使得我们能够以较低的数据重排代价开发出高效的对流2d 算法。然而，新的编码方法也引入了额外的计算，在编码方法之间的转换，这可能会否定其优点。我们发现，融合 conv2d 与自举消除了这种计算，同时降低了自举的成本。然后，我们针对不同类型的 conv2d 设计了优化的执行流程，并将其应用于 CNN 的端到端实现。与最先进的基于 FHE 的 PI 工作相比，NeuJans 可以将 conv2d 的性能提高5.68倍，并且在短短几秒钟内就能以 ImageNet (ResNet18)的规模完成 CNN 的 PI 工作"
    },
    {
        "title": "Improved Efficient Two-Stage Denoising Diffusion Power System\n  Measurement Recovery Against False Data Injection Attacks and Data Losses",
        "url": "http://arxiv.org/abs/2312.04346v1",
        "pub_date": "2023-12-07",
        "summary": "Measurement uncertainties, represented by cyber-attacks and data losses,\nseriously degrade the quality of power system measurements. Fortunately, the\npowerful generation ability of the denoising diffusion models can enable more\nprecise measurement generation for power system data recovery. However, the\ncontrollable data generation and efficient computing methods of denoising\ndiffusion models for deterministic trajectory still need further investigation.\nTo this end, this paper proposes an improved two-stage denoising diffusion\nmodel (TSDM) to identify and reconstruct the measurements with various\nmeasurement uncertainties. The first stage of the model comprises a\nclassifier-guided conditional anomaly detection component, while the second\nstage involves diffusion-based measurement imputation component. Moreover, the\nproposed TSDM adopts precise means and optimal variances to accelerate the\ndiffusion generation process with subsequence sampling. Extensive numerical\ncase studies demonstrate that the proposed TSDM can accurately recover power\nsystem measurements despite strong randomness under renewable energy\nintegration and highly nonlinear dynamics under complex cyber-physical\ncontingencies. Additionally, the proposed TSDM has stronger robustness compared\nto existing reconstruction networks and exhibits lower computational complexity\nthan general denoising diffusion models.",
        "translated": "以网络攻击和数据丢失为代表的测量不确定性严重降低了电力系统测量的质量。幸运的是，去噪扩散模型强大的产生能力可以使电力系统数据恢复更精确的测量产生。然而，确定性轨迹的可控数据生成和去噪扩散模型的有效计算方法仍有待进一步研究。为此，本文提出了一种改进的两阶段去噪扩散模型(TSDM)来识别和重构具有不同测量不确定性的测量值。该模型的第一阶段包括分类器引导的条件异常检测组件，而第二阶段涉及基于扩散的测量插补组件。此外，本文提出的 TSDM 采用精确的均值和最佳方差来加速子序列抽样的扩散生成过程。大量的数值实例表明，在可再生能源集成和复杂的网络物理突发事件下，TSDM 能够准确恢复电力系统的测量数据，尽管其具有很强的随机性和高度非线性动态性。此外，与现有的重构网络相比，所提出的 TSDM 具有更强的鲁棒性，并且比一般的去噪扩散模型具有更低的计算复杂度。"
    },
    {
        "title": "Dynamic Data-Driven Digital Twins for Blockchain Systems",
        "url": "http://arxiv.org/abs/2312.04226v1",
        "pub_date": "2023-12-07",
        "summary": "In recent years, we have seen an increase in the adoption of blockchain-based\nsystems in non-financial applications, looking to benefit from what the\ntechnology has to offer. Although many fields have managed to include\nblockchain in their core functionalities, the adoption of blockchain, in\ngeneral, is constrained by the so-called trilemma trade-off between\ndecentralization, scalability, and security. In our previous work, we have\nshown that using a digital twin for dynamically managing blockchain systems\nduring runtime can be effective in managing the trilemma trade-off. Our Digital\nTwin leverages DDDAS feedback loop, which is responsible for getting the data\nfrom the system to the digital twin, conducting optimisation, and updating the\nphysical system. This paper examines how leveraging DDDAS feedback loop can\nsupport the optimisation component of the trilemma benefiting from\nReinforcement Learning agents and a simulation component to augment the quality\nof the learned model while reducing the computational overhead required for\ndecision-making.",
        "translated": "近年来，我们已经看到在非金融应用中采用基于区块链的系统的增加，希望从该技术提供的东西中获益。虽然许多领域已经设法将区块链纳入其核心功能，但区块链的采用，一般来说，受到所谓的三难地方分权，可扩展性和安全性之间的权衡的限制。在我们以前的工作中，我们已经表明，使用数字双在运行时动态管理区块链系统可以有效地管理三难权衡。我们的数字孪生兄弟利用 DDDAS 反馈回路，这是负责获取数据从系统到数字孪生兄弟，进行优化，并更新物理系统。本文研究了如何利用 DDdAS 反馈回路来支持三难问题的优化部分，这个优化部分受益于强化学习代理和一个模拟部分，以提高学习模型的质量，同时减少决策所需的计算开销。"
    },
    {
        "title": "Topology-Based Reconstruction Prevention for Decentralised Learning",
        "url": "http://arxiv.org/abs/2312.05248v1",
        "pub_date": "2023-12-08",
        "summary": "Decentralised learning has recently gained traction as an alternative to\nfederated learning in which both data and coordination are distributed over its\nusers. To preserve the confidentiality of users' data, decentralised learning\nrelies on differential privacy, multi-party computation, or a combination\nthereof. However, running multiple privacy-preserving summations in sequence\nmay allow adversaries to perform reconstruction attacks. Unfortunately, current\nreconstruction countermeasures either cannot trivially be adapted to the\ndistributed setting, or add excessive amounts of noise.\n  In this work, we first show that passive honest-but-curious adversaries can\nreconstruct other users' private data after several privacy-preserving\nsummations. For example, in subgraphs with 18 users, we show that only three\npassive honest-but-curious adversaries succeed at reconstructing private data\n11.0% of the time, requiring an average of 8.8 summations per adversary. The\nsuccess rate is independent of the size of the full network. We consider weak\nadversaries, who do not control the graph topology and can exploit neither the\nworkings of the summation protocol nor the specifics of users' data.\n  We develop a mathematical understanding of how reconstruction relates to\ntopology and propose the first topology-based decentralised defence against\nreconstruction attacks. Specifically, we show that reconstruction requires a\nnumber of adversaries linear in the length of the network's shortest cycle.\nConsequently, reconstructing private data from privacy-preserving summations is\nimpossible in acyclic networks.\n  Our work is a stepping stone for a formal theory of decentralised\nreconstruction defences based on topology. Such a theory would generalise our\ncountermeasure beyond summation, define confidentiality in terms of entropy,\nand describe the effects of (topology-aware) differential privacy.",
        "translated": "最近，分散式学习作为联邦学习的一种替代方式得到了广泛的关注，在联邦学习中，数据和协调都分布在用户之间。为了保护用户数据的机密性，分散式学习依赖于差分隐私、多方计算或两者的结合。然而，按顺序运行多个保护隐私的汇总可能允许对手执行重构攻击。不幸的是，目前的重建对策要么不能平凡地适应分布式设置，或添加过量的噪声。在这项工作中，我们首先表明，被动诚实但好奇的对手可以重建其他用户的私人数据后，几个保护隐私的汇总。例如，在18个用户的子图中，我们显示只有3个被动诚实但好奇的对手成功重建私人数据的概率为11.0% ，平均每个对手需要8.8次总和。成功率与整个网络的大小无关。我们考虑弱对手，他们不能控制图的拓扑结构，既不能利用总和协议的工作原理，也不能利用用户数据的细节。我们开发了一个数学上的理解，如何重建涉及到拓扑，并提出了第一个基于拓扑的分散防御重建攻击。具体来说，我们表明，重建需要一些线性的对手在网络的最短周期的长度。因此，在无环网络中，根据保护隐私的总和重建私有数据是不可能的。我们的工作是一个基于拓扑的分散重建防御的形式化理论的基石。这样的理论可以概括我们的对策，超越总和，用熵来定义机密性，并描述(拓扑感知的)差分隐私的影响。"
    },
    {
        "title": "Achieving Consensus on Spectrum Usage in the Presence of Faults",
        "url": "http://arxiv.org/abs/2312.05213v1",
        "pub_date": "2023-12-08",
        "summary": "A consensus mechanism is proposed to facilitate radio spectrum sharing with\naccountability in a network of multiple operators, a subset of which may even\nbe adversarial. A distributed ledger is used to securely record and keep track\nof the state of consensus on spectrum usage, including interference incidents\nand the corresponding responsible parties. A key challenge is that the\noperators generally do not have initial agreement due to noise in their analog\nmeasurements. To meet this challenge, two categories of spectrum-sharing\nsolutions are studied in detail. The first category employs an exact Byzantine\nfault tolerant (BFT) agreement model; the second category utilizes an\napproximate BFT agreement model. This paper also delves into the application of\nconsensus protocols to the specific context of low Earth orbit (LEO)\nnon-geostationary satellite networks, also known as mega-constellations.",
        "translated": "建议采用协商一致的机制，以便在一个由多个运营商组成的网络(其中一个子集甚至可能是对抗性的)中促进无线电频谱共享和问责制。分布式分类账用于安全地记录和跟踪关于频谱使用的共识状态，包括干扰事件和相应的责任方。一个关键的挑战是，由于模拟测量中的噪声，运营商通常没有最初的一致意见。为了迎接这一挑战，本文详细研究了两类频谱共享解决方案。第一类采用精确的拜占庭容错(BFT)协议模型; 第二类采用近似的 BFT 协议模型。本文还探讨了协商一致协议在低地球轨道(LEO)非地球静止卫星网络中的具体应用。"
    },
    {
        "title": "Membership Inference Attacks on Diffusion Models via Quantile Regression",
        "url": "http://arxiv.org/abs/2312.05140v1",
        "pub_date": "2023-12-08",
        "summary": "Recently, diffusion models have become popular tools for image synthesis\nbecause of their high-quality outputs. However, like other large-scale models,\nthey may leak private information about their training data. Here, we\ndemonstrate a privacy vulnerability of diffusion models through a\n\\emph{membership inference (MI) attack}, which aims to identify whether a\ntarget example belongs to the training set when given the trained diffusion\nmodel. Our proposed MI attack learns quantile regression models that predict (a\nquantile of) the distribution of reconstruction loss on examples not used in\ntraining. This allows us to define a granular hypothesis test for determining\nthe membership of a point in the training set, based on thresholding the\nreconstruction loss of that point using a custom threshold tailored to the\nexample. We also provide a simple bootstrap technique that takes a majority\nmembership prediction over ``a bag of weak attackers'' which improves the\naccuracy over individual quantile regression models. We show that our attack\noutperforms the prior state-of-the-art attack while being substantially less\ncomputationally expensive -- prior attacks required training multiple ``shadow\nmodels'' with the same architecture as the model under attack, whereas our\nattack requires training only much smaller models.",
        "translated": "近年来，扩散模型以其高质量的输出成为图像合成的主要工具。然而，像其他大型模型一样，它们可能泄露有关其训练数据的私人信息。这里，我们通过一个方法{成员推理(MI)攻击}来证明扩散模型的隐私脆弱性，该方法的目的是在给定训练后的扩散模型的情况下识别目标样本是否属于训练集。我们提出的 MI 攻击学习了分量回归模型，该模型可以预测(一个分位数)未在训练中使用的例子的重建损失分布。这使我们能够定义一个粒度假设检验来确定训练集中的一个点的成员关系，基于阈值化该点的重建损失，使用一个定制的阈值来定制该例子。我们还提供了一个简单的引导技术，它采用多数成员预测，而不是“一群弱攻击者”，从而提高了单个分量回归模型的准确性。我们证明了我们的攻击比之前的最先进的攻击有更好的表现，同时计算成本也大大降低——之前的攻击需要训练多个“影子模型”，这些“影子模型”与受攻击的模型具有相同的架构，而我们的攻击只需要训练更小的模型。"
    },
    {
        "title": "On the Inadequacy of Similarity-based Privacy Metrics: Reconstruction\n  Attacks against \"Truly Anonymous Synthetic Data''",
        "url": "http://arxiv.org/abs/2312.05114v1",
        "pub_date": "2023-12-08",
        "summary": "Training generative models to produce synthetic data is meant to provide a\nprivacy-friendly approach to data release. However, we get robust guarantees\nonly when models are trained to satisfy Differential Privacy (DP). Alas, this\nis not the standard in industry as many companies use ad-hoc strategies to\nempirically evaluate privacy based on the statistical similarity between\nsynthetic and real data. In this paper, we review the privacy metrics offered\nby leading companies in this space and shed light on a few critical flaws in\nreasoning about privacy entirely via empirical evaluations. We analyze the\nundesirable properties of the most popular metrics and filters and demonstrate\ntheir unreliability and inconsistency through counter-examples. We then present\na reconstruction attack, ReconSyn, which successfully recovers (i.e., leaks all\nattributes of) at least 78% of the low-density train records (or outliers) with\nonly black-box access to a single fitted generative model and the privacy\nmetrics. Finally, we show that applying DP only to the model or using\nlow-utility generators does not mitigate ReconSyn as the privacy leakage\npredominantly comes from the metrics. Overall, our work serves as a warning to\npractitioners not to deviate from established privacy-preserving mechanisms.",
        "translated": "培训生成模型以产生合成数据意在为数据发布提供一种有利于保护隐私的方法。然而，只有当模型被训练以满足差分隐私(DP)时，我们才能得到稳健的保证。遗憾的是，这并不是行业标准，因为许多公司使用特别策略，根据合成数据和真实数据之间的统计相似性，对隐私进行经验性评估。在本文中，我们回顾了这一领域的领先企业提供的隐私指标，并通过实证评估揭示了隐私推理中的一些关键性缺陷。我们分析了最流行的度量和过滤器的不良特性，并通过反例证明了它们的不可靠性和不一致性。然后，我们提出一个重构攻击，ReconSyn，它成功恢复(即泄露所有属性)至少78% 的低密度列车记录(或离群值) ，只有黑盒访问一个合适的生成模型和隐私度量。最后，我们展示了仅将 DP 应用于模型或使用低效用的生成器并不能减轻 ReconSyn，因为隐私泄漏主要来自度量。总的来说，我们的工作是对从业者的一个警告，不要偏离既定的隐私保护机制。"
    },
    {
        "title": "A stacked ensemble learning IDS model for Software-defined VANET",
        "url": "http://arxiv.org/abs/2312.04956v1",
        "pub_date": "2023-12-08",
        "summary": "ntrusion Detection Systems (IDS) are widely employed to detect and mitigate\nexternal network security events. VANETs (Vehicle ad-hoc Networks) are\nevolving, especially with the development of Connected Autonomous Vehicles\n(CAVs). So, it is crucial to assess how traditional IDS approaches can be\nutilised for emerging technologies. To address this concern, our work presents\na stacked ensemble learning approach for IDS, which combines multiple machine\nlearning algorithms to detect threats more effectively than single algorithm\nmethods. Using the CICIDS2017 and the VeReMi benchmark data sets, we compare\nthe performance of our approach with existing machine learning methods and find\nthat it is more accurate at identifying threats. Our method also incorporates\nhyperparameter optimization and feature selection to improve its performance\nfurther. Overall, our results suggest that stacked ensemble learning is a\npromising technique for enhancing the effectiveness of IDS.",
        "translated": "入侵检测系统(IDS)广泛应用于检测和缓解外部网络安全事件。车辆自组织网络(VANET)正在发展，特别是随着连通自主车辆(CAV)的发展。因此，评估如何将传统 IDS 方法用于新兴技术至关重要。为了解决这个问题，我们的工作为 IDS 提出了一种叠加集成学习方法，它结合了多种机器学习算法，比单一算法更有效地检测威胁。使用 CICIDS2017和 VeReMi 基准数据集，我们比较了我们的方法和现有的机器学习方法的性能，发现它在识别威胁方面更加准确。该方法还结合了超参数优化和特征选择，进一步提高了性能。总体而言，我们的研究结果表明，叠加集成学习是提高 IDS 有效性的一种有前途的技术。"
    },
    {
        "title": "Detecting DBMS Bugs with Context-Sensitive Instantiation and Multi-Plan\n  Execution",
        "url": "http://arxiv.org/abs/2312.04941v1",
        "pub_date": "2023-12-08",
        "summary": "DBMS bugs can cause serious consequences, posing severe security and privacy\nconcerns. This paper works towards the detection of memory bugs and logic bugs\nin DBMSs, and aims to solve the two innate challenges, including how to\ngenerate semantically correct SQL queries in a test case, and how to propose\neffective oracles to capture logic bugs. To this end, our system proposes two\nkey techniques. The first key technique is called context-sensitive\ninstantiation, which considers all static semantic requirements (including but\nnot limited to the identifier type used by existing systems) to generate\nsemantically valid SQL queries. The second key technique is called multi-plan\nexecution, which can effectively capture logic bugs. Given a test case,\nmulti-plan execution makes the DBMS execute all query plans instead of the\ndefault optimal one, and compares the results. A logic bug is detected if a\ndifference is found among the execution results of the executed query plans. We\nhave implemented a prototype system called Kangaroo and applied it to three\nwidely used and well-tested DBMSs, including SQLite, PostgreSQL, and MySQL. Our\nsystem successfully detected 50 new bugs. The comparison between our system\nwith the state-of-the-art systems shows that our system outperforms them in\nterms of the number of generated semantically valid SQL queries, the explored\ncode paths during testing, and the detected bugs.",
        "translated": "DBMS 错误可能导致严重的后果，引起严重的安全和隐私问题。本文致力于 DBMS 中的内存错误和逻辑错误的检测，旨在解决这两个内在的挑战，包括如何在测试用例中生成语义正确的 SQL 查询，以及如何提出有效的预言来捕获逻辑错误。为此，我们的系统提出了两个关键技术。第一个关键技术称为上下文敏感的实例化，它考虑所有静态语义需求(包括但不限于现有系统使用的标识符类型) ，以生成语义有效的 SQL 查询。第二个关键技术称为多计划执行，它可以有效地捕获逻辑错误。给定一个测试用例，多计划执行使得 DBMS 执行所有查询计划，而不是默认的最优查询计划，并比较结果。如果在执行的查询计划的执行结果中发现差异，则检测到逻辑错误。我们已经实现了一个名为 Kangaroo 的原型系统，并将其应用于三个广泛使用且测试良好的 DBMS，包括 SQLite、 PostgreSQL 和 MySQL。我们的系统成功检测到50个新的错误。我们的系统与最先进的系统之间的比较表明，我们的系统在生成的语义有效的 SQL 查询数量、测试期间探索的代码路径以及检测到的 bug 方面优于它们。"
    },
    {
        "title": "Canaries and Whistles: Resilient Drone Communication Networks with (or\n  without) Deep Reinforcement Learning",
        "url": "http://arxiv.org/abs/2312.04940v1",
        "pub_date": "2023-12-08",
        "summary": "Communication networks able to withstand hostile environments are critically\nimportant for disaster relief operations. In this paper, we consider a\nchallenging scenario where drones have been compromised in the supply chain,\nduring their manufacture, and harbour malicious software capable of\nwide-ranging and infectious disruption. We investigate multi-agent deep\nreinforcement learning as a tool for learning defensive strategies that\nmaximise communications bandwidth despite continual adversarial interference.\nUsing a public challenge for learning network resilience strategies, we propose\na state-of-the-art expert technique and study its superiority over deep\nreinforcement learning agents. Correspondingly, we identify three specific\nmethods for improving the performance of our learning-based agents: (1)\nensuring each observation contains the necessary information, (2) using expert\nagents to provide a curriculum for learning, and (3) paying close attention to\nreward. We apply our methods and present a new mixed strategy enabling expert\nand learning-based agents to work together and improve on all prior results.",
        "translated": "能够抵御恶劣环境的通信网络对于救灾行动至关重要。在本文中，我们考虑了一个具有挑战性的场景，即无人机在制造过程中在供应链中受到损害，并且存在能够造成大范围和传染性破坏的恶意软件。我们研究多代理深度强化学习作为一种学习防御策略的工具，这种策略可以在不断受到敌对干扰的情况下最大化通信带宽。利用学习网络弹性策略的公共挑战，我们提出了一种最先进的专家技术，并研究了其相对于深度强化学习代理的优越性。相应地，我们确定了三种具体的方法来提高我们的基于学习的代理人的表现: (1)确保每个观察包含必要的信息，(2)使用专家代理人提供学习课程，(3)密切关注奖励。我们应用我们的方法，并提出了一个新的混合策略，使专家和基于学习的代理一起工作，改善所有先前的结果。"
    },
    {
        "title": "AHSecAgg and TSKG: Lightweight Secure Aggregation for Federated Learning\n  Without Compromise",
        "url": "http://arxiv.org/abs/2312.04937v1",
        "pub_date": "2023-12-08",
        "summary": "Leveraging federated learning (FL) to enable cross-domain privacy-sensitive\ndata mining represents a vital breakthrough to accomplish privacy-preserving\nlearning. However, attackers can infer the original user data by analyzing the\nuploaded intermediate parameters during the aggregation process. Therefore,\nsecure aggregation has become a critical issue in the field of FL. Many secure\naggregation protocols face the problem of high computation costs, which\nseverely limits their applicability. To this end, we propose AHSecAgg, a\nlightweight secure aggregation protocol using additive homomorphic masks.\nAHSecAgg significantly reduces computation overhead without compromising the\ndropout handling capability or model accuracy. We prove the security of\nAHSecAgg in semi-honest and active adversary settings. In addition, in\ncross-silo scenarios where the group of participants is relatively fixed during\neach round, we propose TSKG, a lightweight Threshold Signature based masking\nkey generation method. TSKG can generate different temporary secrets and shares\nfor different aggregation rounds using the initial key and thus effectively\neliminates the cost of secret sharing and key agreement. We prove TSKG does not\nsacrifice security. Extensive experiments show that AHSecAgg significantly\noutperforms state-of-the-art mask-based secure aggregation protocols in terms\nof computational efficiency, and TSKG effectively reduces the computation and\ncommunication costs for existing secure aggregation protocols.",
        "translated": "利用联邦学习(FL)实现跨域隐私敏感数据挖掘是实现隐私保护学习的重要突破。但是，攻击者可以通过分析聚合过程中上传的中间参数来推断原始用户数据。因此，安全聚合已成为 FL 领域的一个关键问题。许多安全聚合协议都面临着高计算开销的问题，这严重限制了它们的适用性。为此，我们提出了 AHSecAgg，一种使用加性同态掩码的轻量级安全聚合协议。AHSecAgg 在不影响退出处理能力或模型精度的情况下显著降低了计算开销。我们证明了半诚实和积极的对手设置 AHSecAgg 的安全性。此外，在每一轮的参与者群相对固定的交叉竖井场景中，我们提出了 TSKG，一种基于门限签名的轻量级掩蔽密钥生成方法。TSKG 可以利用初始密钥为不同的聚合轮生成不同的临时秘密和共享，从而有效地消除了秘密共享和密钥协商的成本。我们证明 TSKG 不会牺牲安全。大量实验表明，AHSecAgg 在计算效率方面明显优于最先进的基于掩码的安全聚合协议，TSKG 有效地降低了现有安全聚合协议的计算和通信成本。"
    },
    {
        "title": "Towards Efficient Secure Aggregation in FL: Partial Vector Freezing for\n  Cost Compression",
        "url": "http://arxiv.org/abs/2312.04920v1",
        "pub_date": "2023-12-08",
        "summary": "Secure aggregation of user vectors has become a critical issue in the field\nof federated learning. Many Secure Aggregation Protocols (SAP) face exorbitant\ncomputation costs, which severely limit their applicability. We uncover that\ncurrent endeavors to reduce computation costs tend to overlook a crucial fact:\na considerable portion of SAP's computation burden stems from processing each\nentry in the private vectors. Given this observation, we propose PVF, a\nportable module for compressing computation costs. PVF is able to ``freeze'' a\nsubstantial portion of the private vector through specific linear\ntransformations, only requiring $\\frac{1}{\\lambda}$ of the original vector to\nparticipate in SAP. Eventually, users can ``thaw'' the public sum of the\n``frozen entries\" by the result of SAP. To enhance functionality, we introduce\nextensions that can enforce consistency constraints on users' original vectors,\nverify aggregated results, and enhance security when a portion of the private\nvector is known to the server. We demonstrate that PVF can seamlessly integrate\nwith various SAP and prove that it poses no threat to user privacy in the\nsemi-honest and active adversary settings. We select $8$ baselines,\nencompassing $6$ distinct types of SAP, and explore the acceleration effects of\nPVF on these SAP. Empirical investigations indicate that when $\\lambda=100$,\nPVF yields up to $99.5\\times$ speedup and up to $32.3\\times$ communication\nreduction, with the potential to approach nearly $1000\\times$ acceleration as\n$\\lambda$ increases.",
        "translated": "用户向量的安全聚合已经成为联邦学习领域的一个关键问题。许多安全聚合协议(SAP)面临着过高的计算成本，这严重限制了它们的适用性。我们发现，目前为了降低计算成本的努力往往忽略了一个关键的事实: SAP 的计算负担的相当大一部分来自于处理私有向量中的每个条目。考虑到这一点，我们提出 PVF，一个压缩计算开销的可移植模块。PVF 能够通过特定的线性转换“冻结”私有向量的大部分，只需要原始向量的 $frac {1}{ lambda } $参与 SAP。最终，用户可以通过 SAP 的结果“解冻”“冻结的条目”的公共金额。为了增强功能，我们引入了一些扩展，这些扩展可以对用户的原始向量加强一致性约束，验证聚合结果，并在服务器知道部分私有向量时增强安全性。我们证明 PVF 可以与各种 SAP 无缝集成，并证明它在半诚实和主动的对手设置中不会对用户隐私构成威胁。我们选择 $8 $基线，包括 $6 $不同类型的 SAP，并探索 PVF 对这些 SAP 的加速效应。实证研究表明，当 $lambda = 100 $时，PVF 产生高达 $99.5倍的加速和高达 $32.3倍的通信减少，随着 $lambda $的增加，可能接近 $1000倍的加速。"
    },
    {
        "title": "SA-Attack: Improving Adversarial Transferability of Vision-Language\n  Pre-training Models via Self-Augmentation",
        "url": "http://arxiv.org/abs/2312.04913v1",
        "pub_date": "2023-12-08",
        "summary": "Current Visual-Language Pre-training (VLP) models are vulnerable to\nadversarial examples. These adversarial examples present substantial security\nrisks to VLP models, as they can leverage inherent weaknesses in the models,\nresulting in incorrect predictions. In contrast to white-box adversarial\nattacks, transfer attacks (where the adversary crafts adversarial examples on a\nwhite-box model to fool another black-box model) are more reflective of\nreal-world scenarios, thus making them more meaningful for research. By\nsummarizing and analyzing existing research, we identified two factors that can\ninfluence the efficacy of transfer attacks on VLP models: inter-modal\ninteraction and data diversity. Based on these insights, we propose a\nself-augment-based transfer attack method, termed SA-Attack. Specifically,\nduring the generation of adversarial images and adversarial texts, we apply\ndifferent data augmentation methods to the image modality and text modality,\nrespectively, with the aim of improving the adversarial transferability of the\ngenerated adversarial images and texts. Experiments conducted on the FLickr30K\nand COCO datasets have validated the effectiveness of our method. Our code will\nbe available after this paper is accepted.",
        "translated": "目前的视觉语言预训(VLP)模式很容易受到敌对例子的影响。这些对抗性的例子给 VLP 模型带来了巨大的安全风险，因为它们可以利用模型中固有的弱点，导致不正确的预测。与白盒子对抗性攻击相比，转移攻击(对手在白盒子模型上制造对抗性的例子来欺骗另一个黑盒子模型)更能反映现实世界的情景，从而使它们对研究更有意义。通过对现有研究的总结和分析，我们确定了影响 VLP 模型转移攻击效果的两个因素: 模式间的相互作用和数据的多样性。基于这些认识，我们提出了一种基于自增长的传输攻击方法，称为 SA 攻击。具体来说，在对抗性图像和对抗性文本的生成过程中，我们将不同的数据增强方法分别应用于图像模态和文本模态，目的是提高生成的对抗性图像和文本的对抗性可转移性。在 FLickr30K 和 COCO 数据集上进行的实验验证了该方法的有效性。我们的代码将是可用的后，这份文件被接受。"
    },
    {
        "title": "Mean estimation in the add-remove model of differential privacy",
        "url": "http://arxiv.org/abs/2312.06658v1",
        "pub_date": "2023-12-11",
        "summary": "Differential privacy is often studied under two different models of\nneighboring datasets: the add-remove model and the swap model. While the swap\nmodel is used extensively in the academic literature, many practical libraries\nuse the more conservative add-remove model. However, analysis under the\nadd-remove model can be cumbersome, and obtaining results with tight constants\nrequires some additional work. Here, we study the problem of one-dimensional\nmean estimation under the add-remove model of differential privacy. We propose\na new algorithm and show that it is min-max optimal, that it has the correct\nconstant in the leading term of the mean squared error, and that this constant\nis the same as the optimal algorithm in the swap model. Our results show that,\nfor mean estimation, the add-remove and swap model give nearly identical error\neven though the add-remove model cannot treat the size of the dataset as public\ninformation. In addition, we demonstrate empirically that our proposed\nalgorithm yields a factor of two improvement in mean squared error over\nalgorithms often used in practice.",
        "translated": "差分隐私通常在两种不同的相邻数据集模型下进行研究: 添加-删除模型和交换模型。虽然交换模型在学术文献中得到了广泛的应用，但许多实用的图书馆使用的是更为保守的添加-删除模型。但是，在添加-删除模型下进行分析可能非常麻烦，并且要获得具有严格常数的结果需要进行一些额外的工作。在这里，我们研究一维平均估计的问题下的加去除模型的差分隐私。我们提出了一个新的算法，并证明了它是最小-最大最优的，它在均方差的前期具有正确的常数，并且这个常数与交换模型中的最优算法相同。结果表明，对于均值估计，即使加删除模型不能将数据集的大小作为公共信息处理，加删除模型和交换模型的误差也几乎相同。此外，我们通过经验证明，我们提出的算法比实际使用的算法的均方差提高了一倍。"
    },
    {
        "title": "Sparse but Strong: Crafting Adversarially Robust Graph Lottery Tickets",
        "url": "http://arxiv.org/abs/2312.06568v1",
        "pub_date": "2023-12-11",
        "summary": "Graph Lottery Tickets (GLTs), comprising a sparse adjacency matrix and a\nsparse graph neural network (GNN), can significantly reduce the inference\nlatency and compute footprint compared to their dense counterparts. Despite\nthese benefits, their performance against adversarial structure perturbations\nremains to be fully explored. In this work, we first investigate the resilience\nof GLTs against different structure perturbation attacks and observe that they\nare highly vulnerable and show a large drop in classification accuracy. Based\non this observation, we then present an adversarially robust graph\nsparsification (ARGS) framework that prunes the adjacency matrix and the GNN\nweights by optimizing a novel loss function capturing the graph homophily\nproperty and information associated with both the true labels of the train\nnodes and the pseudo labels of the test nodes. By iteratively applying ARGS to\nprune both the perturbed graph adjacency matrix and the GNN model weights, we\ncan find adversarially robust graph lottery tickets that are highly sparse yet\nachieve competitive performance under different untargeted training-time\nstructure attacks. Evaluations conducted on various benchmarks, considering\ndifferent poisoning structure attacks, namely, PGD, MetaAttack, Meta-PGD, and\nPR-BCD demonstrate that the GLTs generated by ARGS can significantly improve\nthe robustness, even when subjected to high levels of sparsity.",
        "translated": "图形彩票票(glt)由稀疏邻接矩阵和稀疏图形神经网络(GNN)组成，可以显著减少推理延迟和计算足迹。尽管有这些好处，他们的表现对抗对手的结构扰动仍有待充分探索。在这项工作中，我们首先研究了 GLTs 对不同结构扰动攻击的恢复能力，并观察到它们非常容易受到攻击，分类精度有很大的下降。在此基础上，我们提出了一个对抗鲁棒的图稀疏化(ARGS)框架，该框架通过优化一个新的损失函数，捕获图同质性和与列车节点的真实标签和测试节点的伪标签相关的信息，来修剪邻接矩阵和 GNN 权重。通过迭代地应用 ARGS 去除扰动的图形邻接矩阵和 GNN 模型权重，我们可以发现对抗鲁棒的图形彩票，它们非常稀疏，但是在不同的非目标训练时间结构攻击下达到了有竞争力的性能。考虑到不同的中毒结构攻击(即 PGD、 MetaAttack、 Meta-pgD 和 PR-BCD) ，以不同基准进行的评估表明，即使在高稀疏度的情况下，ARGS 生成的 GLT 也能显著提高鲁棒性。"
    },
    {
        "title": "On the Impact of CDL and TDL Augmentation for RF Fingerprinting under\n  Impaired Channels",
        "url": "http://arxiv.org/abs/2312.06555v1",
        "pub_date": "2023-12-11",
        "summary": "Cyber-physical systems have recently been used in several areas (such as\nconnected and autonomous vehicles) due to their high maneuverability. On the\nother hand, they are susceptible to cyber-attacks. Radio frequency (RF)\nfingerprinting emerges as a promising approach. This work aims to analyze the\nimpact of decoupling tapped delay line and clustered delay line (TDL+CDL)\naugmentation-driven deep learning (DL) on transmitter-specific fingerprints to\ndiscriminate malicious users from legitimate ones. This work also considers\n5G-only-CDL, WiFi-only-TDL augmentation approaches. RF fingerprinting models\nare sensitive to changing channels and environmental conditions. For this\nreason, they should be considered during the deployment of a DL model. Data\nacquisition can be another option. Nonetheless, gathering samples under various\nconditions for a train set formation may be quite hard. Consequently, data\nacquisition may not be feasible. This work uses a dataset that includes 5G, 4G,\nand WiFi samples, and it empowers a CDL+TDL-based augmentation technique in\norder to boost the learning performance of the DL model. Numerical results show\nthat CDL+TDL, 5G-only-CDL, and WiFi-only-TDL augmentation approaches achieve\n87.59%, 81.63%, 79.21% accuracy on unobserved data while TDL/CDL augmentation\ntechnique and no augmentation approach result in 77.81% and 74.84% accuracy on\nunobserved data, respectively.",
        "translated": "网络物理系统由于其高可操作性，最近已经在几个领域得到应用(如连接和自动驾驶车辆)。另一方面，他们容易受到网络攻击。射频指纹识别技术是一种很有前途的技术。分析了解耦抽头延迟线和簇延迟线(TDL + CDL)增强驱动深度学习(DL)对特定发送者指纹识别的影响，以区分恶意用户和合法用户。这项工作还考虑了5G-only-CDL，WiFi-only-TDL 增强方法。射频指纹识别模型对变化的通道和环境条件非常敏感。因此，在部署 DL 模型时应该考虑这些因素。数据采集可以是另一种选择。尽管如此，在各种条件下采集列车组合样本可能是相当困难的。因此，数据采集可能是不可行的。这项工作使用了一个包括5G，4G 和 WiFi 样本的数据集，它支持基于 CDL + TDL 的增强技术，以提高 DL 模型的学习性能。数值结果表明，CDL + TDL、5G-only-CDL 和 WiFi-only-TDL 增强方法对未观测数据的准确率分别为87.59% 、81.63% 和79.21% ，而 TDL/CDL 增强方法和无增强方法对未观测数据的准确率分别为77.81% 和74.84% 。"
    },
    {
        "title": "A Golden-Free Formal Method for Trojan Detection in Non-Interfering\n  Accelerators",
        "url": "http://arxiv.org/abs/2312.06515v1",
        "pub_date": "2023-12-11",
        "summary": "The threat of hardware Trojans (HTs) in security-critical IPs like\ncryptographic accelerators poses severe security risks. The HT detection\nmethods available today mostly rely on golden models and detailed circuit\nspecifications. Often they are specific to certain HT payload types, making\npre-silicon verification difficult and leading to security gaps. We propose a\nnovel formal verification method for HT detection in non-interfering\naccelerators at the Register Transfer Level (RTL), employing standard formal\nproperty checking. Our method guarantees the exhaustive detection of any\nsequential HT independently of its payload behavior, including physical side\nchannels. It does not require a golden model or a functional specification of\nthe design. The experimental results demonstrate efficient and effective\ndetection of all sequential HTs in accelerators available on Trust-Hub,\nincluding those with complex triggers and payloads.",
        "translated": "安全关键 IP (如加密加速器)中的硬件木马(HTs)威胁构成严重的安全风险。现有的 HT 检测方法主要依赖于黄金模型和详细的电路规范。通常它们是特定于某些 HT 有效载荷类型的，这使得前硅验证变得困难，并导致安全漏洞。我们提出了一种新的形式验证检测方法，在非干扰加速器在寄存器传输级(RTL) ，采用标准的形式属性检查。我们的方法保证任何顺序 HT 的穷举检测独立于其有效载荷行为，包括物理边通道。它不需要黄金模型或设计规格化。实验结果表明，在 Trust-Hub 可用的加速器(包括那些具有复杂触发器和有效载荷的加速器)中，对所有顺序高温超导都进行了高效和有效的检测。"
    },
    {
        "title": "Trusting a Smart Contract Means Trusting Its Owners: Understanding\n  Centralization Risk",
        "url": "http://arxiv.org/abs/2312.06510v1",
        "pub_date": "2023-12-11",
        "summary": "Smart contract access control mechanisms can introduce centralization into\nsupposedly decentralized ecosystems. In our view, such centralization is an\noverlooked risk of smart contracts that underlies well-known smart contract\nsecurity incidents. Critically, mitigating the known vulnerability of missing\npermission verification by implementing authorization patterns can in turn\nintroduce centralization. To delineate the issue, we define centralization risk\nand describe smart contract source code patterns for Ethereum and Algorand that\ncan introduce it to smart contracts. We explain under which circumstances the\ncentralization can be exploited. Finally, we discuss implications of\ncentralization risk for different smart contract stakeholders.",
        "translated": "智能合同访问控制机制可以将集中化引入所谓的分散式生态系统。在我们看来，这种集中化是智能合同的一个被忽视的风险，而智能合同是众所周知的智能合同安全事件的基础。重要的是，通过实现授权模式来减轻已知的缺失权限验证的漏洞反过来又可以引入集中化。为了描述这个问题，我们定义了集中化风险，并描述了以太坊和 Algorand 的智能合同源代码模式，这些模式可以将其引入智能合同。我们解释在什么情况下可以利用中央集权。最后，我们讨论了集中风险对不同智能契约利益相关者的影响。"
    },
    {
        "title": "Performance-lossless Black-box Model Watermarking",
        "url": "http://arxiv.org/abs/2312.06488v1",
        "pub_date": "2023-12-11",
        "summary": "With the development of deep learning, high-value and high-cost models have\nbecome valuable assets, and related intellectual property protection\ntechnologies have become a hot topic. However, existing model watermarking work\nin black-box scenarios mainly originates from training-based backdoor methods,\nwhich probably degrade original task performance. To address this, we propose a\nbranch backdoor-based model watermarking protocol to protect model intellectual\nproperty, where a construction based on a message authentication scheme is\nadopted as the branch indicator. We prove the lossless performance of the\nprotocol by reduction. Taking the language generation task as an instance, we\nshow the effectiveness of the proposed protocol.",
        "translated": "随着深度学习的发展，高价值、高成本的模型已成为宝贵的资产，相关的知识产权保护技术已成为热门话题。然而，现有的黑盒场景中的模型水印工作主要来源于基于训练的后门方法，这可能会降低原始任务的性能。为了解决这一问题，我们提出了一种基于分支后门的模型水印协议来保护模型的知识产权，该协议采用基于消息认证方案的结构作为分支指示器。通过简化证明了该协议的无损性能。以语言生成任务为例，验证了该协议的有效性。"
    },
    {
        "title": "Numeric Truncation Security Predicate",
        "url": "http://arxiv.org/abs/2312.06425v1",
        "pub_date": "2023-12-11",
        "summary": "Numeric truncation is a widely spread error in software written in languages\nwith static data typing, such as C/C++ or Java. It occurs when the significant\nbits of the value with a bigger type size are truncated during value conversion\nto the smaller type. Utilizing one of the most powerful methods for path\nexploration and automated bug detection called dynamic symbolic execution\n(DSE), we propose the symbolic security predicate for numeric truncation error\ndetection, developed on top of DSE tool Sydr. Firstly, we execute the program\non the data, which does not lead to any errors. During program execution we\nupdate symbolic shadow stack and shadow registers to track symbolic sizes of\nthe symbolic variables to avoid false positives. Then, if we meet the\ninstruction, which truncates the symbolic variable, we build the security\npredicate, try to solve it with the SMT-solver and in case of success save new\ninput file to reproduce the error. We tested our approach on Juliet Dynamic\ntest suite for CWE-197 and achieved 100% accuracy. We approved the workability\nof our approach by detecting 12 new errors of numeric truncation in 5 different\nreal-world open source projects within OSS-Sydr-Fuzz project. All of the errors\nwere reported, most of the reports were equipped with appropriate fixes,\nsuccessfully confirmed and applied by project maintainers.",
        "translated": "数值截断是使用具有静态数据类型的语言(如 C/C + + 或 Java)编写的软件中广泛存在的错误。当类型大小较大的值的有效位在值转换为较小类型期间被截断时发生。利用最强大的路径探索和自动缺陷检测方法之一的动态符号执行(DSE) ，我们提出了数字截尾误差检测的符号安全谓词，它是在 DSE 工具 Sydr 的基础上开发的。首先，我们对数据执行程序，这不会导致任何错误。在程序执行期间，我们更新符号阴影堆栈和阴影寄存器来跟踪符号变量的符号大小，以避免误报。然后，如果我们遇到截断符号变量的指令，我们构建安全谓词，尝试用 SMT 求解器解决它，并在成功的情况下保存新的输入文件来重现错误。我们在用于 CWE-197的 Juliet Dynamic 测试套件上测试了我们的方法，并获得了100% 的准确性。我们通过在 OSS-Sydr-Fuzz 项目中的5个不同的实际开源项目中检测到12个新的数值截断错误来验证我们的方法的可行性。所有的错误都被报告了，大多数的报告都配备了适当的修正，成功地被项目维护人员确认和应用。"
    },
    {
        "title": "MalPurifier: Enhancing Android Malware Detection with Adversarial\n  Purification against Evasion Attacks",
        "url": "http://arxiv.org/abs/2312.06423v1",
        "pub_date": "2023-12-11",
        "summary": "Machine learning (ML) has gained significant adoption in Android malware\ndetection to address the escalating threats posed by the rapid proliferation of\nmalware attacks. However, recent studies have revealed the inherent\nvulnerabilities of ML-based detection systems to evasion attacks. While efforts\nhave been made to address this critical issue, many of the existing defensive\nmethods encounter challenges such as lower effectiveness or reduced\ngeneralization capabilities. In this paper, we introduce a novel Android\nmalware detection method, MalPurifier, which exploits adversarial purification\nto eliminate perturbations independently, resulting in attack mitigation in a\nlight and flexible way. Specifically, MalPurifier employs a Denoising\nAutoEncoder (DAE)-based purification model to preprocess input samples,\nremoving potential perturbations from them and then leading to correct\nclassification. To enhance defense effectiveness, we propose a diversified\nadversarial perturbation mechanism that strengthens the purification model\nagainst different manipulations from various evasion attacks. We also\nincorporate randomized \"protective noises\" onto benign samples to prevent\nexcessive purification. Furthermore, we customize a loss function for improving\nthe DAE model, combining reconstruction loss and prediction loss, to enhance\nfeature representation learning, resulting in accurate reconstruction and\nclassification. Experimental results on two Android malware datasets\ndemonstrate that MalPurifier outperforms the state-of-the-art defenses, and it\nsignificantly strengthens the vulnerable malware detector against 37 evasion\nattacks, achieving accuracies over 90.91%. Notably, MalPurifier demonstrates\neasy scalability to other detectors, offering flexibility and robustness in its\nimplementation.",
        "translated": "机器学习(ML)在 Android 恶意软件检测中得到了显著的应用，以应对恶意软件攻击快速增长所带来的不断升级的威胁。然而，最近的研究揭示了基于机器学习的检测系统对于规避攻击的内在弱点。虽然为解决这一关键问题作出了努力，但许多现有的防御方法遇到了效率低下或一般化能力降低等挑战。本文介绍了一种新的 Android 恶意软件检测方法 MalPurifier，该方法利用对抗性净化技术独立消除干扰，从而轻松灵活地减少攻击。具体来说，MalPurifier 采用了一种基于去噪自动编码器(DAE)的净化模型来预处理输入样本，去除潜在的扰动，然后导致正确的分类。为了提高防御效率，我们提出了一种多样化的对抗扰动机制，加强了针对各种规避攻击的不同操作的净化模型。我们还将随机的“保护性噪声”加入到良性样本中，以防止过度纯化。此外，我们定制了一个损失函数来改进 DAE 模型，将重建损失和预测损失结合起来，以增强特征表示学习，从而得到精确的重建和分类。在两个 Android 恶意软件数据集上的实验结果表明，MalPurifier 的性能优于最先进的防御系统，并且显著增强了脆弱的恶意软件检测器对37次规避攻击的防御能力，准确率达到90.91% 以上。值得注意的是，MalPurifier 展示了对其他检测器的简单可伸缩性，在其实现中提供了灵活性和健壮性。"
    },
    {
        "title": "Security and Reliability Evaluation of Countermeasures implemented using\n  High-Level Synthesis",
        "url": "http://arxiv.org/abs/2312.06268v1",
        "pub_date": "2023-12-11",
        "summary": "As the complexity of digital circuits increases, High-Level Synthesis (HLS)\nis becoming a valuable tool to increase productivity and design reuse by\nutilizing relevant Electronic Design Automation (EDA) flows, either for\nApplication-Specific Integrated Circuits (ASIC) or for Field Programmable Gate\nArrays (FPGA). Side Channel Analysis (SCA) and Fault Injection (FI) attacks are\npowerful hardware attacks, capable of greatly weakening the theoretical\nsecurity levels of secure implementations. Furthermore, critical applications\ndemand high levels of reliability including fault tolerance. The lack of\nsecurity and reliability driven optimizations in HLS tools makes it necessary\nfor the HLS-based designs to validate that the properties of the algorithm and\nthe countermeasures have not been compromised due to the HLS flow. In this\nwork, we provide results on the resilience evaluation of HLS-based FPGA\nimplementations for the aforementioned threats. As a test case, we use multiple\nversions of an on-the-fly SBOX algorithm integrating different countermeasures\n(hiding and masking), written in C and implemented using Vivado HLS. We perform\nextensive evaluations for all the designs and their optimization scenarios. The\nresults provide evidence of issues arising due to HLS optimizations on the\nsecurity and the reliability of cryptographic implementations. Furthermore, the\nresults put HLS algorithms to the test of designing secure accelerators and can\nlead to improving them towards the goal of increasing productivity in the\ndomain of secure and reliable cryptographic implementations.",
        "translated": "随着数字电路复杂性的增加，高级综合(HLS)正在成为一种有价值的工具，通过利用相关的电子设计自动化(EDA)流来提高生产率和设计重用，无论是专用集成电路(ASIC)还是现场可编程门阵列(FPGA)。侧信道分析(SCA)和故障注入(FI)攻击是强大的硬件攻击，能够大大削弱安全实现的理论安全水平。此外，关键应用程序需要高水平的可靠性，包括容错性。由于合肥光源工具缺乏安全性和可靠性驱动的优化，因此基于合肥光源的设计需要验证合肥光源的算法和对策的性能没有受到合肥光源流程的影响。在这项工作中，我们提供了基于合肥光源的 FPGA 实现对上述威胁的弹性评估的结果。作为测试用例，我们使用多个版本的动态 SBOX 算法，该算法集成了不同的对策(隐藏和掩蔽) ，用 C 编写，并使用 Vivado HLS 实现。我们对所有的设计及其优化方案进行广泛的评估。这些结果提供了证据，证明由于 HLS 在加密实现的安全性和可靠性方面的优化而产生的问题。此外，研究结果将 HLS 算法应用于安全加速器的设计测试中，并且可以改进算法，使其朝着提高安全和可靠密码实现领域的生产率的目标发展。"
    },
    {
        "title": "On The Effect of Replacement Policies on The Security of Randomized\n  Cache Architectures",
        "url": "http://arxiv.org/abs/2312.06235v1",
        "pub_date": "2023-12-11",
        "summary": "Randomizing the mapping of addresses to cache entries has proven to be an\neffective technique for hardening caches against contention-based attacks like\nPrime+Prome. While attacks and defenses are still evolving, it is clear that\nrandomized caches significantly increase the security against such attacks.\nHowever, one aspect that is missing from most analyses of randomized cache\narchitectures is the choice of the replacement policy. Often, only the random-\nand LRU replacement policies are investigated. However, LRU is not applicable\nto randomized caches due to its immense hardware overhead, while the random\nreplacement policy is not ideal from a performance and security perspective.\n  In this paper, we explore replacement policies for randomized caches. We\ndevelop two new replacement policies and evaluate a total of five replacement\npolicies regarding their security against Prime+Prune+Probe attackers.\nMoreover, we analyze the effect of the replacement policy on the system's\nperformance and quantify the introduced hardware overhead. We implement\nrandomized caches with configurable replacement policies in software and\nhardware using a custom cache simulator, gem5, and the CV32E40P RISC-V core.\nAmong others, we show that the construction of eviction sets with our new\npolicy, VARP-64, requires over 25-times more cache accesses than with the\nrandom replacement policy while also enhancing overall performance.",
        "translated": "随机地址到缓存条目的映射已被证明是一种有效的技术，可以加强缓存对抗基于竞争的攻击，比如 Prime + Prome。虽然攻击和防御仍在不断发展，但很明显，随机缓存显著提高了针对此类攻击的安全性。然而，在大多数随机缓存体系结构的分析中缺少的一个方面是替换策略的选择。通常，只调查随机替换策略和 LRU 替换策略。然而，LRU 由于其巨大的硬件开销而不适用于随机缓存，而从性能和安全角度来看，随机替换策略并不理想。在本文中，我们探讨了随机缓存的替换策略。我们开发了两个新的替换策略，并评估了总共五个替换策略对 Prime + Prune + 探针攻击者的安全性。此外，我们还分析了替换策略对系统性能的影响，并量化了引入的硬件开销。我们使用自定义缓存模拟器 gem5和 CV32E40P RISC-V 核心在软件和硬件上实现具有可配置替换策略的随机缓存。除此之外，我们还展示了使用我们的新策略 VARP-64构建驱逐集需要比使用随机替换策略多25倍的缓存访问，同时还提高了整体性能。"
    },
    {
        "title": "Privacy-Aware Energy Consumption Modeling of Connected Battery Electric\n  Vehicles using Federated Learning",
        "url": "http://arxiv.org/abs/2312.07371v1",
        "pub_date": "2023-12-12",
        "summary": "Battery Electric Vehicles (BEVs) are increasingly significant in modern\ncities due to their potential to reduce air pollution. Precise and real-time\nestimation of energy consumption for them is imperative for effective itinerary\nplanning and optimizing vehicle systems, which can reduce driving range anxiety\nand decrease energy costs. As public awareness of data privacy increases,\nadopting approaches that safeguard data privacy in the context of BEV energy\nconsumption modeling is crucial. Federated Learning (FL) is a promising\nsolution mitigating the risk of exposing sensitive information to third parties\nby allowing local data to remain on devices and only sharing model updates with\na central server. Our work investigates the potential of using FL methods, such\nas FedAvg, and FedPer, to improve BEV energy consumption prediction while\nmaintaining user privacy. We conducted experiments using data from 10 BEVs\nunder simulated real-world driving conditions. Our results demonstrate that the\nFedAvg-LSTM model achieved a reduction of up to 67.84\\% in the MAE value of the\nprediction results. Furthermore, we explored various real-world scenarios and\ndiscussed how FL methods can be employed in those cases. Our findings show that\nFL methods can effectively improve the performance of BEV energy consumption\nprediction while maintaining user privacy.",
        "translated": "电池电动汽车(BEV)由于具有减少空气污染的潜力，在现代城市中越来越重要。精确、实时地估算车辆的能耗对于有效地进行行程规划和优化车辆系统，减少行驶里程焦虑，降低能耗成本具有重要意义。随着公众对数据隐私意识的提高，在 BEV 能源消耗模型中采用保护数据隐私的方法是至关重要的。联邦学习(Federated Learning，FL)是一种很有前途的解决方案，它允许本地数据保留在设备上，只与中央服务器共享模型更新，从而减少了将敏感信息暴露给第三方的风险。我们的工作研究了使用 FL 方法的潜力，如 FedAvg 和 FedPer，以改善 BEV 能源消耗预测，同时维护用户隐私。我们利用来自10辆 BEV 的数据在模拟真实驾驶环境下进行了实验。结果表明，FedAvg-LSTM 模型预测结果的 MAE 值降低了67.84% 。此外，我们还探讨了各种真实场景，并讨论了如何在这些情况下使用 FL 方法。研究结果表明，FL 方法在保护用户隐私的同时，能有效地提高 BEV 能耗预测的性能。"
    },
    {
        "title": "Stop Following Me! Evaluating the Effectiveness of Anti-Stalking\n  Features of Personal Item Tracking Devices",
        "url": "http://arxiv.org/abs/2312.07157v1",
        "pub_date": "2023-12-12",
        "summary": "Personal item tracking devices are popular for locating lost items such as\nkeys, wallets, and suitcases. Originally created to help users find personal\nitems quickly, these devices are now being abused by stalkers and domestic\nabusers to track their victims' location over time. Some device manufacturers\ncreated `anti-stalking features' in response, and later improved on them after\ncriticism that they were insufficient. We analyse the effectiveness of the\nanti-stalking features with five brands of tracking devices through a gamified\nnaturalistic quasi-experiment in collaboration with the Assassins' Guild\nstudent society. Despite participants knowing they might be tracked, and being\nincentivised to detect and remove the tracker, the anti-stalking features were\nnot useful and were rarely used. We also identify additional issues with\nfeature availability, usability, and effectiveness. These failures combined\nimply a need to greatly improve the presence of anti-stalking features to\nprevent trackers being abused.",
        "translated": "个人物品跟踪设备是流行的定位丢失的项目，如钥匙，钱包，行李箱。这些设备最初是为了帮助用户快速找到个人物品而设计的，现在这些设备正被跟踪者和家庭暴力者滥用，以便随着时间的推移追踪受害者的位置。一些设备制造商创造了“反跟踪功能”作为回应，后来在批评它们不够充分之后对它们进行了改进。我们与刺客协会学生会合作，透过游戏化的自然主义准实验，分析五个品牌追踪器的反跟踪功能的成效。尽管参与者知道他们可能被跟踪，并被激励去检测和删除跟踪器，反跟踪功能是没有用的，很少使用。我们还确定了特性可用性、可用性和有效性方面的其他问题。这些故障加在一起意味着需要大大提高反跟踪功能的存在，以防止跟踪器被滥用。"
    },
    {
        "title": "Practical considerations on using private sampling for synthetic data",
        "url": "http://arxiv.org/abs/2312.07139v1",
        "pub_date": "2023-12-12",
        "summary": "Artificial intelligence and data access are already mainstream. One of the\nmain challenges when designing an artificial intelligence or disclosing content\nfrom a database is preserving the privacy of individuals who participate in the\nprocess. Differential privacy for synthetic data generation has received much\nattention due to the ability of preserving privacy while freely using the\nsynthetic data. Private sampling is the first noise-free method to construct\ndifferentially private synthetic data with rigorous bounds for privacy and\naccuracy. However, this synthetic data generation method comes with constraints\nwhich seem unrealistic and not applicable for real-world datasets. In this\npaper, we provide an implementation of the private sampling algorithm and\ndiscuss the realism of its constraints in practical cases.",
        "translated": "人工智能和数据访问已经成为主流。在设计人工智能或从数据库中披露内容时，主要的挑战之一是保护参与过程的个人的隐私。合成数据生成差分隐私由于能够在自由使用合成数据的同时保护隐私，因此备受关注。私有采样是第一种构造差分私有合成数据的无噪声方法，在保密性和准确性方面有着严格的界限。然而，这种合成数据生成方法带有一些约束，这些约束似乎不现实，并且不适用于真实世界的数据集。本文给出了一个私有抽样算法的实现，并讨论了在实际情况下其约束条件的真实性。"
    },
    {
        "title": "LLMs Perform Poorly at Concept Extraction in Cyber-security Research\n  Literature",
        "url": "http://arxiv.org/abs/2312.07110v1",
        "pub_date": "2023-12-12",
        "summary": "The cybersecurity landscape evolves rapidly and poses threats to\norganizations. To enhance resilience, one needs to track the latest\ndevelopments and trends in the domain. It has been demonstrated that standard\nbibliometrics approaches show their limits in such a fast-evolving domain. For\nthis purpose, we use large language models (LLMs) to extract relevant knowledge\nentities from cybersecurity-related texts. We use a subset of arXiv preprints\non cybersecurity as our data and compare different LLMs in terms of entity\nrecognition (ER) and relevance. The results suggest that LLMs do not produce\ngood knowledge entities that reflect the cybersecurity context, but our results\nshow some potential for noun extractors. For this reason, we developed a noun\nextractor boosted with some statistical analysis to extract specific and\nrelevant compound nouns from the domain. Later, we tested our model to identify\ntrends in the LLM domain. We observe some limitations, but it offers promising\nresults to monitor the evolution of emergent trends.",
        "translated": "网络安全领域发展迅速，对组织构成威胁。为了增强弹性，我们需要跟踪该领域的最新发展和趋势。已经证明，标准的文献计量学方法在这样一个快速发展的领域显示了它们的局限性。为此，我们使用大型语言模型(LLM)从网络安全相关的文本中提取相关的知识实体。我们使用网络安全的 arXiv 预印本的一个子集作为我们的数据，并比较不同的 LLM 在实体识别(ER)和相关性方面。结果表明，LLM 不能产生反映网络安全上下文的良好知识实体，但我们的研究结果显示了名词抽取器的一些潜力。为此，我们开发了一个名词提取器，并进行了一些统计分析，以提取具体的和相关的复合名词从领域。后来，我们测试了我们的模型，以确定 LLM 领域的趋势。我们观察到了一些局限性，但它为监测突发趋势的演变提供了有希望的结果。"
    },
    {
        "title": "Focus on Hiders: Exploring Hidden Threats for Enhancing Adversarial\n  Training",
        "url": "http://arxiv.org/abs/2312.07067v1",
        "pub_date": "2023-12-12",
        "summary": "Adversarial training is often formulated as a min-max problem, however,\nconcentrating only on the worst adversarial examples causes alternating\nrepetitive confusion of the model, i.e., previously defended or correctly\nclassified samples are not defensible or accurately classifiable in subsequent\nadversarial training. We characterize such non-ignorable samples as \"hiders\",\nwhich reveal the hidden high-risk regions within the secure area obtained\nthrough adversarial training and prevent the model from finding the real worst\ncases. We demand the model to prevent hiders when defending against adversarial\nexamples for improving accuracy and robustness simultaneously. By rethinking\nand redefining the min-max optimization problem for adversarial training, we\npropose a generalized adversarial training algorithm called Hider-Focused\nAdversarial Training (HFAT). HFAT introduces the iterative evolution\noptimization strategy to simplify the optimization problem and employs an\nauxiliary model to reveal hiders, effectively combining the optimization\ndirections of standard adversarial training and prevention hiders. Furthermore,\nwe introduce an adaptive weighting mechanism that facilitates the model in\nadaptively adjusting its focus between adversarial examples and hiders during\ndifferent training periods. We demonstrate the effectiveness of our method\nbased on extensive experiments, and ensure that HFAT can provide higher\nrobustness and accuracy.",
        "translated": "对抗性训练通常表述为最小最大问题，然而，仅集中于最坏的对抗性例子会导致模型的交替重复混淆，即先前辩护或正确分类的样本在随后的对抗性训练中是不可辩护或不能准确分类的。我们将这些不可忽略的样本描述为“隐藏者”，它们揭示了通过对抗性训练获得的安全区域内的隐藏的高风险区域，从而防止模型发现真正的最坏情况。我们要求该模型在防御对手实例时防止隐藏者，以同时提高准确性和鲁棒性。通过重新思考和重新定义对抗性训练的最小-最大最佳化问题，我们提出了一种广义对抗性训练算法，称为 Hider-focus adversarialTraining (HFAT)。HFAT 引入了迭代进化优化策略来简化最佳化问题，并采用了一个辅助模型来揭示隐藏者，有效地结合了标准对抗训练和预防隐藏者的优化方向。此外，我们还引入了一种自适应加权机制，使得模型能够在不同的训练阶段在对手样本和隐藏者之间自适应地调整焦点。在大量实验的基础上验证了该方法的有效性，保证了 HFAT 算法具有较高的鲁棒性和准确性。"
    },
    {
        "title": "Communication Cost Reduction for Subgraph Counting under Local\n  Differential Privacy via Hash Functions",
        "url": "http://arxiv.org/abs/2312.07055v1",
        "pub_date": "2023-12-12",
        "summary": "We suggest the use of hash functions to cut down the communication costs when\ncounting subgraphs under edge local differential privacy. While various\nalgorithms exist for computing graph statistics, including the count of\nsubgraphs, under the edge local differential privacy, many suffer with high\ncommunication costs, making them less efficient for large graphs. Though data\ncompression is a typical approach in differential privacy, its application in\nlocal differential privacy requires a form of compression that every node can\nreproduce. In our study, we introduce linear congruence hashing. With a\nsampling rate of $s$, our method can cut communication costs by a factor of\n$s^2$, albeit at the cost of increasing variance in the published graph\nstatistic by a factor of $s$. The experimental results indicate that, when\nmatched for communication costs, our method achieves a reduction in the\n$\\ell_2$-error for triangle counts by up to 1000 times compared to the\nperformance of leading algorithms.",
        "translated": "我们建议在计算边局部差分隐私下的子图时，使用哈希函数来减少通信成本。尽管存在各种计算图统计量(包括子图计数)的算法，但在边局部差分隐私下，许多算法都存在较高的通信成本，这使得它们在处理大型图时效率较低。尽管数据压缩是差分隐私中的一种典型方法，但是它在本地差分隐私中的应用需要一种每个节点都可以重现的压缩形式。在我们的研究中，我们引入了线性同余散列。在采样率为 $s $的情况下，我们的方法可以将通信成本降低 $s ^ 2 $的一个因子，尽管代价是在已发布的图统计量中增加 $s $的一个因子的方差。实验结果表明，当与通信代价相匹配时，该方法对三角形计数的 $ell _ 2 $误差比主导算法的性能降低了1000倍。"
    },
    {
        "title": "Patch-MI: Enhancing Model Inversion Attacks via Patch-Based\n  Reconstruction",
        "url": "http://arxiv.org/abs/2312.07040v1",
        "pub_date": "2023-12-12",
        "summary": "Model inversion (MI) attacks aim to reveal sensitive information in training\ndatasets by solely accessing model weights. Generative MI attacks, a prominent\nstrand in this field, utilize auxiliary datasets to recreate target data\nattributes, restricting the images to remain photo-realistic, but their success\noften depends on the similarity between auxiliary and target datasets. If the\ndistributions are dissimilar, existing MI attack attempts frequently fail,\nyielding unrealistic or target-unrelated results. In response to these\nchallenges, we introduce a groundbreaking approach named Patch-MI, inspired by\njigsaw puzzle assembly. To this end, we build upon a new probabilistic\ninterpretation of MI attacks, employing a generative adversarial network\n(GAN)-like framework with a patch-based discriminator. This approach allows the\nsynthesis of images that are similar to the target dataset distribution, even\nin cases of dissimilar auxiliary dataset distribution. Moreover, we artfully\nemploy a random transformation block, a sophisticated maneuver that crafts\ngeneralized images, thus enhancing the efficacy of the target classifier. Our\nnumerical and graphical findings demonstrate that Patch-MI surpasses existing\ngenerative MI methods in terms of accuracy, marking significant advancements\nwhile preserving comparable statistical dataset quality. For reproducibility of\nour results, we make our source code publicly available in\nhttps://github.com/jonggyujang0123/Patch-Attack.",
        "translated": "模型反演(MI)攻击旨在通过单独访问模型权重来揭示训练数据集中的敏感信息。生成性 MI 攻击是这一领域的一个突出方向，它利用辅助数据集重建目标数据属性，限制图像保持真实感，但其成功往往取决于辅助数据集和目标数据集之间的相似性。如果分布不同，现有的 MI 攻击尝试经常失败，产生不切实际或与目标无关的结果。为了应对这些挑战，我们引入了一个开创性的方法，名为补丁-MI，灵感来自拼图游戏组装。为此，我们建立了一个新的 MI 攻击的概率解释，采用了一个生成的对抗网络(GAN)类框架与补丁为基础的鉴别。这种方法允许合成与目标数据集分布相似的图像，即使在不同的辅助数据集分布的情况下也是如此。此外，我们巧妙地运用了一个随机变换块，这是一个复杂的机动，工艺广义的图像，从而提高了目标分类器的效率。我们的数值和图形结果表明，补丁 MI 超过现有的生成 MI 方法在准确性方面，标志着显着的进步，同时保持可比较的统计数据集质量。为了保证结果的可重复性，我们将源代码以 https://github.com/jonggyujang0123/patch-attack 的形式公开。"
    },
    {
        "title": "EdgePruner: Poisoned Edge Pruning in Graph Contrastive Learning",
        "url": "http://arxiv.org/abs/2312.07022v1",
        "pub_date": "2023-12-12",
        "summary": "Graph Contrastive Learning (GCL) is unsupervised graph representation\nlearning that can obtain useful representation of unknown nodes. The node\nrepresentation can be utilized as features of downstream tasks. However, GCL is\nvulnerable to poisoning attacks as with existing learning models. A\nstate-of-the-art defense cannot sufficiently negate adverse effects by poisoned\ngraphs although such a defense introduces adversarial training in the GCL. To\nachieve further improvement, pruning adversarial edges is important. To the\nbest of our knowledge, the feasibility remains unexplored in the GCL domain. In\nthis paper, we propose a simple defense for GCL, EdgePruner. We focus on the\nfact that the state-of-the-art poisoning attack on GCL tends to mainly add\nadversarial edges to create poisoned graphs, which means that pruning edges is\nimportant to sanitize the graphs. Thus, EdgePruner prunes edges that contribute\nto minimizing the contrastive loss based on the node representation obtained\nafter training on poisoned graphs by GCL. Furthermore, we focus on the fact\nthat nodes with distinct features are connected by adversarial edges in\npoisoned graphs. Thus, we introduce feature similarity between neighboring\nnodes to help more appropriately determine adversarial edges. This similarity\nis helpful in further eliminating adverse effects from poisoned graphs on\nvarious datasets. Finally, EdgePruner outputs a graph that yields the minimum\ncontrastive loss as the sanitized graph. Our results demonstrate that pruning\nadversarial edges is feasible on six datasets. EdgePruner can improve the\naccuracy of node classification under the attack by up to 5.55% compared with\nthat of the state-of-the-art defense. Moreover, we show that EdgePruner is\nimmune to an adaptive attack.",
        "translated": "图形对比学习(GCL)是一种无监督的图形表示学习，它可以获得未知节点的有用表示。节点表示可以用作下游任务的特征。然而，与现有的学习模型一样，GCL 容易受到中毒攻击。最先进的防御不能充分消除有毒图表的不利影响，尽管这种防御在 GCL 中引入了对抗性训练。为了达到进一步的改进，修剪敌对边缘是重要的。据我们所知，这种可行性在 GCL 领域仍然没有得到探索。在本文中，我们提出了一个简单的防御 GCL，EdgePruner。我们关注的事实是，对 GCL 的最先进的中毒攻击倾向于主要添加对抗性边来创建中毒图，这意味着修剪边对于消毒图非常重要。因此，EdgePruner 基于 GCL 对中毒图进行训练后获得的节点表示来修剪有助于最小化对比损失的边。此外，我们还研究了中毒图中具有不同特征的节点通过对抗边连接的问题。因此，我们引入相邻节点之间的特征相似性，以帮助更恰当地确定对抗性边缘。这种相似性有助于进一步消除各种数据集上中毒图表的不利影响。最后，EdgePruner 输出一个图，该图产生最小的对比度损失作为经过清理的图。我们的结果表明，修剪敌对边缘是可行的六个数据集。EdgePruner 可以提高攻击下节点分类的准确率，与最先进的防御方法相比提高了5.55% 。此外，我们表明，EdgePruner 是免疫的自适应攻击。"
    },
    {
        "title": "Task-Agnostic Privacy-Preserving Representation Learning for Federated\n  Learning Against Attribute Inference Attacks",
        "url": "http://arxiv.org/abs/2312.06989v1",
        "pub_date": "2023-12-12",
        "summary": "Federated learning (FL) has been widely studied recently due to its property\nto collaboratively train data from different devices without sharing the raw\ndata. Nevertheless, recent studies show that an adversary can still be possible\nto infer private information about devices' data, e.g., sensitive attributes\nsuch as income, race, and sexual orientation. To mitigate the attribute\ninference attacks, various existing privacy-preserving FL methods can be\nadopted/adapted. However, all these existing methods have key limitations: they\nneed to know the FL task in advance, or have intolerable computational\noverheads or utility losses, or do not have provable privacy guarantees.\n  We address these issues and design a task-agnostic privacy-preserving\npresentation learning method for FL ({\\bf TAPPFL}) against attribute inference\nattacks. TAPPFL is formulated via information theory. Specifically, TAPPFL has\ntwo mutual information goals, where one goal learns task-agnostic data\nrepresentations that contain the least information about the private attribute\nin each device's data, and the other goal ensures the learnt data\nrepresentations include as much information as possible about the device data\nto maintain FL utility. We also derive privacy guarantees of TAPPFL against\nworst-case attribute inference attacks, as well as the inherent tradeoff\nbetween utility preservation and privacy protection. Extensive results on\nmultiple datasets and applications validate the effectiveness of TAPPFL to\nprotect data privacy, maintain the FL utility, and be efficient as well.\nExperimental results also show that TAPPFL outperforms the existing\ndefenses\\footnote{Source code and full version:\n\\url{https://github.com/TAPPFL}}.",
        "translated": "联邦学习(FL)由于具有协同训练不同设备数据而不共享原始数据的特性，近年来得到了广泛的研究。尽管如此，最近的研究表明，对手仍然可以推断出设备数据的私人信息，例如，收入、种族和性取向等敏感属性。为了减轻属性推理攻击，可以采用/适应各种现有的保护隐私的 FL 方法。然而，所有这些现有的方法都有关键的局限性: 它们需要提前知道 FL 任务，或者有难以忍受的计算开销或效用损失，或者没有可证明的隐私保障。针对这些问题，我们设计了一种针对属性推理攻击的 FL ({ bf TAPPFL })任务无关隐私保护表示学习方法。TAPPFL 是通过信息论公式化的。具体来说，TAPPFL 有两个相互信息目标，其中一个目标学习任务无关的数据表示，这些数据表示包含每个设备数据中关于私有属性的最少信息，另一个目标确保所学习的数据表示包含尽可能多的关于设备数据的信息，以维护 FL 效用。我们还推导了 TAPPFL 针对最坏情况属性推理攻击的隐私保护，以及实用性保护和隐私保护之间的内在权衡。在多个数据集和应用的广泛结果验证了 TAPPFL 的有效性，以保护数据隐私，维护 FL 的实用性，以及是有效的。实验结果还表明，TAPPFL 的性能优于现有的防御脚注{源代码和完整版本: url { https://github.com/TAPPFL }}。"
    },
    {
        "title": "A new lightweight additive homomorphic encryption algorithm",
        "url": "http://arxiv.org/abs/2312.06987v1",
        "pub_date": "2023-12-12",
        "summary": "This article describes a lightweight additive homomorphic algorithm with the\nsame encryption and decryption keys. Compared to standard additive homomorphic\nalgorithms like Paillier, this algorithm reduces the computational cost of\nencryption and decryption from modular exponentiation to modular\nmultiplication, and reduces the computational cost of ciphertext addition from\nmodular multiplication to modular addition. This algorithm is based on a new\nmathematical problem: in two division operations, whether it is possible to\ninfer the remainder or divisor based on the dividend when two remainders are\nrelated. Currently, it is not obvious how to break this problem, but further\nexploration is needed to determine if it is sufficiently difficult. In addition\nto this mathematical problem, we have also designed two interesting\nmathematical structures for decryption, which are used in the two algorithms\nmentioned in the main text. It is possible that the decryption structure of\nAlgorithm 2 introduces new security vulnerabilities, but we have not\ninvestigated this issue thoroughly.",
        "translated": "本文描述了一种具有相同加密和解密密钥的轻量级加法同态算法。与 Paillier 等标准加法同态算法相比，该算法将加密和解密的计算代价从模幂运算降低到模乘运算，并将密文加法的计算代价从模乘运算降低到模加法运算。该算法基于一个新的数学问题: 在两个除法运算中，当两个除法运算相关时，是否可以根据红利推断余数或除数。目前，如何突破这一难题还不明显，但是是否足够困难还需要进一步的探索。除了这个数学问题之外，我们还设计了两个有趣的数学结构用于解密，它们被用于正文中提到的两个算法。算法2的解密结构可能引入了新的安全漏洞，但是我们还没有彻底地研究这个问题。"
    },
    {
        "title": "On the Computational Hardness of Quantum One-Wayness",
        "url": "http://arxiv.org/abs/2312.08363v1",
        "pub_date": "2023-12-13",
        "summary": "There is a large body of work studying what forms of computational hardness\nare needed to realize classical cryptography. In particular, one-way functions\nand pseudorandom generators can be built from each other, and thus require\nequivalent computational assumptions to be realized. Furthermore, the existence\nof either of these primitives implies that $\\rm{P} \\neq \\rm{NP}$, which gives a\nlower bound on the necessary hardness.\n  One can also define versions of each of these primitives with quantum output:\nrespectively one-way state generators and pseudorandom state generators. Unlike\nin the classical setting, it is not known whether either primitive can be built\nfrom the other. Although it has been shown that pseudorandom state generators\nfor certain parameter regimes can be used to build one-way state generators,\nthe implication has not been previously known in full generality. Furthermore,\nto the best of our knowledge, the existence of one-way state generators has no\nknown implications in complexity theory.\n  We show that pseudorandom states compressing $n$ bits to $\\log n + 1$ qubits\ncan be used to build one-way state generators and pseudorandom states\ncompressing $n$ bits to $\\omega(\\log n)$ qubits are one-way state generators.\nThis is a nearly optimal result since pseudorandom states with fewer than $c\n\\log n$-qubit output can be shown to exist unconditionally. We also show that\nany one-way state generator can be broken by a quantum algorithm with classical\naccess to a $\\rm{PP}$ oracle.\n  An interesting implication of our results is that a $t(n)$-copy one-way state\ngenerator exists unconditionally, for every $t(n) = o(n/\\log n)$. This\ncontrasts nicely with the previously known fact that $O(n)$-copy one-way state\ngenerators require computational hardness. We also outline a new route towards\na black-box separation between one-way state generators and quantum bit\ncommitments.",
        "translated": "有大量的工作研究实现经典密码需要什么形式的计算硬度。特别是单向函数和伪随机生成元可以相互构造，因此需要实现等效的计算假设。此外，这两个基元中任何一个的存在都意味着 $rm { P } neq rm { NP } $，它给出了必要硬度的下界。我们还可以用量子输出定义每个原语的版本: 分别是单向状态生成器和伪随机状态生成器。与经典设置不同的是，不知道是否可以从另一个基元构建任何一个基元。虽然已经证明某些参数情况下的伪随机状态发生器可以用来构造单向状态发生器，但是这种含义以前并没有被完全普遍地认识。此外，据我们所知，单向状态生成器的存在在复杂性理论中没有已知的含义。我们证明了将 $n $bit 压缩为 $log n + 1 $qubits 的伪随机状态可以用来构造单向状态生成器，而将 $n $bit 压缩为 $omega (log n) $qubits 的伪随机状态是单向状态生成器。这是一个近乎最佳的结果，因为少于 $c log n $- qubit 输出的伪随机状态可以被证明是无条件存在的。我们还证明了任何一个单向状态生成器都可以被一个经典访问 $rm { PP } $Oracle 的量子算法破坏。我们的结果的一个有趣的含义是，对于每个 $t (n) = o (n/log n) $，$t (n) $- copy 单向状态生成器是无条件存在的。这与先前已知的事实形成了很好的对比，即 $O (n) $- copy 单向状态生成器需要计算硬度。我们还概述了单向状态生成器和量子位承诺之间实现黑盒分离的新途径。"
    },
    {
        "title": "Prompt Engineering-assisted Malware Dynamic Analysis Using GPT-4",
        "url": "http://arxiv.org/abs/2312.08317v1",
        "pub_date": "2023-12-13",
        "summary": "Dynamic analysis methods effectively identify shelled, wrapped, or obfuscated\nmalware, thereby preventing them from invading computers. As a significant\nrepresentation of dynamic malware behavior, the API (Application Programming\nInterface) sequence, comprised of consecutive API calls, has progressively\nbecome the dominant feature of dynamic analysis methods. Though there have been\nnumerous deep learning models for malware detection based on API sequences, the\nquality of API call representations produced by those models is limited. These\nmodels cannot generate representations for unknown API calls, which weakens\nboth the detection performance and the generalization. Further, the concept\ndrift phenomenon of API calls is prominent. To tackle these issues, we\nintroduce a prompt engineering-assisted malware dynamic analysis using GPT-4.\nIn this method, GPT-4 is employed to create explanatory text for each API call\nwithin the API sequence. Afterward, the pre-trained language model BERT is used\nto obtain the representation of the text, from which we derive the\nrepresentation of the API sequence. Theoretically, this proposed method is\ncapable of generating representations for all API calls, excluding the\nnecessity for dataset training during the generation process. Utilizing the\nrepresentation, a CNN-based detection model is designed to extract the feature.\nWe adopt five benchmark datasets to validate the performance of the proposed\nmodel. The experimental results reveal that the proposed detection algorithm\nperforms better than the state-of-the-art method (TextCNN). Specifically, in\ncross-database experiments and few-shot learning experiments, the proposed\nmodel achieves excellent detection performance and almost a 100% recall rate\nfor malware, verifying its superior generalization performance. The code is\navailable at: github.com/yan-scnu/Prompted_Dynamic_Detection.",
        "translated": "动态分析方法可以有效地识别外壳、包装或混淆的恶意软件，从而防止它们入侵计算机。作为动态恶意软件行为的重要表现形式，由连续 API 调用组成的 API (应用程序编程接口)序列已逐渐成为动态分析方法的主要特征。虽然已经有许多基于 API 序列的恶意软件检测的深度学习模型，但是由这些模型产生的 API 调用表示的质量是有限的。这些模型不能为未知的 API 调用生成表示，这削弱了检测性能和泛化能力。此外，API 调用的概念漂移现象也很突出。为了解决这些问题，我们引入了一个快速的工程辅助恶意软件动态分析使用 GPT-4。在这种方法中，使用 GPT-4为 API 序列中的每个 API 调用创建说明文本。然后，使用预训练的语言模型 BERT 来获得文本的表示，从而得到 API 序列的表示。从理论上讲，该方法能够生成所有 API 调用的表示，不需要在生成过程中对数据集进行训练。利用该表示方法，设计了一种基于细胞神经网络的特征提取模型。我们采用了五个基准数据集来验证模型的性能。实验结果表明，所提出的检测算法性能优于最新的检测算法(TextCNN)。具体地说，在跨数据库实验和少镜头学习实验中，该模型取得了良好的检测性能和几乎100% 的恶意软件召回率，验证了其优越的泛化性能。密码可于以下 github.com/yan-scnu/prompted_dynamic_detection 索取:。"
    },
    {
        "title": "Differentially Private Gradient Flow based on the Sliced Wasserstein\n  Distance for Non-Parametric Generative Modeling",
        "url": "http://arxiv.org/abs/2312.08227v1",
        "pub_date": "2023-12-13",
        "summary": "Safeguarding privacy in sensitive training data is paramount, particularly in\nthe context of generative modeling. This is done through either differentially\nprivate stochastic gradient descent, or with a differentially private metric\nfor training models or generators. In this paper, we introduce a novel\ndifferentially private generative modeling approach based on parameter-free\ngradient flows in the space of probability measures. The proposed algorithm is\na new discretized flow which operates through a particle scheme, utilizing\ndrift derived from the sliced Wasserstein distance and computed in a private\nmanner. Our experiments show that compared to a generator-based model, our\nproposed model can generate higher-fidelity data at a low privacy budget,\noffering a viable alternative to generator-based approaches.",
        "translated": "保护敏感训练数据中的隐私是至关重要的，特别是在生成建模的环境中。这要么通过差别私有随机梯度下降完成，要么通过差别私有的培训模型或发电机指标完成。提出了一种基于概率测度空间中无参数梯度流的差分私有生成建模方法。提出的算法是一种新的离散化的流动，通过粒子格式操作，利用漂移导出的切片 Wasserstein 距离和私人方式计算。我们的实验表明，与基于生成器的模型相比，我们提出的模型可以在低隐私预算下生成更高保真度的数据，为基于生成器的方法提供了一个可行的替代方案。"
    },
    {
        "title": "Black-box Membership Inference Attacks against Fine-tuned Diffusion\n  Models",
        "url": "http://arxiv.org/abs/2312.08207v1",
        "pub_date": "2023-12-13",
        "summary": "With the rapid advancement of diffusion-based image-generative models, the\nquality of generated images has become increasingly photorealistic. Moreover,\nwith the release of high-quality pre-trained image-generative models, a growing\nnumber of users are downloading these pre-trained models to fine-tune them with\ndownstream datasets for various image-generation tasks. However, employing such\npowerful pre-trained models in downstream tasks presents significant privacy\nleakage risks. In this paper, we propose the first reconstruction-based\nmembership inference attack framework, tailored for recent diffusion models,\nand in the more stringent black-box access setting. Considering four distinct\nattack scenarios and three types of attacks, this framework is capable of\ntargeting any popular conditional generator model, achieving high precision,\nevidenced by an impressive AUC of $0.95$.",
        "translated": "随着基于扩散的图像生成模型的快速发展，生成的图像质量越来越逼真。此外，随着高质量预先训练的图像生成模型的发布，越来越多的用户正在下载这些预先训练的模型，用下游数据集对它们进行微调，以完成各种图像生成任务。然而，在下游任务中使用如此强大的预先训练的模型会带来显著的隐私泄漏风险。在本文中，我们提出了第一个基于重构的成员推理攻击框架，适用于最近的扩散模型，并在更严格的黑盒访问设置。考虑到四种不同的攻击场景和三种类型的攻击，该框架能够针对任何流行的条件生成器模型，实现高精度，证明了令人印象深刻的 AUC $0.95 $。"
    },
    {
        "title": "Towards Evaluating the Security of Wearable Devices in the Internet of\n  Medical Things",
        "url": "http://arxiv.org/abs/2312.08160v1",
        "pub_date": "2023-12-13",
        "summary": "The Internet of Medical Things (IoMT) offers a promising solution to improve\npatient health and reduce human error. Wearable smart infusion pumps that\naccurately administer medication and integrate with electronic health records\nare an example of technology that can improve healthcare. They can even alert\nhealthcare professionals or remote servers during operational failure,\npreventing distressing incidents. However, as the number of connected medical\ndevices increases, the risk of cyber threats also increases. Wearable\nmedication devices based on IoT attached to patients' bodies are prone to\nsignificant cyber threats. Being connected to the Internet exposes these\ndevices to potential harm, which could disrupt or degrade device performance\nand harm patients. To ensure patient safety and well-being, it is crucial to\nestablish secure data authentication for internet-connected medical devices. It\nis also important to note that the wearability option of such devices might\ndowngrade the computational resources, making them more susceptible to security\nrisks. This paper implements a security approach to a wearable infusion pump.\nWe discuss practical challenges in implementing security-enabled devices and\npropose initial solutions to mitigate cyber threats.",
        "translated": "医疗物品互联网(IoMT)为改善患者健康和减少人为错误提供了一个有前途的解决方案。可穿戴的智能输液泵能够准确地管理药物并与电子健康记录相结合，这是能够改善医疗保健的技术的一个例子。它们甚至可以在操作失败时向医疗专业人员或远程服务器发出警报，防止令人痛苦的事件发生。然而，随着联网医疗设备数量的增加，网络威胁的风险也随之增加。基于物联网连接到患者身体的可穿戴医疗设备容易受到严重的网络威胁。连接到互联网使这些设备暴露在潜在的危害之下，这可能会破坏或降低设备的性能，并对患者造成伤害。为了保障患者的安全和健康，建立安全的医疗设备网络数据认证体系至关重要。同样需要注意的是，这些设备的可穿戴性选项可能会降低计算资源的等级，使其更容易受到安全风险的影响。本文对可穿戴式输液泵进行了安全性研究。我们讨论实施安全设备的实际挑战，并提出初步解决方案，以减轻网络威胁。"
    },
    {
        "title": "Okapi: A Lightweight Architecture for Secure Speculation Exploiting\n  Locality of Memory Accesses",
        "url": "http://arxiv.org/abs/2312.08156v1",
        "pub_date": "2023-12-13",
        "summary": "This paper introduces Okapi, an innovative hardware/software cross-layer\narchitecture designed to mitigate Transient Execution Side Channel (TES)\nattacks, including Spectre variants, in modern computing systems. A key\ncontribution of Okapi is a set of security features building upon each other to\noffer various trade-offs between performance and security. At its core, Okapi\nallows for speculative data accesses if the targeted memory region has already\nbeen accessed non-speculatively before in the same trust domain. It delays\nfirst-time accesses until the speculation is resolved.\n  Okapi stands out for its flexibility in security implementation. For\nenvironments with less stringent security needs, Okapi's features can be\ndeactivated to eliminate performance overhead. When activated, the hardware\nmodifications alone provide robust protection against transient execution\nattacks at a thread-level granularity, including all universal read gadgets\nlike Spectre-PHT and Spectre-BTB. This incurs an average performance overhead\nof only 3.6 % for the SPEC CPU2017 benchmark suite.\n  On top, Okapi introduces the OkapiReset instruction for additional\nsoftware-level security support. This instruction, which can be manually\ninserted by developers or automatically via a compiler extension, allows for\nfully secure speculation and for trust domain sizes smaller than a thread.\nWhile the manual insertion of OkapiReset incurs an additional 0.6 % performance\noverhead, the automated compiler extension approach results in a 23.1 %\noverhead for making a cryptographic library fully secure. With an approximate\n0.4 % hardware overhead, Okapi provides a highly scalable and adaptable\nsolution for secure speculation in state-of-the-art processor design.",
        "translated": "本文介绍了 Okapi，一种创新的软硬件跨层架构，旨在减轻现代计算系统中瞬态执行边通道(TES)攻击，包括 Spectre 变体。Okapi 的一个关键贡献是一组相互基础的安全特性，以提供性能和安全之间的各种权衡。Okapi 的核心是，如果目标内存区域以前已经在同一个信任域中被非推测地访问过，那么 Okapi 允许推测性数据访问。它会推迟首次访问，直到猜测得到解决。Okapi 以其在安全实现方面的灵活性而脱颖而出。对于安全需求不那么严格的环境，可以停用 Okapi 的特性以消除性能开销。当激活时，硬件修改本身提供了针对线程级粒度的短暂执行攻击的强大保护，包括所有通用读取小工具，如 Spectre-PHT 和 Spectre-BTB。这使得 SPEC CPU2017基准测试套件的平均性能开销仅为3.6% 。最重要的是，Okapi 引入了 OkapiReset 指令，以提供额外的软件级安全支持。该指令可以由开发人员手动插入，也可以通过编译器扩展自动插入，它允许完全安全的猜测和比线程小的信任域大小。虽然手动插入 OkapiReset 会带来0.6% 的额外性能开销，但自动编译器扩展方法会导致23.1% 的开销，从而使加密库完全安全。Okapi 的硬件开销大约为0.4% ，它为最先进的处理器设计中的安全推测提供了一个高度可伸缩和适应性强的解决方案。"
    },
    {
        "title": "Security aspects in Smart Meters: Analysis and Prevention",
        "url": "http://arxiv.org/abs/2312.08101v1",
        "pub_date": "2023-12-13",
        "summary": "Smart meters are of the basic elements in the so-called Smart Grid. These\ndevices, connected to the Internet, keep bidirectional communication with other\ndevices in the Smart Grid structure to allow remote readings and maintenance.\nAs any other device connected to a network, smart meters become vulnerable to\nattacks with different purposes, like stealing data or altering readings.\nNowadays, it is becoming more and more popular to buy and plug-and-play smart\nmeters, additionally to those installed by the energy providers, to directly\nmonitor the energy consumption at home. This option inherently entails security\nrisks that are under the responsibility of householders. In this paper, we\nfocus on an open solution based on Smartpi 2.0 devices with two purposes. On\nthe one hand, we propose a network configuration and different data flows to\nexchange data (energy readings) in the home. These flows are designed to\nsupport collaborative among the devices in order to prevent external attacks\nand attempts of corrupting the data. On the other hand, we check the\nvulnerability by performing two kind of attacks (denial of service and stealing\nand changing data by using a malware). We conclude that, as expected, these\ndevices are vulnerable to these attacks, but we provide mechanisms to detect\nboth of them and to solve, by applying cooperation techniques",
        "translated": "智能电表是所谓智能电网的基本组成部分。这些设备连接到互联网，与智能电网结构中的其他设备保持双向通信，以便进行远程读取和维护。正如任何连接到网络的其他设备一样，智能仪表容易受到不同目的的攻击，比如窃取数据或改变读数。如今，除了能源供应商安装的智能电表外，购买和即插即用智能电表直接监测家庭能源消耗也变得越来越流行。这种选择必然会带来由住户承担的安全风险。在本文中，我们主要讨论一个基于 Smartpi 2.0设备的开放式解决方案，目的有两个。一方面，我们提出了一个网络配置和不同的数据流交换数据(能源读数)在家里。这些流程旨在支持设备之间的协作，以防止外部攻击和破坏数据的企图。另一方面，我们通过执行两种攻击(分布式拒绝服务攻击攻击和使用恶意软件窃取和更改数据)来检查漏洞。我们的结论是，正如预期的那样，这些设备很容易受到这些攻击，但是我们提供了机制来检测这两种攻击，并通过应用合作技术来解决这些问题"
    },
    {
        "title": "Recursive Augmented Fernet (RAF) Token: Alleviating the Pain of Stolen\n  Tokens",
        "url": "http://arxiv.org/abs/2312.08086v1",
        "pub_date": "2023-12-13",
        "summary": "A robust authentication and authorization mechanism is imperative in modular\nsystem development, where modularity and modular thinking are pivotal.\nTraditional systems often employ identity modules responsible for\nauthentication and token issuance. Tokens, representing user credentials, offer\nadvantages such as reduced reliance on passwords, limited lifespan, and scoped\naccess. Despite these benefits, the \"bearer token\" problem persists, leaving\nsystems vulnerable to abuse if tokens are compromised. We propose a token-based\nauthentication mechanism addressing modular systems' critical bearer token\nproblem. The proposed mechanism includes a novel RAF (Recursive Augmented\nFernet) token, a blacklist component, and a policy enforcer component. RAF\ntokens are one-time-use tokens, like tickets. They carry commands, and the\nreceiver of an RAF token can issue new tokens using the received RAF token. The\nblacklist component guarantees an RAF token can not be approved more than once,\nand the policy enforcer checks the compatibility of commands carried by an RAF\ntoken. We introduce two variations of RAF tokens: User-tied RAF, offering\nsimplicity and compatibility, and Fully-tied RAF, providing enhanced security\nthrough service-specific secret keys. We thoroughly discuss the security\nguarantees, technical definitions, and construction of RAF tokens backed by\ngame-based proofs. We demonstrate a proof of concept in the context of\nOpenStack, involving modifications to Keystone and creating an RAFT library.\nThe experimental results reveal minimal overhead in typical scenarios,\nestablishing the practicality and effectiveness of RAF. Our experiments show\nthat the RAF mechanism beats the idea of using short-life Fernet tokens while\nproviding much better security.",
        "translated": "在模块化系统开发中，健壮的身份验证和授权机制势在必行，模块化和模块化思想是关键。传统系统通常使用负责身份验证和令牌发放的身份模块。代表用户凭证的令牌提供了诸如减少对密码的依赖、有限的生命周期和作用域访问等优势。尽管有这些好处，但是“无记名令牌”问题仍然存在，如果令牌被破坏，系统很容易被滥用。提出了一种基于令牌的身份认证机制，解决了模块化系统的关键承载令牌问题。提出的机制包括一个新的 RAF (递归增强 Fernet)令牌、一个黑名单组件和一个策略执行组件。RAF 令牌是一次性使用的令牌，就像票证一样。它们携带命令，RAF 令牌的接收者可以使用接收到的 RAF 令牌发出新令牌。黑名单组件保证 RAF 令牌不能被批准一次以上，策略执行者检查 RAF 令牌所携带命令的兼容性。我们引入了 RAF 令牌的两种变体: 用户绑定 RAF (提供简单性和兼容性)和完全绑定 RAF (通过特定于服务的密钥提供增强的安全性)。我们深入讨论了基于游戏的证明支持的 RAF 令牌的安全保证、技术定义和构造。我们在 OpenStack 的上下文中演示了一个概念验证，包括对 Keystone 的修改和创建一个 RAFT 库。实验结果表明，在典型场景中，RAF 的开销最小，建立了 RAF 的实用性和有效性。我们的实验表明，RAF 机制打败了使用短寿命 Fernet 令牌的想法，同时提供了更好的安全性。"
    },
    {
        "title": "Individualized Deepfake Detection Exploiting Traces Due to Double\n  Neural-Network Operations",
        "url": "http://arxiv.org/abs/2312.08034v1",
        "pub_date": "2023-12-13",
        "summary": "In today's digital landscape, journalists urgently require tools to verify\nthe authenticity of facial images and videos depicting specific public figures\nbefore incorporating them into news stories. Existing deepfake detectors are\nnot optimized for this detection task when an image is associated with a\nspecific and identifiable individual. This study focuses on the deepfake\ndetection of facial images of individual public figures. We propose to\ncondition the proposed detector on the identity of the identified individual\ngiven the advantages revealed by our theory-driven simulations. While most\ndetectors in the literature rely on perceptible or imperceptible artifacts\npresent in deepfake facial images, we demonstrate that the detection\nperformance can be improved by exploiting the idempotency property of neural\nnetworks. In our approach, the training process involves double neural-network\noperations where we pass an authentic image through a deepfake simulating\nnetwork twice. Experimental results show that the proposed method improves the\narea under the curve (AUC) from 0.92 to 0.94 and reduces its standard deviation\nby 17\\%. For evaluating the detection performance of individual public figures,\na facial image dataset with individuals' names is required, a criterion not met\nby the current deepfake datasets. To address this, we curated a dataset\ncomprising 32k images featuring 45 public figures, which we intend to release\nto the public after the paper is published.",
        "translated": "在当今的数字环境下，记者们迫切需要工具来验证特定公众人物的面部图像和视频的真实性，然后才能将其纳入新闻报道。当一幅图像与一个特定的、可识别的个体相关联时，现有的深度伪造检测器并没有针对这个检测任务进行优化。本研究主要针对个别公众人物的面部图像进行深度伪造检测。鉴于我们的理论驱动模拟所揭示的优势，我们建议将所提出的检测器的条件与所识别的个体的身份相关联。虽然文献中大多数检测器依赖于深度假面部图像中可感知或不可感知的伪影，但是我们证明了利用神经网络的幂等性能可以提高检测性能。在我们的方法中，训练过程包括双神经网络操作，其中我们通过一个深伪造的模拟网络传递一个真实的图像两次。实验结果表明，该方法将曲线下方面积(aUC)由0.92提高到0.94，标准差降低了17% 。为了评估个人公众人物的检测性能，需要一个包含个人姓名的面部图像数据集，这是目前深度伪造数据集所不能达到的标准。为了解决这个问题，我们策划了一个包含45位公众人物的32k 图像的数据集，我们打算在论文发表后向公众发布这些图像。"
    },
    {
        "title": "Provable Security for the Onion Routing and Mix Network Packet Format\n  Sphinx",
        "url": "http://arxiv.org/abs/2312.08028v1",
        "pub_date": "2023-12-13",
        "summary": "Onion routing and mix networks are fundamental concepts to provide users with\nanonymous access to the Internet. Various corresponding solutions rely on the\nefficient Sphinx packet format. However, flaws in Sphinx's underlying proof\nstrategy were found recently. It is thus currently unclear which guarantees\nSphinx actually provides, and, even worse, there is no suitable proof strategy\navailable. In this paper, we restore the security foundation for all these\nworks by building a theoretical framework for Sphinx. We discover that the\npreviously-used DDH assumption is insufficient for a security proof and show\nthat the Gap Diffie-Hellman (GDH) assumption is required instead. We apply it\nto prove that a slightly adapted version of the Sphinx packet format is secure\nunder the GDH assumption. Ours is the first work to provide a detailed,\nin-depth security proof for Sphinx in this manner. Our adaptations to Sphinx\nare necessary, as we demonstrate with an attack on sender privacy that would be\npossible otherwise.",
        "translated": "洋葱路由和混合网络是为用户提供匿名访问互联网的基本概念。各种相应的解决方案都依赖于高效的 Sphinx 包格式。然而，斯芬克斯的基本证明策略最近被发现存在缺陷。因此，目前还不清楚狮身人面像实际上提供了哪些保证，更糟糕的是，没有合适的证明策略可用。本文通过构建狮身人面像的理论框架，还原了这些工作的安全基础。我们发现以前使用的 DDH 假设不足以作为安全性证明，因此需要使用 Gap Divie-Hellman (GDH)假设。我们应用它来证明在 GDH 假设下，Sphinx 数据包格式的一个稍微适应的版本是安全的。我们的第一个工作是以这种方式为 Sphinx 提供详细的、深入的安全性证明。我们对狮身人面像的适应是必要的，因为我们展示了对发件人隐私的攻击，这在其他情况下是可能的。"
    },
    {
        "title": "Pseudorandomness from Subset States",
        "url": "http://arxiv.org/abs/2312.09206v1",
        "pub_date": "2023-12-14",
        "summary": "We show it is possible to obtain quantum pseudorandomness and\npseudoentanglement from random subset states -- i.e. quantum states which are\nequal superpositions over (pseudo)random subsets of strings. This answers an\nopen question of Aaronson et al. [arXiv:2211.00747], who devised a similar\nconstruction augmented by pseudorandom phases. Our result follows from a direct\ncalculation of the trace distance between $t$ copies of random subset states\nand the Haar measure, via the representation theory of the symmetric group. We\nshow that the trace distance is negligibly small, as long as the subsets are of\nan appropriate size which is neither too big nor too small. In particular, we\nanalyze the action of basis permutations on the symmetric subspace, and show\nthat the largest component is described by the Johnson scheme: the\ndouble-cosets of the symmetric group $\\mathbb{S}_N$ by the subgroup\n$\\mathbb{S}_t \\times \\mathbb{S}_{N-t}$. The Gelfand pair property of this\nsetting implies that the matrix eigenbasis coincides with the symmetric group\nirreducible blocks, with the largest eigenblock asymptotically approaching the\nHaar average. An immediate corollary of our result is that quantum pseudorandom\nand pseudoentangled state ensembles do not require relative phases.",
        "translated": "我们展示了从随机子集态获得量子伪随机数和伪纠缠是可能的——即在(伪)随机子集上等重叠的量子态。这回答了 Aaronson 等人的一个开放性问题[ arXiv: 2211.00747] ，他们设计了一个类似的结构，由伪随机相增强。我们的结果是通过对称群的表示论直接计算随机子集态的 $t $拷贝与 Haar 测度之间的跟踪距离得出的。证明了只要子集大小适当，不太大也不太小，跟踪距离可以忽略不计。特别地，我们分析了基排列在对称子空间上的作用，证明了 Johnson 格式描述的最大分量: 对称群 $mathbb { S } _ N $的双陪集由子群 $mathbb { S } _ t 乘以 mathbb { S } _ { N-t } $描述。这种设置的 Gelfand 对性质意味着矩阵本征基与对称群不可约块一致，最大本征块渐近接近 Haar 平均。我们的结果的一个直接推论是，量子伪随机和伪纠缠态集合不需要相对相位。"
    },
    {
        "title": "Puppy: A Publicly Verifiable Watermarking Protocol",
        "url": "http://arxiv.org/abs/2312.09125v1",
        "pub_date": "2023-12-14",
        "summary": "In this paper, we propose Puppy, the first formally defined framework for\nconverting any symmetric watermarking into a publicly verifiable one. Puppy\nallows anyone to verify a watermark any number of times with the help of an\nuntrusted third party, without requiring owner presence during detection. We\nformally define and prove security of Puppy using the ideal/real-world\nsimulation paradigm and construct two practical and secure instances: (1)\nPuppy-TEE that uses Trusted Execution Environments (TEEs), and (2) Puppy-2PC\nthat relies on two-party computation (2PC) based on garbled circuits. We then\nconvert four current symmetric watermarking schemes into publicly verifiable\nones and run extensive experiments using Puppy-TEE and Puppy-2PC. Evaluation\nresults show that, while Puppy-TEE incurs some overhead, its total latency is\non the order of milliseconds for three out of four watermarking schemes.\nAlthough the overhead of Puppy-2PC is higher (on the order of seconds), it is\nviable for settings that lack a TEE or where strong trust assumptions about a\nTEE need to be avoided. We further optimize the solution to increase its\nscalability and resilience to denial of service attacks via memoization.",
        "translated": "在本文中，我们提出了第一个正式定义的框架 Puppy，用于将任何对称水印转换成一个公开可验证的水印。Puppy 允许任何人在不可信任的第三方的帮助下多次验证水印，而不需要检测过程中所有者在场。我们正式定义和证明 Puppy 的安全性使用理想/现实世界的模拟范例，并构建两个实际和安全的实例: (1) Puppy-TEE 使用可信执行环境(TEE) ，和(2) Puppy-2PC 依赖于两方计算(2PC)基于乱码电路。然后，我们将四个现有的对称水印方案转换成可公开验证的方案，并使用 Puppy-TEE 和 Puppy-2PC 进行了广泛的实验。评估结果表明，虽然 Puppy-TEE 会带来一些开销，但是四种水印方案中有三种的总延迟是毫秒级的。尽管 Puppy-2PC 的开销较高(以秒为单位) ，但对于缺少 TEE 或需要避免对 TEE 进行强信任假设的设置是可行的。我们进一步优化了解决方案，通过制表提高了其可伸缩性和对分布式拒绝服务攻击攻击的适应能力。"
    },
    {
        "title": "MRL-PoS: A Multi-agent Reinforcement Learning based Proof of Stake\n  Consensus Algorithm for Blockchain",
        "url": "http://arxiv.org/abs/2312.09123v1",
        "pub_date": "2023-12-14",
        "summary": "The core of a blockchain network is its consensus algorithm. Starting with\nthe Proof-of-Work, there have been various versions of consensus algorithms,\nsuch as Proof-of-Stake (PoS), Proof-of-Authority (PoA), and Practical Byzantine\nFault Tolerance (PBFT). Each of these algorithms focuses on different aspects\nto ensure efficient and reliable processing of transactions. Blockchain\noperates in a decentralized manner where there is no central authority and the\nnetwork is composed of diverse users. This openness creates the potential for\nmalicious nodes to disrupt the network in various ways. Therefore, it is\ncrucial to embed a mechanism within the blockchain network to constantly\nmonitor, identify, and eliminate these malicious nodes. However, there is no\none-size-fits-all mechanism to identify all malicious nodes. Hence, the dynamic\nadaptability of the blockchain network is important to maintain security and\nreliability at all times. This paper introduces MRL-PoS, a Proof-of-Stake\nconsensus algorithm based on multi-agent reinforcement learning. MRL-PoS\nemploys reinforcement learning for dynamically adjusting to the behavior of all\nusers. It incorporates a system of rewards and penalties to eliminate malicious\nnodes and incentivize honest ones. Additionally, MRL-PoS has the capability to\nlearn and respond to new malicious tactics by continually training its agents.",
        "translated": "区块链网络的核心是其一致性算法。从工作证明开始，已经出现了各种版本的共识算法，比如股份证明(proof-stake，PoS)、权威证明(proof-authority，PoA)和实用拜占庭将军问题证明(實用计算程序)。这些算法都侧重于不同的方面，以确保事务处理的高效性和可靠性。区块链以分散的方式运作，没有中央主管部门，网络由不同的用户组成。这种开放性可能导致恶意节点以各种方式破坏网络。因此，在区块链网络中嵌入一种机制来不断监视、识别和消除这些恶意节点是至关重要的。但是，没有一种通用的机制来识别所有恶意节点。因此，区块链网络的动态适应性在任何时候都对维护网络的安全性和可靠性至关重要。本文介绍了 MRL-poS，一种基于多智能体强化学习的一致性风险证明算法。MRL-poS 使用强化学习来动态调整以适应所有用户的行为。它包括一个奖励和惩罚系统，以消除恶意节点和激励诚实的节点。此外，MRL-PoS 还能够通过不断地训练其代理来学习和响应新的恶意策略。"
    },
    {
        "title": "LayerZero",
        "url": "http://arxiv.org/abs/2312.09118v1",
        "pub_date": "2023-12-14",
        "summary": "In this paper, we present the first instrinsically secure and semantically\nuniversal omnichain interoperability protocol: LayerZero. Utilizing an\nimmutable endpoint, append-only verification modules, and fully-configurable\nverification infrastructure, LayerZero provides the security, configurability,\nand extensibility necessary to achieve omnichain interoperability. LayerZero\nenforces strict application-exclusive ownership of protocol security and cost\nthrough its novel trust-minimized modular security framework which is designed\nto universally support all blockchains and use cases. Omnichain applications\n(OApps) built on the LayerZero protocol achieve frictionless\nblockchain-agnostic interoperation through LayerZero's universal network\nsemantics.",
        "translated": "本文提出了第一个本质安全且语义通用的全链互操作协议 LayerZero。LayerZero 利用不可变的端点、只附加验证模块和完全可配置的验证基础设施，提供了实现全链互操作性所必需的安全性、可配置性和可扩展性。LayerZero 通过其新颖的信任最小化模块化安全框架强制执行协议安全性和成本的严格应用程序独占所有权，该框架旨在普遍支持所有区块链和用例。基于 LayerZero 协议的全链应用程序(OApps)通过 LayerZero 的通用网络语义实现了无摩擦的区块链无关互操作。"
    },
    {
        "title": "A Comprehensive Trusted Runtime for WebAssembly with Intel SGX",
        "url": "http://arxiv.org/abs/2312.09087v1",
        "pub_date": "2023-12-14",
        "summary": "In real-world scenarios, trusted execution environments (TEEs) frequently\nhost applications that lack the trust of the infrastructure provider, as well\nas data owners who have specifically outsourced their data for remote\nprocessing. We present Twine, a trusted runtime for running\nWebAssembly-compiled applications within TEEs, establishing a two-way sandbox.\nTwine leverages memory safety guarantees of WebAssembly (Wasm) and abstracts\nthe complexity of TEEs, empowering the execution of legacy and\nlanguage-agnostic applications. It extends the standard WebAssembly system\ninterface (WASI), providing controlled OS services, focusing on I/O.\nAdditionally, through built-in TEE mechanisms, Twine delivers attestation\ncapabilities to ensure the integrity of the runtime and the OS services\nsupplied to the application. We evaluate its performance using general-purpose\nbenchmarks and real-world applications, showing it compares on par with\nstate-of-the-art solutions. A case study involving fintech company Credora\nreveals that Twine can be deployed in production with reasonable performance\ntrade-offs, ranging from a 0.7x slowdown to a 1.17x speedup compared to native\nrun time. Finally, we identify performance improvement through library\noptimisation, showcasing one such adjustment that leads up to 4.1x speedup.\nTwine is open-source and has been upstreamed into the original Wasm runtime,\nWAMR.",
        "translated": "在真实场景中，受信任的执行环境(TEE)经常承载缺乏基础设施提供者信任的应用程序，以及专门将数据外包用于远程处理的数据所有者。我们介绍 Twine，一个在 TEE 中运行 WebAssembly 编译的应用程序的可信运行时，它建立了一个双向沙箱。Twine 利用 WebAssembly (Wasm)的内存安全保证，抽象出 TEE 的复杂性，授权执行遗留应用程序和语言无关的应用程序。它扩展了标准的 WebAssembly 系统接口(WASI) ，提供受控的操作系统服务，侧重于 I/O。此外，通过内置的 TEE 机制，Twine 提供认证功能，以确保运行时和提供给应用程序的操作系统服务的完整性。我们使用通用基准测试和实际应用程序来评估它的性能，表明它与最先进的解决方案相当。一个涉及金融科技公司 Credora 的案例研究表明，Twine 可以在生产中部署，并且具有合理的性能权衡，与本地运行时间相比，从0.7倍的减速到1.17倍的加速。最后，我们通过库优化确定性能改进，展示了一个这样的调整，导致了4.1倍的加速。Twine 是开源的，并且已经上传到原始的 Wasm 运行时 WAMR。"
    },
    {
        "title": "The Earth is Flat because...: Investigating LLMs' Belief towards\n  Misinformation via Persuasive Conversation",
        "url": "http://arxiv.org/abs/2312.09085v1",
        "pub_date": "2023-12-14",
        "summary": "Large Language Models (LLMs) encapsulate vast amounts of knowledge but still\nremain vulnerable to external misinformation. Existing research mainly studied\nthis susceptibility behavior in a single-turn setting. However, belief can\nchange during a multi-turn conversation, especially a persuasive one.\nTherefore, in this study, we delve into LLMs' susceptibility to persuasive\nconversations, particularly on factual questions that they can answer\ncorrectly. We first curate the Farm (i.e., Fact to Misinform) dataset, which\ncontains factual questions paired with systematically generated persuasive\nmisinformation. Then, we develop a testing framework to track LLMs' belief\nchanges in a persuasive dialogue. Through extensive experiments, we find that\nLLMs' correct beliefs on factual knowledge can be easily manipulated by various\npersuasive strategies.",
        "translated": "大型语言模型(LLM)封装了大量的知识，但仍然容易受到外部错误信息的影响。现有的研究主要是研究这种敏感性行为在一个单一的回合设置。然而，信念可以改变在一个多回合的谈话，尤其是一个有说服力的。因此，在这项研究中，我们深入探讨了 LLM 的易感性说服性会话，特别是在事实问题，他们可以正确地回答。我们首先策划的农场(即，事实误导)数据集，其中包含事实问题配对系统生成的有说服力的错误信息。然后，我们开发了一个测试框架来跟踪 LLM 的信念变化在一个有说服力的对话。通过大量的实验，我们发现 LLM 对事实知识的正确信念很容易被各种说服策略所操纵。"
    },
    {
        "title": "On the Difficulty of Defending Contrastive Learning against Backdoor\n  Attacks",
        "url": "http://arxiv.org/abs/2312.09057v1",
        "pub_date": "2023-12-14",
        "summary": "Recent studies have shown that contrastive learning, like supervised\nlearning, is highly vulnerable to backdoor attacks wherein malicious functions\nare injected into target models, only to be activated by specific triggers.\nHowever, thus far it remains under-explored how contrastive backdoor attacks\nfundamentally differ from their supervised counterparts, which impedes the\ndevelopment of effective defenses against the emerging threat.\n  This work represents a solid step toward answering this critical question.\nSpecifically, we define TRL, a unified framework that encompasses both\nsupervised and contrastive backdoor attacks. Through the lens of TRL, we\nuncover that the two types of attacks operate through distinctive mechanisms:\nin supervised attacks, the learning of benign and backdoor tasks tends to occur\nindependently, while in contrastive attacks, the two tasks are deeply\nintertwined both in their representations and throughout their learning\nprocesses. This distinction leads to the disparate learning dynamics and\nfeature distributions of supervised and contrastive attacks. More importantly,\nwe reveal that the specificities of contrastive backdoor attacks entail\nimportant implications from a defense perspective: existing defenses for\nsupervised attacks are often inadequate and not easily retrofitted to\ncontrastive attacks. We also explore several alternative defenses and discuss\ntheir potential challenges. Our findings highlight the need for defenses\ntailored to the specificities of contrastive backdoor attacks, pointing to\npromising directions for future research.",
        "translated": "最近的研究表明，对比学习，像监督式学习一样，非常容易受到后门攻击，恶意功能被注入目标模型，只是被特定的触发器激活。然而，到目前为止，对比后门攻击如何从根本上区别于它们受监督的对应方，这阻碍了对新出现的威胁的有效防御的发展，仍然没有得到充分的探讨。这项工作代表着朝着回答这个关键问题迈出了坚实的一步。具体来说，我们定义了 TRL，一个包含监督和对比后门攻击的统一框架。通过 TRL 的研究，我们发现这两种攻击都是通过不同的机制来实现的: 在有监督的攻击中，良性和后门任务的学习往往是独立发生的，而在对比攻击中，这两种任务在表征和整个学习过程中都是紧密交织在一起的。这种区别导致了不同的学习动力学和特征分布的监督和对比攻击。更重要的是，我们揭示了对比后门攻击的特殊性从防御的角度来说意味着重要的含义: 现有的针对监督攻击的防御往往是不充分的，不容易改造成对比攻击。我们还探讨了几种可供选择的防御方式，并讨论了它们的潜在挑战。我们的研究结果强调了针对对比性后门攻击的特殊性进行防御的必要性，为未来的研究指明了有希望的方向。"
    },
    {
        "title": "Attestation with Constrained Relying Party",
        "url": "http://arxiv.org/abs/2312.08903v1",
        "pub_date": "2023-12-14",
        "summary": "Allowing a compromised device to receive privacy-sensitive sensor readings,\nor to operate a safety-critical actuator, carries significant risk. Usually,\nsuch risks are mitigated by validating the device's security state with remote\nattestation, but current remote attestation protocols are not suitable when the\nbeneficiary of attestation, the relying party, is a constrained device such as\na small sensor or actuator. These devices typically lack the power and memory\nto operate public-key cryptography needed by such protocols, and may only be\nable to communicate with devices in their physical proximity, such as with the\ncontroller whose security state they wish to evaluate. In this paper, we\npresent a remote platform attestation protocol suitable for relying parties\nthat are limited to symmetric-key cryptography and a single communication\nchannel. We show that our protocol, including the needed cryptography and\nmessage processing, can be implemented with a code size of 6 KB and validate\nits security via model checking with the ProVerif tool.",
        "translated": "允许受损的设备接收隐私敏感的传感器读数，或操作安全关键的执行器，具有重大风险。通常，通过远程认证来验证设备的安全状态可以降低这种风险，但是当认证的受益者，依赖方，是一个受限制的设备，如小型传感器或执行器时，现有的远程认证协议就不适用了。这些设备通常缺乏能力和内存来操作这些协议所需的公开密钥加密，并且可能只能与物理上接近的设备通信，例如与他们希望评估其安全状态的控制器通信。本文提出了一种适用于仅限于对称密钥加密和单一通信信道的依赖方的远程平台认证协议。我们展示了我们的协议，包括所需的加密和消息处理，可以用6KB 的代码实现，并通过 ProVerif 工具的模型检查来验证其安全性。"
    },
    {
        "title": "Detection and Defense of Unlearnable Examples",
        "url": "http://arxiv.org/abs/2312.08898v1",
        "pub_date": "2023-12-14",
        "summary": "Privacy preserving has become increasingly critical with the emergence of\nsocial media. Unlearnable examples have been proposed to avoid leaking personal\ninformation on the Internet by degrading generalization abilities of deep\nlearning models. However, our study reveals that unlearnable examples are\neasily detectable. We provide theoretical results on linear separability of\ncertain unlearnable poisoned dataset and simple network based detection methods\nthat can identify all existing unlearnable examples, as demonstrated by\nextensive experiments. Detectability of unlearnable examples with simple\nnetworks motivates us to design a novel defense method. We propose using\nstronger data augmentations coupled with adversarial noises generated by simple\nnetworks, to degrade the detectability and thus provide effective defense\nagainst unlearnable examples with a lower cost. Adversarial training with large\nbudgets is a widely-used defense method on unlearnable examples. We establish\nquantitative criteria between the poison and adversarial budgets which\ndetermine the existence of robust unlearnable examples or the failure of the\nadversarial defense.",
        "translated": "随着社交媒体的出现，隐私保护变得越来越重要。为了避免因特网上的个人信息泄露，提出了不可学习的例子，降低了深度学习模型的泛化能力。然而，我们的研究表明，不可学的例子很容易被发现。通过大量的实验，我们提供了关于某些不可学习的中毒数据集的线性可分的和简单的基于网络的检测方法的理论结果，这些方法可以识别所有现有的不可学习的例子。简单网络中不可学习例子的可检测性促使我们设计一种新的防御方法。我们建议使用更强的数据增强加上由简单网络产生的对抗性噪声，以降低可检测性，从而以较低的成本对不可学习的例子提供有效的防御。大预算的对抗性训练是一种广泛使用的针对不可学案例的辩护方法。我们建立了毒药和对抗性预算之间的定量标准，这些标准决定了是否存在无法学习的强有力的例子或者对抗性防御的失败。"
    },
    {
        "title": "How to Raise a Robot -- A Case for Neuro-Symbolic AI in Constrained Task\n  Planning for Humanoid Assistive Robots",
        "url": "http://arxiv.org/abs/2312.08820v1",
        "pub_date": "2023-12-14",
        "summary": "Humanoid robots will be able to assist humans in their daily life, in\nparticular due to their versatile action capabilities. However, while these\nrobots need a certain degree of autonomy to learn and explore, they also should\nrespect various constraints, for access control and beyond. We explore the\nnovel field of incorporating privacy, security, and access control constraints\nwith robot task planning approaches. We report preliminary results on the\nclassical symbolic approach, deep-learned neural networks, and modern ideas\nusing large language models as knowledge base. From analyzing their trade-offs,\nwe conclude that a hybrid approach is necessary, and thereby present a new use\ncase for the emerging field of neuro-symbolic artificial intelligence.",
        "translated": "人形机器人将能够在日常生活中帮助人类，特别是由于他们多才多艺的行动能力。然而，虽然这些机器人需要一定程度的自主学习和探索，他们也应该尊重各种约束，访问控制和超越。我们探索了将隐私、安全和访问控制约束与机器人任务规划方法相结合的新领域。我们报告的初步结果，经典的符号方法，深入学习的神经网络，和现代思想使用大型语言模型作为知识库。通过分析他们的权衡，我们得出结论，混合方法是必要的，从而为新兴的神经符号人工智能领域提供了一个新的用例。"
    },
    {
        "title": "An artificial neural network approach to finding the key length of the\n  Vigenère cipher",
        "url": "http://arxiv.org/abs/2312.09956v1",
        "pub_date": "2023-12-15",
        "summary": "In this article, we create an artificial neural network (ANN) that combines\nboth classical and modern techniques for determining the key length of a\nVigen\\`{e}re cipher. We provide experimental evidence supporting the accuracy\nof our model for a wide range of parameters. We also discuss the creation and\nfeatures of this ANN along with a comparative analysis between our ANN, the\nindex of coincidence, and the twist-based algorithms.",
        "translated": "在本文中，我们创建了一个人工神经网络(ANN) ，它结合了经典和现代技术来确定一个 Vigen‘ e } re 密码的密钥长度。我们提供的实验证据支持我们的模型的准确性的参数范围广泛。我们还讨论了这种人工神经网络的产生和特点，并对我们的人工神经网络、符合度指数和基于扭曲的算法进行了比较分析。"
    },
    {
        "title": "LogoStyleFool: Vitiating Video Recognition Systems via Logo Style\n  Transfer",
        "url": "http://arxiv.org/abs/2312.09935v1",
        "pub_date": "2023-12-15",
        "summary": "Video recognition systems are vulnerable to adversarial examples. Recent\nstudies show that style transfer-based and patch-based unrestricted\nperturbations can effectively improve attack efficiency. These attacks,\nhowever, face two main challenges: 1) Adding large stylized perturbations to\nall pixels reduces the naturalness of the video and such perturbations can be\neasily detected. 2) Patch-based video attacks are not extensible to targeted\nattacks due to the limited search space of reinforcement learning that has been\nwidely used in video attacks recently. In this paper, we focus on the video\nblack-box setting and propose a novel attack framework named LogoStyleFool by\nadding a stylized logo to the clean video. We separate the attack into three\nstages: style reference selection, reinforcement-learning-based logo style\ntransfer, and perturbation optimization. We solve the first challenge by\nscaling down the perturbation range to a regional logo, while the second\nchallenge is addressed by complementing an optimization stage after\nreinforcement learning. Experimental results substantiate the overall\nsuperiority of LogoStyleFool over three state-of-the-art patch-based attacks in\nterms of attack performance and semantic preservation. Meanwhile, LogoStyleFool\nstill maintains its performance against two existing patch-based defense\nmethods. We believe that our research is beneficial in increasing the attention\nof the security community to such subregional style transfer attacks.",
        "translated": "视频识别系统容易受到敌对实例的攻击。最近的研究表明，基于样式转移和基于补丁的无限制扰动可以有效地提高攻击效率。然而，这些攻击面临两个主要挑战: 1)向所有像素添加大的程式化扰动会降低视频的自然性，而且这种扰动很容易被检测到。2)由于强化学习搜索空间有限，基于补丁的视频攻击不能扩展到目标攻击，而这种搜索空间近来已广泛应用于视频攻击。本文以视频黑盒设置为研究对象，提出了一种新的攻击框架 LogoStyleFool，该框架通过在清晰视频中添加一个程式化的 logo 来实现。我们将攻击分为三个阶段: 风格参考选择、基于强化学习的 logo 风格转移和扰动优化。我们通过将扰动范围缩小到一个地区标志来解决第一个挑战，而第二个挑战则通过补充一个又一个优化阶段来解决强化学习。实验结果证实了 LogoStyleFool 在攻击性能和语义保护方面优于三种最先进的补丁攻击。与此同时，LogoStyleFool 仍然保持其针对两种现有的基于补丁的防御方法的性能。我们认为，我们的研究有助于提高安全界对这种次区域风格的转移攻击的关注。"
    },
    {
        "title": "ESAT:Extended Security in the Air using TESLA",
        "url": "http://arxiv.org/abs/2312.09870v1",
        "pub_date": "2023-12-15",
        "summary": "The Automatic Dependent Surveillance-Broadcast (ADS-B) is a surveillance\ntechnology that becomes mandatory in many airspaces. It improves safety,\nincreases efficiency and reduces air traffic congestion by broadcasting\naircraft navigation data. Yet, ADS-B is vulnerable to spoofing attacks as it\nlacks mechanisms to ensure the integrity and authenticity of the data being\nsupplied. None of the existing cryptographic solutions fully meet the backward\ncompatibility and bandwidth preservation requirements of the standard. Hence,\nwe propose Extended Security in the Air using TESLA (ESAT), an enhanced\napproach that integrates TESLA, phase-overlay modulation techniques and\ncertificate-based PKI. As a result, entity authentication, data origin\nauthentication, and data integrity are the security services that ESAT offers.\nTo assess compliance with the standard, we designed an SDR-based implementation\nof ESAT and performed backwards compatibility tests on commercial and general\naviation (GA) ADS-B in receivers. Besides, we calculated the 1090ES band's\nactivity factor and analyzed the channel occupancy rate according to ITU-R\nSM.2256-1 recommendation. Also, we performed a bit error rate analysis of ESAT\nmessages. The results suggest that ESAT is backward compatible, does not incur\nsignificant communication overhead, and has an error rate that is acceptable\nfor Eb/No values above 14 dB.",
        "translated": "该广播式自动相关监视(ADS-B)是一种监视技术，在许多空域成为强制性的。它通过广播飞机导航数据，提高安全性、效率并减少空中交通堵塞。然而，ADS-B 很容易受到欺骗攻击，因为它缺乏确保所提供数据的完整性和真实性的机制。现有的加密解决方案均未能完全符合该标准的向下兼容和带宽保存要求。因此，我们提出了利用 TESLA (ESAT)的空中扩展安全(Extended Security in Air) ，这是一种集成了 TESLA、相位叠加调制技术和基于证书的 PKI 的增强型方法。因此，实体身份验证、数据源身份验证和数据完整性是 ESAT 提供的安全服务。为了评估是否符合该标准，我们设计了一个基于 SDR 的 ESAT 实现，并在接收机中对商用和通用航空(GA) ADS-B 进行了向后兼容性测试。此外，我们计算了1090ES 波段的活动因子，并根据 ITU-R SM.2256-1的建议分析了频道占用率。此外，我们还对 ESAT 消息进行了比特错误率分析。结果表明，ESAT 是向后兼容的，不会产生显著的通信开销，并且对于 Eb/No 值大于14 dB 的情况下具有可接受的错误率。"
    },
    {
        "title": "Directed Acyclic Graph Based Blockchain Systems",
        "url": "http://arxiv.org/abs/2312.09816v1",
        "pub_date": "2023-12-15",
        "summary": "Blockchain technology has been revolutionizing many fields since last decade.\nIts true potential is not practically utilized yet. In a very short period of\ntime, it has evolved twice - Smart contracts and Directed Acyclic Graph (DAG).\nDAG based blockchains currently referred to as Blockchain 3.0 solves many\nissues in the current conventional blockchain technologies including\ntransaction fees, transaction approval times and scalability. In this paper, we\npresent a comparative analysis of blockchain implementations based on DAG\nincluding IOTA, NxT, Byteball, Nano, DAGCoin, Fantom, XDAG and Caixapay. We\ndiscuss limitations of both conventional and DAG based blockchains and suggest\nwhen to prefer DAG based blockchains.",
        "translated": "区块链技术自上个十年以来已经在许多领域发生了革命性的变化。它的真正潜力尚未得到实际利用。在非常短的时间内，它已经发展了两次-智能合同和有向无环图(DAG)。目前基于 DAG 的区块链被称为区块链3.0，它解决了传统区块链技术中的许多问题，包括交易费用、交易批准时间和可扩展性。在本文中，我们对基于区块链的实现方法进行了比较分析，包括 IOTA、 NxT、 Byteball、 Nano、 DAGCoin、 Fantom、 XDAG 和 Caixapay。我们讨论了传统和 DAG 为基础的区块链的局限性，并建议何时更喜欢 DAG 为基础的区块链。"
    },
    {
        "title": "Beyond Over-Protection: A Targeted Approach to Spectre Mitigation and\n  Performance Optimization",
        "url": "http://arxiv.org/abs/2312.09770v1",
        "pub_date": "2023-12-15",
        "summary": "Since the advent of Spectre attacks, researchers and practitioners have\ndeveloped a range of hardware and software measures to counter transient\nexecution attacks. A prime example of such mitigation is speculative load\nhardening in LLVM, which protects against leaks by tracking the speculation\nstate and masking values during misspeculation. LLVM relies on static analysis\nto harden programs using slh that often results in over-protection, which\nincurs performance overhead. We extended an existing side-channel model\nvalidation framework, Scam-V, to check the vulnerability of programs to\nSpectre-PHT attacks and optimize the protection of programs using the slh\napproach. We illustrate the efficacy of Scam-V by first demonstrating that it\ncan automatically identify Spectre vulnerabilities in real programs, e.g.,\nfragments of crypto-libraries. We then develop an optimization mechanism that\nvalidates the necessity of slh hardening w.r.t. the target platform. Our\nexperiments showed that hardening introduced by LLVM in most cases could be\nsignificantly improved when the underlying microarchitecture properties are\nconsidered.",
        "translated": "自幽灵攻击出现以来，研究人员和从业人员已经开发了一系列的硬件和软件措施，以对抗短暂的执行攻击。这种缓解的一个主要例子是 LLVM 中的推测性负载硬化，它通过跟踪推测状态和在推测错误期间屏蔽值来防止泄漏。LLVM 依赖于静态分析来加固使用 slh 的程序，这通常会导致过度保护，从而带来性能开销。我们扩展了现有的边通道模型验证框架 Scam-V，以检查程序对 Spectre-PHT 攻击的脆弱性，并使用 slh 方法优化程序的保护。我们通过证明 Scam-V 可以自动识别实际程序中的 Spectre 漏洞，例如密码库的片段，来说明 Scam-V 的有效性。然后我们开发了一个优化机制，验证了 SLH 加固目标平台的必要性。我们的实验表明，如果考虑到基本的微架构特性，LLVM 在大多数情况下引入的硬化效果可以得到显著改善。"
    },
    {
        "title": "Silent Guardian: Protecting Text from Malicious Exploitation by Large\n  Language Models",
        "url": "http://arxiv.org/abs/2312.09669v1",
        "pub_date": "2023-12-15",
        "summary": "The rapid development of large language models (LLMs) has yielded impressive\nsuccess in various downstream tasks. However, the vast potential and remarkable\ncapabilities of LLMs also raise new security and privacy concerns if they are\nexploited for nefarious purposes due to their open-endedness. For example, LLMs\nmay be used to plagiarize or imitate writing, thereby infringing the copyright\nof the original content, or to create indiscriminate fake information based on\na certain source text. In some cases, LLMs can even analyze text from the\nInternet to infer personal privacy. Unfortunately, previous text protection\nresearch could not foresee the emergence of powerful LLMs, rendering it no\nlonger effective in this new context. To bridge this gap, we introduce Silent\nGuardian (SG), a text protection mechanism against LLMs, which allows LLMs to\nrefuse to generate response when receiving protected text, preventing the\nmalicious use of text from the source. Specifically, we first propose the\nconcept of Truncation Protection Examples (TPE). By carefully modifying the\ntext to be protected, TPE can induce LLMs to first sample the end token, thus\ndirectly terminating the interaction. In addition, to efficiently construct TPE\nin the discrete space of text data, we propose a novel optimization algorithm\ncalled Super Taliored Protection (STP), which is not only highly efficient but\nalso maintains the semantic consistency of the text during the optimization\nprocess. The comprehensive experimental evaluation demonstrates that SG can\neffectively protect the target text under various configurations and achieve\nalmost 100% protection success rate in some cases. Notably, SG also exhibits\nrelatively good transferability and robustness, making its application in\npractical scenarios possible.",
        "translated": "大型语言模型(LLM)的快速发展已经在各种下游任务中取得了令人印象深刻的成功。然而，LLM 的巨大潜力和显著能力也引起了新的安全和隐私问题，如果它们由于其开放性而被用于邪恶目的。例如，LLM 可能被用于剽窃或模仿写作，从而侵犯原始内容的版权，或者根据某一源文本制造不分青红皂白的虚假信息。在某些情况下，LLM 甚至可以通过分析互联网上的文本来推断个人隐私。不幸的是，以前的文本保护研究没有预见到强大的 LLM 的出现，使其不再有效在这个新的背景下。为了弥补这一差距，我们引入了 Silent Guardian (SG) ，这是一种针对 LLM 的文本保护机制，允许 LLM 在接收受保护的文本时拒绝生成响应，从而防止恶意使用源文本。具体来说，我们首先提出截断保护示例(TPE)的概念。通过仔细修改要保护的文本，TPE 可以诱导 LLM 首先采样结束标记，从而直接终止交互。此外，为了在文本数据的离散空间中有效地构造 TPE，我们提出了一种新的优化算法，称为超级塔利奥保护(STP) ，它不仅高效，而且在优化过程中保持了文本的语义一致性。综合实验结果表明，在不同的配置下，SG 可以有效地保护目标文本，在某些情况下可以达到100% 的保护成功率。值得注意的是，SG 还具有相对较好的可迁移性和鲁棒性，使其在实际场景中的应用成为可能。"
    },
    {
        "title": "FlowMur: A Stealthy and Practical Audio Backdoor Attack with Limited\n  Knowledge",
        "url": "http://arxiv.org/abs/2312.09665v1",
        "pub_date": "2023-12-15",
        "summary": "Speech recognition systems driven by DNNs have revolutionized human-computer\ninteraction through voice interfaces, which significantly facilitate our daily\nlives. However, the growing popularity of these systems also raises special\nconcerns on their security, particularly regarding backdoor attacks. A backdoor\nattack inserts one or more hidden backdoors into a DNN model during its\ntraining process, such that it does not affect the model's performance on\nbenign inputs, but forces the model to produce an adversary-desired output if a\nspecific trigger is present in the model input. Despite the initial success of\ncurrent audio backdoor attacks, they suffer from the following limitations: (i)\nMost of them require sufficient knowledge, which limits their widespread\nadoption. (ii) They are not stealthy enough, thus easy to be detected by\nhumans. (iii) Most of them cannot attack live speech, reducing their\npracticality. To address these problems, in this paper, we propose FlowMur, a\nstealthy and practical audio backdoor attack that can be launched with limited\nknowledge. FlowMur constructs an auxiliary dataset and a surrogate model to\naugment adversary knowledge. To achieve dynamicity, it formulates trigger\ngeneration as an optimization problem and optimizes the trigger over different\nattachment positions. To enhance stealthiness, we propose an adaptive data\npoisoning method according to Signal-to-Noise Ratio (SNR). Furthermore, ambient\nnoise is incorporated into the process of trigger generation and data poisoning\nto make FlowMur robust to ambient noise and improve its practicality. Extensive\nexperiments conducted on two datasets demonstrate that FlowMur achieves high\nattack performance in both digital and physical settings while remaining\nresilient to state-of-the-art defenses. In particular, a human study confirms\nthat triggers generated by FlowMur are not easily detected by participants.",
        "translated": "由 DNN 驱动的语音识别系统通过语音界面彻底改变了人机交互，极大地方便了我们的日常生活。然而，这些系统的日益普及也引起了对其安全性的特别关注，特别是关于后门攻击。后门攻击在 DNN 模型的训练过程中插入一个或多个隐藏的后门，这样它不会影响模型在良性输入上的性能，但是如果模型输入中存在特定的触发器，它会迫使模型产生一个对手期望的输出。尽管目前的音频后门攻击取得了初步成功，但它们受到以下限制: (i)大多数攻击需要足够的知识，这限制了它们的广泛应用。(ii)它们不够隐蔽，容易被人类发现。(iii)大多数不能攻击现场演讲，降低了实用性。为了解决这些问题，在本文中，我们提出了 FlowMur，一种隐蔽和实用的音频后门攻击，可以发动与有限的知识。FlowMur 构造了一个辅助数据集和一个代理模型来增加对手知识。为了达到动态性，它将触发器的生成制定为一个最佳化问题，并针对不同的连接位置优化触发器。为了提高隐蔽性，我们提出了一种根据信噪比(SNR)自适应的数据中毒方法。此外，将环境噪声引入到触发器生成和数据中毒过程中，使 FlowMur 对环境噪声具有鲁棒性，提高了其实用性。在两个数据集上进行的大量实验表明，FlowMur 在数字和物理设置上都取得了很高的攻击性能，同时保持了对最先进防御的弹性。特别是，一项人体研究证实，由 FlowMur 产生的触发因素不容易被参与者发现。"
    },
    {
        "title": "When and How to Aggregate Message Authentication Codes on Lossy\n  Channels?",
        "url": "http://arxiv.org/abs/2312.09660v1",
        "pub_date": "2023-12-15",
        "summary": "Aggregation of message authentication codes (MACs) is a proven and efficient\nmethod to preserve valuable bandwidth in resource-constrained environments:\nInstead of appending a long authentication tag to each message, the integrity\nprotection of multiple messages is aggregated into a single tag. However, while\nsuch aggregation saves bandwidth, a single lost message typically means that\nauthentication information for multiple messages cannot be verified anymore.\nWith the significant increase of bandwidth-constrained lossy communication, as\napplications shift towards wireless channels, it thus becomes paramount to\nstudy the impact of packet loss on the diverse MAC aggregation schemes proposed\nover the past 15 years to assess when and how to aggregate message\nauthentication. Therefore, we empirically study all relevant MAC aggregation\nschemes in the context of lossy channels, investigating achievable goodput\nimprovements, the resulting verification delays, processing overhead, and\nresilience to denial-of-service attacks. Our analysis shows the importance of\ncarefully choosing and configuring MAC aggregation, as selecting and correctly\nparameterizing the right scheme can, e.g., improve goodput by 39% to 444%,\ndepending on the scenario. However, since no aggregation scheme performs best\nin all scenarios, we provide guidelines for network operators to select optimal\nschemes and parameterizations suiting specific network settings.",
        "translated": "在资源受限的环境中，聚合消息身份验证码(MAC)是一种行之有效的方法，可以保留有价值的带宽: 多个消息的完整性保护被聚合到一个标记中，而不是向每个消息添加一个长的身份验证标记。然而，尽管这种聚合可以节省带宽，但单个丢失的消息通常意味着无法再验证多个消息的身份验证信息。随着带宽受限有损通信的显著增加，随着应用向无线信道的转移，因此研究数据包丢失对过去15年来提出的各种 MAC 聚合方案的影响以评估何时以及如何聚合消息认证变得至关重要。因此，我们实证研究所有相关的 MAC 聚合方案在有损信道的情况下，调查可实现的好输出改进，由此产生的验证延迟，处理开销，以及拒绝服务攻击的弹性。我们的分析表明了仔细选择和配置 MAC 聚合的重要性，因为选择和正确的参数化正确的方案可以，例如，提高39% 至444% 的良好输出，这取决于场景。然而，由于没有一种聚合方案在所有场景中都表现得最好，我们为网络运营商提供了根据特定的网络设置选择最佳方案和参数化的指导方针。"
    },
    {
        "title": "What to Remember: Self-Adaptive Continual Learning for Audio Deepfake\n  Detection",
        "url": "http://arxiv.org/abs/2312.09651v1",
        "pub_date": "2023-12-15",
        "summary": "The rapid evolution of speech synthesis and voice conversion has raised\nsubstantial concerns due to the potential misuse of such technology, prompting\na pressing need for effective audio deepfake detection mechanisms. Existing\ndetection models have shown remarkable success in discriminating known deepfake\naudio, but struggle when encountering new attack types. To address this\nchallenge, one of the emergent effective approaches is continual learning. In\nthis paper, we propose a continual learning approach called Radian Weight\nModification (RWM) for audio deepfake detection. The fundamental concept\nunderlying RWM involves categorizing all classes into two groups: those with\ncompact feature distributions across tasks, such as genuine audio, and those\nwith more spread-out distributions, like various types of fake audio. These\ndistinctions are quantified by means of the in-class cosine distance, which\nsubsequently serves as the basis for RWM to introduce a trainable gradient\nmodification direction for distinct data types. Experimental evaluations\nagainst mainstream continual learning methods reveal the superiority of RWM in\nterms of knowledge acquisition and mitigating forgetting in audio deepfake\ndetection. Furthermore, RWM's applicability extends beyond audio deepfake\ndetection, demonstrating its potential significance in diverse machine learning\ndomains such as image recognition.",
        "translated": "语音合成和语音转换技术的迅速发展引起了人们的高度关注，因为这些技术可能被滥用，这促使人们迫切需要有效的音频深度伪造检测机制。现有的检测模型在识别已知的深度伪造音频方面取得了显著的成功，但是在遇到新的攻击类型时却很难做到。为了应对这一挑战，一种新出现的有效方法是持续学习。本文提出了一种基于连续学习的声音深伪检测方法-径向权值修正(RWM)。RWM 的基本概念涉及到将所有类分为两组: 一组具有跨任务的紧凑特征分布(如真实音频) ，另一组具有更多分布式分布(如各种类型的假音频)。这些区别通过类内余弦距离来量化，这随后成为 RWM 为不同数据类型引入可训练的梯度修正方向的基础。针对主流连续学习方法的实验评估揭示了 RWM 在音频深度伪造检测中在知识获取和减轻遗忘方面的优越性。此外，RWM 的适用性超越了音频深伪检测，表明其潜在的重要性，在不同的机器学习领域，如图像识别。"
    },
    {
        "title": "Madtls: Fine-grained Middlebox-aware End-to-end Security for Industrial\n  Communication",
        "url": "http://arxiv.org/abs/2312.09650v1",
        "pub_date": "2023-12-15",
        "summary": "Industrial control systems increasingly rely on middlebox functionality such\nas intrusion detection or in-network processing. However, traditional\nend-to-end security protocols interfere with the necessary access to in-flight\ndata. While recent work on middlebox-aware end-to-end security protocols for\nthe traditional Internet promises to address the dilemma between end-to-end\nsecurity guarantees and middleboxes, the current state-of-the-art lacks\ncritical features for industrial communication. Most importantly, industrial\nsettings require fine-grained access control for middleboxes to truly operate\nin a least-privilege mode. Likewise, advanced applications even require that\nmiddleboxes can inject specific messages (e.g., emergency shutdowns).\nMeanwhile, industrial scenarios often expose tight latency and bandwidth\nconstraints not found in the traditional Internet. As the current\nstate-of-the-art misses critical features, we propose Middlebox-aware DTLS\n(Madtls), a middlebox-aware end-to-end security protocol specifically tailored\nto the needs of industrial networks. Madtls provides bit-level read and write\naccess control of middleboxes to communicated data with minimal bandwidth and\nprocessing overhead, even on constrained hardware.",
        "translated": "工业控制系统越来越依赖于中间盒功能，例如入侵检测或网络内处理。但是，传统的端到端安全协议会干扰对飞行中数据的必要访问。尽管最近针对传统互联网端到端安全协议的研究有望解决端到端安全保证和中间盒之间的困境，但目前的最新技术缺乏工业通信的关键特性。最重要的是，工业设置需要对中间件进行细粒度的访问控制，以便真正以最低特权模式进行操作。同样，高级应用程序甚至要求中间盒可以注入特定的消息(例如，紧急关闭)。同时，工业场景经常暴露出传统 Internet 中没有的严格的延迟和带宽限制。由于当前最先进的技术没有提供关键的特性，我们提出支持 Middlebox 的 DTLS (Madtls) ，这是一种支持中间盒的端到端安全协议，专门适用于工业网络的需求。Madtls 提供中间盒的位级读写访问控制，以最小的带宽和处理开销传输数据，即使在受限的硬件上也是如此。"
    },
    {
        "title": "Disclosure Avoidance for the 2020 Census Demographic and Housing\n  Characteristics File",
        "url": "http://arxiv.org/abs/2312.10863v1",
        "pub_date": "2023-12-18",
        "summary": "In \"The 2020 Census Disclosure Avoidance System TopDown Algorithm,\" Abowd et\nal. (2022) describe the concepts and methods used by the Disclosure Avoidance\nSystem (DAS) to produce formally private output in support of the 2020 Census\ndata product releases, with a particular focus on the DAS implementation that\nwas used to create the 2020 Census Redistricting Data (P.L. 94-171) Summary\nFile. In this paper we describe the updates to the DAS that were required to\nrelease the Demographic and Housing Characteristics (DHC) File, which provides\nmore granular tables than other data products, such as the Redistricting Data\nSummary File. We also describe the final configuration parameters used for the\nproduction DHC DAS implementation, as well as subsequent experimental data\nproducts to facilitate development of tools that provide confidence intervals\nfor confidential 2020 Census tabulations.",
        "translated": ""
    },
    {
        "title": "Federated learning with differential privacy and an untrusted aggregator",
        "url": "http://arxiv.org/abs/2312.10789v1",
        "pub_date": "2023-12-17",
        "summary": "Federated learning for training models over mobile devices is gaining\npopularity. Current systems for this task exhibit significant trade-offs\nbetween model accuracy, privacy guarantee, and device efficiency. For instance,\nOort (OSDI 2021) provides excellent accuracy and efficiency but requires a\ntrusted central server. On the other hand, Orchard (OSDI 2020) provides good\naccuracy and the rigorous guarantee of differential privacy over an untrusted\nserver, but creates huge overhead for the devices. This paper describes Aero, a\nnew federated learning system that significantly improves this trade-off. Aero\nguarantees good accuracy, differential privacy over an untrusted server, and\nkeeps the device overhead low. The key idea of Aero is to tune system\narchitecture and design to a specific set of popular, federated learning\nalgorithms. This tuning requires novel optimizations and techniques, e.g., a\nnew protocol to securely aggregate updates from devices. An evaluation of Aero\ndemonstrates that it provides comparable accuracy to plain federated learning\n(without differential privacy), and it improves efficiency (CPU and network)\nover Orchard by up to $10^5\\times$.",
        "translated": ""
    },
    {
        "title": "A Mutation-Based Method for Multi-Modal Jailbreaking Attack Detection",
        "url": "http://arxiv.org/abs/2312.10766v1",
        "pub_date": "2023-12-17",
        "summary": "Large Language Models and Multi-Modal LLMs have become pervasive, and so does\nthe importance of their security; yet, modern LLMs are known to be vulnerable\nto jailbreaking attacks. These attacks can allow malicious users to exploit the\nmodels, making the case for effective jailbreak detection mechanisms an\nessential aspect of maintaining the integrity and trustworthiness of LLM-based\napplications. However, existing detection works on jailbreak attacks have\nlimitations. Existing post-query-based strategies require target domain\nknowledge, and pre-query-based methods mainly focus on text-level attacks and\nfail to meet the increasingly complex multi-modal security requirements placed\nupon contemporary LLMs. This gap underscores the need for a more comprehensive\napproach to safeguarding these influential systems.\n  In this work, we propose JailGuard, the first mutation-based jailbreaking\ndetection framework which supports both image and text modalities. Our key\nobservation is that attack queries inherently possess less robustness compared\nto benign queries. Specifically, to confuse the model, attack queries are\nusually crafted with well-designed templates or complicate perturbations,\nleading to a fact that a slight disturbance in input may result in a drastic\nchange in the response. This lack of robustness can be utilized in attack\ndetection. Based on this intuition, we designed and implemented a detection\nframework comprising 19 different mutators and a divergence-based detection\nformula. To fully understand the effectiveness of our framework, we built the\nfirst multi-modal LLM jailbreaking attack dataset, which has 304 items of data,\ncovering ten types of known jailbreaking attacks on image and text modalities.\nThe evaluation suggests that JailGuard achieves the best detection accuracy of\n89.38%/85.42% on image and text inputs, outperforming state-of-the-art defense\nmethods by 15.28%.",
        "translated": ""
    },
    {
        "title": "HE-DKSAP: Privacy-Preserving Stealth Address Protocol via Additively\n  Homomorphic Encryption",
        "url": "http://arxiv.org/abs/2312.10698v1",
        "pub_date": "2023-12-17",
        "summary": "Blockchain transactions have gained widespread adoption across various\nindustries, largely attributable to their unparalleled transparency and robust\nsecurity features. Nevertheless, this technique introduces various privacy\nconcerns, including pseudonymity, Sybil attacks, and potential susceptibilities\nto quantum computing, to name a few. In response to these challenges,\ninnovative privacy-enhancing solutions like zero-knowledge proofs, homomorphic\nencryption, and stealth addresses (SA) have been developed. Among the various\nschemes, SA stands out as it prevents the association of a blockchain\ntransaction's output with the recipient's public address, thereby ensuring\ntransactional anonymity. However, the basic SA schemes have exhibited\nvulnerabilities to key leakage and quantum computing attacks. To address these\nshortcomings, we present a pioneering solution - Homomorphic Encryption-based\nDual-Key Stealth Address Protocol (HE-DKSAP), which can be further extended to\nFully HE-DKSAP (FHE-DKSAP). By leveraging the power of homomorphic encryption,\nHE-DKSAP introduces a novel approach to safeguarding transaction privacy and\npreventing potential quantum computing attacks. This paper delves into the core\nprinciples of HE-DKSAP, highlighting its capacity to enhance privacy,\nscalability, and security in programmable blockchains. Through a comprehensive\nexploration of its design architecture, security analysis, and practical\nimplementations, this work establishes a privacy-preserving, practical, and\nefficient stealth address protocol via additively homomorphic encryption.",
        "translated": ""
    },
    {
        "title": "Analisis Eksploratif Dan Augmentasi Data NSL-KDD Menggunakan Deep\n  Generative Adversarial Networks Untuk Meningkatkan Performa Algoritma Extreme\n  Gradient Boosting Dalam Klasifikasi Jenis Serangan Siber",
        "url": "http://arxiv.org/abs/2312.10669v1",
        "pub_date": "2023-12-17",
        "summary": "This study proposes the implementation of Deep Generative Adversarial\nNetworks (GANs) for augmenting the NSL-KDD dataset. The primary objective is to\nenhance the efficacy of eXtreme Gradient Boosting (XGBoost) in the\nclassification of cyber-attacks on the NSL-KDD dataset. As a result, the method\nproposed in this research achieved an accuracy of 99.53% using the XGBoost\nmodel without data augmentation with GAN, and 99.78% with data augmentation\nusing GAN.",
        "translated": ""
    },
    {
        "title": "UltraClean: A Simple Framework to Train Robust Neural Networks against\n  Backdoor Attacks",
        "url": "http://arxiv.org/abs/2312.10657v1",
        "pub_date": "2023-12-17",
        "summary": "Backdoor attacks are emerging threats to deep neural networks, which\ntypically embed malicious behaviors into a victim model by injecting poisoned\nsamples. Adversaries can activate the injected backdoor during inference by\npresenting the trigger on input images. Prior defensive methods have achieved\nremarkable success in countering dirty-label backdoor attacks where the labels\nof poisoned samples are often mislabeled. However, these approaches do not work\nfor a recent new type of backdoor -- clean-label backdoor attacks that\nimperceptibly modify poisoned data and hold consistent labels. More complex and\npowerful algorithms are demanded to defend against such stealthy attacks. In\nthis paper, we propose UltraClean, a general framework that simplifies the\nidentification of poisoned samples and defends against both dirty-label and\nclean-label backdoor attacks. Given the fact that backdoor triggers introduce\nadversarial noise that intensifies in feed-forward propagation, UltraClean\nfirst generates two variants of training samples using off-the-shelf denoising\nfunctions. It then measures the susceptibility of training samples leveraging\nthe error amplification effect in DNNs, which dilates the noise difference\nbetween the original image and denoised variants. Lastly, it filters out\npoisoned samples based on the susceptibility to thwart the backdoor\nimplantation. Despite its simplicity, UltraClean achieves a superior detection\nrate across various datasets and significantly reduces the backdoor attack\nsuccess rate while maintaining a decent model accuracy on clean data,\noutperforming existing defensive methods by a large margin. Code is available\nat https://github.com/bxz9200/UltraClean.",
        "translated": ""
    },
    {
        "title": "A Novel RFID Authentication Protocol Based on A Block-Order-Modulus\n  Variable Matrix Encryption Algorithm",
        "url": "http://arxiv.org/abs/2312.10593v1",
        "pub_date": "2023-12-17",
        "summary": "In this paper, authentication for mobile radio frequency identification\n(RFID) systems with low-cost tags is studied. Firstly, a diagonal block key\nmatrix (DBKM) encryption algorithm is proposed, which effectively expands the\nfeasible domain of the key space. Subsequently, in order to enhance the\nsecurity, a self updating encryption order (SUEO) algorithm is conceived. To\nfurther weaken the correlation between plaintext and ciphertext, a self\nupdating modulus (SUM) algorithm is constructed. Based on the above three\nalgorithms, a new joint DBKM-SUEO-SUM matrix encryption algorithm is\nestablished, which intends to enhance security without the need of additional\nstorage for extra key matrices. Making full use of the advantages of the\nproposed joint algorithm, a two-way RFID authentication protocol named\nDBKM-SUEO-SUM-RFID is proposed for mobile RFID systems. In addition, the\nBurrows-Abadi-Needham (BAN) logic and security analysis indicate that the newly\nproposed DBKM-SUEO-SUM-RFID protocol can effectively resist various typical\nattacks, such as replay attacks and de-synchronization. Finally, numerical\nresults demonstrate that the DBKM-SUEO-SUM algorithm can save at least 90.46\\%\nof tag storage compared to traditional algorithms, and thus, is friendly to be\nemployed with low-cost RFID tags.",
        "translated": ""
    },
    {
        "title": "SAME: Sample Reconstruction Against Model Extraction Attacks",
        "url": "http://arxiv.org/abs/2312.10578v1",
        "pub_date": "2023-12-17",
        "summary": "While deep learning models have shown significant performance across various\ndomains, their deployment needs extensive resources and advanced computing\ninfrastructure. As a solution, Machine Learning as a Service (MLaaS) has\nemerged, lowering the barriers for users to release or productize their deep\nlearning models. However, previous studies have highlighted potential privacy\nand security concerns associated with MLaaS, and one primary threat is model\nextraction attacks. To address this, there are many defense solutions but they\nsuffer from unrealistic assumptions and generalization issues, making them less\npractical for reliable protection. Driven by these limitations, we introduce a\nnovel defense mechanism, SAME, based on the concept of sample reconstruction.\nThis strategy imposes minimal prerequisites on the defender's capabilities,\neliminating the need for auxiliary Out-of-Distribution (OOD) datasets, user\nquery history, white-box model access, and additional intervention during model\ntraining. It is compatible with existing active defense methods. Our extensive\nexperiments corroborate the superior efficacy of SAME over state-of-the-art\nsolutions. Our code is available at https://github.com/xythink/SAME.",
        "translated": ""
    },
    {
        "title": "Rethinking Robustness of Model Attributions",
        "url": "http://arxiv.org/abs/2312.10534v1",
        "pub_date": "2023-12-16",
        "summary": "For machine learning models to be reliable and trustworthy, their decisions\nmust be interpretable. As these models find increasing use in safety-critical\napplications, it is important that not just the model predictions but also\ntheir explanations (as feature attributions) be robust to small\nhuman-imperceptible input perturbations. Recent works have shown that many\nattribution methods are fragile and have proposed improvements in either these\nmethods or the model training. We observe two main causes for fragile\nattributions: first, the existing metrics of robustness (e.g., top-k\nintersection) over-penalize even reasonable local shifts in attribution,\nthereby making random perturbations to appear as a strong attack, and second,\nthe attribution can be concentrated in a small region even when there are\nmultiple important parts in an image. To rectify this, we propose simple ways\nto strengthen existing metrics and attribution methods that incorporate\nlocality of pixels in robustness metrics and diversity of pixel locations in\nattributions. Towards the role of model training in attributional robustness,\nwe empirically observe that adversarially trained models have more robust\nattributions on smaller datasets, however, this advantage disappears in larger\ndatasets. Code is available at https://github.com/ksandeshk/LENS.",
        "translated": ""
    },
    {
        "title": "The Evolution of Keylogger Technologies: A Survey from Historical\n  Origins to Emerging Opportunities",
        "url": "http://arxiv.org/abs/2312.10445v1",
        "pub_date": "2023-12-16",
        "summary": "As the digital world evolves, so do the threats to our security do too.\nKeyloggers were once a large threat to the cyber world. Though undergoing many\ntransformations alongside the technological advancements of today, it is\nimportant to raise questions about the importance of Anti-Keyloggers in our\ncurrent state of cyber security. This survey dives into the historical\nevolution of Keyloggers and investigates their current day forms. Within this\ninspection of Keyloggers, we must propose whether Anti-Keyloggers serve a\npurpose to this ever-changing landscape before us or if emerging strategies\nhave rendered them obsolete.",
        "translated": ""
    },
    {
        "title": "Terrapin Attack: Breaking SSH Channel Integrity By Sequence Number\n  Manipulation",
        "url": "http://arxiv.org/abs/2312.12422v1",
        "pub_date": "2023-12-19",
        "summary": "The SSH protocol provides secure access to network services, particularly\nremote terminal login and file transfer within organizational networks and to\nover 15 million servers on the open internet. SSH uses an authenticated key\nexchange to establish a secure channel between a client and a server, which\nprotects the confidentiality and integrity of messages sent in either\ndirection. The secure channel prevents message manipulation, replay, insertion,\ndeletion, and reordering. In this paper, we show that as new encryption\nalgorithms and mitigations were added to SSH, the SSH Binary Packet Protocol is\nno longer a secure channel: SSH channel integrity (INT-PST) is broken for three\nwidely used encryption modes. This allows prefix truncation attacks where some\nencrypted packets at the beginning of the SSH channel can be deleted without\nthe client or server noticing it. We demonstrate several real-world\napplications of this attack. We show that we can fully break SSH extension\nnegotiation (RFC 8308), such that an attacker can downgrade the public key\nalgorithms for user authentication or turn off a new countermeasure against\nkeystroke timing attacks introduced in OpenSSH 9.5. We also identified an\nimplementation flaw in AsyncSSH that, together with prefix truncation, allows\nan attacker to redirect the victim's login into a shell controlled by the\nattacker. In an internet-wide scan for vulnerable encryption modes and support\nfor extension negotiation, we find that 77% of SSH servers support an\nexploitable encryption mode, while 57% even list it as their preferred choice.\nWe identify two root causes that enable these attacks: First, the SSH handshake\nsupports optional messages that are not authenticated. Second, SSH does not\nreset message sequence numbers when encryption is enabled. Based on this\nanalysis, we propose effective and backward-compatible changes to SSH that\nmitigate our attacks.",
        "translated": "SSH 协议提供对网络服务的安全访问，特别是远程终端登录和组织网络内的文件传输，以及对开放互联网上超过1500万台服务器的访问。SSH 使用经过身份验证的密钥交换在客户端和服务器之间建立安全通道，这保护了向任一方向发送的消息的机密性和完整性。安全通道防止消息操作、重播、插入、删除和重新排序。在本文中，我们发现随着新的加密算法和解决方案的加入，SSH 二进制分组协议不再是一个安全的信道: SSH 信道完整性(INT-PST)被三种广泛使用的加密模式所破坏。这允许前缀截断攻击，在这种攻击中，可以删除 SSH 通道开始处的一些加密数据包，而不会引起客户机或服务器的注意。我们展示了这种攻击的几个实际应用。我们展示了我们可以完全破坏 SSH 扩展协商(RFC 8308) ，使攻击者可以降低用户身份验证的公钥算法或关闭 OpenSSH 9.5中引入的针对击键计时攻击的新对策。我们还发现了 AsyncSSH 中的一个实现缺陷，它与前缀截断一起允许攻击者将受害者的登录重定向到由攻击者控制的 shell 中。通过对互联网上易受攻击的加密模式和支持扩展协商的扫描，我们发现77% 的 SSH 服务器支持可利用的加密模式，而57% 的服务器甚至将其列为首选。我们确定了启用这些攻击的两个根本原因: 首先，SSH 握手支持未经身份验证的可选消息。其次，启用加密时，SSH 不会重置消息序列号。在此分析的基础上，我们提出了对 SSH 进行有效且向后兼容的更改，以减轻我们的攻击。"
    },
    {
        "title": "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks",
        "url": "http://arxiv.org/abs/2312.12321v1",
        "pub_date": "2023-12-19",
        "summary": "With the recent surge in popularity of LLMs has come an ever-increasing need\nfor LLM safety training. In this paper, we show that SOTA open-source LLMs are\nvulnerable to simple, optimization-free attacks we refer to as $\\textit{priming\nattacks}$, which are easy to execute and effectively bypass alignment from\nsafety training. Our proposed attack improves the Attack Success Rate on\nHarmful Behaviors, as measured by Llama Guard, by up to $3.3\\times$ compared to\nbaselines. Source code and data are available at\nhttps://github.com/uiuc-focal-lab/llm-priming-attacks .",
        "translated": "随着近年来 LLM 的流行，对 LLM 安全培训的需求越来越大。在本文中，我们证明了 SOTA 开源 LLM 容易受到简单的、无需优化的攻击，我们称之为 $textit { priming 攻击} $，这种攻击易于执行并且有效地绕过了安全训练中的对齐。我们提出的攻击提高了对有害行为的攻击成功率，根据美洲驼守卫的测量，与基线相比，提高了3.3倍。源代码和数据可在 https://github.com/uiuc-focal-lab/llm-priming-attacks 查阅。"
    },
    {
        "title": "Web 3.0 and a Decentralized Approach to Education",
        "url": "http://arxiv.org/abs/2312.12268v1",
        "pub_date": "2023-12-19",
        "summary": "With the natural evolution of the web, the need for decentralization has\nrendered the current centralized education system out of date. The student does\nnot \"own\" their credentials, as the only way their accomplishments are directly\nlinked to their person and considered valuable is by verification through a\nstamp of an expensive, prestigious institution. However, going to a university\nis no longer the only way to acquire an education; open-source learning\nmaterial is widely available and accessible through the internet. However, our\nsociety does not deem these methods of education as verifiable if they do not\ninclude a degree or certificate. Additionally, a valid certificate for the vast\nmajority of open-source courses costs a few hundred dollars to obtain. The\ncentralized nature of education inadvertently places students in\nunderprivileged communities at a disadvantage in comparison to students in\neconomically advantaged communities, thus a decentralized approach to education\nwould eliminate the vast majority of such discrepancies. In the present paper,\nwe integrate Decentralized Identity (DID) with Web 3.0 to upload credentials\nlinked directly to the user. Each credential is appended to an Ethereum\nblockchain that, by design, cannot be altered once uploaded. We include DID\ndocument based access controls to display the candidate's upload and\nverification history. Finally, we utilize TLS protocols to provide a secure\nconnection to the internet for ensuring non-fungibility of credentials and\nauthentication of users.",
        "translated": "随着互联网的自然发展，对地方分权的需求使得当前的中央教育系统已经过时。学生并不“拥有”他们的证书，因为他们的成就与他们的个人直接相关，并被认为是有价值的唯一方式是通过一个昂贵的，有声望的机构的印章验证。然而，上大学不再是获得教育的唯一途径; 开源学习材料可以通过互联网广泛获得。然而，我们的社会并不认为这些教育方法是可验证的，如果它们不包括学位或证书。此外，绝大多数开源课程的有效证书需要花费几百美元才能获得。教育的集中性无意中使贫困社区的学生与经济条件较好社区的学生相比处于不利地位，因此，分散的教育办法将消除绝大多数此类差异。在本文中，我们将分散身份与 Web 3.0结合起来，直接上传与用户相关的凭证。每个凭证被附加到一个以太区块链，根据设计，一旦上传就不能被更改。我们包括基于差分文档的访问控制，以显示候选人的上传和验证历史。最后，我们利用 TLS 协议提供一个安全的互联网连接，以确保证书和用户认证的不可替代性。"
    },
    {
        "title": "Sharing is CAIRing: Characterizing Principles and Assessing Properties\n  of Universal Privacy Evaluation for Synthetic Tabular Data",
        "url": "http://arxiv.org/abs/2312.12216v1",
        "pub_date": "2023-12-19",
        "summary": "Data sharing is a necessity for innovative progress in many domains,\nespecially in healthcare. However, the ability to share data is hindered by\nregulations protecting the privacy of natural persons. Synthetic tabular data\nprovide a promising solution to address data sharing difficulties but does not\ninherently guarantee privacy. Still, there is a lack of agreement on\nappropriate methods for assessing the privacy-preserving capabilities of\nsynthetic data, making it difficult to compare results across studies. To the\nbest of our knowledge, this is the first work to identify properties that\nconstitute good universal privacy evaluation metrics for synthetic tabular\ndata. The goal of such metrics is to enable comparability across studies and to\nallow non-technical stakeholders to understand how privacy is protected. We\nidentify four principles for the assessment of metrics: Comparability,\nApplicability, Interpretability, and Representativeness (CAIR). To quantify and\nrank the degree to which evaluation metrics conform to the CAIR principles, we\ndesign a rubric using a scale of 1-4. Each of the four properties is scored on\nfour parameters, yielding 16 total dimensions. We study the applicability and\nusefulness of the CAIR principles and rubric by assessing a selection of\nmetrics popular in other studies. The results provide granular insights into\nthe strengths and weaknesses of existing metrics that not only rank the metrics\nbut highlight areas of potential improvements. We expect that the CAIR\nprinciples will foster agreement among researchers and organizations on which\nuniversal privacy evaluation metrics are appropriate for synthetic tabular\ndata.",
        "translated": "数据共享是许多领域创新进步的必要条件，特别是在医疗领域。然而，保护自然人隐私的法规阻碍了数据共享的能力。合成表格数据为解决数据共享困难提供了一个有前途的解决方案，但并不能从本质上保证隐私。尽管如此，在评估合成数据的隐私保护能力的适当方法上仍然缺乏一致意见，因此很难比较不同研究的结果。据我们所知，这是第一个工作，以确定属性，构成良好的通用隐私评估指标的综合表格数据。这种衡量标准的目的是使各项研究具有可比性，并使非技术利益相关者了解如何保护隐私。我们确定了评估指标的四个原则: 可比性、适用性、可解释性和代表性(CAIR)。为了量化和排序评估指标符合 CAIR 原则的程度，我们设计了一个1-4的评分表。四个属性中的每一个都在四个参数上进行评分，得到16个总维度。我们通过评估其他研究中流行的指标来研究 CAIR 原则和标准的适用性和有用性。这些结果提供了对现有度量标准优缺点的详细了解，这些度量标准不仅对度量标准进行了排名，而且突出了可能改进的领域。我们期望 CAIR 原则将促进研究人员和组织之间的一致性，在这一点上，通用隐私评估指标适用于合成表格数据。"
    },
    {
        "title": "Decentralised and collaborative machine learning framework for IoT",
        "url": "http://arxiv.org/abs/2312.12190v1",
        "pub_date": "2023-12-19",
        "summary": "Decentralised machine learning has recently been proposed as a potential\nsolution to the security issues of the canonical federated learning approach.\nIn this paper, we propose a decentralised and collaborative machine learning\nframework specially oriented to resource-constrained devices, usual in IoT\ndeployments. With this aim we propose the following construction blocks. First,\nan incremental learning algorithm based on prototypes that was specifically\nimplemented to work in low-performance computing elements. Second, two\nrandom-based protocols to exchange the local models among the computing\nelements in the network. Finally, two algorithmics approaches for prediction\nand prototype creation. This proposal was compared to a typical centralized\nincremental learning approach in terms of accuracy, training time and\nrobustness with very promising results.",
        "translated": "分散式机器学习最近被提出作为规范联邦学习方法安全性问题的一个潜在解决方案。在这篇文章中，我们提出了一个分散的和协作的机器学习框架，专门面向资源受限的设备，通常在物联网部署。为此，我们提出以下建筑模块。首先，一个基于原型的在线机机器学习算法，专门用于低性能计算元件。其次，两个基于随机的协议在网络中的计算单元之间交换本地模型。最后，给出了两种预测和原型创建的算法方法。该方案与典型的集中式在线机机器学习方法在准确性、训练时间和稳健性方面进行了比较，取得了非常有希望的结果。"
    },
    {
        "title": "Poincaré Differential Privacy for Hierarchy-aware Graph Embedding",
        "url": "http://arxiv.org/abs/2312.12183v1",
        "pub_date": "2023-12-19",
        "summary": "Hierarchy is an important and commonly observed topological property in\nreal-world graphs that indicate the relationships between supervisors and\nsubordinates or the organizational behavior of human groups. As hierarchy is\nintroduced as a new inductive bias into the Graph Neural Networks (GNNs) in\nvarious tasks, it implies latent topological relations for attackers to improve\ntheir inference attack performance, leading to serious privacy leakage issues.\nIn addition, existing privacy-preserving frameworks suffer from reduced\nprotection ability in hierarchical propagation due to the deficiency of\nadaptive upper-bound estimation of the hierarchical perturbation boundary. It\nis of great urgency to effectively leverage the hierarchical property of data\nwhile satisfying privacy guarantees. To solve the problem, we propose the\nPoincar\\'e Differential Privacy framework, named PoinDP, to protect the\nhierarchy-aware graph embedding based on hyperbolic geometry. Specifically,\nPoinDP first learns the hierarchy weights for each entity based on the\nPoincar\\'e model in hyperbolic space. Then, the Personalized Hierarchy-aware\nSensitivity is designed to measure the sensitivity of the hierarchical\nstructure and adaptively allocate the privacy protection strength. Besides, the\nHyperbolic Gaussian Mechanism (HGM) is proposed to extend the Gaussian\nmechanism in Euclidean space to hyperbolic space to realize random\nperturbations that satisfy differential privacy under the hyperbolic space\nmetric. Extensive experiment results on five real-world datasets demonstrate\nthe proposed PoinDP's advantages of effective privacy protection while\nmaintaining good performance on the node classification task.",
        "translated": "在现实世界的图表中，等级制度是一个重要且常见的拓扑不变性，它表明了上司和下属之间的关系，或者人类群体的组织行为学。由于层次结构作为一种新的归纳偏差被引入到图神经网络(GNN)的各种任务中，这意味着潜在的拓扑关系可以提高攻击者的推理攻击性能，从而导致严重的隐私泄漏问题。此外，现有的隐私保护框架在分层传播中由于缺乏对分层扰动边界的自适应上限估计而导致保护能力下降。在满足隐私保护的同时，有效地利用数据的层次属性是当务之急。为了解决这个问题，我们提出了 Poincar’e 差分隐私框架，命名为 PoinDP，以保护基于双曲几何的层次感知图形嵌入。具体来说，PoinDP 首先根据 Poincar’e 模型在双曲空间中学习每个实体的层次权重。然后，设计个性化层次感知灵敏度度量层次结构的灵敏度，并自适应地分配隐私保护强度。此外，本文还提出了双曲高斯机制(hGM) ，将欧氏空间中的高斯机制扩展到双曲空间，以实现满足差分隐私双曲空间度量下的随机扰动。在五个实际数据集上的大量实验结果表明，PoinDP 算法在保持节点分类性能的同时，具有有效保护隐私的优点。"
    },
    {
        "title": "Towards an in-depth detection of malware using distributed QCNN",
        "url": "http://arxiv.org/abs/2312.12161v1",
        "pub_date": "2023-12-19",
        "summary": "Malware detection is an important topic of current cybersecurity, and Machine\nLearning appears to be one of the main considered solutions even if certain\nproblems to generalize to new malware remain. In the aim of exploring the\npotential of quantum machine learning on this domain, our previous work showed\nthat quantum neural networks do not perform well on image-based malware\ndetection when using a few qubits. In order to enhance the performances of our\nquantum algorithms for malware detection using images, without increasing the\nresources needed in terms of qubits, we implement a new preprocessing of our\ndataset using Grayscale method, and we couple it with a model composed of five\ndistributed quantum convolutional networks and a scoring function. We get an\nincrease of around 20 \\% of our results, both on the accuracy of the test and\nits F1-score.",
        "translated": "恶意软件检测是当前网络安全的一个重要课题，机器学习似乎是主要考虑的解决方案之一，即使某些问题仍然推广到新的恶意软件。为了探索量子机器学习在这一领域的潜力，我们以前的工作表明，当使用少量量子位时，量子神经网络在基于图像的恶意软件检测方面表现不佳。为了提高利用图像检测恶意软件的量子算法的性能，在不增加量子位所需资源的情况下，我们使用灰度方法对数据集实现了一种新的预处理，并将其与一个由五个分布式量子卷积网络和一个评分函数组成的模型耦合。在测试的准确性和 F1得分上，我们的结果增加了大约20% 。"
    },
    {
        "title": "Elliptic Curve Pairing Stealth Address Protocols",
        "url": "http://arxiv.org/abs/2312.12131v1",
        "pub_date": "2023-12-19",
        "summary": "The protection of transactions privacy is extremely important for the user.\nWith stealth address protocols (SAP), users can receive assets on stealth\naddresses that they do not link to their stealth meta-addresses. SAP can be\ngenerated using various cryptographic approaches. DKSAP uses elliptic curve\nmultiplication and hashing of the resulting shared secret. Another approach is\nto use a bilinear mapping. The paper presents two SA protocols that use\nelliptic curve pairing as a cryptographic solution. ECPDKSAP is a pairing-based\nprotocol that includes viewing key and spending key, while ECPSKSAP is a\npairing-based protocol that uses a single key with which spending and the\nviewing key are derived. The expected efficiency of the proposed protocols is\nsimilar to that of DKSAP with a view tag.",
        "translated": "交易隐私的保护对于用户来说非常重要。使用隐形地址协议(SAP) ，用户可以接收隐形地址上的资产，而这些资产不会链接到他们的隐形元地址。可以使用各种加密方法生成 SAP。DKSAP 使用椭圆曲线乘法和散列结果共享秘密。另一种方法是使用双线性映射。本文提出了两种使用椭圆曲线对作为密码解决方案的 SA 协议。ECPDKSAP 是一个基于配对的协议，它包括查看密钥和查看密钥，而 ECPSKSAP 是一个基于配对的协议，它使用一个密钥来获得查看密钥和查看密钥。所提议的协议的预期效率与带有视图标记的 DKSAP 类似。"
    },
    {
        "title": "A Survey on Property-Preserving Database Encryption Techniques in the\n  Cloud",
        "url": "http://arxiv.org/abs/2312.12075v1",
        "pub_date": "2023-12-19",
        "summary": "Outsourcing a relational database to the cloud offers several benefits,\nincluding scalability, availability, and cost-effectiveness. However, there are\nconcerns about the security and confidentiality of the outsourced data. A\ngeneral approach here would be to encrypt the data with a standardized\nencryption algorithm and then store the data only encrypted in the cloud. The\nproblem with this approach, however, is that with encryption, important\nproperties of the data such as sorting, format or comparability, which are\nessential for the functioning of database queries, are lost. One solution to\nthis problem is the use of encryption algorithms, which also preserve these\nproperties in the encrypted data, thus enabling queries to encrypted data.\nThese algorithms range from simple algorithms like Caesar encryption to secure\nalgorithms like mOPE. The report at hand presents a survey on common encryption\ntechniques used for storing data in relation Cloud database services. It\npresents the applied methods and identifies their characteristics.",
        "translated": "将关系数据库外包到云计算有几个好处，包括可伸缩性、可用性和成本效益。然而，对于外包数据的安全性和机密性还存在一些问题。这里的通用方法是使用标准化的加密算法对数据进行加密，然后将仅加密的数据存储在云中。但是，这种方法的问题在于，加密会丢失数据的重要属性，如对数据库查询的功能至关重要的排序、格式或可比性。这个问题的一个解决方案是使用加密算法，该算法还在加密数据中保留这些属性，从而允许对加密数据进行查询。这些算法的范围从像凯撒加密这样的简单算法到像 mOPE 这样的安全算法。手头的报告介绍了一个关于在关系云数据库服务中用于存储数据的常用加密技术的调查。介绍了应用的方法，并确定了它们的特点。"
    },
    {
        "title": "EncryIP: A Practical Encryption-Based Framework for Model Intellectual\n  Property Protection",
        "url": "http://arxiv.org/abs/2312.12049v1",
        "pub_date": "2023-12-19",
        "summary": "In the rapidly growing digital economy, protecting intellectual property (IP)\nassociated with digital products has become increasingly important. Within this\ncontext, machine learning (ML) models, being highly valuable digital assets,\nhave gained significant attention for IP protection. This paper introduces a\npractical encryption-based framework called \\textit{EncryIP}, which seamlessly\nintegrates a public-key encryption scheme into the model learning process. This\napproach enables the protected model to generate randomized and confused\nlabels, ensuring that only individuals with accurate secret keys, signifying\nauthorized users, can decrypt and reveal authentic labels. Importantly, the\nproposed framework not only facilitates the protected model to multiple\nauthorized users without requiring repetitive training of the original ML model\nwith IP protection methods but also maintains the model's performance without\ncompromising its accuracy. Compared to existing methods like watermark-based,\ntrigger-based, and passport-based approaches, \\textit{EncryIP} demonstrates\nsuperior effectiveness in both training protected models and efficiently\ndetecting the unauthorized spread of ML models.",
        "translated": "在快速增长的数字经济中，保护与数字产品相关的知识产权变得越来越重要。在这种背景下，机器学习(ML)模型作为一种非常有价值的数字资产，在知识产权保护方面得到了极大的关注。本文介绍了一个实用的基于加密的框架 texttit { EncryIP } ，该框架将公钥加密方案无缝集成到模型学习过程中。这种方法使受保护的模型能够生成随机和混淆的标签，确保只有个人准确的秘密密钥，表示授权用户，可以解密和揭示真实的标签。重要的是，提出的框架不仅方便了多个授权用户的保护模型，而不需要重复训练的原始机器学习模型与 IP 保护方法，但也保持了模型的性能，而不影响其准确性。与现有的基于水印、基于触发器和基于护照的方法相比，texttit { EncryIP }在训练保护模型和有效检测机器学习模型未经授权的传播方面表现出了优越的效率。"
    },
    {
        "title": "Dynamic Mining Interval to Improve Blockchain Throughput",
        "url": "http://arxiv.org/abs/2312.14038v1",
        "pub_date": "2023-12-21",
        "summary": "Decentralized Finance (DeFi), propelled by Blockchain technology, has\nrevolutionized traditional financial systems, improving transparency, reducing\ncosts, and fostering financial inclusion. However, transaction activities in\nthese systems fluctuate significantly and the throughput can be effected. To\naddress this issue, we propose a Dynamic Mining Interval (DMI) mechanism that\nadjusts mining intervals in response to block size and trading volume to\nenhance the transaction throughput of Blockchain platforms. Besides, in the\ncontext of public Blockchains such as Bitcoin, Ethereum, and Litecoin, a shift\ntowards transaction fees dominance over coin-based rewards is projected in near\nfuture. As a result, the ecosystem continues to face threats from deviant\nmining activities such as Undercutting Attacks, Selfish Mining, and Pool\nHopping, among others. In recent years, Dynamic Transaction Storage (DTS)\nstrategies were proposed to allocate transactions dynamically based on fees\nthereby stabilizing block incentives. However, DTS' utilization of Merkle tree\nleaf nodes can reduce system throughput. To alleviate this problem, in this\npaper, we propose an approach for combining DMI and DTS. Besides, we also\ndiscuss the DMI selection mechanism for adjusting mining intervals based on\nvarious factors.",
        "translated": "在 Blockchain 技术的推动下，分散金融(deFi)已经彻底改变了传统的金融体系，提高了透明度，降低了成本，促进了普惠金融。但是，这些系统中的事务活动会有很大的波动，并且会影响吞吐量。为了解决这个问题，我们提出了一种动态挖掘间隔(DMI)机制，根据块大小和交易量调整挖掘间隔，以提高区块链平台的事务吞吐量。此外，在比特币(Bitcoin)、以太坊(Etherum)和莱特币(Litecoin)等公共区块链的背景下，预计在不久的将来，交易费用将超过基于硬币的奖励。因此，生态系统继续面临着来自不正常采矿活动的威胁，如削弱攻击、自私采矿和池跳等。近年来，动态事务存储(DTS)策略被提出，以费用为基础动态分配事务，从而稳定了块激励。然而，DTS 对默克尔树叶节点的利用会降低系统吞吐量。为了解决这一问题，本文提出了一种 DMI 和 DTS 相结合的方法。此外，还讨论了基于各种因素调整采矿间隔的 DMI 选择机制。"
    },
    {
        "title": "Efficient quantum algorithms for some instances of the semidirect\n  discrete logarithm problem",
        "url": "http://arxiv.org/abs/2312.14028v1",
        "pub_date": "2023-12-21",
        "summary": "The semidirect discrete logarithm problem (SDLP) is the following analogue of\nthe standard discrete logarithm problem in the semidirect product semigroup\n$G\\rtimes \\mathrm{End}(G)$ for a finite semigroup $G$. Given $g\\in G, \\sigma\\in\n\\mathrm{End}(G)$, and $h=\\prod_{i=0}^{t-1}\\sigma^i(g)$ for some integer $t$,\nthe SDLP$(G,\\sigma)$, for $g$ and $h$, asks to determine $t$. As Shor's\nalgorithm crucially depends on commutativity, it is believed not to be\napplicable to the SDLP. Previously, the best known algorithm for the SDLP was\nbased on Kuperberg's subexponential time quantum algorithm. Still, the problem\nplays a central role in the security of certain proposed cryptosystems in the\nfamily of \\textit{semidirect product key exchange}. This includes a recently\nproposed signature protocol called SPDH-Sign. In this paper, we show that the\nSDLP is even easier in some important special cases. Specifically, for a finite\ngroup $G$, we describe quantum algorithms for the SDLP in $G\\rtimes\n\\mathrm{Aut}(G)$ for the following two classes of instances: the first one is\nwhen $G$ is solvable and the second is when $G$ is a matrix group and a power\nof $\\sigma$ with a polynomially small exponent is an inner automorphism of $G$.\nWe further extend the results to groups composed of factors from these classes.\nA consequence is that SPDH-Sign and similar cryptosystems whose security\nassumption is based on the presumed hardness of the SDLP in the cases described\nabove are insecure against quantum attacks. The quantum ingredients we rely on\nare not new: these are Shor's factoring and discrete logarithm algorithms and\nwell-known generalizations.",
        "translated": "半直接离散对数问题(SDLP)类似于有限半群 $g $的离散对数半直积半群 $g rtimes 数学{ End }(G) $中的标准问题。给定 G 中的 $g，数学中的 sigma { End }(G) $，以及 $h = prod _ { i = 0} ^ { t-1} sigma ^ i (g) $对于某个整数 $t $，对于 $g $和 $h $，SDLP $(G，sigma) $要求确定 $t $。由于 Shor 算法严重依赖于交换性，因此不适用于 SDLP 算法。此前，SDLP 最著名的算法是基于库珀伯格的次指数时间量子算法。尽管如此，这个问题仍然在 textit {半直积密钥交换}家族中某些被提议的密码系统的安全性中扮演着核心角色。这包括最近提出的名为 SPDH-Sign 的签名协议。在本文中，我们证明了 SDLP 在一些重要的特殊情况下更加容易。特别地，对于一个有限群 $g $，我们描述了在 $g rtimes 数学{ Aut }(G) $中 SDLP 的量子算法，用于以下两类实例: 第一类是当 $g $是可解的时候，第二类是当 $g $是一个矩阵群，并且一个具有多项式小指数的 $sigma $的幂是 $g $的内自同构时。我们进一步将结果推广到由这些类中的因子组成的组。结果表明，在上述情况下，基于 SDLP 硬度假设的 SPDH-Sign 和类似的密码体制对量子攻击是不安全的。我们所依赖的量子成分并不新鲜，这些是肖尔的分解和离散对数算法以及众所周知的概括。"
    },
    {
        "title": "Deep Learning Based Face Recognition Method using Siamese Network",
        "url": "http://arxiv.org/abs/2312.14001v1",
        "pub_date": "2023-12-21",
        "summary": "Achieving state-of-the-art results in face verification systems typically\nhinges on the availability of labeled face training data, a resource that often\nproves challenging to acquire in substantial quantities. In this research\nendeavor, we proposed employing Siamese networks for face recognition,\neliminating the need for labeled face images. We achieve this by strategically\nleveraging negative samples alongside nearest neighbor counterparts, thereby\nestablishing positive and negative pairs through an unsupervised methodology.\nThe architectural framework adopts a VGG encoder, trained as a double branch\nsiamese network. Our primary aim is to circumvent the necessity for labeled\nface image data, thus proposing the generation of training pairs in an entirely\nunsupervised manner. Positive training data are selected within a dataset based\non their highest cosine similarity scores with a designated anchor, while\nnegative training data are culled in a parallel fashion, though drawn from an\nalternate dataset. During training, the proposed siamese network conducts\nbinary classification via cross-entropy loss. Subsequently, during the testing\nphase, we directly extract face verification scores from the network's output\nlayer. Experimental results reveal that the proposed unsupervised system\ndelivers a performance on par with a similar but fully supervised baseline.",
        "translated": "在人脸验证系统中实现最先进的结果通常取决于标记的人脸训练数据的可用性，这种资源往往被证明难以获得大量的数据。在这项研究工作中，我们提出使用暹罗网络进行人脸识别，消除了标记人脸图像的需要。我们通过策略性地利用负面样本和最近邻的对应样本来实现这一点，从而通过无监督的方法建立正负对。结构框架采用 VGG 编码器，训练成双分支暹罗网络。我们的主要目的是规避标记人脸图像数据的必要性，从而提出一个完全无监督的方式生成训练对。正面的训练数据是根据他们的最高余弦距离得分和指定的锚在数据集中选择的，而负面的训练数据是从一个替代的数据集中以平行的方式筛选出来的。在训练过程中，该网络通过交叉熵损失进行二值分类。随后，在测试阶段，我们直接从网络的输出层提取人脸验证得分。实验结果表明，提出的无监督系统提供了一个相似的性能，但完全监督的基线。"
    },
    {
        "title": "Open-Set: ID Card Presentation Attack Detection using Neural Transfer\n  Style",
        "url": "http://arxiv.org/abs/2312.13993v1",
        "pub_date": "2023-12-21",
        "summary": "The accurate detection of ID card Presentation Attacks (PA) is becoming\nincreasingly important due to the rising number of online/remote services that\nrequire the presentation of digital photographs of ID cards for digital\nonboarding or authentication. Furthermore, cybercriminals are continuously\nsearching for innovative ways to fool authentication systems to gain\nunauthorized access to these services. Although advances in neural network\ndesign and training have pushed image classification to the state of the art,\none of the main challenges faced by the development of fraud detection systems\nis the curation of representative datasets for training and evaluation. The\nhandcrafted creation of representative presentation attack samples often\nrequires expertise and is very time-consuming, thus an automatic process of\nobtaining high-quality data is highly desirable. This work explores ID card\nPresentation Attack Instruments (PAI) in order to improve the generation of\nsamples with four Generative Adversarial Networks (GANs) based image\ntranslation models and analyses the effectiveness of the generated data for\ntraining fraud detection systems. Using open-source data, we show that\nsynthetic attack presentations are an adequate complement for additional real\nattack presentations, where we obtain an EER performance increase of 0.63%\npoints for print attacks and a loss of 0.29% for screen capture attacks.",
        "translated": "由于越来越多的网上/远程服务要求提供身份证的数码照片，以便进行数码登机或认证，因此，准确侦测身份证显示攻击(PA)正变得越来越重要。此外，网络罪犯不断寻找创新的方法来欺骗身份验证系统，以获得对这些服务的未经授权的访问。尽管神经网络设计和训练的进步已经将图像分类推向了最高水平，但欺诈检测系统发展所面临的主要挑战之一是如何管理用于训练和评估的代表性数据集。手工创建具有代表性的表示攻击样本通常需要专业知识，而且非常耗时，因此非常需要自动获取高质量数据的过程。为了改进基于生成对抗网络(GANs)的图像翻译模型的样本生成，本文对身份证呈现攻击工具(PAI)进行了研究，并分析了生成的数据对培训欺诈检测系统的有效性。使用开源数据，我们表明合成攻击演示是额外的真实攻击演示的充分补充，其中我们获得了打印攻击的 EER 性能提高0.63% ，屏幕捕获攻击的损失0.29% 。"
    },
    {
        "title": "Rényi Pufferfish Privacy: General Additive Noise Mechanisms and\n  Privacy Amplification by Iteration",
        "url": "http://arxiv.org/abs/2312.13985v1",
        "pub_date": "2023-12-21",
        "summary": "Pufferfish privacy is a flexible generalization of differential privacy that\nallows to model arbitrary secrets and adversary's prior knowledge about the\ndata. Unfortunately, designing general and tractable Pufferfish mechanisms that\ndo not compromise utility is challenging. Furthermore, this framework does not\nprovide the composition guarantees needed for a direct use in iterative machine\nlearning algorithms. To mitigate these issues, we introduce a R\\'enyi\ndivergence-based variant of Pufferfish and show that it allows us to extend the\napplicability of the Pufferfish framework. We first generalize the Wasserstein\nmechanism to cover a wide range of noise distributions and introduce several\nways to improve its utility. We also derive stronger guarantees against\nout-of-distribution adversaries. Finally, as an alternative to composition, we\nprove privacy amplification results for contractive noisy iterations and\nshowcase the first use of Pufferfish in private convex optimization. A common\ningredient underlying our results is the use and extension of shift reduction\nlemmas.",
        "translated": "河豚鱼隐私权是一种灵活的概括差分隐私，允许建模任意的秘密和对手的先前知识的数据。不幸的是，设计通用和易处理的河豚机制，不妥协的效用是具有挑战性的。此外，这个框架没有提供直接使用迭代机器学习算法所需的组合保证。为了缓解这些问题，我们引入了一个基于 R‘ enyi 散度的 Pufferfish 变体，并表明它允许我们扩展 Pufferfish 框架的适用性。我们首先将 Wasserstein 机制推广到广泛的噪声分布，并介绍了几种提高其效用的方法。我们还得到了针对分配不足的对手的更强有力的保障。最后，作为合成的替代方案，我们证明了压缩噪声迭代的隐私放大效果，并展示了河豚鱼在私人凸优化中的首次使用。我们的结果的一个共同因素是移位缩减引理的使用和扩展。"
    },
    {
        "title": "Asynchronous Authentication",
        "url": "http://arxiv.org/abs/2312.13967v1",
        "pub_date": "2023-12-21",
        "summary": "A myriad of authentication mechanisms embody a continuous evolution from\nverbal passwords in ancient times to contemporary multi-factor authentication.\nNevertheless, digital asset heists and numerous identity theft cases illustrate\nthe urgent need to revisit the fundamentals of user authentication. We abstract\naway credential details and formalize the general, common case of asynchronous\nauthentication, with unbounded message propagation time. Our model, which might\nbe of independent interest, allows for eventual message delivery, while\nbounding execution time to maintain cryptographic guarantees. Given\ncredentials' fault probabilities (e.g., loss or leak), we seek mechanisms with\nthe highest success probability. We show that every mechanism is dominated by\nsome Boolean mechanism -- defined by a monotonic Boolean function on presented\ncredentials. We present an algorithm for finding approximately optimal\nmechanisms. Previous work analyzed Boolean mechanisms specifically, but used\nbrute force, which quickly becomes prohibitively complex. We leverage the\nproblem structure to reduce complexity by orders of magnitude. The algorithm is\nreadily applicable to practical settings. For example, we revisit the common\napproach in cryptocurrency wallets that use a handful of high-quality\ncredentials. We show that adding low-quality credentials improves security by\norders of magnitude.",
        "translated": "无数的认证机制体现了从古代口头密码到现代双重身份验证密码的不断演变。然而，数字资产盗窃和大量身份盗窃案件表明，迫切需要重新审视用户认证的基本原则。我们抽象出凭据的细节，并用无限的消息传播时间将异步身份验证的一般常见情况形式化。我们的模型可能具有独立的兴趣，它允许最终的消息传递，同时限制执行时间以维护加密保证。给定凭据的故障概率(例如丢失或泄漏) ，我们寻找具有最高成功概率的机制。我们证明了每个机制都是由一些布尔机制支配的——定义为呈现凭证的单调布尔函数。我们提出了一个近似最优机构的算法。先前的工作专门分析了布尔机制，但使用了蛮力，这很快就变得非常复杂。我们利用问题结构通过数量级来降低复杂性。该算法适用于实际环境。例如，我们重新讨论使用少量高质量凭证的加密货币钱包中的常用方法。我们的研究表明，添加低质量的凭证可以通过数量级提高安全性。"
    },
    {
        "title": "Measures of Resilience to Cyber Contagion -- An Axiomatic Approach for\n  Complex Systems",
        "url": "http://arxiv.org/abs/2312.13884v1",
        "pub_date": "2023-12-21",
        "summary": "We introduce a novel class of risk measures for the management of systemic\nrisk in networks. In contrast to most existing approaches, our measures target\nthe topological structure of the network in order to control the risk of a\npandemic spread of some contagious peril throughout the network. While the main\ndiscussion of the paper is tailored to the management of systemic cyber risk in\ndigital networks, we also draw parallels to similar risk management frameworks\nfor other types of complex systems.",
        "translated": "提出了一类新的风险度量方法，用于网络系统风险管理。与大多数现有方法不同，我们的措施针对网络的拓扑结构，以控制某些传染性危险在整个网络中大流行蔓延的风险。虽然本文的主要讨论是针对数字网络中系统性网络风险的管理进行的，但我们也将其与其他类型复杂系统的类似风险管理框架进行了比较。"
    },
    {
        "title": "Manipulating Trajectory Prediction with Backdoors",
        "url": "http://arxiv.org/abs/2312.13863v1",
        "pub_date": "2023-12-21",
        "summary": "Autonomous vehicles ought to predict the surrounding agents' trajectories to\nallow safe maneuvers in uncertain and complex traffic situations. As companies\nincreasingly apply trajectory prediction in the real world, security becomes a\nrelevant concern. In this paper, we focus on backdoors - a security threat\nacknowledged in other fields but so far overlooked for trajectory prediction.\nTo this end, we describe and investigate four triggers that could affect\ntrajectory prediction. We then show that these triggers (for example, a braking\nvehicle), when correlated with a desired output (for example, a curve) during\ntraining, cause the desired output of a state-of-the-art trajectory prediction\nmodel. In other words, the model has good benign performance but is vulnerable\nto backdoors. This is the case even if the trigger maneuver is performed by a\nnon-casual agent behind the target vehicle. As a side-effect, our analysis\nreveals interesting limitations within trajectory prediction models. Finally,\nwe evaluate a range of defenses against backdoors. While some, like simple\noffroad checks, do not enable detection for all triggers, clustering is a\npromising candidate to support manual inspection to find backdoors.",
        "translated": "自动驾驶车辆应该预测周围车辆的轨迹，以便在不确定和复杂的交通情况下进行安全机动。随着公司越来越多地在现实世界中应用轨迹预测，安全成为一个相关的问题。在本文中，我们重点介绍了后门技术——一种在其他领域得到承认但在轨迹预测中被忽视的安全威胁。为此，我们描述和研究了影响轨迹预测的四个触发因素。然后我们展示了这些触发器(例如，刹车车辆) ，当与期望的输出(例如，曲线)在训练期间，导致期望的输出最先进的轨迹预测模型。换句话说，该模型具有良好的性能，但容易受到后门的影响。即使触发机动是由目标车辆后面的非临时特工执行的，情况也是如此。作为一个副作用，我们的分析揭示了轨迹预测模型中有趣的局限性。最后，我们评估了一系列防御后门的措施。虽然有些检查，比如简单的越野检查，不能检测所有触发器，但集群是一个很有希望的候选方案，可以支持人工检查以发现后门。"
    },
    {
        "title": "An Approach to Abstract Multi-stage Cyberattack Data Generation for\n  ML-Based IDS in Smart Grids",
        "url": "http://arxiv.org/abs/2312.13737v1",
        "pub_date": "2023-12-21",
        "summary": "Power grids are becoming more digitized, resulting in new opportunities for\nthe grid operation but also new challenges, such as new threats from the\ncyber-domain. To address these challenges, cybersecurity solutions are being\nconsidered in the form of preventive, detective, and reactive measures. Machine\nlearning-based intrusion detection systems are used as part of detection\nefforts to detect and defend against cyberattacks. However, training and\ntesting data for these systems are often not available or suitable for use in\nmachine learning models for detecting multi-stage cyberattacks in smart grids.\nIn this paper, we propose a method to generate synthetic data using a\ngraph-based approach for training machine learning models in smart grids. We\nuse an abstract form of multi-stage cyberattacks defined via graph formulations\nand simulate the propagation behavior of attacks in the network. Within the\nselected scenarios, we observed promising results, but a larger number of\nscenarios need to be studied to draw a more informed conclusion about the\nsuitability of synthesized data.",
        "translated": "电网数字化程度的提高，给电网运行带来了新的机遇，同时也带来了新的挑战，如网络领域的新威胁。为了应对这些挑战，网络安全解决方案正在考虑采取预防性、侦查性和反应性措施。基于机器学习的入侵检测系统被用作检测工作的一部分，以检测和防御网络攻击。然而，这些系统的训练和测试数据往往不可用或不适合用于机器学习模型，以检测智能电网中的多阶段网络攻击。本文提出了一种基于图的智能电网机器学习模型生成方法。我们使用一种抽象的多阶段网络攻击形式，通过图论公式定义，并模拟攻击在网络中的传播行为。在选定的情景中，我们观察到有希望的结果，但需要对更多的情景进行研究，以便就综合数据的适用性得出更明智的结论。"
    },
    {
        "title": "Conciliating Privacy and Utility in Data Releases via Individual\n  Differential Privacy and Microaggregation",
        "url": "http://arxiv.org/abs/2312.13712v1",
        "pub_date": "2023-12-21",
        "summary": "$\\epsilon$-Differential privacy (DP) is a well-known privacy model that\noffers strong privacy guarantees. However, when applied to data releases, DP\nsignificantly deteriorates the analytical utility of the protected outcomes. To\nkeep data utility at reasonable levels, practical applications of DP to data\nreleases have used weak privacy parameters (large $\\epsilon$), which dilute the\nprivacy guarantees of DP. In this work, we tackle this issue by using an\nalternative formulation of the DP privacy guarantees, named\n$\\epsilon$-individual differential privacy (iDP), which causes less data\ndistortion while providing the same protection as DP to subjects. We enforce\niDP in data releases by relying on attribute masking plus a pre-processing step\nbased on data microaggregation. The goal of this step is to reduce the\nsensitivity to record changes, which determines the amount of noise required to\nenforce iDP (and DP). Specifically, we propose data microaggregation strategies\ndesigned for iDP whose sensitivities are significantly lower than those used in\nDP. As a result, we obtain iDP-protected data with significantly better utility\nthan with DP. We report on experiments that show how our approach can provide\nstrong privacy (small $\\epsilon$) while yielding protected data that do not\nsignificantly degrade the accuracy of secondary data analysis.",
        "translated": "$epsilon $- 差分隐私(DP)是一种著名的隐私模式，提供强大的隐私保障。然而，当应用于数据发布时，DP 显著地降低了受保护结果的分析效用。为了使数据实用性保持在合理的水平，DP 在数据发布中的实际应用使用了弱隐私参数(大的 $epsilon $) ，这稀释了 DP 的隐私保障。在这项工作中，我们通过使用 DP 隐私保护的另一种形式来解决这个问题，命名为 $epsilon $- 个人差分隐私(iDP) ，它在为受试者提供与 DP 相同的保护的同时，减少了数据失真。我们在数据发布中通过依赖属性屏蔽加上基于数据微聚合的预处理步骤来实施 iDP。此步骤的目标是降低对记录更改的敏感性，这决定了实施 iDP (和 DP)所需的噪声量。具体来说，我们提出了针对灵敏度明显低于 DP 的 iDP 的数据微聚集策略。因此，我们获得的 iDP 保护数据的实用性明显优于 DP。我们报告的实验表明，我们的方法可以提供强大的隐私(小 $epsilon $) ，同时产生保护数据，不会显着降低准确性的二次数据分析。"
    },
    {
        "title": "On rate-optimal classification from non-private and from private data",
        "url": "http://arxiv.org/abs/2312.14889v1",
        "pub_date": "2023-12-22",
        "summary": "In this paper we revisit the classical problem of classification, but impose\nprivacy constraints. Under such constraints, the raw data\n$(X_1,Y_1),\\ldots,(X_n,Y_n)$ cannot be directly observed, and all classifiers\nare functions of the randomised outcome of a suitable local differential\nprivacy mechanism. The statistician is free to choose the form of this privacy\nmechanism, and here we add Laplace distributed noise to a discretisation of the\nlocation of each feature vector $X_i$ and to its label $Y_i$. The\nclassification rule is the privatized version of the well-studied partitioning\nclassification rule. In addition to the standard Lipschitz and margin\nconditions, a novel characteristic is introduced, by which the exact rate of\nconvergence of the classification error probability is calculated, both for\nnon-private and private data.",
        "translated": "在本文中，我们重新讨论了经典的分类问题，但是加入了隐私约束。在这种约束下，原始数据 $(X _ 1，Y _ 1) ，ldot，(X _ n，Y _ n) $不能直接观察到，所有分类器都是适当的本地差分隐私机制的随机结果的函数。统计人员可以自由选择这种隐私机制的形式，在这里我们将拉普拉斯分布式噪声添加到每个特征向量 $x _ i $和它的标签 $Y _ i $的离散位置。分类规则是经过深入研究的划分分类规则的私有化版本。除了标准的 Lipschitz 条件和边界条件外，还引入了一种新的特征，通过这种特征可以计算非私有和私有数据的分类错误概率的精确收敛速度。"
    },
    {
        "title": "Can Machines Learn Robustly, Privately, and Efficiently?",
        "url": "http://arxiv.org/abs/2312.14712v1",
        "pub_date": "2023-12-22",
        "summary": "The success of machine learning (ML) applications relies on vast datasets and\ndistributed architectures, which, as they grow, present challenges for ML. In\nreal-world scenarios, where data often contains sensitive information, issues\nlike data poisoning and hardware failures are common. Ensuring privacy and\nrobustness is vital for the broad adoption of ML in public life. This paper\nexamines the costs associated with achieving these objectives in distributed\narchitectures. We overview the meanings of privacy and robustness in\ndistributed ML, and clarify how they can be achieved efficiently in isolation.\nHowever, we contend that the integration of these objectives entails a notable\ncompromise in computational efficiency. We delve into this intricate balance,\nexploring the challenges and solutions for privacy, robustness, and\ncomputational efficiency in ML applications.",
        "translated": "机器学习(ML)应用的成功依赖于庞大的数据集和分布式体系结构，随着它们的发展，机器学习面临着挑战。在真实场景中，数据通常包含敏感信息，像数据中毒和硬件故障这样的问题很常见。确保隐私和健壮性对于机器学习在公共生活中的广泛应用至关重要。本文研究了在分布式体系结构中实现这些目标的相关成本。我们概述了隐私和健壮性在分布式机器学习中的意义，并阐明了如何有效地隔离它们。然而，我们认为，这些目标的整合需要一个显着的折衷计算效率。我们深入研究这种错综复杂的平衡，探索机器学习应用中隐私、健壮性和计算效率的挑战和解决方案。"
    },
    {
        "title": "Cybersecurity in Motion: A Survey of Challenges and Requirements for\n  Future Test Facilities of CAVs",
        "url": "http://arxiv.org/abs/2312.14687v1",
        "pub_date": "2023-12-22",
        "summary": "The way we travel is changing rapidly, and Cooperative Intelligent\nTransportation Systems (C-ITSs) are at the forefront of this evolution.\nHowever, the adoption of C-ITSs introduces new risks and challenges, making\ncybersecurity a top priority for ensuring safety and reliability. Building on\nthis premise, this paper presents an envisaged Cybersecurity Centre of\nExcellence (CSCE) designed to bolster research, testing, and evaluation of the\ncybersecurity of C-ITSs. We explore the design, functionality, and challenges\nof CSCE's testing facilities, outlining the technological, security, and\nsocietal requirements. Through a thorough survey and analysis, we assess the\neffectiveness of these systems in detecting and mitigating potential threats,\nhighlighting their flexibility to adapt to future C-ITSs. Finally, we identify\ncurrent unresolved challenges in various C-ITS domains, with the aim of\nmotivating further research into the cybersecurity of C-ITSs.",
        "translated": "我们的旅行方式正在迅速改变，协同智能交通系统(C-ITS)处于这一进化的前沿。然而，C-ITS 的采用带来了新的风险和挑战，使得网络安全成为确保安全和可靠性的首要任务。在此前提下，本文提出了一个设想的网络安全卓越中心(CSCE) ，旨在支持研究，测试和评估的网络安全的 C-ITS。我们探讨了 CSCE 测试设施的设计、功能和挑战，概述了技术、安全和社会需求。通过彻底的调查和分析，我们评估了这些系统在探测和减轻潜在威胁方面的有效性，突出了它们适应未来 C-ITS 的灵活性。最后，我们确定了目前在不同的 C-ITS 领域尚未解决的挑战，目的是推动 C-ITS 系统的网络安全的进一步研究。"
    },
    {
        "title": "MEAOD: Model Extraction Attack against Object Detectors",
        "url": "http://arxiv.org/abs/2312.14677v1",
        "pub_date": "2023-12-22",
        "summary": "The widespread use of deep learning technology across various industries has\nmade deep neural network models highly valuable and, as a result, attractive\ntargets for potential attackers. Model extraction attacks, particularly\nquery-based model extraction attacks, allow attackers to replicate a substitute\nmodel with comparable functionality to the victim model and present a\nsignificant threat to the confidentiality and security of MLaaS platforms.\nWhile many studies have explored threats of model extraction attacks against\nclassification models in recent years, object detection models, which are more\nfrequently used in real-world scenarios, have received less attention. In this\npaper, we investigate the challenges and feasibility of query-based model\nextraction attacks against object detection models and propose an effective\nattack method called MEAOD. It selects samples from the attacker-possessed\ndataset to construct an efficient query dataset using active learning and\nenhances the categories with insufficient objects. We additionally improve the\nextraction effectiveness by updating the annotations of the query dataset.\nAccording to our gray-box and black-box scenarios experiments, we achieve an\nextraction performance of over 70% under the given condition of a 10k query\nbudget.",
        "translated": "深度学习技术在各个行业的广泛应用使得深度神经网络模型具有很高的价值，因此对潜在的攻击者来说，深度神经网络模型是很有吸引力的目标。模型提取攻击，尤其是基于查询的模型提取攻击，允许攻击者复制具有与受害者模型相似功能的替代模型，并对 MLaaS 平台的机密性和安全性构成重大威胁。尽管近年来许多研究探讨了模型提取攻击对分类模型的威胁，但在现实世界中使用较多的目标检测模型却没有得到足够的重视。在本文中，我们研究了基于查询的模型抽取攻击对目标检测模型的挑战和可行性，并提出了一种有效的攻击方法，称为 MEAOD。该方法从攻击者所拥有的数据集中选取样本，利用主动学习技术构造高效的查询数据集，增强对象不足的类别。我们还通过更新查询数据集的注释来提高提取的有效性。根据我们的灰盒和黑盒场景实验，在给定的10k 查询预算条件下，我们实现了70% 以上的提取性能。"
    },
    {
        "title": "Evaluating the Security and Privacy Risk Postures of Virtual Assistants",
        "url": "http://arxiv.org/abs/2312.14633v1",
        "pub_date": "2023-12-22",
        "summary": "Virtual assistants (VAs) have seen increased use in recent years due to their\nease of use for daily tasks. Despite their growing prevalence, their security\nand privacy implications are still not well understood. To address this gap, we\nconducted a study to evaluate the security and privacy postures of eight widely\nused voice assistants: Alexa, Braina, Cortana, Google Assistant, Kalliope,\nMycroft, Hound, and Extreme. We used three vulnerability testing tools,\nAndroBugs, RiskInDroid, and MobSF, to assess the security and privacy of these\nVAs. Our analysis focused on five areas: code, access control, tracking, binary\nanalysis, and sensitive data confidentiality. The results revealed that these\nVAs are vulnerable to a range of security threats, including not validating SSL\ncertificates, executing raw SQL queries, and using a weak mode of the AES\nalgorithm. These vulnerabilities could allow malicious actors to gain\nunauthorized access to users' personal information. This study is a first step\ntoward understanding the risks associated with these technologies and provides\na foundation for future research to develop more secure and privacy-respecting\nVAs.",
        "translated": "虚拟助理(VAs)由于其易于用于日常任务，近年来得到了越来越多的使用。尽管它们越来越流行，但它们对安全和隐私的影响仍然没有得到很好的理解。为了弥补这一差距，我们进行了一项研究，以评估八个广泛使用的语音助手的安全和隐私姿态: Alexa，Braina，Cortana，谷歌助手，Kalliope，麦考夫，猎犬和极限。我们使用了三个漏洞测试工具 AndroBugs、 RiskInDroid 和 MobSF 来评估这些增值服务的安全性和隐私性。我们的分析集中在五个方面: 代码、访问控制、跟踪、二进制分析和敏感数据保密性。结果表明，这些增值服务容易受到一系列安全威胁，包括不验证 SSL 证书、执行原始 SQL 查询和使用弱模式的 AES 算法。这些漏洞可能允许恶意行为者未经授权访问用户的个人信息。这项研究是了解与这些技术相关的风险的第一步，并为未来研究开发更安全和尊重隐私的增值服务提供了基础。"
    },
    {
        "title": "Hierarchical Multi-Agent Reinforcement Learning for Assessing False-Data\n  Injection Attacks on Transportation Networks",
        "url": "http://arxiv.org/abs/2312.14625v1",
        "pub_date": "2023-12-22",
        "summary": "The increasing reliance of drivers on navigation applications has made\ntransportation networks more susceptible to data-manipulation attacks by\nmalicious actors. Adversaries may exploit vulnerabilities in the data\ncollection or processing of navigation services to inject false information,\nand to thus interfere with the drivers' route selection. Such attacks can\nsignificantly increase traffic congestions, resulting in substantial waste of\ntime and resources, and may even disrupt essential services that rely on road\nnetworks. To assess the threat posed by such attacks, we introduce a\ncomputational framework to find worst-case data-injection attacks against\ntransportation networks. First, we devise an adversarial model with a threat\nactor who can manipulate drivers by increasing the travel times that they\nperceive on certain roads. Then, we employ hierarchical multi-agent\nreinforcement learning to find an approximate optimal adversarial strategy for\ndata manipulation. We demonstrate the applicability of our approach through\nsimulating attacks on the Sioux Falls, ND network topology.",
        "translated": "司机对导航应用程序的日益依赖使得交通网络更容易受到恶意行为者的数据操纵攻击。对手可能利用数据收集或导航服务处理中的漏洞注入虚假信息，从而干扰驾驶员的路径选择。这种攻击会大大增加交通堵塞，造成大量的时间和资源浪费，甚至可能扰乱依赖道路网络的基本服务。为了评估此类攻击所构成的威胁，我们引入了一个计算框架来寻找针对交通网络的最坏情况数据注入攻击。首先，我们设计了一个对抗模型，模型中的威胁行为者可以通过增加驾驶员在特定道路上的行驶时间来操纵驾驶员。然后，我们使用层次化的多代理强化学习来找到一个近似最优的数据操作对抗策略。我们通过模拟对苏福尔斯网络拓扑的攻击来证明我们方法的适用性。"
    },
    {
        "title": "ChatGPT, Llama, can you write my report? An experiment on assisted\n  digital forensics reports written using (Local) Large Language Models",
        "url": "http://arxiv.org/abs/2312.14607v1",
        "pub_date": "2023-12-22",
        "summary": "Generative AIs, especially Large Language Models (LLMs) such as ChatGPT or\nLlama, have advanced significantly, positioning them as valuable tools for\ndigital forensics. While initial studies have explored the potential of ChatGPT\nin the context of investigations, the question of to what extent LLMs can\nassist the forensic report writing process remains unresolved. To answer the\nquestion, this article first examines forensic reports with the goal of\ngeneralization (e.g., finding the `average structure' of a report). We then\nevaluate the strengths and limitations of LLMs for generating the different\nparts of the forensic report using a case study. This work thus provides\nvaluable insights into the automation of report writing, a critical facet of\ndigital forensics investigations. We conclude that combined with thorough\nproofreading and corrections, LLMs may assist practitioners during the report\nwriting process but at this point cannot replace them.",
        "translated": "生成式人工智能，特别是大型语言模型(LLM) ，如 ChatGPT 或美洲驼，已经取得了显著的进步，将它们定位为数位鑑识的宝贵工具。虽然初步研究探讨了 ChatGPT 在调查方面的潜力，但长期法律援助在多大程度上可以协助法医报告编写过程的问题仍然没有得到解决。为了回答这个问题，本文首先以概括为目标检验法医报告(例如，找出报告的“平均结构”)。然后，我们通过案例研究评估 LLM 在生成法医报告的不同部分方面的优势和局限性。因此，这项工作提供了有价值的见解自动化的报告书写，一个关键的方面的数位鑑识调查。我们的结论是，结合彻底的校对和更正，LLM 可以帮助从业人员在撰写报告的过程中，但在这一点上不能取代他们。"
    },
    {
        "title": "Strategic Bidding Wars in On-chain Auctions",
        "url": "http://arxiv.org/abs/2312.14510v1",
        "pub_date": "2023-12-22",
        "summary": "The Ethereum block-building process has changed significantly since the\nemergence of Proposer-Builder Separation. Validators access blocks through a\nmarketplace, where block builders bid for the right to construct the block and\nearn MEV (Maximal Extractable Value) rewards in an on-chain competition, known\nas the MEV-boost auction. While more than 90% of blocks are currently built via\nMEV-Boost, trade-offs between builders' strategic behaviors and auction design\nremain poorly understood. In this paper we address this gap. We introduce a\ngame-theoretic model for MEV-Boost auctions and use simulations to study\ndifferent builders' bidding strategies observed in practice. We study various\nstrategic interactions and auction setups and evaluate how the interplay\nbetween critical elements such as access to MEV opportunities and improved\nconnectivity to relays impact bidding performance. Our results demonstrate the\nimportance of latency on the effectiveness of builders' strategies and the\noverall auction outcome from the proposer's perspective.",
        "translated": "自从建议者-建造者分离出现以来，以太坊的建造过程发生了重大变化。验证者通过一个市场进入区块，在这里区块建造者竞标建造区块的权利，并获得 MEV (最大可提取价值)奖励在一个连锁竞争中，被称为 MEV 提高拍卖。虽然目前超过90% 的街区是通过 MEV-Boost 建造的，但建筑商的战略行为和拍卖设计之间的权衡仍然知之甚少。在本文中，我们处理这一差距。本文提出了一个基于博弈论的 MEV-Boost 拍卖模型，并通过仿真研究了不同建筑商在实际拍卖中的投标策略。我们研究各种战略互动和拍卖设置，并评估关键因素之间的相互作用，如获得 MEV 的机会和改善连接的继电器影响投标业绩。我们的研究结果从提出者的角度证明了延迟对建筑商策略的有效性和整体拍卖结果的重要性。"
    },
    {
        "title": "Concurrent Asynchronous Byzantine Agreement in Expected-Constant Rounds,\n  Revisited",
        "url": "http://arxiv.org/abs/2312.14506v1",
        "pub_date": "2023-12-22",
        "summary": "It is well known that without randomization, Byzantine agreement (BA)\nrequires a linear number of rounds in the synchronous setting, while it is flat\nout impossible in the asynchronous setting. The primitive which allows to\nbypass the above limitation is known as oblivious common coin (OCC). It allows\nparties to agree with constant probability on a random coin, where agreement is\noblivious, i.e., players are not aware whether or not agreement has been\nachieved.\n  The starting point of our work is the observation that no known protocol\nexists for information-theoretic multi-valued OCC with optimal resiliency in\nthe asynchronous setting (with eventual message delivery). This apparent hole\nin the literature is particularly problematic, as multi-valued OCC is\nimplicitly or explicitly used in several constructions.\n  In this paper, we present the first information-theoretic multi-valued OCC\nprotocol in the asynchronous setting with optimal resiliency, i.e., tolerating\n$t &lt; n/3$ corruptions, thereby filling this important gap. Further, our\nprotocol efficiently implements OCC with an exponential-size domain, a property\nwhich is not even achieved by known constructions in the simpler, synchronous\nsetting.\n  We then turn to the problem of round-preserving parallel composition of\nasynchronous BA. A protocol for this task was proposed by Ben-Or and El-Yaniv\n[Distributed Computing '03]. Their construction, however, is flawed in several\nways. Thus, as a second contribution, we provide a simpler, more modular\nprotocol for the above task. Finally, and as a contribution of independent\ninterest, we provide proofs in Canetti's Universal Composability framework;\nthis makes our work the first one offering composability guarantees, which are\nimportant as BA is a core building block of secure multi-party computation\nprotocols.",
        "translated": "众所周知，如果没有随机化，拜占庭协议(BA)在同步设置中需要线性轮数，而在异步设置中则是不可能的。允许绕过上述限制的原语称为遗忘共同硬币(OCC)。它允许各方在一个随机的硬币上以常数概率达成一致意见，即玩家不知道是否已经达成了一致意见。我们工作的出发点是观察到信息论多值 OCC 不存在已知的协议，这种协议在异步设置中具有最佳弹性(最终消息传递)。文献中这个明显的漏洞是特别有问题的，因为多值 OCC 隐式或显式地用于几个结构。在本文中，我们提出了第一个信息论多值 OCC 协议在异步环境下具有最佳弹性，即，容忍 $t < n/3 $损坏，从而填补了这一重要的空白。此外，我们的协议有效地实现了一个指数大小的域的 OCC，这一特性甚至不能通过已知的构造在更简单的同步设置中实现。然后我们讨论了异步 BA 的保圆并行复合问题。本-奥尔和埃尔-亚尼夫[2003年分布式计算]为这项任务提出了一个协议。然而，它们的结构在几个方面存在缺陷。因此，作为第二个贡献，我们为上述任务提供了一个更简单、更模块化的协议。最后，作为独立利益的贡献，我们在 Canetti 的通用可组合性框架中提供了证明，这使我们的工作成为第一个提供可组合性保证的工作，这很重要，因为 BA 是安全多方计算协议的核心构件。"
    },
    {
        "title": "MetaAID 2.5: A Secure Framework for Developing Metaverse Applications\n  via Large Language Models",
        "url": "http://arxiv.org/abs/2312.14480v1",
        "pub_date": "2023-12-22",
        "summary": "Large language models (LLMs) are increasingly being used in Metaverse\nenvironments to generate dynamic and realistic content and to control the\nbehavior of non-player characters (NPCs). However, the cybersecurity concerns\nassociated with LLMs have become increasingly prominent. Previous research has\nprimarily focused on patching system vulnerabilities to enhance cybersecurity,\nbut these approaches are not well-suited to the Metaverse, where the virtual\nspace is more complex, LLMs are vulnerable, and ethical user interaction is\ncritical. Moreover, the scope of cybersecurity in the Metaverse is expected to\nexpand significantly. This paper proposes a method for enhancing cybersecurity\nthrough the simulation of user interaction with LLMs. Our goal is to educate\nusers and strengthen their defense capabilities through exposure to a\ncomprehensive simulation system. This system includes extensive Metaverse\ncybersecurity Q&amp;A and attack simulation scenarios. By engaging with these,\nusers will improve their ability to recognize and withstand risks.\nAdditionally, to address the ethical implications of user input, we propose\nusing LLMs as evaluators to assess user content across five dimensions. We\nfurther adapt the models through vocabulary expansion training to better\nunderstand personalized inputs and emoticons. We conduct experiments on\nmultiple LLMs and find that our approach is effective.",
        "translated": "大型语言模型(LLM)越来越多地被用于 Metaverse 环境中，以生成动态和真实的内容，并控制非玩家角色(NPC)的行为。然而，与 LLM 相关的网络安全问题日益突出。以前的研究主要集中在修补系统漏洞以增强网络安全，但这些方法并不适合元宇宙，因为元宇宙中的虚拟空间更加复杂，LLM 易受攻击，合乎道德的用户交互至关重要。此外，元宇宙的网络安全范围预计将显著扩大。提出了一种通过仿真用户与 LLM 的交互来提高网络安全性的方法。我们的目标是教育用户和加强他们的防御能力，通过暴露在一个全面的模拟系统。该系统包括广泛的 Metaverse 网络安全问答和攻击模拟场景。通过参与这些活动，用户将提高识别和承受风险的能力。此外，为了解决用户输入的伦理含义，我们建议使用 LLM 作为评估者来评估五个维度的用户内容。我们通过词汇扩展训练进一步调整模型，以更好地理解个性化输入和表情符号。我们在多个 LLM 上进行了实验，发现我们的方法是有效的。"
    },
    {
        "title": "A Note on Output Length of One-Way State Generators",
        "url": "http://arxiv.org/abs/2312.16025v1",
        "pub_date": "2023-12-26",
        "summary": "We study output length of one-way state generators (OWSGs) and their weaker\nvariants.\n  - Standard OWSGs. Recently, Cavalar et al. (arXiv:2312.08363) give OWSGs with\n$m$-qubit outputs for any $m=\\omega(\\log \\lambda)$, where $\\lambda$ is the\nsecurity parameter, and conjecture that there do not exist OWSGs with $O(\\log\n\\log \\lambda)$-qubit outputs. We prove their conjecture in a stronger manner by\nshowing that there do not exist OWSGs with $O(\\log \\lambda)$-qubit outputs.\nThis means that their construction is optimal in terms of output length.\n  - Constant-advantage OWSGs. Let $\\epsilon$-OWSGs be a parameterized variant\nof OWSGs where a quantum polynomial-time adversary's advantage is at most\n$\\epsilon$. For any constant $\\epsilon&gt;0$, we construct $\\epsilon$-OWSGs with\n$O(\\log \\log \\lambda)$-qubit outputs assuming the existence of subexponentially\nsecure OWFs. We show that this is almost tight by proving that there do not\nexist $O(1)$-OWSGs with $(\\log \\log \\lambda)/2+O(1)$-qubit outputs.\n  - Weak OWSGs. We refer to $(1-1/\\mathsf{poly}(\\lambda))$-OWSGs as weak OWSGs.\nWe construct weak OWSGs with $m$-qubit outputs for any $m=\\omega(1)$ assuming\nthe existence of exponentially secure injective OWFs with linear expansion. We\nshow that this is tight by proving that there do not exist weak OWSGs with\n$O(1)$-qubit outputs.",
        "translated": "我们研究了单向状态发生器(OWSG)的输出长度及其弱变量。- 标准工作小组。最近，Cavalar 等人(arXiv: 2312.08363)给出了任何 $m = omega (log lambda) $的 $m $- 量子位输出的 OWSG，其中 $lambda $是安全参数，并推测不存在具有 $O (log log lambda) $- 量子位输出的 OWSG。我们通过展示不存在具有 $O (log lambda) $- qubit 输出的 OWSG 来更有力地证明它们的猜想。这意味着它们的结构在输出长度方面是最优的。- 持续优势 OWSG。让 $epsilon $- OWSG 成为 OWSG 的参数化变体，其中量子多项式时间对手的优势最多为 $epsilon $。对于任何常数 $epsilon > 0 $，我们假设存在次指数安全的 OWF，构造具有 $O (log log lambda) $- qubit 输出的 $epsilon $- OWSG。通过证明不存在具有 $(log log lambda)/2 + O (1) $- qubit 输出的 $O (1) $- OWSG，我们表明这几乎是紧密的。- 弱小的职业安全及保安队。我们将 $(1-1/mathsf { poly}(lambda)) $- OWSG 称为弱 OWSG。我们假设存在线性展开的指数安全内射 OWF，对任意 $m = omega (1) $，构造了具有 $m $- 量子位输出的弱 OWSG。我们通过证明不存在具有 $O (1) $- 量子位输出的弱 OWSG 来证明这一点。"
    },
    {
        "title": "A fully decentralized auditing approach for edge computing: A\n  Game-Theoretic Perspective",
        "url": "http://arxiv.org/abs/2312.16007v1",
        "pub_date": "2023-12-26",
        "summary": "Edge storage presents a viable data storage alternative for application\nvendors (AV), offering benefits such as reduced bandwidth overhead and latency\ncompared to cloud storage. However, data cached in edge computing systems is\nsusceptible to intentional or accidental disturbances. This paper proposes a\ndecentralized integrity auditing scheme to safeguard data integrity and counter\nthe traditional reliance on centralized third-party auditors (TPA), which are\nunfit for distributed systems. Our novel approach employs edge servers (ES) as\nmutual auditors, eliminating the need for a centralized entity. This\ndecentralization minimizes potential collusion with malicious auditors and\nbiases in audit outcomes. Using a strategic game model, we demonstrate that ESs\nare more motivated to audit each other than TPAs. The auditing process is\naddressed as a Nash Equilibrium problem, assuring accurate integrity proof\nthrough incentives for ESs. Our scheme's security and performance are\nrigorously assessed, showing it is secure within the random oracle model,\noffers improved speed, and is cost-effective compared to existing methods.",
        "translated": "边缘存储为应用程序供应商(AV)提供了一种可行的数据存储替代方案，与云存储相比，边缘存储提供了诸如减少带宽开销和延迟等好处。然而，边缘计算系统中缓存的数据容易受到有意或偶然的干扰。本文提出了一种分散式完整性审计方案，以保证数据的完整性，克服传统审计方法对集中式第三方审计师(TPA)的依赖，这种方法不适合于分布式系统。我们的新方法使用边缘服务器(ES)作为相互审计，消除了对集中实体的需要。这项地方分权可减少与恶意核数师的潜在合谋及审核结果的偏差。通过一个战略博弈模型，我们证明了 ESs 比 TPA 更有动力去审计对方。审核过程被视为一个纳什均衡点问题，通过激励 ESs 来确保准确的完整性证明。我们对方案的安全性和性能进行了严格的评估，表明它在随机预言模型中是安全的，提供了改进的速度，并且与现有方法相比具有成本效益。"
    },
    {
        "title": "Cryptoanalysis McEliece-type cryptosystem based on correction of errors\n  and erasures",
        "url": "http://arxiv.org/abs/2312.15912v1",
        "pub_date": "2023-12-26",
        "summary": "Krouk, Tavernier and Kabatiansky proposed new variants of the McEliece\ncryptosystem. In this letter, it is shown that cryptosystem based on correction\nof errors erasures is equal to the Mc-Eliece cryptosystem with worse parametrs\npublic key. It will also add an organic extension of the authors' idea,\nalthough one that has its flaws...",
        "translated": "Krouk，Tavernier 和 Kabatiansky 提出了 McEliece 密码系统的新变体。本文证明了基于纠错消除的密码体制等同于参数较差的 Mc-Eliece 密码体制。它也将增加一个有机扩展的作者的想法，虽然一个它的缺陷..。"
    },
    {
        "title": "Reinforcement Unlearning",
        "url": "http://arxiv.org/abs/2312.15910v1",
        "pub_date": "2023-12-26",
        "summary": "Machine unlearning refers to the process of mitigating the influence of\nspecific training data on machine learning models based on removal requests\nfrom data owners. However, one important area that has been largely overlooked\nin the research of unlearning is reinforcement learning. Reinforcement learning\nfocuses on training an agent to make optimal decisions within an environment to\nmaximize its cumulative rewards. During the training, the agent tends to\nmemorize the features of the environment, which raises a significant concern\nabout privacy. As per data protection regulations, the owner of the environment\nholds the right to revoke access to the agent's training data, thus\nnecessitating the development of a novel and pressing research field, known as\n\\emph{reinforcement unlearning}. Reinforcement unlearning focuses on revoking\nentire environments rather than individual data samples. This unique\ncharacteristic presents three distinct challenges: 1) how to propose unlearning\nschemes for environments; 2) how to avoid degrading the agent's performance in\nremaining environments; and 3) how to evaluate the effectiveness of unlearning.\nTo tackle these challenges, we propose two reinforcement unlearning methods.\nThe first method is based on decremental reinforcement learning, which aims to\nerase the agent's previously acquired knowledge gradually. The second method\nleverages environment poisoning attacks, which encourage the agent to learn\nnew, albeit incorrect, knowledge to remove the unlearning environment.\nParticularly, to tackle the third challenge, we introduce the concept of\n``environment inference attack'' to evaluate the unlearning outcomes. The\nsource code is available at\n\\url{https://anonymous.4open.science/r/Reinforcement-Unlearning-D347}.",
        "translated": "机器学习是指基于数据所有者的删除请求，减轻特定训练数据对机器学习模型的影响的过程。然而，在遗忘研究中一个被忽视的重要领域是强化学习。强化学习专注于训练代理人在一个环境中做出最佳决策，以最大化其累积回报。在训练过程中，代理倾向于记忆环境的特征，这引起了对隐私的重大关注。根据数据保护条例，环境所有者有权撤销对代理人训练数据的访问，因此需要开发一个新的、紧迫的研究领域，称为强化忘却学习。强化取消学习关注于撤销整个环境，而不是单个数据样本。这种独特的特性提出了三个明显的挑战: 1)如何为环境提出忘记方案; 2)如何避免在剩余环境中降低代理的性能; 3)如何评估忘记的有效性。为了应对这些挑战，我们提出了两种强化去学习方法。第一种方法是基于递减强化学习，目的是逐渐清除代理人先前获得的知识。第二种方法利用环境中毒攻击，鼓励代理学习新的(尽管是不正确的)知识，以消除忘记学习的环境。特别地，为了解决第三个挑战，我们引入了“环境推理攻击”的概念来评估遗忘的结果。源代码可在 url { https://anonymous.4open.science/r/reinforcement-unlearning-d347}获得。"
    },
    {
        "title": "Punctuation Matters! Stealthy Backdoor Attack for Language Models",
        "url": "http://arxiv.org/abs/2312.15867v1",
        "pub_date": "2023-12-26",
        "summary": "Recent studies have pointed out that natural language processing (NLP) models\nare vulnerable to backdoor attacks. A backdoored model produces normal outputs\non the clean samples while performing improperly on the texts with triggers\nthat the adversary injects. However, previous studies on textual backdoor\nattack pay little attention to stealthiness. Moreover, some attack methods even\ncause grammatical issues or change the semantic meaning of the original texts.\nTherefore, they can easily be detected by humans or defense systems. In this\npaper, we propose a novel stealthy backdoor attack method against textual\nmodels, which is called \\textbf{PuncAttack}. It leverages combinations of\npunctuation marks as the trigger and chooses proper locations strategically to\nreplace them. Through extensive experiments, we demonstrate that the proposed\nmethod can effectively compromise multiple models in various tasks. Meanwhile,\nwe conduct automatic evaluation and human inspection, which indicate the\nproposed method possesses good performance of stealthiness without bringing\ngrammatical issues and altering the meaning of sentences.",
        "translated": "最近的研究指出，自然语言处理(NLP)模型容易受到后门攻击。一个后门模型产生正常输出的干净样本，而执行不当的文本与触发器，对手注入。然而，以往关于文本后门攻击的研究很少关注隐蔽性。此外，一些攻击方法甚至会引起语法问题或改变原文的语义。因此，它们很容易被人类或防御系统发现。本文提出了一种新的针对文本模型的隐蔽后门攻击方法，称为 textbf { Punc冷攻击}。它利用句读的组合作为触发点，并战略性地选择合适的位置来替代它们。通过大量的实验证明，该方法能够有效地折衷不同任务中的多个模型。同时进行了自动评价和人工检测，表明该方法在不引起语法问题和改变句子意义的情况下，具有良好的隐蔽性。"
    },
    {
        "title": "SecQA: A Concise Question-Answering Dataset for Evaluating Large\n  Language Models in Computer Security",
        "url": "http://arxiv.org/abs/2312.15838v1",
        "pub_date": "2023-12-26",
        "summary": "In this paper, we introduce SecQA, a novel dataset tailored for evaluating\nthe performance of Large Language Models (LLMs) in the domain of computer\nsecurity. Utilizing multiple-choice questions generated by GPT-4 based on the\n\"Computer Systems Security: Planning for Success\" textbook, SecQA aims to\nassess LLMs' understanding and application of security principles. We detail\nthe structure and intent of SecQA, which includes two versions of increasing\ncomplexity, to provide a concise evaluation across various difficulty levels.\nAdditionally, we present an extensive evaluation of prominent LLMs, including\nGPT-3.5-Turbo, GPT-4, Llama-2, Vicuna, Mistral, and Zephyr models, using both\n0-shot and 5-shot learning settings. Our results, encapsulated in the SecQA v1\nand v2 datasets, highlight the varying capabilities and limitations of these\nmodels in the computer security context. This study not only offers insights\ninto the current state of LLMs in understanding security-related content but\nalso establishes SecQA as a benchmark for future advancements in this critical\nresearch area.",
        "translated": "在本文中，我们介绍了 SecQA，一个新的数据集，专门用于评估大型语言模型(LLM)在计算机安全领域的性能。SecQA 利用 GPT-4根据《计算机系统安全: 成功规划》教科书产生的多项选择题，旨在评估 LLM 对安全原则的理解和应用。我们详细介绍了 SecQA 的结构和意图，其中包括两个版本的日益增加的复杂性，以提供一个跨越不同难度级别的简明评估。此外，我们还使用0-shot 和5-shot 学习设置，对著名的 LLM 进行了广泛的评估，包括 GPT-3.5-Turbo，GPT-4，lama-2，Vicuna，mistro 和 Zephyr 模型。我们的结果封装在 SecQA v1和 v2数据集中，突出了这些模型在计算机安全上下文中的不同功能和局限性。这项研究不仅为深入了解 LLM 在理解安全相关内容方面的现状提供了见解，而且还将 SecQA 作为这一关键研究领域未来发展的基准。"
    },
    {
        "title": "GanFinger: GAN-Based Fingerprint Generation for Deep Neural Network\n  Ownership Verification",
        "url": "http://arxiv.org/abs/2312.15617v1",
        "pub_date": "2023-12-25",
        "summary": "Deep neural networks (DNNs) are extensively employed in a wide range of\napplication scenarios. Generally, training a commercially viable neural network\nrequires significant amounts of data and computing resources, and it is easy\nfor unauthorized users to use the networks illegally. Therefore, network\nownership verification has become one of the most crucial steps in safeguarding\ndigital assets. To verify the ownership of networks, the existing network\nfingerprinting approaches perform poorly in the aspects of efficiency,\nstealthiness, and discriminability. To address these issues, we propose a\nnetwork fingerprinting approach, named as GanFinger, to construct the network\nfingerprints based on the network behavior, which is characterized by network\noutputs of pairs of original examples and conferrable adversarial examples.\nSpecifically, GanFinger leverages Generative Adversarial Networks (GANs) to\neffectively generate conferrable adversarial examples with imperceptible\nperturbations. These examples can exhibit identical outputs on copyrighted and\npirated networks while producing different results on irrelevant networks.\nMoreover, to enhance the accuracy of fingerprint ownership verification, the\nnetwork similarity is computed based on the accuracy-robustness distance of\nfingerprint examples'outputs. To evaluate the performance of GanFinger, we\nconstruct a comprehensive benchmark consisting of 186 networks with five\nnetwork structures and four popular network post-processing techniques. The\nbenchmark experiments demonstrate that GanFinger significantly outperforms the\nstate-of-the-arts in efficiency, stealthiness, and discriminability. It\nachieves a remarkable 6.57 times faster in fingerprint generation and boosts\nthe ARUC value by 0.175, resulting in a relative improvement of about 26%.",
        "translated": "深层神经网络(DNN)广泛应用于各种应用场合。一般来说，训练一个商业上可行的神经网络需要大量的数据和计算资源，而且未经授权的用户很容易非法使用网络。因此，网络所有权验证已成为保护数字资产的关键步骤之一。为了验证网络的所有权，现有的网络指纹识别方法在效率、隐蔽性和可识别性方面表现不佳。为了解决这些问题，我们提出了一种基于网络行为的网络指纹识别方法，称为 GanFinger，该方法将网络输出的原始样本和可授予的对手样本进行拥有属性化。具体来说，GanFinger 利用生成对抗网络(GANs)有效地生成具有不可察觉扰动的可授权对抗例子。这些例子可以在版权和盗版网络上显示相同的输出，而在不相关的网络上产生不同的结果。此外，为了提高指纹所有权验证的准确性，基于指纹样本输出的准确性-鲁棒性距离计算网络相似度。为了评价 GanFinger 的性能，我们构建了一个由186个具有5种网络结构的网络和4种流行的网络后处理技术组成的综合基准。基准实验表明，GanFinger 在效率、隐蔽性和区分能力方面明显优于最新技术。该方法指纹生成速度提高了6.57倍，ARUC 值提高了0.175，相对提高了26% 左右。"
    },
    {
        "title": "Federated learning-outcome prediction with multi-layer privacy\n  protection",
        "url": "http://arxiv.org/abs/2312.15608v1",
        "pub_date": "2023-12-25",
        "summary": "Learning-outcome prediction (LOP) is a long-standing and critical problem in\neducational routes. Many studies have contributed to developing effective\nmodels while often suffering from data shortage and low generalization to\nvarious institutions due to the privacy-protection issue. To this end, this\nstudy proposes a distributed grade prediction model, dubbed FecMap, by\nexploiting the federated learning (FL) framework that preserves the private\ndata of local clients and communicates with others through a global generalized\nmodel. FecMap considers local subspace learning (LSL), which explicitly learns\nthe local features against the global features, and multi-layer privacy\nprotection (MPP), which hierarchically protects the private features, including\nmodel-shareable features and not-allowably shared features, to achieve\nclient-specific classifiers of high performance on LOP per institution. FecMap\nis then achieved in an iteration manner with all datasets distributed on\nclients by training a local neural network composed of a global part, a local\npart, and a classification head in clients and averaging the global parts from\nclients on the server. To evaluate the FecMap model, we collected three\nhigher-educational datasets of student academic records from engineering\nmajors. Experiment results manifest that FecMap benefits from the proposed LSL\nand MPP and achieves steady performance on the task of LOP, compared with the\nstate-of-the-art models. This study makes a fresh attempt at the use of\nfederated learning in the learning-analytical task, potentially paving the way\nto facilitating personalized education with privacy protection.",
        "translated": "学习结果预测(LOP)是教育路径中一个长期存在的关键问题。许多研究有助于开发有效的模型，但由于隐私保护问题，这些模型往往存在数据短缺和对各种机构的推广程度低的问题。为此，本研究提出了一种分布式成绩预测模型，称为 FecMap，该模型利用联邦学习(FL)框架来保存本地客户的私有数据，并通过一个全局通用模型与他人进行通信。FecMap 考虑局部子空间学习(LSL)和多层隐私保护(MPP) ，前者针对全局特征显式地学习局部特征，后者分层保护包括模型共享特征和不允许共享特征在内的私有特征，以实现每个机构 LOP 上高性能的客户特定分类器。然后，通过训练由客户端的全局部分、局部部分和分类头组成的局部神经网络，并对服务器上的客户端的全局部分进行平均，以迭代的方式实现分布在客户端上的所有数据集的 FecMap。为了评价 FecMap 模型，我们收集了三个来自工科专业学生的高等教育学习成绩数据集。实验结果表明，与现有的 LSL 和 MPP 模型相比，FecMap 在 LOP 任务上取得了稳定的性能。本研究对联邦学习在学习分析任务中的应用做出了新的尝试，为个性化教育和隐私保护的实现铺平了道路。"
    },
    {
        "title": "Mining Domain-Based Policies",
        "url": "http://arxiv.org/abs/2312.15596v1",
        "pub_date": "2023-12-25",
        "summary": "Protection domains are one of the most enduring concepts in Access Control.\nEntities with identical access control characteristics are grouped under the\nsame protection domain, and domain-based policies assign access privileges to\nthe protection domain as a whole. With the advent of the Internet of Things\n(IoT), devices play the roles of both subjects and objects. Domain-based\npolicies are particularly suited to support this symmetry of roles.\n  This paper studies the mining of domain-based policies from incomplete access\nlogs. We began by building a theory of domain-based policies, resulting in a\npolynomial-time algorithm that constructs the optimal domain-based policy out\nof a given access control matrix. We then showed that the problem of\ndomain-based policy mining (DBPM) and the related problem of mining policies\nfor domain and type enforcement (DTEPM) are both NP-complete. Next, we looked\nat the practical problem of using a MaxSAT solver to solve DBPM. We devised\nsophisticated encodings for this purpose, and empirically evaluated their\nrelative performance. This paper thus lays the groundwork for future study of\nDBPM.",
        "translated": "保护域是访问控制中最经久不衰的概念之一。具有相同访问控制特征的实体被分组到相同的保护域下，基于域的策略将访问权限分配给整个保护域。随着物联网(IoT)的出现，设备扮演着主体和客体的双重角色。基于域的策略特别适合支持这种角色对称性。本文研究了从不完全访问日志中挖掘基于域的策略。我们开始建立一个领域为基础的政策理论，导致一个多项式时间算法，构造出最佳的领域为基础的政策给定的存取控制矩阵。然后我们证明了基于领域的策略挖掘问题(DBPM)和相关的领域和类型强制策略挖掘问题(DTEPM)都是 NP 完全的。接下来，我们研究了使用 MaxSAT 求解器解决 DBPM 的实际问题。我们为此设计了复杂的编码，并根据经验评估了它们的相对性能。本文的研究为 DBPM 的进一步研究奠定了基础。"
    },
    {
        "title": "Privacy-Preserving Neural Graph Databases",
        "url": "http://arxiv.org/abs/2312.15591v1",
        "pub_date": "2023-12-25",
        "summary": "In the era of big data and rapidly evolving information systems, efficient\nand accurate data retrieval has become increasingly crucial. Neural graph\ndatabases (NGDBs) have emerged as a powerful paradigm that combines the\nstrengths of graph databases (graph DBs) and neural networks to enable\nefficient storage, retrieval, and analysis of graph-structured data. The usage\nof neural embedding storage and complex neural logical query answering provides\nNGDBs with generalization ability. When the graph is incomplete, by extracting\nlatent patterns and representations, neural graph databases can fill gaps in\nthe graph structure, revealing hidden relationships and enabling accurate query\nanswering. Nevertheless, this capability comes with inherent trade-offs, as it\nintroduces additional privacy risks to the database. Malicious attackers can\ninfer more sensitive information in the database using well-designed\ncombinatorial queries, such as by comparing the answer sets of where Turing\nAward winners born before 1950 and after 1940 lived, the living places of\nTuring Award winner Hinton are probably exposed, although the living places may\nhave been deleted in the training due to the privacy concerns. In this work,\ninspired by the privacy protection in graph embeddings, we propose a\nprivacy-preserving neural graph database (P-NGDB) to alleviate the risks of\nprivacy leakage in NGDBs. We introduce adversarial training techniques in the\ntraining stage to force the NGDBs to generate indistinguishable answers when\nqueried with private information, enhancing the difficulty of inferring\nsensitive information through combinations of multiple innocuous queries.\nExtensive experiment results on three datasets show that P-NGDB can effectively\nprotect private information in the graph database while delivering high-quality\npublic answers responses to queries.",
        "translated": "在海量数据和信息系统快速发展的时代，高效、准确的数据检索已经变得越来越重要。神经图形数据库(NGDB)已经成为一种强大的范式，它结合了图形数据库(GDB)和神经网络的优势，能够有效地存储、检索和分析图形结构数据。神经嵌入存储和复杂神经逻辑查询应答的使用为 NGDB 提供了泛化能力。当图不完整时，神经图数据库通过提取潜在模式和表征，可以填补图结构中的空白，揭示隐藏的关系，实现准确的查询回答。尽管如此，这种能力带来了内在的权衡，因为它给数据库带来了额外的隐私风险。恶意攻击者可以通过精心设计的组合查询在数据库中推断出更敏感的信息，比如通过比较图灵奖获得者在1950年之前出生和1940年之后居住的地方，图灵奖获得者 Hinton 的生活场所可能会暴露出来，尽管由于隐私问题，生活场所可能已经在训练中被删除。本文受到图嵌入中隐私保护的启发，提出了一种保护隐私的神经图数据库(P-NGDB) ，以降低 NGDB 中隐私泄露的风险。我们在训练阶段引入对抗性训练技术，迫使 NGDB 在用私人信息查询时生成难以区分的答案，通过多个无害查询的组合增加推断敏感信息的难度。在三个数据集上的大量实验结果表明，P-NGDB 能够有效地保护图形数据库中的私有信息，同时对查询提供高质量的公开答复。"
    },
    {
        "title": "Scalable and automated Evaluation of Blue Team cyber posture in Cyber\n  Ranges",
        "url": "http://arxiv.org/abs/2312.17221v1",
        "pub_date": "2023-12-28",
        "summary": "Cyber ranges are virtual training ranges that have emerged as indispensable\nenvironments for conducting secure exercises and simulating real or\nhypothetical scenarios. These complex computational infrastructures enable the\nsimulation of attacks, facilitating the evaluation of defense tools and\nmethodologies and developing novel countermeasures against threats. One of the\nmain challenges of cyber range scalability is the exercise evaluation that\noften requires the manual intervention of human operators, the White team. This\npaper proposes a novel approach that uses Blue and Red team reports and\nwell-known databases to automate the evaluation and assessment of the exercise\noutcomes, overcoming the limitations of existing assessment models. Our\nproposal encompasses evaluating various aspects and metrics, explicitly\nemphasizing Blue Teams' actions and strategies and allowing the automated\ngeneration of their cyber posture.",
        "translated": "网络范围是虚拟的训练范围，已成为进行安全演习和模拟真实或假设情景的不可或缺的环境。这些复杂的计算基础设施能够模拟攻击，促进防御工具和方法的评估，并开发新的对抗威胁的对策。网络范围可扩展性的主要挑战之一是练习评估，这通常需要人工操作员白队的干预。本文提出了一种新的方法，利用蓝色和红色的团队报告和著名的数据库，自动评估和评估的演习结果，克服现有的评估模型的局限性。我们的建议包括评估各个方面和指标，明确强调蓝队的行动和战略，并允许自动生成他们的网络态势。"
    },
    {
        "title": "Securing NextG Systems against Poisoning Attacks on Federated Learning:\n  A Game-Theoretic Solution",
        "url": "http://arxiv.org/abs/2312.17164v1",
        "pub_date": "2023-12-28",
        "summary": "This paper studies the poisoning attack and defense interactions in a\nfederated learning (FL) system, specifically in the context of wireless signal\nclassification using deep learning for next-generation (NextG) communications.\nFL collectively trains a global model without the need for clients to exchange\ntheir data samples. By leveraging geographically dispersed clients, the trained\nglobal model can be used for incumbent user identification, facilitating\nspectrum sharing. However, in this distributed learning system, the presence of\nmalicious clients introduces the risk of poisoning the training data to\nmanipulate the global model through falsified local model exchanges. To address\nthis challenge, a proactive defense mechanism is employed in this paper to make\ninformed decisions regarding the admission or rejection of clients\nparticipating in FL systems. Consequently, the attack-defense interactions are\nmodeled as a game, centered around the underlying admission and poisoning\ndecisions. First, performance bounds are established, encompassing the best and\nworst strategies for attackers and defenders. Subsequently, the attack and\ndefense utilities are characterized within the Nash equilibrium, where no\nplayer can unilaterally improve its performance given the fixed strategies of\nothers. The results offer insights into novel operational modes that safeguard\nFL systems against poisoning attacks by quantifying the performance of both\nattacks and defenses in the context of NextG communications.",
        "translated": "本文研究了联邦学习(FL)系统中的中毒攻击和防御交互，特别是在下一代(NextG)通信中使用深度学习进行无线信号分类的情况下。FL 集体训练一个全球模型，而不需要客户交换他们的数据样本。通过利用地理上分散的客户端，经过训练的全球模型可以用于在职用户识别，促进频谱共享。然而，在这个分布式学习系统中，恶意客户端的存在引入了中毒训练数据的风险，通过伪造的局部模型交换操纵全局模型。为了应对这一挑战，本文采用了一种积极的防御机制，对参与 FL 系统的客户的接纳或拒绝做出明智的决定。因此，攻防互动被建模为一个游戏，围绕着潜在的承认和中毒决定。首先，建立性能界限，包括攻击者和防守者的最佳和最差策略。随后，攻击和防御工具的特点是在纳什均衡点内，没有球员可以单方面提高其性能给予其他固定的战略。研究结果通过量化 NextG 通信环境中攻击和防御的性能，为保护 FL 系统免受中毒攻击的新型操作模式提供了见解。"
    },
    {
        "title": "Kirchhoff-Law Johnson Noise Meets Web 3.0: A Statistical Physical Method\n  of Random Key Generation for Decentralized Identity Protocols",
        "url": "http://arxiv.org/abs/2312.17113v1",
        "pub_date": "2023-12-28",
        "summary": "This paper presents a statistical physical generation of random keys for a\ndecentralized identity ecosystem that uses Web 3.0 protocols. Web 3.0 is driven\nby secure keys, typically represented in hexadecimal, that are pseudo-randomly\ngenerated by an initialization vector and complex computational algorithms. We\ndemonstrate that the statistical physical Kirchhoff-law-Johnson-noise (KLJN)\nscheme eliminates the additional computational power by naturally generating\ntruly random binary keys to drive the creation of decentralized identifiers\n(DIDs) that are appended to an Ethereum blockchain.",
        "translated": "本文提出了一个使用 Web 3.0协议的分散身份生态系统的随机密钥的统计物理生成方法。Web 3.0由安全密钥驱动，通常以十六进制表示，这些密钥由初始向量复杂的计算算法伪随机生成。我们证明了统计物理 Kirchhoff-law-Johnson- 噪(KLJN)方案通過自然地產生真正的随机二进制密钥来驱动附加到以太区块链上的分散式標識符(DID)的產生而消除了额外的計算能力。"
    },
    {
        "title": "Multi-Tier Computing-Enabled Digital Twin in 6G Networks",
        "url": "http://arxiv.org/abs/2312.16999v1",
        "pub_date": "2023-12-28",
        "summary": "Digital twin (DT) is the recurrent and common feature in discussions about\nfuture technologies, bringing together advanced communication, computation, and\nartificial intelligence, to name a few. In the context of Industry 4.0,\nindustries such as manufacturing, automotive, and healthcare are rapidly\nadopting DT-based development. The main challenges to date have been the high\ndemands on communication and computing resources, as well as privacy and\nsecurity concerns, arising from the large volumes of data exchanges. To achieve\nlow latency and high security services in the emerging DT, multi-tier computing\nhas been proposed by combining edge/fog computing and cloud computing.\nSpecifically, low latency data transmission, efficient resource allocation, and\nvalidated security strategies of multi-tier computing systems are used to solve\nthe operational problems of the DT system. In this paper, we introduce the\narchitecture and applications of DT using examples from manufacturing, the\nInternet-of-Vehicles and healthcare. At the same time, the architecture and\ntechnology of multi-tier computing systems are studied to support DT. This\npaper will provide valuable reference and guidance for the theory, algorithms,\nand applications in collaborative multi-tier computing and DT.",
        "translated": "数字孪生兄弟(DT)是关于未来技术讨论中反复出现的共同特征，它将先进的通信、计算和人工智能等结合在一起。在行业4.0的背景下，制造业、汽车业和医疗保健等行业正在迅速采用基于 DT 的开发。迄今为止，主要的挑战是大量数据交换对通信和计算资源的高需求，以及对隐私和安全的关切。为了在新兴的 DT 中实现低延迟和高安全性的服务，将边缘/雾计算和云计算相结合，提出了多层计算。具体来说，低延迟数据传输、高效率的资源分配，以及经过验证的多层计算系统保安策略，都被用来解决数码地面电视系统的运作问题。本文以制造业、车辆互联网和医疗保健为例，介绍了 DT 的体系结构和应用。同时，研究了支持 DT 的多层计算系统的体系结构和技术。本文对协同多层计算和数据挖掘的理论、算法和应用提供了有价值的参考和指导。"
    },
    {
        "title": "BlackboxBench: A Comprehensive Benchmark of Black-box Adversarial\n  Attacks",
        "url": "http://arxiv.org/abs/2312.16979v1",
        "pub_date": "2023-12-28",
        "summary": "Adversarial examples are well-known tools to evaluate the vulnerability of\ndeep neural networks (DNNs). Although lots of adversarial attack algorithms\nhave been developed, it is still challenging in the practical scenario that the\nmodel's parameters and architectures are inaccessible to the\nattacker/evaluator, i.e., black-box adversarial attacks. Due to the practical\nimportance, there has been rapid progress from recent algorithms, reflected by\nthe quick increase in attack success rate and the quick decrease in query\nnumbers to the target model. However, there is a lack of thorough evaluations\nand comparisons among these algorithms, causing difficulties of tracking the\nreal progress, analyzing advantages and disadvantages of different technical\nroutes, as well as designing future development roadmap of this field. Thus, in\nthis work, we aim at building a comprehensive benchmark of black-box\nadversarial attacks, called BlackboxBench. It mainly provides: 1) a unified,\nextensible and modular-based codebase, implementing 25 query-based attack\nalgorithms and 30 transfer-based attack algorithms; 2) comprehensive\nevaluations: we evaluate the implemented algorithms against several\nmainstreaming model architectures on 2 widely used datasets (CIFAR-10 and a\nsubset of ImageNet), leading to 14,106 evaluations in total; 3) thorough\nanalysis and new insights, as well analytical tools. The website and source\ncodes of BlackboxBench are available at https://blackboxbench.github.io/ and\nhttps://github.com/SCLBD/BlackboxBench/, respectively.",
        "translated": "对抗性的例子是众所周知的工具，以评估深层神经网络(DNN)的脆弱性。尽管已经开发了大量的对抗性攻击算法，但是在实际场景中，模型的参数和体系结构不能被攻击者/评估者访问，即黑盒对抗性攻击仍然是一个挑战。由于实际应用的重要性，最近的算法取得了快速的进展，反映在攻击成功率的快速增长和查询数量对目标模型的快速减少。然而，这些算法之间缺乏深入的评价和比较，给跟踪实际进展、分析不同技术路线的优缺点以及设计该领域的未来发展路线带来了困难。因此，在这项工作中，我们的目标是建立一个黑盒对抗性攻击的综合基准，称为 BlackboxBench。它主要提供: 1)一个统一的、可扩展的、基于模块的代码库，实现了25个基于查询的攻击算法和30个基于转移的攻击算法; 2)综合评估: 我们评估了针对2个广泛使用的数据集(CIFAR-10和 ImageNet 的一个子集)上的几个主流模型架构的实现算法，导致总共14,106个评估; 3)彻底的分析和新的见解，以及分析工具。BlackboxBench 的网站及源码分别可于 https://BlackboxBench.github.io/及 https://github.com/sclbd/BlackboxBench/下载。"
    },
    {
        "title": "Attack Tree Analysis for Adversarial Evasion Attacks",
        "url": "http://arxiv.org/abs/2312.16957v1",
        "pub_date": "2023-12-28",
        "summary": "Recently, the evolution of deep learning has promoted the application of\nmachine learning (ML) to various systems. However, there are ML systems, such\nas autonomous vehicles, that cause critical damage when they misclassify.\nConversely, there are ML-specific attacks called adversarial attacks based on\nthe characteristics of ML systems. For example, one type of adversarial attack\nis an evasion attack, which uses minute perturbations called \"adversarial\nexamples\" to intentionally misclassify classifiers. Therefore, it is necessary\nto analyze the risk of ML-specific attacks in introducing ML base systems. In\nthis study, we propose a quantitative evaluation method for analyzing the risk\nof evasion attacks using attack trees. The proposed method consists of the\nextension of the conventional attack tree to analyze evasion attacks and the\nsystematic construction method of the extension. In the extension of the\nconventional attack tree, we introduce ML and conventional attack nodes to\nrepresent various characteristics of evasion attacks. In the systematic\nconstruction process, we propose a procedure to construct the attack tree. The\nprocedure consists of three steps: (1) organizing information about attack\nmethods in the literature to a matrix, (2) identifying evasion attack scenarios\nfrom methods in the matrix, and (3) constructing the attack tree from the\nidentified scenarios using a pattern. Finally, we conducted experiments on\nthree ML image recognition systems to demonstrate the versatility and\neffectiveness of our proposed method.",
        "translated": "近年来，深度学习的发展促进了机器学习在各种系统中的应用。然而，有机器学习系统，如自动驾驶汽车，造成重大损害时，他们错误的分类。相反地，根据机器学习系统的特点，有一种特定于机器学习的攻击叫做对抗性攻击。例如，一种类型的敌对攻击是规避攻击，它使用被称为“敌对例子”的微小扰动来故意错误分类。因此，在引入机器学习基础系统时，有必要对机器学习专用攻击的风险进行分析。在这项研究中，我们提出了一个定量的评估方法来分析风险的规避攻击使用攻击树。该方法包括对传统攻击树进行扩展以分析规避攻击和扩展的系统构造方法。在常规攻击树的扩展中，我们引入了 ML 和常规攻击节点来表示规避攻击的各种特征。在系统构建过程中，提出了一种攻击树的构建方法。该过程包括三个步骤: (1)将文献中关于攻击方法的信息组织到一个矩阵中; (2)从矩阵中的方法中识别出规避攻击场景; (3)使用模式从识别出的场景中构建攻击树。最后，在三个机器学习图像识别系统上进行了实验，验证了该方法的通用性和有效性。"
    },
    {
        "title": "Blockchain-based Privacy-Preserving Public Key Searchable Encryption\n  with Strong Traceability",
        "url": "http://arxiv.org/abs/2312.16954v1",
        "pub_date": "2023-12-28",
        "summary": "Public key searchable encryption (PKSE) scheme allows data users to search\nover encrypted data. To identify illegal users, many traceable PKSE schemes\nhave been proposed. However, existing schemes cannot trace the keywords which\nillegal users searched and protect users' privacy simultaneously. In some\npractical applications, tracing both illegal users' identities and the keywords\nwhich they searched is quite important to against the abuse of data. It is a\nchallenge to bind users' identities and keywords while protecting their\nprivacy. Moreover, existing traceable PKSE schemes do not consider the\nunforgeability and immutability of trapdoor query records, which can lead to\nthe occurrence of frame-up and denying. In this paper, to solve these problems,\nwe propose a blockchain-based privacy-preserving PKSE with strong traceability\n(BP3KSEST) scheme. Our scheme provides the following features: (1) authorized\nusers can authenticate to trapdoor generation center and obtain trapdoors\nwithout releasing their identities and keywords; (2) when data users misbehave\nin the system, the trusted third party (TTP) can trace both their identities\nand the keywords which they searched; (3) trapdoor query records are\nunforgeable; (4) trapdoor query records are immutable because records are\nstored in blockchain. Notably, this scheme is suitable to the scenarios where\nprivacy must be considered, e.g., electronic health record (EHR). We formalize\nboth the definition and security model of our BP3KSEST scheme, and present a\nconcrete construction. Furthermore, the security of the proposed scheme is\nformally proven. Finally, the implementation and evaluation are conducted to\nanalyze its efficiency.",
        "translated": "公钥可搜索加密(PKSE)方案允许数据用户搜索加密的数据。为了识别非法用户，人们提出了许多可追踪的 PKSE 方案。然而，现有的方案不能同时追踪非法用户搜索的关键词，保护用户的隐私。在一些实际应用中，追踪非法用户的身份和他们搜索的关键词对于防止数据滥用非常重要。在保护用户隐私的同时绑定用户的身份和关键字是一个挑战。此外，现有的可跟踪 PKSE 方案没有考虑到陷门查询记录的不可伪造性和不可变性，从而导致了框架和否认的发生。为了解决这些问题，本文提出了一种基于区块链的强跟踪保密 PKSE (BP3KSEST)方案。我们的方案提供了以下特点: (1)授权用户可以在不泄露身份和关键字的情况下向活门生成中心认证并获得活门; (2)当数据用户在系统中行为不当时，可信第三方(TTP)可以追踪他们的身份和他们搜索的关键字; (3)活门查询记录是不可伪造的; (4)活门查询记录是不可变的，因为记录存储在区块链中。值得注意的是，这个方案适用于必须考虑隐私的情况，例如电子健康记录(EHR)。我们对 BP3KSEST 方案的定义和安全模型进行了形式化描述，并给出了具体的构造。进一步证明了该方案的安全性。最后，通过实施和评估分析了该方法的有效性。"
    },
    {
        "title": "Adversarial Attacks on Image Classification Models: Analysis and Defense",
        "url": "http://arxiv.org/abs/2312.16880v1",
        "pub_date": "2023-12-28",
        "summary": "The notion of adversarial attacks on image classification models based on\nconvolutional neural networks (CNN) is introduced in this work. To classify\nimages, deep learning models called CNNs are frequently used. However, when the\nnetworks are subject to adversarial attacks, extremely potent and previously\ntrained CNN models that perform quite effectively on image datasets for image\nclassification tasks may perform poorly. In this work, one well-known\nadversarial attack known as the fast gradient sign method (FGSM) is explored\nand its adverse effects on the performances of image classification models are\nexamined. The FGSM attack is simulated on three pre-trained image classifier\nCNN architectures, ResNet-101, AlexNet, and RegNetY 400MF using randomly chosen\nimages from the ImageNet dataset. The classification accuracies of the models\nare computed in the absence and presence of the attack to demonstrate the\ndetrimental effect of the attack on the performances of the classifiers.\nFinally, a mechanism is proposed to defend against the FGSM attack based on a\nmodified defensive distillation-based approach. Extensive results are presented\nfor the validation of the proposed scheme.",
        "translated": "提出了基于卷积神经网络(CNN)的图像分类模型的对抗性攻击的概念。为了对图像进行分类，人们经常使用称为 CNN 的深度学习模型。然而，当网络受到敌对攻击时，在图像数据集上运行非常有效的、先前训练过的 CNN 模型在图像分类任务中的表现可能会很差。本文研究了一种著名的对抗性攻击——快速梯度符号法(FGSM) ，并分析了它对图像分类模型性能的不利影响。使用从 ImageNet 数据集中随机选择的图像，在三个预先训练的图像分类器 CNN 架构，ResNet-101，AlexNet 和 RegNetY 400MF 上模拟 FGSM 攻击。在攻击不存在和存在的情况下，计算模型的分类精度，以证明攻击对分类器性能的不利影响。最后，提出了一种基于改进的防御精馏方法的 FGSM 攻击防御机制。为验证所提方案的有效性，给出了广泛的结果。"
    },
    {
        "title": "Hiding in Plain Sight: Towards the Science of Linguistic Steganography",
        "url": "http://arxiv.org/abs/2312.16840v1",
        "pub_date": "2023-12-28",
        "summary": "Covert communication (also known as steganography) is the practice of\nconcealing a secret inside an innocuous-looking public object (cover) so that\nthe modified public object (covert code) makes sense to everyone but only\nsomeone who knows the code can extract the secret (message). Linguistic\nsteganography is the practice of encoding a secret message in natural language\ntext such as spoken conversation or short public communications such as\ntweets.. While ad hoc methods for covert communications in specific domains\nexist ( JPEG images, Chinese poetry, etc), there is no general model for\nlinguistic steganography specifically. We present a novel mathematical\nformalism for creating linguistic steganographic codes, with three parameters:\nDecodability (probability that the receiver of the coded message will decode\nthe cover correctly), density (frequency of code words in a cover code), and\ndetectability (probability that an attacker can tell the difference between an\nuntampered cover compared to its steganized version). Verbal or linguistic\nsteganography is most challenging because of its lack of artifacts to hide the\nsecret message in. We detail a practical construction in Python of a\nsteganographic code for Tweets using inserted words to encode hidden digits\nwhile using n-gram frequency distortion as the measure of detectability of the\ninsertions. Using the publicly accessible Stanford Sentiment Analysis dataset\nwe implemented the tweet steganization scheme -- a codeword (an existing word\nin the data set) inserted in random positions in random existing tweets to find\nthe tweet that has the least possible n-gram distortion. We argue that this\napproximates KL distance in a localized manner at low cost and thus we get a\nlinguistic steganography scheme that is both formal and practical and permits a\ntradeoff between codeword density and detectability of the covert message.",
        "translated": "秘密通信(也称为隐写术)是将秘密隐藏在一个看起来无害的公共对象(掩护)中，以便修改后的公共对象(隐蔽代码)对每个人都有意义，但只有知道代码的人才能提取秘密(信息)。语言隐写术是将秘密信息编码到自然语言文本(如口头对话或短公共交流(如推特)中的一种实践。.虽然在特定领域(JPEG 图像、汉语诗歌等)存在特别的隐蔽通信方法，但是没有一个通用的语言隐写模型。我们提出了一种创建语言隐写代码的新型数学形式，它有三个参数: 可解码性(编码信息的接收者将正确解码封面的概率)、密度(封面代码中代码词的频率)和可检测性(攻击者能够区分未被篡改的封面与其隐写版本之间的区别的概率)。口头或语言隐写是最具挑战性的，因为它缺乏隐藏秘密信息的工件。我们详细介绍了一个在 Python 中实际构建的 Tweets 隐写代码，该代码使用插入的单词对隐藏的数字进行编码，同时使用 n-gram 频率失真度量插入的可检测性。我们使用公开的斯坦福情绪分析(Stanford Sentiment Analysis)数据集实现了 tweet 隐藏方案——一个代码词(数据集中的一个现有单词)随机插入现有 tweet 中的位置，以找到具有最小可能的 n-gram 失真的 tweet。我们认为这种方法可以低成本地近似 KL 距离，因此我们得到了一种正式和实用的语言隐写方案，并允许在码字密度和隐蔽信息的可检测性之间进行权衡。"
    },
    {
        "title": "Layer Attack Unlearning: Fast and Accurate Machine Unlearning via Layer\n  Level Attack and Knowledge Distillation",
        "url": "http://arxiv.org/abs/2312.16823v1",
        "pub_date": "2023-12-28",
        "summary": "Recently, serious concerns have been raised about the privacy issues related\nto training datasets in machine learning algorithms when including personal\ndata. Various regulations in different countries, including the GDPR grant\nindividuals to have personal data erased, known as 'the right to be forgotten'\nor 'the right to erasure'. However, there has been less research on effectively\nand practically deleting the requested personal data from the training set\nwhile not jeopardizing the overall machine learning performance. In this work,\nwe propose a fast and novel machine unlearning paradigm at the layer level\ncalled layer attack unlearning, which is highly accurate and fast compared to\nexisting machine unlearning algorithms. We introduce the Partial-PGD algorithm\nto locate the samples to forget efficiently. In addition, we only use the last\nlayer of the model inspired by the Forward-Forward algorithm for unlearning\nprocess. Lastly, we use Knowledge Distillation (KD) to reliably learn the\ndecision boundaries from the teacher using soft label information to improve\naccuracy performance. We conducted extensive experiments with SOTA machine\nunlearning models and demonstrated the effectiveness of our approach for\naccuracy and end-to-end unlearning performance.",
        "translated": "近年来，在机器学习算法中，当包含个人数据时，与训练数据集相关的隐私问题引起了人们的严重关注。不同国家的各种规定，包括 GDPR 授予个人删除个人信息的权利，被称为“被遗忘的权利”或“删除的权利”。然而，关于在不损害整体机器学习性能的情况下有效和实际地从训练集中删除所请求的个人数据的研究较少。在这项工作中，我们提出了一个快速和新颖的机器去学习范式在层级称为层攻击去学习，这是高准确性和快速的比较现有的机器去学习算法。我们引入了部分 PGD 算法来有效地定位样本进行遗忘。此外，我们只使用模型的最后一层，灵感来自前进算法的去学习过程。最后，利用知识提取技术(KD) ，通过软标签信息可靠地从教师那里学习决策边界，提高决策的准确性。我们对 SOTA 机器学习模型进行了广泛的实验，并证明了我们的方法在准确性和端到端学习性能方面的有效性。"
    },
    {
        "title": "Comparing Effectiveness and Efficiency of Interactive Application\n  Security Testing (IAST) and Runtime Application Self-Protection (RASP) Tools\n  in a Large Java-based System",
        "url": "http://arxiv.org/abs/2312.17726v1",
        "pub_date": "2023-12-29",
        "summary": "Security resources are scarce, and practitioners need guidance in the\neffective and efficient usage of techniques and tools available in the\ncybersecurity industry. Two emerging tool types, Interactive Application\nSecurity Testing (IAST) and Runtime Application Self-Protection (RASP), have\nnot been thoroughly evaluated against well-established counterparts such as\nDynamic Application Security Testing (DAST) and Static Application Security\nTesting (SAST). The goal of this research is to aid practitioners in making\ninformed choices about the use of Interactive Application Security Testing\n(IAST) and Runtime Application Self-Protection (RASP) tools through an analysis\nof their effectiveness and efficiency in comparison with different\nvulnerability detection and prevention techniques and tools. We apply IAST and\nRASP on OpenMRS, an open-source Java-based online application. We compare the\nefficiency and effectiveness of IAST and RASP with techniques applied on\nOpenMRS in prior work. We measure efficiency and effectiveness in terms of the\nnumber and type of vulnerabilities detected and prevented per hour. Our study\nshows IAST performed relatively well compared to other techniques, performing\nsecond-best in both efficiency and effectiveness. IAST detected eight Top-10\nOWASP security risks compared to nine by SMPT and seven for EMPT, DAST, and\nSAST. IAST found more vulnerabilities than SMPT. The efficiency of IAST (2.14\nVpH) is second to only EMPT (2.22 VpH). These findings imply that our study\nbenefited from using IAST when conducting black-box security testing. In the\ncontext of a large, enterprise-scale web application such as OpenMRS, RASP does\nnot replace vulnerability detection, while IAST is a powerful tool that\ncomplements other techniques.",
        "translated": "安全资源稀缺，从业人员需要指导如何有效和高效地使用网络安全行业现有的技术和工具。两种新兴的工具类型，交互式应用程序安全测试(IAST)和运行时应用程序自我保护(RASP) ，还没有被充分地评估，比如动态应用程序安全测试(DAST)和静态应用程序安全测试(SAST)。本研究的目的是通过分析交互式应用程序安全测试(IAST)和运行时应用程序自我保护(RASP)工具的有效性和效率，并与不同的漏洞检测和预防技术和工具进行比较，帮助实践者做出明智的选择。我们将 IAST 和 RASP 应用于 OpenMRS，这是一个基于 Java 的开源在线应用程序。我们比较了 IAST 和 RASP 的效率和有效性，以及在 OpenMRS 上应用的技术在以前的工作。我们根据每小时检测到和防止的漏洞的数量和类型来衡量效率和有效性。我们的研究表明，与其他技术相比，IAST 表现相对较好，在效率和有效性方面表现次佳。IAST 检测到8个 OWASP 安全风险，而 SMPT 检测到9个，EMPT、 DAST 和 SAST 检测到7个。IAST 发现了比 SMPT 更多的漏洞。IAST (2.14 VpH)的效率仅次于 EMPT (2.22 VpH)。这些发现意味着我们的研究受益于使用 IAST 时进行黑盒安全性测试。在 OpenMRS 这样的大型企业级 Web 应用程序中，RASP 并不能取代漏洞检测，而 IAST 是一个强大的工具，可以补充其他技术。"
    },
    {
        "title": "Simple client-side encryption of personal information with Web Assembly",
        "url": "http://arxiv.org/abs/2312.17689v1",
        "pub_date": "2023-12-29",
        "summary": "The HTTPS protocol has enforced a higher level of robustness to several\nattacks; however, it is not easy to set up the required certificates on\nintranets, nor is it effective in the case the server confidentiality is not\nreliable, as in the case of cloud services, or it could be compromised. A\nsimple method is proposed to encrypt the data on the client side, using Web\nAssembly. It never transfers data to the server as clear text. Searching fields\nin the server is made possible by an encoding scheme that ensures a stable\nprefix correspondence between ciphertext and plaintext. The method has been\ndeveloped for a semantic medical database, and allows accessing personal data\nusing an additional password while maintaining non-sensitive information in\nclear form. Web Assembly has been chosen to guarantee the fast and efficient\nexecution of encrypting/decrypting operations and because of its characteristic\nof producing modules that are very robust against reverse engineering. The code\nis available at https://github.com/mfalda/client-encdec.",
        "translated": "HTTPS 协议增强了对多种攻击的鲁棒性; 然而，在内联网上设置所需的证书并不容易，在服务器机密性不可靠(如云服务)或可能受到破坏的情况下，也不是有效的。提出了一种在客户端使用 WebAssembly 对数据进行加密的简单方法。它从不将数据以明文形式传输到服务器。服务器中的搜索字段是通过一种编码方案实现的，该编码方案确保了密文和明文之间稳定的前缀对应关系。该方法是为一个语义医学数据库开发的，它允许使用附加密码访问个人数据，同时保持非敏感信息的清晰形式。Web Assembly 被选择来保证加密/解密操作的快速有效执行，并且因为它的特点产生的模块对逆向工程非常健壮。密码可在 https://github.com/mfalda/client-encdec 查阅。"
    },
    {
        "title": "Malware Detection in IOT Systems Using Machine Learning Techniques",
        "url": "http://arxiv.org/abs/2312.17683v1",
        "pub_date": "2023-12-29",
        "summary": "Malware detection in IoT environments necessitates robust methodologies. This\nstudy introduces a CNN-LSTM hybrid model for IoT malware identification and\nevaluates its performance against established methods. Leveraging K-fold\ncross-validation, the proposed approach achieved 95.5% accuracy, surpassing\nexisting methods. The CNN algorithm enabled superior learning model\nconstruction, and the LSTM classifier exhibited heightened accuracy in\nclassification. Comparative analysis against prevalent techniques demonstrated\nthe efficacy of the proposed model, highlighting its potential for enhancing\nIoT security. The study advocates for future exploration of SVMs as\nalternatives, emphasizes the need for distributed detection strategies, and\nunderscores the importance of predictive analyses for a more powerful IOT\nsecurity. This research serves as a platform for developing more resilient\nsecurity measures in IoT ecosystems.",
        "translated": "物联网环境中的恶意软件检测需要健壮的方法论。本研究介绍了一个用于物联网恶意软件识别的 CNN-LSTM 混合模型，并根据已建立的方法对其性能进行了评估。利用 k 倍交叉验证，提出的方法达到了95.5% 的准确率，超过了现有的方法。细胞神经网络算法支持高级学习模型的建立，而 LSTM 分类器表现出更高的分类精度。通过与流行技术的比较分析，证明了该模型的有效性，突出了其增强物联网安全性的潜力。该研究提倡将来探索支持向量机作为替代方案，强调分布式检测策略的必要性，并强调预测分析对于更强大的物联网安全的重要性。该研究为物联网生态系统中开发更具弹性的安全措施提供了一个平台。"
    },
    {
        "title": "Prompt Fuzzing for Fuzz Driver Generation",
        "url": "http://arxiv.org/abs/2312.17677v1",
        "pub_date": "2023-12-29",
        "summary": "Writing high-quality fuzz drivers is time-consuming and requires a deep\nunderstanding of the library. However, the performance of the state-of-the-art\nautomatic fuzz driver generation techniques leaves a lot to be desired. Fuzz\ndrivers, which are learned from consumer code, can reach deep states but are\nrestricted to their external inputs. On the other hand, interpretative fuzzing\ncan explore most APIs but requires numerous attempts in a vast search space. We\npropose PromptFuzz, a coverage-guided fuzzer for prompt fuzzing that\niteratively generates fuzz drivers to explore undiscovered library code. To\nexplore API usage in fuzz drivers during prompt fuzzing, we proposed several\nkey techniques: instructive program generation, erroneous program sanitization,\ncoverage-guided prompt mutation, and constrained fuzzer fusion. We implemented\nPromptFuzz and evaluated its effectiveness on 14 real-world libraries,\ncomparing it against OSS-Fuzz and the state-of-the-art fuzz driver generation\nsolution (i.e., Hopper). The experiment results demonstrate that the fuzz\ndrivers generated by PromptFuzz achieve higher branch coverage that is 1.61\ntimes greater than that of OSS-Fuzz and 1.67 times greater than that of Hopper.\nIn addition, the fuzz drivers generated by PromptFuzz successfully detect 33\ntrue bugs out of a total of 44 crashes, which were previously unknown, and 27\nof these bugs have been confirmed by the respective communities.",
        "translated": "编写高质量的模糊驱动程序非常耗时，并且需要对库有深入的理解。然而，最先进的自动模糊驱动程序生成技术的性能还有很多不足之处。从使用者代码中学习的模糊驱动程序可以达到深层状态，但仅限于它们的外部输入。另一方面，解释性模糊化可以探索大多数 API，但需要在广阔的搜索空间中进行大量尝试。我们提出 PromptFuzz，一个覆盖率引导的 fuzzer，用于提示 fuzzing，迭代地生成 fuzz 驱动程序以探索未发现的库代码。为了探索模糊驱动程序在提示模糊化过程中的应用，我们提出了几个关键技术: 指导性程序生成、错误程序消毒、覆盖引导的提示变异和约束模糊融合。我们实现了 PromptFuzz，并评估了它在14个现实世界库中的有效性，将其与 OSS-Fuzz 和最先进的模糊驱动程序生成解决方案(即 Hopper)进行了比较。实验结果表明，PromptFuzz 生成的模糊驱动程序的分支覆盖率比 OSS-Fuzz 高1.61倍，比 Hopper 高1.67倍。此外，PromptFuzz 生成的模糊驱动程序成功地检测到了总共44次碰撞中的33个真正的错误，这些错误以前是未知的，其中27个已经得到了相关社区的确认。"
    },
    {
        "title": "Jatmo: Prompt Injection Defense by Task-Specific Finetuning",
        "url": "http://arxiv.org/abs/2312.17673v1",
        "pub_date": "2023-12-29",
        "summary": "Large Language Models (LLMs) are attracting significant research attention\ndue to their instruction-following abilities, allowing users and developers to\nleverage LLMs for a variety of tasks. However, LLMs are vulnerable to\nprompt-injection attacks: a class of attacks that hijack the model's\ninstruction-following abilities, changing responses to prompts to undesired,\npossibly malicious ones. In this work, we introduce Jatmo, a method for\ngenerating task-specific models resilient to prompt-injection attacks. Jatmo\nleverages the fact that LLMs can only follow instructions once they have\nundergone instruction tuning. It harnesses a teacher instruction-tuned model to\ngenerate a task-specific dataset, which is then used to fine-tune a base model\n(i.e., a non-instruction-tuned model). Jatmo only needs a task prompt and a\ndataset of inputs for the task: it uses the teacher model to generate outputs.\nFor situations with no pre-existing datasets, Jatmo can use a single example,\nor in some cases none at all, to produce a fully synthetic dataset. Our\nexperiments on six tasks show that Jatmo models provide the same quality of\noutputs on their specific task as standard LLMs, while being resilient to\nprompt injections. The best attacks succeeded in less than 0.5% of cases\nagainst our models, versus over 90% success rate against GPT-3.5-Turbo. We\nrelease Jatmo at https://github.com/wagner-group/prompt-injection-defense.",
        "translated": "大型语言模型(LLM)由于其指令跟踪能力而吸引了大量的研究注意力，允许用户和开发人员利用 LLM 完成各种任务。然而，LLM 很容易受到提示注入攻击: 一类劫持模型的指令跟踪能力的攻击，改变对不希望的、可能是恶意的提示的响应。在这项工作中，我们介绍了 Jatmo，一种生成任务特定模型的方法，该模型能够抵御注入攻击。Jatmo 利用了这样一个事实，即 LLM 只能在经过指令调优之后才能遵循指令。它利用教师指令调优模型来生成特定于任务的数据集，然后使用该数据集对基本模型(即非指令调优模型)进行微调。Jatmo 只需要任务提示符和任务输入的数据集: 它使用教师模型来生成输出。对于没有预先存在的数据集的情况，Jatmo 可以使用一个例子，或者在某些情况下根本没有，来生成一个完全合成的数据集。我们在六个任务上的实验表明，Jatmo 模型在其特定任务上提供了与标准 LLM 相同的输出质量，同时具有迅速注射的弹性。针对我们的模型，最好的攻击成功率不到0.5% ，而针对 GPT-3.5-Turbo 的成功率超过90% 。我们 https://github.com/wagner-group/prompt-injection-defense 释放杰特莫。"
    },
    {
        "title": "AIJack: Security and Privacy Risk Simulator for Machine Learning",
        "url": "http://arxiv.org/abs/2312.17667v1",
        "pub_date": "2023-12-29",
        "summary": "This paper introduces AIJack, an open-source library designed to assess\nsecurity and privacy risks associated with the training and deployment of\nmachine learning models. Amid the growing interest in big data and AI,\nadvancements in machine learning research and business are accelerating.\nHowever, recent studies reveal potential threats, such as the theft of training\ndata and the manipulation of models by malicious attackers. Therefore, a\ncomprehensive understanding of machine learning's security and privacy\nvulnerabilities is crucial for the safe integration of machine learning into\nreal-world products. AIJack aims to address this need by providing a library\nwith various attack and defense methods through a unified API. The library is\npublicly available on GitHub (https://github.com/Koukyosyumei/AIJack).",
        "translated": "本文介绍了 AIJack，一个旨在评估与机器学习模型的培训和部署相关的安全和隐私风险的开源库。随着人们对大数据和人工智能的兴趣日益浓厚，机器学习研究和商业领域的进展正在加速。然而，最近的研究揭示了潜在的威胁，如盗取训练数据和恶意攻击者操纵模型。因此，全面了解机器学习的安全和隐私漏洞对于机器学习与现实世界产品的安全集成至关重要。AIJack 的目标是通过统一的 API 为库提供各种攻击和防御方法来满足这一需求。这个图书馆可以在 GitHub 上公开使用( https://GitHub.com/koukyosyumei/aijack )。"
    },
    {
        "title": "Differentially Private Low-Rank Adaptation of Large Language Model Using\n  Federated Learning",
        "url": "http://arxiv.org/abs/2312.17493v1",
        "pub_date": "2023-12-29",
        "summary": "The surge in interest and application of large language models (LLMs) has\nsparked a drive to fine-tune these models to suit specific applications, such\nas finance and medical science. However, concerns regarding data privacy have\nemerged, especially when multiple stakeholders aim to collaboratively enhance\nLLMs using sensitive data. In this scenario, federated learning becomes a\nnatural choice, allowing decentralized fine-tuning without exposing raw data to\ncentral servers. Motivated by this, we investigate how data privacy can be\nensured in LLM fine-tuning through practical federated learning approaches,\nenabling secure contributions from multiple parties to enhance LLMs. Yet,\nchallenges arise: 1) despite avoiding raw data exposure, there is a risk of\ninferring sensitive information from model outputs, and 2) federated learning\nfor LLMs incurs notable communication overhead. To address these challenges,\nthis article introduces DP-LoRA, a novel federated learning algorithm tailored\nfor LLMs. DP-LoRA preserves data privacy by employing a Gaussian mechanism that\nadds noise in weight updates, maintaining individual data privacy while\nfacilitating collaborative model training. Moreover, DP-LoRA optimizes\ncommunication efficiency via low-rank adaptation, minimizing the transmission\nof updated weights during distributed training. The experimental results across\nmedical, financial, and general datasets using various LLMs demonstrate that\nDP-LoRA effectively ensures strict privacy constraints while minimizing\ncommunication overhead.",
        "translated": "人们对大型语言模型(LLM)的兴趣和应用激增，促使人们对这些模型进行微调，以适应金融和医学等特定应用。然而，关于数据隐私的担忧已经出现，特别是当多个利益相关者旨在使用敏感数据协同增强 LLM 时。在这种情况下，联合学习成为一种自然的选择，允许分散的微调，而不需要将原始数据暴露给中央服务器。基于此，我们研究如何通过实用的联邦学习方法在 LLM 微调中确保数据隐私，从而使多方的安全贡献能够增强 LLM。然而，挑战出现了: 1)尽管避免了原始数据的暴露，但仍存在从模型输出推断敏感信息的风险，2) LLM 的联邦学习会带来显著的通信开销。为了应对这些挑战，本文介绍了 DP-LoRA，一种专为 LLM 设计的新型联邦学习算法。DP-LoRA 通过采用高斯机制在权重更新中添加噪声来保护数据隐私，在促进协作模型训练的同时维护个人数据隐私。此外，DP-LoRA 通过低秩自适应优化通信效率，最小化了分布式训练中更新权值的传输。实验结果表明，DP-LoRA 有效地保证了严格的隐私约束，同时最大限度地减少了通信开销。"
    },
    {
        "title": "MVPatch: More Vivid Patch for Adversarial Camouflaged Attacks on Object\n  Detectors in the Physical World",
        "url": "http://arxiv.org/abs/2312.17431v1",
        "pub_date": "2023-12-29",
        "summary": "Recent research has shown that adversarial patches can manipulate outputs\nfrom object detection models. However, the conspicuous patterns on these\npatches may draw more attention and raise suspicions among humans. Moreover,\nexisting works have primarily focused on the attack performance of individual\nmodels and have neglected the generation of adversarial patches for ensemble\nattacks on multiple object detection models. To tackle these concerns, we\npropose a novel approach referred to as the More Vivid Patch (MVPatch), which\naims to improve the transferability and stealthiness of adversarial patches\nwhile considering the limitations observed in prior paradigms, such as easy\nidentification and poor transferability. Our approach incorporates an attack\nalgorithm that decreases object confidence scores of multiple object detectors\nby using the ensemble attack loss function, thereby enhancing the\ntransferability of adversarial patches. Additionally, we propose a lightweight\nvisual similarity measurement algorithm realized by the Compared Specified\nImage Similarity (CSS) loss function, which allows for the generation of\nnatural and stealthy adversarial patches without the reliance on additional\ngenerative models. Extensive experiments demonstrate that the proposed MVPatch\nalgorithm achieves superior attack transferability compared to similar\nalgorithms in both digital and physical domains, while also exhibiting a more\nnatural appearance. These findings emphasize the remarkable stealthiness and\ntransferability of the proposed MVPatch attack algorithm.",
        "translated": "最近的研究表明，对抗性补丁可以操纵目标检测模型的输出。然而，这些斑块上的显著图案可能会引起更多的注意，并引起人类的怀疑。此外，现有的研究主要集中在单个模型的攻击性能上，而忽略了针对多个目标检测模型的集合攻击的对抗补丁的生成。为了解决这些问题，我们提出了一种称为 More Vivid Patch (MVPatch)的新方法，其目的是改善对抗性补丁的可转移性和隐蔽性，同时考虑到先前范例中观察到的局限性，如易于识别和可转移性差。我们的方法结合了一个攻击算法，通过使用集合攻击损失函数来降低多个目标检测器的目标置信度得分，从而增强了对抗补丁的可转移性。此外，本文还提出了一种轻量级的视觉相似度度量算法，该算法通过比较指定图像相似度(CSS)损失函数实现，可以生成自然的和隐蔽的对抗性补丁，而不需要依赖额外的生成模型。大量实验表明，与同类算法相比，MVPatch 算法在数字和物理领域具有更好的攻击可转移性，同时也表现出更加自然的外观。这些发现强调了所提出的 MVPatch 攻击算法的显著的隐蔽性和可转移性。"
    },
    {
        "title": "Seqnature: Extracting Network Fingerprints from Packet Sequences",
        "url": "http://arxiv.org/abs/2312.17370v1",
        "pub_date": "2023-12-28",
        "summary": "This paper proposes a general network fingerprinting framework, Seqnature,\nthat uses packet sequences as its basic data unit and that makes it simple to\nimplement any fingerprinting technique that can be formulated as a problem of\nidentifying packet exchanges that consistently occur when the fingerprinted\nevent is triggered. We demonstrate the versatility of Seqnature by using it to\nimplement five different fingerprinting techniques, as special cases of the\nframework, which broadly fall into two categories: (i) fingerprinting\ntechniques that consider features of each individual packet in a packet\nsequence, e.g., size and direction; and (ii) fingerprinting techniques that\nonly consider stream-wide features, specifically what Internet endpoints are\ncontacted. We illustrate how Seqnature facilitates comparisons of the relative\nperformance of different fingerprinting techniques by applying the five\nfingerprinting techniques to datasets from the literature. The results confirm\nfindings in prior work, for example that endpoint information alone is\ninsufficient to differentiate between individual events on Internet of Things\ndevices, but also show that smart TV app fingerprints based exclusively on\nendpoint information are not as distinct as previously reported.",
        "translated": "本文提出了一个通用的网络指纹识别框架 Seqnature，该框架使用数据包序列作为其基本数据单元，并使得实现任何指纹识别技术变得简单，这些技术可以表述为识别指纹识别事件触发时始终发生的数据包交换的问题。我们通过使用它来实现五种不同的指纹技术，作为框架的特殊情况，证明了 Seqnature 的多功能性，这些技术大致分为两类: (i)考虑包序列中每个包的特征的指纹技术，例如大小和方向; 以及(ii)只考虑流范围特征的指纹技术，特别是联系的 Internet 端点。我们举例说明 Seqnature 如何通过将五种指纹技术应用于文献中的数据集来促进不同指纹技术的相对性能的比较。结果证实了之前工作中的发现，例如端点信息本身不足以区分物联网设备上的个别事件，但也表明完全基于端点信息的智能电视应用指纹不像之前报道的那样明显。"
    },
    {
        "title": "An Introduction to Adaptive Software Security",
        "url": "http://arxiv.org/abs/2312.17358v1",
        "pub_date": "2023-12-28",
        "summary": "This paper presents the adaptive software security model, an innovative\napproach integrating the MAPE-K loop and the Software Development Life Cycle\n(SDLC). It proactively embeds security policies throughout development,\nreducing vulnerabilities from different levels of software engineering. Three\nprimary contributions-MAPE-K integration, SDLC embedding, and analytical\ninsights-converge to create a comprehensive approach for strengthening software\nsystems against security threats. This research represents a paradigm shift,\nadapting security measures with agile software development and ensuring\ncontinuous improvement in the face of evolving threats. The model emerges as a\nrobust solution, addressing the crucial need for adaptive software security\nstrategies in modern software development. We analytically discuss the\nadvantages of the proposed model.",
        "translated": "提出了一种集成 MAPE-K 循环和软件开发生命周期(SDLC)的自适应软件安全模型。它在整个开发过程中积极地嵌入安全策略，减少来自不同级别的软件工程的漏洞。三个主要贡献—— MAPE-K 集成、 SDLC 嵌入和分析见解——汇聚在一起，创建了一个针对安全威胁加强软件系统的综合方法。这项研究代表了一种范式转变，使安全措施与敏捷软件开发相适应，并确保在面对不断变化的威胁时不断改进。该模型是一个健壮的解决方案，满足了现代软件开发中自适应软件安全策略的关键需求。我们分析讨论了该模型的优点。"
    },
    {
        "title": "Experimental Validation of Sensor Fusion-based GNSS Spoofing Attack\n  Detection Framework for Autonomous Vehicles",
        "url": "http://arxiv.org/abs/2401.01304v1",
        "pub_date": "2024-01-02",
        "summary": "In this paper, we validate the performance of the a sensor fusion-based\nGlobal Navigation Satellite System (GNSS) spoofing attack detection framework\nfor Autonomous Vehicles (AVs). To collect data, a vehicle equipped with a GNSS\nreceiver, along with Inertial Measurement Unit (IMU) is used. The detection\nframework incorporates two strategies: The first strategy involves comparing\nthe predicted location shift, which is the distance traveled between two\nconsecutive timestamps, with the inertial sensor-based location shift. For this\npurpose, data from low-cost in-vehicle inertial sensors such as the\naccelerometer and gyroscope sensor are fused and fed into a long short-term\nmemory (LSTM) neural network. The second strategy employs a Random-Forest\nsupervised machine learning model to detect and classify turns, distinguishing\nbetween left and right turns using the output from the steering angle sensor.\nIn experiments, two types of spoofing attack models: turn-by-turn and wrong\nturn are simulated. These spoofing attacks are modeled as SQL injection\nattacks, where, upon successful implementation, the navigation system perceives\ninjected spoofed location information as legitimate while being unable to\ndetect legitimate GNSS signals. Importantly, the IMU data remains uncompromised\nthroughout the spoofing attack. To test the effectiveness of the detection\nframework, experiments are conducted in Tuscaloosa, AL, mimicking urban road\nstructures. The results demonstrate the framework's ability to detect various\nsophisticated GNSS spoofing attacks, even including slow position drifting\nattacks. Overall, the experimental results showcase the robustness and efficacy\nof the sensor fusion-based spoofing attack detection approach in safeguarding\nAVs against GNSS spoofing threats.",
        "translated": "本文对基于传感器融合的全球导航卫星系统(GNSS)自主车辆欺骗攻击检测框架的性能进行了验证。为了收集数据，我们使用了一辆装有全球导航卫星系统接收器和惯性导航系统(IMU)的车辆。该检测框架包含两种策略: 第一种策略包括比较预测的位置偏移(即两个连续时间戳之间的距离)和基于惯性传感器的位置偏移。为此，将低成本车载惯性传感器(如加速度计和陀螺仪传感器)的数据进行融合，并将其输入到一个长期短期记忆(LSTM)神经网络中。第二种策略采用 Random-Forest 的监督式学习模型来检测和分类转弯，利用转向角传感器的输出来区分左转弯和右转弯。在实验中，模拟了两种类型的欺骗攻击模型: 逐拐攻击和错误转向攻击。这些欺骗性攻击被建模为 SQL 注入式攻击，一旦成功实施，导航系统就会认为注入的欺骗性位置信息是合法的，而无法检测到合法的 GNSS 信号。重要的是，在整个欺骗攻击过程中，IMU 数据保持不受损害。为了测试检测框架的有效性，我们在塔斯卡卢萨县进行了模拟城市道路结构的实验。结果表明，该框架能够检测各种复杂的 GNSS 欺骗攻击，甚至包括慢位置漂移攻击。总的来说，实验结果表明了基于传感器融合的欺骗攻击检测方法在保护 AVS 对抗 GNSS 欺骗威胁方面的鲁棒性和有效性。"
    },
    {
        "title": "LLbezpeky: Leveraging Large Language Models for Vulnerability Detection",
        "url": "http://arxiv.org/abs/2401.01269v1",
        "pub_date": "2024-01-02",
        "summary": "Despite the continued research and progress in building secure systems,\nAndroid applications continue to be ridden with vulnerabilities, necessitating\neffective detection methods. Current strategies involving static and dynamic\nanalysis tools come with limitations like overwhelming number of false\npositives and limited scope of analysis which make either difficult to adopt.\nOver the past years, machine learning based approaches have been extensively\nexplored for vulnerability detection, but its real-world applicability is\nconstrained by data requirements and feature engineering challenges. Large\nLanguage Models (LLMs), with their vast parameters, have shown tremendous\npotential in understanding semnatics in human as well as programming languages.\nWe dive into the efficacy of LLMs for detecting vulnerabilities in the context\nof Android security. We focus on building an AI-driven workflow to assist\ndevelopers in identifying and rectifying vulnerabilities. Our experiments show\nthat LLMs outperform our expectations in finding issues within applications\ncorrectly flagging insecure apps in 91.67% of cases in the Ghera benchmark. We\nuse inferences from our experiments towards building a robust and actionable\nvulnerability detection system and demonstrate its effectiveness. Our\nexperiments also shed light on how different various simple configurations can\naffect the True Positive (TP) and False Positive (FP) rates.",
        "translated": "尽管在构建安全系统方面不断进行研究和取得进展，但 Android 应用程序仍然充斥着漏洞，需要有效的检测方法。目前使用静态和动态分析工具的策略都存在一些局限性，比如大量的假阳性和有限的分析范围，这使得任何一种方法都难以采用。在过去的几年中，基于机器学习的脆弱性检测方法得到了广泛的探索，但是它在现实世界中的适用性受到数据需求和特征工程挑战的限制。大型语言模型(LLM)具有巨大的参数，在理解人类语言和编程语言中的语义方面显示出巨大的潜力。我们深入研究 LLM 在 Android 安全环境中检测漏洞的功效。我们专注于构建一个人工智能驱动的工作流，以帮助开发人员识别和纠正漏洞。我们的实验表明，在 Ghera 基准测试的91.67% 的情况下，LLM 在发现应用程序中正确标记不安全应用程序的问题方面超出了我们的预期。我们使用我们的实验推论建立一个健壮的和可操作的漏洞检测系统，并证明其有效性。我们的实验还揭示了不同的简单配置如何影响真阳性(TP)和假阳性(FP)率。"
    },
    {
        "title": "PPBFL: A Privacy Protected Blockchain-based Federated Learning Model",
        "url": "http://arxiv.org/abs/2401.01204v1",
        "pub_date": "2024-01-02",
        "summary": "With the rapid development of machine learning and growing concerns about\ndata privacy, federated learning has become an increasingly prominent focus.\nHowever, challenges such as attacks on model parameters and the lack of\nincentive mechanisms hinder the effectiveness of federated learning. Therefore,\nwe propose a Privacy Protected Blockchain-based Federated Learning Model\n(PPBFL) to enhance the security of federated learning and promote the active\nparticipation of nodes in model training. Blockchain ensures that model\nparameters stored in the InterPlanetary File System (IPFS) remain unaltered. A\nnovel adaptive differential privacy addition algorithm is simultaneously\napplied to local and global models, preserving the privacy of local models and\npreventing a decrease in the security of the global model due to the presence\nof numerous local models in federated learning. Additionally, we introduce a\nnew mix transactions mechanism to better protect the identity privacy of local\ntraining clients. Security analysis and experimental results demonstrate that\nPPBFL outperforms baseline methods in both model performance and security.",
        "translated": "随着机器学习的迅速发展和对数据隐私的日益关注，联邦学习已经成为一个日益突出的焦点。然而，模型参数的攻击和缺乏激励机制等挑战阻碍了联邦学习的有效性。为此，提出了一种基于隐私保护区块链的联邦学习模型(PPBFL) ，以提高联邦学习的安全性，促进节点积极参与模型训练。区块链确保存储在行星文件系统(IPFS)中的模型参数保持不变。一种新的自适应差分隐私加法算法同时应用于局部和全局模型，保护了局部模型的隐私性，并防止了由于联邦学习中存在大量的局部模型而导致的全局模型安全性下降。此外，为了更好地保护本地培训客户端的身份隐私，我们引入了一种新的混合事务机制。安全性分析和实验结果表明，PPBFL 在模型性能和安全性方面均优于基线方法。"
    },
    {
        "title": "FedQV: Leveraging Quadratic Voting in Federated Learning",
        "url": "http://arxiv.org/abs/2401.01168v1",
        "pub_date": "2024-01-02",
        "summary": "Federated Learning (FL) permits different parties to collaboratively train a\nglobal model without disclosing their respective local labels. A crucial step\nof FL, that of aggregating local models to produce the global one, shares many\nsimilarities with public decision-making, and elections in particular. In that\ncontext, a major weakness of FL, namely its vulnerability to poisoning attacks,\ncan be interpreted as a consequence of the one person one vote (henceforth\n1p1v) principle underpinning most contemporary aggregation rules. In this\npaper, we propose FedQV, a novel aggregation algorithm built upon the quadratic\nvoting scheme, recently proposed as a better alternative to 1p1v-based\nelections. Our theoretical analysis establishes that FedQV is a truthful\nmechanism in which bidding according to one's true valuation is a dominant\nstrategy that achieves a convergence rate that matches those of\nstate-of-the-art methods. Furthermore, our empirical analysis using multiple\nreal-world datasets validates the superior performance of FedQV against\npoisoning attacks. It also shows that combining FedQV with unequal voting\n``budgets'' according to a reputation score increases its performance benefits\neven further. Finally, we show that FedQV can be easily combined with\nByzantine-robust privacy-preserving mechanisms to enhance its robustness\nagainst both poisoning and privacy attacks.",
        "translated": "联合学习(FL)允许不同的团体协作培训一个全球模型，而不必透露他们各自的本地标签。FL 的一个关键步骤，即聚合地方模型以产生全球模型，与公共决策，特别是选举有许多相似之处。在这种情况下，FL 的一个主要弱点，即它容易受到中毒攻击，可以解释为一个人一票(从今以后1p1v)原则的结果，支持大多数当代聚合规则。本文提出了一种新的基于二次投票方案的聚合算法 FedQV，作为基于1p1v 的选举的一种更好的替代方案。我们的理论分析表明，FedQV 是一种真实的机制，在这种机制中，根据真实价值进行报价是一种优势策略，它的收敛速度与最先进的报价方法相匹配。此外，我们使用多个实际数据集的实证分析验证了 FedQV 在中毒攻击下的优越性能。它还表明，将 FedQV 与不平等的投票“预算”相结合，根据声誉评分进一步提高了其绩效效益。最后，我们表明，FedQV 可以很容易地与拜占庭稳健的隐私保护机制相结合，以增强其对中毒和隐私攻击的鲁棒性。"
    },
    {
        "title": "Static Deadlock Detection for Rust Programs",
        "url": "http://arxiv.org/abs/2401.01114v1",
        "pub_date": "2024-01-02",
        "summary": "Rust relies on its unique ownership mechanism to ensure thread and memory\nsafety. However, numerous potential security vulnerabilities persist in\npractical applications. New language features in Rust pose new challenges for\nvulnerability detection. This paper proposes a static deadlock detection method\ntailored for Rust programs, aiming to identify various deadlock types,\nincluding double lock, conflict lock, and deadlock associated with conditional\nvariables. With due consideration for Rust's ownership and lifetimes, we first\ncomplete the pointer analysis. Then, based on the obtained points-to\ninformation, we analyze dependencies among variables to identify potential\ndeadlocks. We develop a tool and conduct experiments based on the proposed\nmethod. The experimental results demonstrate that our method outperforms\nexisting deadlock detection methods in precision.",
        "translated": "Rust 依靠其独特的所有权机制来确保线程和内存安全。然而，许多潜在的安全漏洞持续存在于实际应用中。Rust 中新的语言特性对漏洞检测提出了新的挑战。本文针对 Rust 程序提出了一种静态死锁检测方法，目的是识别各种死锁类型，包括双重锁、冲突锁和与条件变量相关的死锁。考虑到 Rust 的所有权和生存期，我们首先完成指针分析。然后，根据获得的点到信息，分析变量之间的依赖关系，识别潜在的死锁。我们开发了一个工具，并在此基础上进行了实验。实验结果表明，该方法在检测精度上优于现有的死锁检测方法。"
    },
    {
        "title": "Imperio: Language-Guided Backdoor Attacks for Arbitrary Model Control",
        "url": "http://arxiv.org/abs/2401.01085v1",
        "pub_date": "2024-01-02",
        "summary": "Revolutionized by the transformer architecture, natural language processing\n(NLP) has received unprecedented attention. While advancements in NLP models\nhave led to extensive research into their backdoor vulnerabilities, the\npotential for these advancements to introduce new backdoor threats remains\nunexplored. This paper proposes Imperio, which harnesses the language\nunderstanding capabilities of NLP models to enrich backdoor attacks. Imperio\nprovides a new model control experience. It empowers the adversary to control\nthe victim model with arbitrary output through language-guided instructions.\nThis is achieved using a language model to fuel a conditional trigger\ngenerator, with optimizations designed to extend its language understanding\ncapabilities to backdoor instruction interpretation and execution. Our\nexperiments across three datasets, five attacks, and nine defenses confirm\nImperio's effectiveness. It can produce contextually adaptive triggers from\ntext descriptions and control the victim model with desired outputs, even in\nscenarios not encountered during training. The attack maintains a high success\nrate across complex datasets without compromising the accuracy of clean inputs\nand also exhibits resilience against representative defenses. The source code\nis available at \\url{https://khchow.com/Imperio}.",
        "translated": "自然语言处理(NLP)在变压器体系结构的革命性变革中受到了前所未有的重视。虽然 NLP 模型的进步已经导致了对其后门漏洞的广泛研究，但是这些进步引入新的后门威胁的潜力仍然没有得到探索。本文提出 Imperio，它利用自然语言处理模型的语言理解能力来丰富后门攻击。Imperio 提供了一种新的模型控制体验。它授权对手通过语言引导的指令控制任意输出的受害者模型。这是通过使用语言模型为条件触发器提供燃料来实现的，优化的目的是将其语言理解能力扩展到后门指令解释和执行。我们在三个数据集，五次攻击和九次防御上的实验证实了 Imperio 的有效性。它可以从文本描述中产生上下文自适应触发器，并用期望的输出控制受害者模型，即使在训练期间没有遇到的情况下也是如此。这种攻击在不损害干净输入的准确性的情况下，在复杂数据集中保持了高成功率，并且对代表性的防御显示出了弹性。源代码可在 url { https://khchow.com/imperio }获得。"
    },
    {
        "title": "Safety and Performance, Why Not Both? Bi-Objective Optimized Model\n  Compression against Heterogeneous Attacks Toward AI Software Deployment",
        "url": "http://arxiv.org/abs/2401.00996v1",
        "pub_date": "2024-01-02",
        "summary": "The size of deep learning models in artificial intelligence (AI) software is\nincreasing rapidly, hindering the large-scale deployment on resource-restricted\ndevices (e.g., smartphones). To mitigate this issue, AI software compression\nplays a crucial role, which aims to compress model size while keeping high\nperformance. However, the intrinsic defects in a big model may be inherited by\nthe compressed one. Such defects may be easily leveraged by adversaries, since\na compressed model is usually deployed in a large number of devices without\nadequate protection. In this article, we aim to address the safe model\ncompression problem from the perspective of safety-performance co-optimization.\nSpecifically, inspired by the test-driven development (TDD) paradigm in\nsoftware engineering, we propose a test-driven sparse training framework called\nSafeCompress. By simulating the attack mechanism as safety testing,\nSafeCompress can automatically compress a big model to a small one following\nthe dynamic sparse training paradigm. Then, considering two kinds of\nrepresentative and heterogeneous attack mechanisms, i.e., black-box membership\ninference attack and white-box membership inference attack, we develop two\nconcrete instances called BMIA-SafeCompress and WMIA-SafeCompress. Further, we\nimplement another instance called MMIA-SafeCompress by extending SafeCompress\nto defend against the occasion when adversaries conduct black-box and white-box\nmembership inference attacks simultaneously. We conduct extensive experiments\non five datasets for both computer vision and natural language processing\ntasks. The results show the effectiveness and generalizability of our\nframework. We also discuss how to adapt SafeCompress to other attacks besides\nmembership inference attack, demonstrating the flexibility of SafeCompress.",
        "translated": "人工智能(AI)软件中深度学习模型的规模正在迅速增长，阻碍了资源受限设备(如智能手机)的大规模部署。为了缓解这一问题，人工智能软件压缩技术发挥了重要作用，它的目标是在保持高性能的同时压缩模型的大小。然而，大型模型中的内在缺陷可能会被压缩模型所继承。这样的缺陷可能很容易被对手利用，因为压缩模型通常部署在大量设备中，而没有足够的保护。本文旨在从安全-性能协同优化的角度解决安全模型的压缩问题。具体来说，受到软件工程中的测试驱动开发(tDD)范式的启发，我们提出了一个测试驱动的稀疏培训框架，称为 SafeCompress。通过模拟攻击机制作为安全测试，SafeCompress 可以按照动态稀疏训练模式自动将一个大模型压缩为一个小模型。然后，考虑到黑盒子成员推理攻击和白盒子成员推理攻击这两种具有代表性和异构性的攻击机制，开发了两个具体的攻击实例: BMIA-SafeCompress 和 WMIA-SafeCompress。此外，我们通过扩展 SafeCompress 来实现另一个名为 MMIA-SafeCompress 的实例，以防止对手同时进行黑盒和白盒成员推断攻击。我们对五个数据集进行了广泛的实验，用于计算机视觉和自然语言处理任务。结果表明了该框架的有效性和可推广性。我们还讨论了如何使 SafeCompress 适应除了成员推断攻击之外的其他攻击，展示了 SafeCompress 的灵活性。"
    },
    {
        "title": "Detection and Defense Against Prominent Attacks on Preconditioned\n  LLM-Integrated Virtual Assistants",
        "url": "http://arxiv.org/abs/2401.00994v1",
        "pub_date": "2024-01-02",
        "summary": "The emergence of LLM (Large Language Model) integrated virtual assistants has\nbrought about a rapid transformation in communication dynamics. During virtual\nassistant development, some developers prefer to leverage the system message,\nalso known as an initial prompt or custom prompt, for preconditioning purposes.\nHowever, it is important to recognize that an excessive reliance on this\nfunctionality raises the risk of manipulation by malicious actors who can\nexploit it with carefully crafted prompts. Such malicious manipulation poses a\nsignificant threat, potentially compromising the accuracy and reliability of\nthe virtual assistant's responses. Consequently, safeguarding the virtual\nassistants with detection and defense mechanisms becomes of paramount\nimportance to ensure their safety and integrity. In this study, we explored\nthree detection and defense mechanisms aimed at countering attacks that target\nthe system message. These mechanisms include inserting a reference key,\nutilizing an LLM evaluator, and implementing a Self-Reminder. To showcase the\nefficacy of these mechanisms, they were tested against prominent attack\ntechniques. Our findings demonstrate that the investigated mechanisms are\ncapable of accurately identifying and counteracting the attacks. The\neffectiveness of these mechanisms underscores their potential in safeguarding\nthe integrity and reliability of virtual assistants, reinforcing the importance\nof their implementation in real-world scenarios. By prioritizing the security\nof virtual assistants, organizations can maintain user trust, preserve the\nintegrity of the application, and uphold the high standards expected in this\nera of transformative technologies.",
        "translated": "大语言模型(LLM)集成虚拟助手的出现，带来了通信动力学的快速转变。在虚拟助理开发期间，一些开发人员更喜欢利用系统消息(也称为初始提示或自定义提示)进行预处理。然而，重要的是要认识到，过度依赖这一功能会增加恶意行为者操纵的风险，这些恶意行为者可以通过精心设计的提示来利用这一功能。这种恶意操纵构成了重大威胁，可能会损害虚拟助理响应的准确性和可靠性。因此，使用检测和防御机制来保护虚拟助手对于确保其安全性和完整性至关重要。在这项研究中，我们探索了三种检测和防御机制，旨在反击针对系统消息的攻击。这些机制包括插入引用键、利用 LLM 计算器和实现 Self-Reminder。为了展示这些机制的功效，他们对突出的攻击技术进行了测试。我们的研究结果表明，所研究的机制能够准确地识别和抵消的攻击。这些机制的有效性强调了它们在保障虚拟助理的完整性和可靠性方面的潜力，强调了在现实世界中实施这些机制的重要性。通过优先考虑虚拟助手的安全性，组织可以维护用户的信任，保持应用程序的完整性，并坚持在这个变革性技术时代所期望的高标准。"
    },
    {
        "title": "A Novel Evaluation Framework for Assessing Resilience Against Prompt\n  Injection Attacks in Large Language Models",
        "url": "http://arxiv.org/abs/2401.00991v1",
        "pub_date": "2024-01-02",
        "summary": "Prompt injection attacks exploit vulnerabilities in large language models\n(LLMs) to manipulate the model into unintended actions or generate malicious\ncontent. As LLM integrated applications gain wider adoption, they face growing\nsusceptibility to such attacks. This study introduces a novel evaluation\nframework for quantifying the resilience of applications. The framework\nincorporates innovative techniques designed to ensure representativeness,\ninterpretability, and robustness. To ensure the representativeness of simulated\nattacks on the application, a meticulous selection process was employed,\nresulting in 115 carefully chosen attacks based on coverage and relevance. For\nenhanced interpretability, a second LLM was utilized to evaluate the responses\ngenerated from these simulated attacks. Unlike conventional malicious content\nclassifiers that provide only a confidence score, the LLM-based evaluation\nproduces a score accompanied by an explanation, thereby enhancing\ninterpretability. Subsequently, a resilience score is computed by assigning\nhigher weights to attacks with greater impact, thus providing a robust\nmeasurement of the application resilience. To assess the framework's efficacy,\nit was applied on two LLMs, namely Llama2 and ChatGLM. Results revealed that\nLlama2, the newer model exhibited higher resilience compared to ChatGLM. This\nfinding substantiates the effectiveness of the framework, aligning with the\nprevailing notion that newer models tend to possess greater resilience.\nMoreover, the framework exhibited exceptional versatility, requiring only\nminimal adjustments to accommodate emerging attack techniques and\nclassifications, thereby establishing itself as an effective and practical\nsolution. Overall, the framework offers valuable insights that empower\norganizations to make well-informed decisions to fortify their applications\nagainst potential threats from prompt injection.",
        "translated": "提示注入攻击利用大型语言模型(LLM)中的漏洞，将模型操纵为无意的操作或生成恶意内容。随着 LLM 集成应用程序得到更广泛的采用，它们面临着越来越多的此类攻击的可能性。这项研究介绍了一个新的评估框架来量化应用程序的弹性。该框架采用了创新技术，旨在确保代表性、可解释性和健壮性。为了确保对应用程序的模拟攻击的代表性，采用了一个细致的选择过程，最终根据覆盖率和相关性精心选择了115个攻击。为了提高可解释性，第二个 LLM 被用来评估这些模拟攻击所产生的响应。与传统的恶意内容分类器只提供置信度评分不同，基于 LLM 的评估产生一个附带解释的评分，从而提高了可解释性。随后，通过为具有更大影响的攻击赋予更高的权重来计算弹性得分，从而为应用程序弹性提供一个可靠的度量。为了评估该框架的有效性，它被应用于两个 LLM，即 Llama2和 ChatGLM。结果显示，Llama2，较新的模型表现出更高的弹性相比，ChatGLM。这一发现证实了该框架的有效性，符合较新的模型往往具有更大弹性的流行观念。此外，该框架表现出非凡的多功能性，只需要最小限度的调整，以适应新兴的攻击技术和分类，从而确立了自己作为一个有效和实用的解决方案。总的来说，该框架提供了宝贵的见解，使组织能够作出知情的决定，以加强其应用程序，应对迅速注入的潜在威胁。"
    },
    {
        "title": "CCA-Secure Hybrid Encryption in Correlated Randomness Model and KEM\n  Combiners",
        "url": "http://arxiv.org/abs/2401.00983v1",
        "pub_date": "2024-01-02",
        "summary": "A hybrid encryption (HE) system is an efficient public key encryption system\nfor arbitrarily long messages. An HE system consists of a public key component\ncalled key encapsulation mechanism (KEM), and a symmetric key component called\ndata encapsulation mechanism (DEM). The HE encryption algorithm uses a KEM\ngenerated key k to encapsulate the message using DEM, and send the ciphertext\ntogether with the encapsulaton of k, to the decryptor who decapsulates k and\nuses it to decapsulate the message using the corresponding KEM and DEM\ncomponents. The KEM/DEM composition theorem proves that if KEM and DEM satisfy\nwell-defined security notions, then HE will be secure with well defined\nsecurity. We introduce HE in correlated randomness model where the encryption\nand decryption algorithms have samples of correlated random variables that are\npartially leaked to the adversary. Security of the new KEM/DEM paradigm is\ndefined against computationally unbounded or polynomially bounded adversaries.\nWe define iKEM and cKEM with respective information theoretic computational\nsecurity, and prove a composition theorem for them and a computationally secure\nDEM, resulting in secure HEs with proved computational security (CPA and CCA)\nand without any computational assumption. We construct two iKEMs that provably\nsatisfy the required security notions of the composition theorem. The iKEMs are\nused to construct two efficient quantum-resistant HEs when used with an AES\nbased DEM. We also define and construct combiners with proved security that\ncombine the new KEM/DEM paradigm of HE with the traditional public key based\nparadigm of HE.",
        "translated": "混合加密(HE)系统是一种适用于任意长消息的高效公钥加密系统。HE 系统由称为密钥封装机制(KEM)的公钥组件和称为数据封装机制(DEM)的对称密钥组件组成。HE 加密算法使用 KEM 生成的密钥 k 对消息进行 DEM 封装，并将密文和 k 的封装一起发送给解密器，解密器解封 k 并使用相应的 KEM 和 DEM 组件对消息进行解封。KEM/DEM 合成定理证明，如果 KEM 和 DEM 满足定义良好的安全性概念，那么 HE 就是具有定义良好的安全性的。我们在相关随机模型中引入 HE，其中加密和解密算法都有相关随机变量的样本部分泄漏给对手。新的 KEM/DEM 范式的安全性是针对计算无界或多项式有界的对手定义的。定义了具有各自信息论计算安全性的 iKEM 和 cKEM，并证明了它们的一个组合定理和一个计算安全的 DEM，得到了具有经过证明的计算安全性(CPA 和 CCA)且没有任何计算假设的安全 HEs。我们构造了两个可证明满足复合定理所要求的安全性概念的 iKEM。当与基于 AES 的 DEM 一起使用时，iKEM 被用来构造两个有效的量子抗性 HEs。我们还定义和构造了具有证明安全性的组合子，将 HE 的新 KEM/DEM 范式与传统的基于公钥的 HE 范式结合起来。"
    },
    {
        "title": "Architectural Design for Secure Smart Contract Development",
        "url": "http://arxiv.org/abs/2401.01891v1",
        "pub_date": "2024-01-03",
        "summary": "As time progresses, the need for more secure applications grows\nexponentially. The different types of sensitive information that is being\ntransferred virtually has sparked a rise in systems that leverage blockchain.\nDifferent sectors are beginning to use this disruptive technology to evaluate\nthe risks and benefits. Sectors like finance, medicine, higher education, and\nwireless communication have research regarding blockchain. Futhermore, the need\nfor security standards in this area of research is pivotal. In recent past,\nseveral attacks on blockchain infrastructures have resulted in hundreds of\nmillions dollars lost and sensitive information compromised. Some of these\nattacks include DAO attacks, bZx attacks, and Parity Multisignature Wallet\nDouble Attacks which targeted vulnerabilities within smart contracts on the\nEthereum network. These attacks exposed the weaknesses of current smart\ncontract development practices which has led to the increase in distrust and\nadoption of systems that leverage blockchain for its functionality. In this\npaper, I identify common software vulnerabilities and attacks on blockchain\ninfrastructures, thoroughly detail the smart contract development process and\npropose a model for ensuring a stronger security standard for future systems\nleveraging smart contracts. The purpose for proposing a model is to promote\ntrust among end users in the system which is a foundational element for\nblockchain adoption in the future.",
        "translated": "随着时间的推移，对更安全的应用程序的需求呈指数级增长。正在传输的不同类型的敏感信息实际上已经引发了利用区块链的系统的上升。不同行业开始使用这种破坏性创新来评估风险和收益。金融、医药、高等教育和无线通信等部门对区块链进行了研究。此外，在这个研究领域对安全标准的需求也是至关重要的。在最近的过去，对区块链基础设施的几次攻击导致了数亿美元的损失和敏感信息的泄露。其中一些攻击包括 DAO 攻击、 bZx 攻击和奇偶多重签名钱包双重攻击，这些攻击针对以太网上智能合同中的漏洞。这些攻击暴露了当前智能合同开发做法的弱点，导致不信任的增加和采用的系统，利用区块链的功能。在本文中，我确定常见的软件漏洞和对区块链基础设施的攻击，彻底详细的智能合同开发过程，并提出了一个模型，以确保未来系统更强大的安全标准，利用智能合同。提出模型的目的是促进系统中最终用户之间的信任，这是未来采用区块链的基本要素。"
    },
    {
        "title": "Mining Temporal Attack Patterns from Cyberthreat Intelligence Reports",
        "url": "http://arxiv.org/abs/2401.01883v1",
        "pub_date": "2024-01-03",
        "summary": "Defending from cyberattacks requires practitioners to operate on high-level\nadversary behavior. Cyberthreat intelligence (CTI) reports on past cyberattack\nincidents describe the chain of malicious actions with respect to time. To\navoid repeating cyberattack incidents, practitioners must proactively identify\nand defend against recurring chain of actions - which we refer to as temporal\nattack patterns. Automatically mining the patterns among actions provides\nstructured and actionable information on the adversary behavior of past\ncyberattacks. The goal of this paper is to aid security practitioners in\nprioritizing and proactive defense against cyberattacks by mining temporal\nattack patterns from cyberthreat intelligence reports. To this end, we propose\nChronoCTI, an automated pipeline for mining temporal attack patterns from\ncyberthreat intelligence (CTI) reports of past cyberattacks. To construct\nChronoCTI, we build the ground truth dataset of temporal attack patterns and\napply state-of-the-art large language models, natural language processing, and\nmachine learning techniques. We apply ChronoCTI on a set of 713 CTI reports,\nwhere we identify 124 temporal attack patterns - which we categorize into nine\npattern categories. We identify that the most prevalent pattern category is to\ntrick victim users into executing malicious code to initiate the attack,\nfollowed by bypassing the anti-malware system in the victim network. Based on\nthe observed patterns, we advocate organizations to train users about\ncybersecurity best practices, introduce immutable operating systems with\nlimited functionalities, and enforce multi-user authentications. Moreover, we\nadvocate practitioners to leverage the automated mining capability of ChronoCTI\nand design countermeasures against the recurring attack patterns.",
        "translated": "防御网络攻击需要从业者对高级别的对手行为进行操作。网络威胁情报(CTI)对过去网络攻击事件的报告描述了恶意行为的时间链。为了避免再次发生网络攻击事件，从业人员必须主动识别并防范反复出现的一系列行动——我们称之为暂时性攻击模式。自动挖掘动作之间的模式可以提供关于过去网络攻击的对手行为的结构化和可操作的信息。本文的目的是通过从网络威胁情报报告中挖掘时间攻击模式，帮助安全从业人员对网络攻击进行优先级排序和主动防御。为此，我们提出 ChronoCTI，一个从网络威胁情报(CTI)过去的网络攻击报告中挖掘时间攻击模式的自动管道。为了构建 ChronoCTI，我们建立了时间攻击模式的地面真相数据集，并应用了最先进的大型语言模型、自然语言处理和机器学习技术。我们将 ChronoCTI 应用于一组713个 CTI 报告，其中我们确定了124个时间攻击模式——我们将其分为9个模式类别。我们发现，最普遍的模式类别是欺骗受害者用户执行恶意代码来发起攻击，然后绕过受害者网络中的反恶意软件系统。基于观察到的模式，我们提倡组织对用户进行网络安全最佳实践培训，引入功能有限的不可变操作系统，并实施多用户认证。此外，我们提倡从业者利用 ChronoCTI 的自动挖掘能力，并针对反复出现的攻击模式设计对策。"
    },
    {
        "title": "Attackers reveal their arsenal: An investigation of adversarial\n  techniques in CTI reports",
        "url": "http://arxiv.org/abs/2401.01865v1",
        "pub_date": "2024-01-03",
        "summary": "Context: Cybersecurity vendors often publish cyber threat intelligence (CTI)\nreports, referring to the written artifacts on technical and forensic analysis\nof the techniques used by the malware in APT attacks. Objective: The goal of\nthis research is to inform cybersecurity practitioners about how adversaries\nform cyberattacks through an analysis of adversarial techniques documented in\ncyberthreat intelligence reports. Dataset: We use 594 adversarial techniques\ncataloged in MITRE ATT\\&amp;CK. We systematically construct a set of 667 CTI\nreports that MITRE ATT\\&amp;CK used as citations in the descriptions of the\ncataloged adversarial techniques. Methodology: We analyze the frequency and\ntrend of adversarial techniques, followed by a qualitative analysis of the\nimplementation of techniques. Next, we perform association rule mining to\nidentify pairs of techniques recurring in APT attacks. We then perform\nqualitative analysis to identify the underlying relations among the techniques\nin the recurring pairs. Findings: The set of 667 CTI reports documents 10,370\ntechniques in total, and we identify 19 prevalent techniques accounting for\n37.3\\% of documented techniques. We also identify 425 statistically significant\nrecurring pairs and seven types of relations among the techniques in these\npairs. The top three among the seven relationships suggest that techniques used\nby the malware inter-relate with one another in terms of (a) abusing or\naffecting the same system assets, (b) executing in sequences, and (c)\noverlapping in their implementations. Overall, the study quantifies how\nadversaries leverage techniques through malware in APT attacks based on\npublicly reported documents. We advocate organizations prioritize their defense\nagainst the identified prevalent techniques and actively hunt for potential\nmalicious intrusion based on the identified pairs of techniques.",
        "translated": "背景: 网络安全供应商经常发布网络威胁情报(CTI)报告，指的是恶意软件在 APT 攻击中使用的技术和取证分析技术的书面文件。目标: 本研究的目的是通过分析网络威胁情报报告中记录的对抗技巧，向网络安全从业人员介绍对手是如何形成网络攻击的。数据集: 我们使用 MITRE ATT & CK 目录中的594种对抗性技术。我们系统地构建了一套667份 CTI 报告，MITRE ATT & CK 在对抗性技术的分类描述中作为引文使用。方法: 我们分析对抗性技巧的频率和趋势，然后对技巧的实施进行定性分析。接下来，我们执行关联规则挖掘来识别 APT 攻击中重复出现的技术对。然后，我们进行定性分析，以确定之间的潜在关系的技术重复对。结果: 667份 CTI 报告总共记录了10,370种技术，我们确定了19种流行的技术，占记录技术的37.3% 。我们还确定了425个具有统计学意义的循环对和这些对中技术之间的7种类型的关系。这七种关系中的前三种表明，恶意软件所使用的技术在以下方面是相互关联的: (a)滥用或影响相同的系统资产，(b)按顺序执行，(c)在其实现中重叠。总的来说，这项研究量化了对手如何利用技术通过恶意软件在 APT 攻击的基础上公开报告的文件。我们主张组织优先考虑对已识别的流行技术的防御，并根据已识别的技术对积极寻找潜在的恶意入侵。"
    },
    {
        "title": "EPA: Neural Collapse Inspired Robust Out-of-Distribution Detector",
        "url": "http://arxiv.org/abs/2401.01710v1",
        "pub_date": "2024-01-03",
        "summary": "Out-of-distribution (OOD) detection plays a crucial role in ensuring the\nsecurity of neural networks. Existing works have leveraged the fact that\nIn-distribution (ID) samples form a subspace in the feature space, achieving\nstate-of-the-art (SOTA) performance. However, the comprehensive characteristics\nof the ID subspace still leave under-explored. Recently, the discovery of\nNeural Collapse ($\\mathcal{NC}$) sheds light on novel properties of the ID\nsubspace. Leveraging insight from $\\mathcal{NC}$, we observe that the Principal\nAngle between the features and the ID feature subspace forms a superior\nrepresentation for measuring the likelihood of OOD. Building upon this\nobservation, we propose a novel $\\mathcal{NC}$-inspired OOD scoring function,\nnamed Entropy-enhanced Principal Angle (EPA), which integrates both the global\ncharacteristic of the ID subspace and its inner property. We experimentally\ncompare EPA with various SOTA approaches, validating its superior performance\nand robustness across different network architectures and OOD datasets.",
        "translated": "OOD 检测在保证神经网络的安全性方面起着至关重要的作用。现有作品利用了 In-distribution (ID)样本在特性空间中形成子空间的事实，从而实现了最先进的(SOTA)性能。然而，ID 子空间的综合特性仍然没有得到充分的研究。最近，神经崩溃的发现揭示了 ID 子空间的一些新的性质。利用 $mathcal { NC } $的洞察力，我们观察到特征和 ID 特征子空间之间的主角形成了测量面向对象的可能性的优越表示。在此基础上，我们提出了一种新的基于数学计算的面向对象设计(OOD)评分函数，称为熵增主角(EPA) ，它综合了 ID 子空间的全局特性及其内部性质。我们通过实验比较 EPA 与各种 SOTA 方法，验证了其在不同网络架构和 OOD 数据集上的优越性能和鲁棒性。"
    },
    {
        "title": "A Cybersecurity Risk Analysis Framework for Systems with Artificial\n  Intelligence Components",
        "url": "http://arxiv.org/abs/2401.01630v1",
        "pub_date": "2024-01-03",
        "summary": "The introduction of the European Union Artificial Intelligence Act, the NIST\nArtificial Intelligence Risk Management Framework, and related norms demands a\nbetter understanding and implementation of novel risk analysis approaches to\nevaluate systems with Artificial Intelligence components. This paper provides a\ncybersecurity risk analysis framework that can help assessing such systems. We\nuse an illustrative example concerning automated driving systems.",
        "translated": "欧盟人工智能法案、 NIST 人工智能风险管理框架和相关规范的引入，要求更好地理解和实施新的风险分析方法，以评估具有人工智能组件的系统。本文提供了一个网络安全风险分析框架，可以帮助评估此类系统。我们使用一个关于自动驾驶系统的例子。"
    },
    {
        "title": "The Security and Privacy of Mobile Edge Computing: An Artificial\n  Intelligence Perspective",
        "url": "http://arxiv.org/abs/2401.01589v1",
        "pub_date": "2024-01-03",
        "summary": "Mobile Edge Computing (MEC) is a new computing paradigm that enables cloud\ncomputing and information technology (IT) services to be delivered at the\nnetwork's edge. By shifting the load of cloud computing to individual local\nservers, MEC helps meet the requirements of ultralow latency, localized data\nprocessing, and extends the potential of Internet of Things (IoT) for\nend-users. However, the crosscutting nature of MEC and the multidisciplinary\ncomponents necessary for its deployment have presented additional security and\nprivacy concerns. Fortunately, Artificial Intelligence (AI) algorithms can cope\nwith excessively unpredictable and complex data, which offers a distinct\nadvantage in dealing with sophisticated and developing adversaries in the\nsecurity industry. Hence, in this paper we comprehensively provide a survey of\nsecurity and privacy in MEC from the perspective of AI. On the one hand, we use\nEuropean Telecommunications Standards Institute (ETSI) MEC reference\narchitecture as our based framework while merging the Software Defined Network\n(SDN) and Network Function Virtualization (NFV) to better illustrate a\nserviceable platform of MEC. On the other hand, we focus on new security and\nprivacy issues, as well as potential solutions from the viewpoints of AI.\nFinally, we comprehensively discuss the opportunities and challenges associated\nwith applying AI to MEC security and privacy as possible future research\ndirections.",
        "translated": "移动边缘计算(MEC)是一种新的计算范式，它使云计算和信息技术(IT)服务能够在网络的边缘交付。通过将云计算的负载转移到单个本地服务器，MEC 帮助满足超低延迟、本地化数据处理的需求，并为终端用户扩展物联网(IoT)的潜力。然而，MEC 的跨领域性质及其部署所需的多学科组成部分引起了额外的安全和隐私问题。幸运的是，人工智能(AI)算法可以处理过于不可预测和复杂的数据，这在处理安全行业中复杂和发展中的对手方面提供了明显的优势。因此，本文从人工智能的角度对 MEC 的安全性和隐私性进行了全面的研究。一方面，我们使用欧洲电信标准协会(ETSI)的 MEC 参考体系结构作为我们的基础框架，同时融合了软件定义网络(SDN)和网络功能虚拟化(NFV) ，以更好地说明 MEC 的可服务平台。另一方面，我们关注新的安全和隐私问题，以及从人工智能的观点潜在的解决方案。最后，作为未来可能的研究方向，我们全面讨论了人工智能应用于 MEC 安全和隐私的机遇和挑战。"
    },
    {
        "title": "A Survey of Protocol Fuzzing",
        "url": "http://arxiv.org/abs/2401.01568v1",
        "pub_date": "2024-01-03",
        "summary": "Communication protocols form the bedrock of our interconnected world, yet\nvulnerabilities within their implementations pose significant security threats.\nRecent developments have seen a surge in fuzzing-based research dedicated to\nuncovering these vulnerabilities within protocol implementations. However,\nthere still lacks a systematic overview of protocol fuzzing for answering the\nessential questions such as what the unique challenges are, how existing works\nsolve them, etc. To bridge this gap, we conducted a comprehensive investigation\nof related works from both academia and industry. Our study includes a detailed\nsummary of the specific challenges in protocol fuzzing, and provides a\nsystematic categorization and overview of existing research efforts.\nFurthermore, we explore and discuss potential future research directions in\nprotocol fuzzing. This survey serves as a foundational guideline for\nresearchers and practitioners in the field.",
        "translated": "通信协议构成了我们相互联系的世界的基石，然而其实现中的漏洞构成了重大的安全威胁。最近的事态发展表明，在协议实施过程中，致力于发现这些漏洞的基于模糊的研究激增。然而，对于协议模糊化在回答诸如什么是独特的挑战、现有工作如何解决这些挑战等基本问题方面还缺乏系统的概述。为了弥补这一差距，我们对学术界和业界的相关工作进行了全面的调查。我们的研究包括协议模糊化的具体挑战的详细总结，并提供了一个系统的分类和现有研究工作的概述。此外，本文还对协议模糊化的研究方向进行了探讨。这项调查为该领域的研究人员和从业人员提供了基本指南。"
    },
    {
        "title": "The Art of Deception: Robust Backdoor Attack using Dynamic Stacking of\n  Triggers",
        "url": "http://arxiv.org/abs/2401.01537v1",
        "pub_date": "2024-01-03",
        "summary": "The area of Machine Learning as a Service (MLaaS) is experiencing increased\nimplementation due to recent advancements in the AI (Artificial Intelligence)\nindustry. However, this spike has prompted concerns regarding AI defense\nmechanisms, specifically regarding potential covert attacks from third-party\nproviders that cannot be entirely trusted. Recent research has uncovered that\nauditory backdoors may use certain modifications as their initiating mechanism.\nDynamicTrigger is introduced as a methodology for carrying out dynamic backdoor\nattacks that use cleverly designed tweaks to ensure that corrupted samples are\nindistinguishable from clean. By utilizing fluctuating signal sampling rates\nand masking speaker identities through dynamic sound triggers (such as the\nclapping of hands), it is possible to deceive speech recognition systems (ASR).\nOur empirical testing demonstrates that DynamicTrigger is both potent and\nstealthy, achieving impressive success rates during covert attacks while\nmaintaining exceptional accuracy with non-poisoned datasets.",
        "translated": "机器学习作为一种服务(MLaaS)的领域正在经历增加的实施，由于最近在人工智能(AI)行业的进步。然而，这种激增已经引起了人们对人工智能防御机制的担忧，特别是不能完全信任的来自第三方供应商的潜在秘密攻击。最近的研究发现，听觉后门可能使用某些修改作为其启动机制。DynamicTrigger 是一种用于执行动态后门攻击的方法，它使用设计巧妙的调整来确保损坏的样本与干净的样本无法区分。通过利用波动的信号采样率和通过动态声音触发(如拍手)来掩盖说话人身份，有可能欺骗语音识别系统(ASR)。我们的实证测试表明，DynamicTrigger 既有效又隐蔽，在秘密攻击中取得了令人印象深刻的成功率，同时保持了非中毒数据集的异常准确性。"
    },
    {
        "title": "Will 6G be Semantic Communications? Opportunities and Challenges from\n  Task Oriented and Secure Communications to Integrated Sensing",
        "url": "http://arxiv.org/abs/2401.01531v1",
        "pub_date": "2024-01-03",
        "summary": "This paper explores opportunities and challenges of task (goal)-oriented and\nsemantic communications for next-generation (NextG) communication networks\nthrough the integration of multi-task learning. This approach employs deep\nneural networks representing a dedicated encoder at the transmitter and\nmultiple task-specific decoders at the receiver, collectively trained to handle\ndiverse tasks including semantic information preservation, source input\nreconstruction, and integrated sensing and communications. To extend the\napplicability from point-to-point links to multi-receiver settings, we envision\nthe deployment of decoders at various receivers, where decentralized learning\naddresses the challenges of communication load and privacy concerns, leveraging\nfederated learning techniques that distribute model updates across\ndecentralized nodes. However, the efficacy of this approach is contingent on\nthe robustness of the employed deep learning models. We scrutinize potential\nvulnerabilities stemming from adversarial attacks during both training and\ntesting phases. These attacks aim to manipulate both the inputs at the encoder\nat the transmitter and the signals received over the air on the receiver side,\nhighlighting the importance of fortifying semantic communications against\npotential multi-domain exploits. Overall, the joint and robust design of\ntask-oriented communications, semantic communications, and integrated sensing\nand communications in a multi-task learning framework emerges as the key\nenabler for context-aware, resource-efficient, and secure communications\nultimately needed in NextG network systems.",
        "translated": "本文通过多任务学习的集成，探讨了下一代通信网络中面向任务(目标)和语义通信的机遇和挑战。这种方法使用深层神经网络，在发射端代表一个专用的编码器，在接收端代表多个任务特定的解码器，集体训练以处理不同的任务，包括语义信息保存、源输入重建以及综合传感和通信。为了将适用性从点对点链接扩展到多接收器设置，我们设想在各种接收器上部署解码器，其中分散学习解决通信负载和隐私问题的挑战，利用联邦学习技术将模型更新分布在分散的节点上。然而，这种方法的有效性取决于所使用的深度学习模型的稳健性。在训练和测试阶段，我们都会仔细检查来自对手攻击的潜在漏洞。这些攻击的目的是操纵发射端编码器的输入和接收端空中接收的信号，突出了加强语义通信以对抗潜在的多领域利用的重要性。总的来说，在一个多任务学习框架中，面向任务的通信、语义通信以及综合传感和通信的联合和稳健设计成为了下一代网络系统中最终需要的上下文感知、资源高效和安全通信的关键推动者。"
    },
    {
        "title": "Specific Emitter Identification Based on Joint Variational Mode\n  Decomposition",
        "url": "http://arxiv.org/abs/2401.01503v1",
        "pub_date": "2024-01-03",
        "summary": "Specific emitter identification (SEI) technology is significant in device\nadministration scenarios, such as self-organized networking and spectrum\nmanagement, owing to its high security. For nonlinear and non-stationary\nelectromagnetic signals, SEI often employs variational modal decomposition\n(VMD) to decompose the signal in order to effectively characterize the distinct\ndevice fingerprint. However, the trade-off of VMD between the robustness to\nnoise and the ability to preserve signal information has not been investigated\nin the current literature. Moreover, the existing VMD algorithm does not\nutilize the stability of the intrinsic distortion of emitters within a certain\ntemporal span, consequently constraining its practical applicability in SEI. In\nthis paper, we propose a joint variational modal decomposition (JVMD)\nalgorithm, which is an improved version of VMD by simultaneously implementing\nmodal decomposition on multi-frame signals. The consistency of multi-frame\nsignals in terms of the central frequencies and the inherent modal functions\n(IMFs) is exploited, which effectively highlights the distinctive\ncharacteristics among emitters and reduces noise. Additionally, the complexity\nof JVMD is analyzed, which is proven to be more computational-friendly than\nVMD. Simulations of both modal decomposition and SEI that involve real-world\ndatasets are presented to illustrate that when compared with VMD, the JVMD\nalgorithm improves the accuracy of device classification and the robustness\ntowards noise.",
        "translated": "特定辐射源识别(SEI)技术具有高度的安全性，在设备管理场景中具有重要意义，如自组织网络和频谱管理。对于非线性和非平稳电磁信号，SEI 通常采用变分模态分解(VMD)对信号进行分解，以有效地表征不同器件的指纹特征。然而，VMD 对噪声的鲁棒性和保持信号信息的能力之间的权衡在目前的文献中还没有被研究。此外，现有的 VMD 算法没有利用发射极在一定时间跨度内固有畸变的稳定性，从而限制了其在 SEI 中的实际应用。本文提出了一种联合变分模态分解(JVMD)算法，它是 VMD 的一个改进版本，同时对多帧信号进行模态分解。利用多帧信号在中心频率和固有模态函数(IMF)方面的一致性，有效地突出了发射器之间的独特性，降低了噪声。此外，分析了 JVMD 的复杂性，结果表明 JVMD 比 VMD 具有更好的计算友好性。仿真结果表明，与 VMD 算法相比，JVMD 算法提高了器件分类的准确性和对噪声的鲁棒性。"
    },
    {
        "title": "Evasive Hardware Trojan through Adversarial Power Trace",
        "url": "http://arxiv.org/abs/2401.02342v1",
        "pub_date": "2024-01-04",
        "summary": "The globalization of the Integrated Circuit (IC) supply chain, driven by\ntime-to-market and cost considerations, has made ICs vulnerable to hardware\nTrojans (HTs). Against this threat, a promising approach is to use Machine\nLearning (ML)-based side-channel analysis, which has the advantage of being a\nnon-intrusive method, along with efficiently detecting HTs under golden\nchip-free settings. In this paper, we question the trustworthiness of ML-based\nHT detection via side-channel analysis. We introduce a HT obfuscation (HTO)\napproach to allow HTs to bypass this detection method. Rather than\ntheoretically misleading the model by simulated adversarial traces, a key\naspect of our approach is the design and implementation of adversarial noise as\npart of the circuitry, alongside the HT. We detail HTO methodologies for ASICs\nand FPGAs, and evaluate our approach using TrustHub benchmark. Interestingly,\nwe found that HTO can be implemented with only a single transistor for ASIC\ndesigns to generate adversarial power traces that can fool the defense with\n100% efficiency. We also efficiently implemented our approach on a Spartan 6\nXilinx FPGA using 2 different variants: (i) DSP slices-based, and (ii)\nring-oscillator-based design. Additionally, we assess the efficiency of\ncountermeasures like spectral domain analysis, and we show that an adaptive\nattacker can still design evasive HTOs by constraining the design with a\nspectral noise budget. In addition, while adversarial training (AT) offers\nhigher protection against evasive HTs, AT models suffer from a considerable\nutility loss, potentially rendering them unsuitable for such security\napplication. We believe this research represents a significant step in\nunderstanding and exploiting ML vulnerabilities in a hardware security context,\nand we make all resources and designs openly available online:\nhttps://dev.d18uu4lqwhbmka.amplifyapp.com",
        "translated": "集成电路(IC)供应链的全球化，由上市时间和成本的考虑驱动，使集成电路容易受到硬件木马(HTs)。针对这一威胁，一个有前途的方法是使用基于机器学习(ML)的边通道分析，其优点是非侵入性的方法，以及在无黄金芯片设置下有效检测高温超导。本文通过边信道分析对基于 ML 的 HT 检测的可信性提出了质疑。我们引入了一种 HT 模糊(HTO)方法来允许 HT 绕过这种检测方法。我们的方法的一个关键方面是设计和实现对抗噪声作为电路的一部分，与 HT 一起，而不是通过模拟对抗迹在理论上误导模型。我们详细介绍了 ASIC 和 FPGA 的 HTO 方法，并使用 TrustHub 基准评估了我们的方法。有趣的是，我们发现 HTO 可以只用一个专用集成电路设计的晶体管来实现，从而产生对抗性的功率轨迹，可以100% 有效地欺骗防御者。我们还有效地实现了我们的方法在斯巴达6 Xilinx FPGA 上使用2种不同的变种: (i)基于 DSP 片和(ii)基于环振荡器的设计。此外，我们还评估了像谱域分析这样的对抗措施的效率，并且我们表明自适应攻击者仍然可以通过限制设计带有谱噪声预算的设计来设计规避性的高清晰度目标。此外，虽然对抗性训练(AT)提供了更高的保护，以避免回避 HTs，AT 模型遭受了相当大的效用损失，潜在地使他们不适合这种安全应用。我们相信这项研究代表了在硬件安全背景下理解和利用机器学习漏洞的重要一步，我们将所有的资源和设计公开在网上:  https://dev.d18uu4lqwhbmka.amplifyapp.com"
    },
    {
        "title": "Kernel Search approach to solve the Minimum Spanning Tree Problem with\n  conflicting edge pairs",
        "url": "http://arxiv.org/abs/2401.02222v1",
        "pub_date": "2024-01-04",
        "summary": "The Minimum Spanning Tree Problem with Conflicts consists in finding the\nminimum conflict-free spanning tree of a graph, i.e., the spanning tree of\nminimum cost, including no pairs of edges that are in conflict. In this paper,\nwe solve this problem using a tailored Kernel Search heuristic method, which\nconsists in solving iteratively improved restrictions of the problem. The main\nnovelty of the approach consists in using an independent set of the conflict\ngraph within the algorithm. We test our approach on the benchmark instances and\nwe compare our results with the ones obtained by other heuristics available in\nthe literature.",
        "translated": "带冲突的最小生成树问题包括寻找图的最小无冲突生成树，即最小开销生成树，不包括任何冲突边对。在本文中，我们使用一个剪裁核搜索启发式方法来解决这个问题，其中包括迭代求解改进的问题的约束条件。该方法的主要新颖之处在于在算法中使用了一个独立的冲突图集。我们在基准实例上测试我们的方法，并将我们的结果与文献中可用的其他启发式方法所获得的结果进行比较。"
    },
    {
        "title": "Disentangle Estimation of Causal Effects from Cross-Silo Data",
        "url": "http://arxiv.org/abs/2401.02154v1",
        "pub_date": "2024-01-04",
        "summary": "Estimating causal effects among different events is of great importance to\ncritical fields such as drug development. Nevertheless, the data features\nassociated with events may be distributed across various silos and remain\nprivate within respective parties, impeding direct information exchange between\nthem. This, in turn, can result in biased estimations of local causal effects,\nwhich rely on the characteristics of only a subset of the covariates. To tackle\nthis challenge, we introduce an innovative disentangle architecture designed to\nfacilitate the seamless cross-silo transmission of model parameters, enriched\nwith causal mechanisms, through a combination of shared and private branches.\nBesides, we introduce global constraints into the equation to effectively\nmitigate bias within the various missing domains, thereby elevating the\naccuracy of our causal effect estimation. Extensive experiments conducted on\nnew semi-synthetic datasets show that our method outperforms state-of-the-art\nbaselines.",
        "translated": "估计不同事件之间的因果效应对药物开发等关键领域具有重要意义。尽管如此，与事件有关的数据特征可能分布在各个筒仓中，在各方内部保持私密，妨碍它们之间的直接信息交流。这反过来又可能导致局部因果效应的有偏估计，这种估计仅依赖于协变量的一个子集的特征。为了应对这一挑战，我们引入了一个创新的分离架构，旨在促进模型参数的无缝跨筒仓传输，通过共享和私有分支的组合，丰富了因果机制。此外，我们在方程中引入全局约束，以有效地减轻各种缺失域内的偏差，从而提高因果效应估计的准确性。在新的半合成数据集上进行的大量实验表明，我们的方法优于最先进的基线。"
    },
    {
        "title": "PosCUDA: Position based Convolution for Unlearnable Audio Datasets",
        "url": "http://arxiv.org/abs/2401.02135v1",
        "pub_date": "2024-01-04",
        "summary": "Deep learning models require large amounts of clean data to acheive good\nperformance. To avoid the cost of expensive data acquisition, researchers use\nthe abundant data available on the internet. This raises significant privacy\nconcerns on the potential misuse of personal data for model training without\nauthorisation. Recent works such as CUDA propose solutions to this problem by\nadding class-wise blurs to make datasets unlearnable, i.e a model can never use\nthe acquired dataset for learning. However these methods often reduce the\nquality of the data making it useless for practical applications. We introduce\nPosCUDA, a position based convolution for creating unlearnable audio datasets.\nPosCUDA uses class-wise convolutions on small patches of audio. The location of\nthe patches are based on a private key for each class, hence the model learns\nthe relations between positional blurs and labels, while failing to generalize.\nWe empirically show that PosCUDA can achieve unlearnability while maintaining\nthe quality of the original audio datasets. Our proposed method is also robust\nto different audio feature representations such as MFCC, raw audio and\ndifferent architectures such as transformers, convolutional networks etc.",
        "translated": "深度学习模型需要大量清晰的数据才能获得良好的性能。为了避免昂贵的数据采集成本，研究人员利用互联网上的大量数据。这引起了人们对未经授权将个人数据用于模型培训的潜在误用的重大隐私关切。最近的工作，如 CUDA 提出了解决这个问题的方案，增加类的模糊，使数据集不可学习，即一个模型永远不能使用获得的数据集学习。然而，这些方法往往会降低数据的质量，使其无法用于实际应用。我们介绍了 PosCUDA，一个位置为基础的卷积创建不可学习的音频数据集。PosCUDA 在小的音频片段上使用类卷积。补丁的位置基于每个类的私有密钥，因此模型学习位置模糊和标签之间的关系，而不能泛化。实验结果表明，PosCUDA 可以在保持原始音频数据质量的同时实现不可学习性。该方法对不同的音频特征表示如 MFCC、原始音频和不同的结构如变压器、卷积网络等具有较强的鲁棒性。"
    },
    {
        "title": "Travelers: A scalable fair ordering BFT system",
        "url": "http://arxiv.org/abs/2401.02030v1",
        "pub_date": "2024-01-04",
        "summary": "Many blockchain platform are subject to maximal value extraction (MEV), and\nusers on the platform are losing money while sending transactions because the\ntransaction order can be manipulated to extract value from them. Consensus\nprotocols have been augmented with different notion of fair ordering in order\nto counter the problem. Out of all practical protocols, the most efficient BFT\nconsensus requires $O(nTL + n^2T)$ communication complexity, where $n$ is\nnumber node, $T$ is number of transactions and $L$ is average transaction size.\nIn this work, we propose a new system of BFT fair ordering protocols,\nTravelers, that substantially reduce the communication complexity. The proposed\nsystem of protocols satisfy a new notion of fair ordering, called probabilistic\nfair ordering, which is an extension to some existing notions of fairness. The\nnew notion allows a small probability of error $\\epsilon$, that adversary can\ninsert some transactions at any location in a block, but for the remaining\n$1-\\epsilon$ the a modified version of ordering linearizability holds. Our\nmechanism neither require a dissemination network nor direct submissions to all\nconsensus nodes. The key innovation comes from a routing protocol, that is both\nflexible and efficient. We construct a protocol with $O(c\\log({n})TL + n^2)$\ncommunication complexity with $\\epsilon = 1/n^c$ for some system parameter\n$c\\ge 1$.",
        "translated": "许多区块链平台都受到最大价值提取(MEV)的影响，平台上的用户在发送交易时由于交易顺序可以被操纵以从中提取价值而蒙受损失。为了解决这一问题，协商一致的协议已经增加了公平排序的不同概念。在所有实用协议中，最有效的 BFT 共识需要 $O (nTL + n ^ 2T) $通信复杂度，其中 $n $是数字节点，$T $是事务数，$L $是平均事务大小。在这项工作中，我们提出了一个新的 BFT 公平订购协议系统，旅行者，大大降低了通信的复杂性。所提出的协议体系满足一种新的公平排序概念，即概率公平排序，它是对现有公平概念的扩展。新的概念允许出现错误的可能性很小，即对手可以在块中的任何位置插入一些事务，但是对于剩下的 $1-epsilon $，修改后的排序线性化版本仍然有效。我们的机制既不需要一个传播网络，也不需要向所有共识节点直接提交材料。关键的创新来自于一种灵活而高效的路由协议。我们构造了一个具有 $O (c log ({ n }) TL + n ^ 2) $通信复杂度的协议，该协议对于某个系统参数 $c ge 1 $具有 $epsilon = 1/n ^ c $。"
    },
    {
        "title": "DApps Ecosystems: Mapping the Network Structure of Smart Contract\n  Interactions",
        "url": "http://arxiv.org/abs/2401.01991v1",
        "pub_date": "2024-01-03",
        "summary": "In recent years, decentralized applications (dApps) built on blockchain\nplatforms such as Ethereum and coded in languages such as Solidity, have gained\nattention for their potential to disrupt traditional centralized systems.\nDespite their rapid adoption, limited research has been conducted to understand\nthe underlying code structure of these applications. In particular, each dApp\nis composed of multiple smart contracts, each containing a number of functions\nthat can be called to trigger a specific event, e.g., a token transfer. In this\npaper, we reconstruct and analyse the network of contracts and functions calls\nwithin the dApp, which is helpful to unveil vulnerabilities that can be\nexploited by malicious attackers. We show how decentralization is\narchitecturally implemented, identifying common development patterns and\nanomalies that could influence the system's robustness and efficiency. We find\na consistent network structure characterized by modular, self-sufficient\ncontracts and a complex web of function interactions, indicating common coding\npractices across the blockchain community. Critically, a small number of key\nfunctions within each dApp play a pivotal role in maintaining network\nconnectivity, making them potential targets for cyber attacks and highlighting\nthe need for robust security measures.",
        "translated": "近年来，分散式应用(dApps)建立在区块链平台(如 Etherum)之上，并用 Solidy 等语言进行编码，由于它们有可能破坏传统的集中式系统而引起了人们的关注。尽管它们被迅速采用，但是为了理解这些应用程序的底层代码结构而进行的研究有限。特别是，每个 dApp 由多个智能契约组成，每个契约包含许多可以调用以触发特定事件的函数，例如令牌传输。在本文中，我们重构和分析了 dApp 内部的契约和函数调用网络，这有助于揭示恶意攻击者可以利用的漏洞。我们展示如何在架构上实现地方分权，识别可能影响系统健壮性和效率的常见开发模式和异常情况。我们发现一个一致的网络结构，拥有属性模块化，自给自足的契约和一个复杂的功能交互网络，表明共同的编码实践整个区块链社区。重要的是，每个 dApp 中的少数关键功能在维持网络连接方面发挥关键作用，使其成为网络攻击的潜在目标，并突出强有力的安全措施的必要性。"
    },
    {
        "title": "Shadow Blade: A tool to interact with attack vectors",
        "url": "http://arxiv.org/abs/2401.01960v1",
        "pub_date": "2024-01-03",
        "summary": "The increased demand of cyber security professionals has also increased the\ndevelopment of new platforms and tools that help those professionals to improve\ntheir offensive skills. One of these platforms is HackTheBox, an online cyber\nsecurity training platform that delivers a controlled and safe environment for\nthose professionals to explore virtual machines in a Capture the Flag (CTF)\ncompetition style.\n  Most of the tools used in a CTF, or even on real-world Penetration Testing\n(Pentest), were developed for specific reasons so each tool usually has\ndifferent input and output formats. These different formats make it hard for\ncyber security professionals and CTF competitors to develop an attack graph. In\norder to help cyber security professionals and CTF competitors to discover,\nselect and exploit an attack vector, this paper presents Shadow Blade, a tool\nto aid users to interact with their attack vectors.",
        "translated": "对网络安全专业人员需求的增加也促进了新平台和工具的开发，以帮助这些专业人员提高进攻技能。其中一个平台是 HackTheBox，这是一个在线网络安全培训平台，为那些专业人士提供一个可控的、安全的环境，让他们以捕获旗帜(CTF)竞赛风格探索虚拟机。CTF 中使用的大多数工具，甚至在现实世界中的渗透测试(Pentest)中使用的大多数工具，都是出于特定的原因而开发的，因此每个工具通常具有不同的输入和输出格式。这些不同的格式使得网络安全专家和 CTF 的竞争对手很难开发出攻击图表。为了帮助网络安全专业人员和 CTF 竞争对手发现、选择和利用攻击矢量，本文提出了一种帮助用户与攻击矢量进行交互的工具 Shadow Blade。"
    },
    {
        "title": "Towards a zk-SNARK compiler for Wolfram language",
        "url": "http://arxiv.org/abs/2401.02935v1",
        "pub_date": "2024-01-05",
        "summary": "Zero-knowledge proofs (zk-Proofs) are communication protocols by which a\nprover can demonstrate to a verifier that it possesses a solution to a given\npublic problem without revealing the content of the solution. Arbitrary\ncomputations can be transformed into an interactive zk-Proof so anyone is\nconvinced that it was executed correctly without knowing what was executed on,\nhaving huge implications for digital currency. Despite this, interactive proofs\nare not suited for blockchain applications but novel protocols such as\nzk-SNARKs have made zero-knowledge ledgers like Zcash possible. This project\nbuilds upon Wolfram's ZeroKnowledgeProofs paclet and implements a zk-SNARK\ncompiler based on Pinocchio protocol.",
        "translated": "零知识证明(zk-Proofs)是一种通信协议，通过这种协议，证明者可以向验证者证明它拥有一个给定公共问题的解决方案，而不需要透露解决方案的内容。任意计算可以转换成交互式 zk-Proof，因此任何人都可以确信它在不知道执行内容的情况下正确执行，这对数字货币具有巨大的影响。尽管如此，交互式证明并不适合区块链应用，但是像 zk-SNARKs 这样的新协议使得 Zcash 这样的零知识分类账成为可能。该项目建立在 Wolfram 的 ZeroKnowledgeProofs 小包的基础上，并实现了一个基于匹诺曹协议的 zk-SNARK 编译器。"
    },
    {
        "title": "MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance",
        "url": "http://arxiv.org/abs/2401.02906v1",
        "pub_date": "2024-01-05",
        "summary": "The deployment of multimodal large language models (MLLMs) has brought forth\na unique vulnerability: susceptibility to malicious attacks through visual\ninputs. We delve into the novel challenge of defending MLLMs against such\nattacks. We discovered that images act as a \"foreign language\" that is not\nconsidered during alignment, which can make MLLMs prone to producing harmful\nresponses. Unfortunately, unlike the discrete tokens considered in text-based\nLLMs, the continuous nature of image signals presents significant alignment\nchallenges, which poses difficulty to thoroughly cover the possible scenarios.\nThis vulnerability is exacerbated by the fact that open-source MLLMs are\npredominantly fine-tuned on limited image-text pairs that is much less than the\nextensive text-based pretraining corpus, which makes the MLLMs more prone to\ncatastrophic forgetting of their original abilities during explicit alignment\ntuning. To tackle these challenges, we introduce MLLM-Protector, a\nplug-and-play strategy combining a lightweight harm detector and a response\ndetoxifier. The harm detector's role is to identify potentially harmful outputs\nfrom the MLLM, while the detoxifier corrects these outputs to ensure the\nresponse stipulates to the safety standards. This approach effectively\nmitigates the risks posed by malicious visual inputs without compromising the\nmodel's overall performance. Our results demonstrate that MLLM-Protector offers\na robust solution to a previously unaddressed aspect of MLLM security.",
        "translated": "多模态大语言模型(MLLM)的部署带来了一个独特的弱点: 通过视觉输入容易受到恶意攻击。我们深入探讨了保护 MLLM 免受此类攻击的新挑战。我们发现，图像作为一种“外语”，在对齐过程中不被考虑，这可能使 MLLM 容易产生有害的反应。遗憾的是，与基于文本的 LLM 中考虑的离散标记不同，图像信号的连续性质提出了重大的对齐挑战，这给彻底覆盖可能的场景带来了困难。开源的 MLLM 主要在有限的图像-文本对上进行微调，这比广泛的基于文本的预训练语料库要少得多，这使得 MLLM 在显式对齐调整期间更容易灾难性地忘记它们的原始能力，这一事实加剧了这种脆弱性。为了应对这些挑战，我们引入了 MLLM-Protector，这是一种即插即用的策略，结合了轻量级的伤害检测器和响应解毒器。危害检测器的作用是识别来自 MLLM 的潜在有害输出，而解毒剂纠正这些输出以确保响应符合安全标准。这种方法在不损害模型整体性能的情况下，有效地减轻了恶意视觉输入带来的风险。我们的结果表明，MLLM-Protector 提供了一个健壮的解决方案，以前未解决的方面的 MLLM 安全。"
    },
    {
        "title": "Lotto: Secure Participant Selection against Adversarial Servers in\n  Federated Learning",
        "url": "http://arxiv.org/abs/2401.02880v1",
        "pub_date": "2024-01-05",
        "summary": "In Federated Learning (FL), common privacy-preserving technologies, such as\nsecure aggregation and distributed differential privacy, rely on the critical\nassumption of an honest majority among participants to withstand various\nattacks. In practice, however, servers are not always trusted, and an\nadversarial server can strategically select compromised clients to create a\ndishonest majority, thereby undermining the system's security guarantees. In\nthis paper, we present Lotto, an FL system that addresses this fundamental, yet\nunderexplored issue by providing secure participant selection against an\nadversarial server. Lotto supports two selection algorithms: random and\ninformed. To ensure random selection without a trusted server, Lotto enables\neach client to autonomously determine their participation using verifiable\nrandomness. For informed selection, which is more vulnerable to manipulation,\nLotto approximates the algorithm by employing random selection within a refined\nclient pool. Our theoretical analysis shows that Lotto effectively restricts\nthe number of server-selected compromised clients, thus ensuring an honest\nmajority among participants. Large-scale experiments further reveal that Lotto\nachieves time-to-accuracy performance comparable to that of insecure selection\nmethods, indicating a low computational overhead for secure selection.",
        "translated": "在联邦学习(Federated Learning，FL)中，常见的保护隐私的技术，如安全聚合和分布式差分隐私，依赖于参与者中诚实多数的关键假设，以抵御各种攻击。然而，在实践中，服务器并不总是可信的，敌对服务器可以战略性地选择受到损害的客户端，从而创造不诚实的大多数，从而破坏系统的安全保障。在本文中，我们提出了 Lotto，一个 FL 系统，解决这个基本的，但未被充分探讨的问题，通过提供安全的参与者选择对抗服务器。Lotto 支持两种选择算法: 随机选择和知情选择。为了确保在没有可信服务器的情况下进行随机选择，Lotto 允许每个客户使用可验证的随机性自主确定他们的参与。对于更容易受到操纵的知情选择，Lotto 通过在精化的客户机池中使用随机选择来近似算法。我们的理论分析表明，Lotto 有效地限制了服务器选择的妥协客户端的数量，从而确保了参与者中的诚实多数。大规模的实验进一步表明，Lotto 实现了与不安全选择方法相当的时间-精度性能，表明安全选择的计算开销较低。"
    },
    {
        "title": "Benchmark Performance of Homomorphic Polynomial Public Key Cryptography\n  for Key Encapsulation and Digital Signature Schemes",
        "url": "http://arxiv.org/abs/2401.02803v1",
        "pub_date": "2024-01-05",
        "summary": "This paper conducts a comprehensive benchmarking analysis of the performance\nof two innovative cryptographic schemes: Homomorphic Polynomial Public Key\n(HPPK)-Key Encapsulation Mechanism (KEM) and Digital Signature (DS), recently\nproposed by Kuang et al. These schemes represent a departure from traditional\ncryptographic paradigms, with HPPK leveraging the security of homomorphic\nsymmetric encryption across two hidden rings without reliance on NP-hard\nproblems. HPPK can be viewed as a specialized variant of Multivariate Public\nKey Cryptography (MPKC), intricately associated with two vector spaces: the\npolynomial vector space for the secret exchange and the multivariate vector\nspace for randomized encapsulation.\n  The unique integration of asymmetric, symmetric, and homomorphic cryptography\nwithin HPPK necessitates a careful examination of its performance metrics. This\nstudy focuses on the thorough benchmarking of HPPK KEM and DS across key\ncryptographic operations, encompassing key generation, encapsulation,\ndecapsulation, signing, and verification. The results highlight the exceptional\nefficiency of HPPK, characterized by compact key sizes, cipher sizes, and\nsignature sizes. The use of symmetric encryption in HPPK enhances its overall\nperformance. Key findings underscore the outstanding performance of HPPK KEM\nand DS across various security levels, emphasizing their superiority in crucial\ncryptographic operations. This research positions HPPK as a promising and\ncompetitive solution for post-quantum cryptographic applications in a wide\nrange of applications, including blockchain, digital currency, and Internet of\nThings (IoT) devices.",
        "translated": "本文对 Kuang 等人最近提出的同态多项式公钥(HPPK)-密钥封装机制(KEM)和数字签名(DS)两种新型密码体制的性能进行了全面的基准测试分析。这些方案背离了传统的密码学范式，HPPK 利用同态对称加密在两个隐环之间的安全性，而不依赖于 NP 难问题。HPPK 可以被视为多变量公钥密码术(MPKC)的专门变体，与两个向量空间错综复杂地相关联: 用于秘密交换的多项式向量空间和用于随机封装的多变量向量空间。HPPK 中不对称、对称和同态密码的独特集成需要仔细检查其性能指标。本文主要研究 HPPK KEM 和 DS 跨密钥加密操作的全面基准测试，包括密钥生成、封装、解封装、签名和验证。研究结果突出显示 HPPK 的卓越效率、拥有属性密钥大小、密码大小和签名大小。在 HPPK 中使用对称加密提高了它的整体性能。重要发现强调了 HPPK KEM 和 DS 在不同安全级别上的出色性能，强调了它们在关键密码操作中的优越性。这项研究将 HPPK 定位为后量子密码应用的一个有前途和有竞争力的解决方案，在广泛的应用领域，包括区块链、数字货币和物联网(IoT)设备。"
    },
    {
        "title": "Ejafa_protocol: A custom INC secure protocol",
        "url": "http://arxiv.org/abs/2401.02787v1",
        "pub_date": "2024-01-05",
        "summary": "\"EJAFA_PROTOCOL: A CUSTOM INC SECURE PROTOCOL\" presents a cryptographic\nsolution tailored for lightweight devices, striking a delicate balance between\nsecurity and efficiency. The protocol incorporates modern cryptographic\nprimitives, including X25519 for key exchange and ChaCha20 for encryption,\nwhile adhering to established RFC standards. The report explores the protocol's\ndesign, implementation over various network protocols, and its performance\ncharacteristics. A key feature of the protocol is its adaptability to\nresource-constrained environments without compromising on security. This work\ncontributes to the evolving landscape of secure communication protocols,\nproviding a robust solution for practical deployment across a spectrum of\napplications.",
        "translated": "“ EJAFA _ PROTOCOL: 自定义 INC 安全协议”提供了一种为轻量级设备量身定制的加密解决方案，在安全性和效率之间突出了人海万花筒(电影)。该协议采用了现代加密原语，包括用于密钥交换的 X25519和用于加密的 ChaCha20，同时遵循已建立的 RFC 标准。报告探讨了该协议的设计、在各种网络协议上的实现及其性能特点。该协议的一个关键特征是其对资源受限环境的适应性，同时又不影响安全性。这项工作有助于不断发展的安全通信协议，为跨各种应用程序的实际部署提供了一个健壮的解决方案。"
    },
    {
        "title": "Calibration Attack: A Framework For Adversarial Attacks Targeting\n  Calibration",
        "url": "http://arxiv.org/abs/2401.02718v1",
        "pub_date": "2024-01-05",
        "summary": "We introduce a new framework of adversarial attacks, named calibration\nattacks, in which the attacks are generated and organized to trap victim models\nto be miscalibrated without altering their original accuracy, hence seriously\nendangering the trustworthiness of the models and any decision-making based on\ntheir confidence scores. Specifically, we identify four novel forms of\ncalibration attacks: underconfidence attacks, overconfidence attacks, maximum\nmiscalibration attacks, and random confidence attacks, in both the black-box\nand white-box setups. We then test these new attacks on typical victim models\nwith comprehensive datasets, demonstrating that even with a relatively low\nnumber of queries, the attacks can create significant calibration mistakes. We\nfurther provide detailed analyses to understand different aspects of\ncalibration attacks. Building on that, we investigate the effectiveness of\nwidely used adversarial defences and calibration methods against these types of\nattacks, which then inspires us to devise two novel defences against such\ncalibration attacks.",
        "translated": "我们引入了一个新的对抗性攻击框架，称为校准攻击，在这个框架中，攻击被生成并组织起来，在不改变其原始准确性的情况下将受害者模型错误校准，从而严重危及模型的可信度和任何基于其置信度得分的决策。具体来说，我们确定了四种新的校准攻击形式: 不自信攻击，过度自信攻击，最大误校正攻击和随机置信攻击，在黑盒和白盒设置。然后，我们使用全面的数据集在典型的受害者模型上测试这些新的攻击，证明即使查询数量相对较少，这些攻击也会产生显著的校准错误。我们进一步提供详细的分析，以了解校准攻击的不同方面。在此基础上，我们调查了广泛使用的对抗性防御和校准方法对这些类型的攻击的有效性，从而启发我们设计两种针对这种校准攻击的新型防御。"
    },
    {
        "title": "Beyond Fidelity: Explaining Vulnerability Localization of Learning-based\n  Detectors",
        "url": "http://arxiv.org/abs/2401.02686v1",
        "pub_date": "2024-01-05",
        "summary": "Vulnerability detectors based on deep learning (DL) models have proven their\neffectiveness in recent years. However, the shroud of opacity surrounding the\ndecision-making process of these detectors makes it difficult for security\nanalysts to comprehend. To address this, various explanation approaches have\nbeen proposed to explain the predictions by highlighting important features,\nwhich have been demonstrated effective in other domains such as computer vision\nand natural language processing. Unfortunately, an in-depth evaluation of\nvulnerability-critical features, such as fine-grained vulnerability-related\ncode lines, learned and understood by these explanation approaches remains\nlacking. In this study, we first evaluate the performance of ten explanation\napproaches for vulnerability detectors based on graph and sequence\nrepresentations, measured by two quantitative metrics including fidelity and\nvulnerability line coverage rate. Our results show that fidelity alone is not\nsufficient for evaluating these approaches, as fidelity incurs significant\nfluctuations across different datasets and detectors. We subsequently check the\nprecision of the vulnerability-related code lines reported by the explanation\napproaches, and find poor accuracy in this task among all of them. This can be\nattributed to the inefficiency of explainers in selecting important features\nand the presence of irrelevant artifacts learned by DL-based detectors.",
        "translated": "基于深度学习(DL)模型的漏洞检测器近年来已被证明是有效的。然而，围绕这些检测器的决策过程的不透明性使得安全分析师难以理解。为了解决这一问题，人们提出了各种解释方法，通过突出重要的特征来解释预测，这些特征已经在计算机视觉和自然语言处理等其他领域得到证明。不幸的是，仍然缺乏对脆弱性关键特性的深入评估，例如通过这些解释方法学习和理解的细粒度脆弱性相关代码行。在本研究中，我们首先评估了基于图和序列表示的十种漏洞检测器解释方法的性能，通过保真度和漏洞线覆盖率两个量化指标来衡量。我们的研究结果表明，保真度本身不足以评估这些方法，因为保真度会在不同的数据集和检测器之间产生显著的波动。随后，我们检查了解释方法报告的与漏洞相关的代码行的精度，发现在所有这些方法中，这个任务的精度都很差。这可以归因于解释者在选择重要特征时的低效率以及基于 DL 的检测器学习到的不相关伪影的存在。"
    },
    {
        "title": "A backdoor attack against link prediction tasks with graph neural\n  networks",
        "url": "http://arxiv.org/abs/2401.02663v1",
        "pub_date": "2024-01-05",
        "summary": "Graph Neural Networks (GNNs) are a class of deep learning models capable of\nprocessing graph-structured data, and they have demonstrated significant\nperformance in a variety of real-world applications. Recent studies have found\nthat GNN models are vulnerable to backdoor attacks. When specific patterns\n(called backdoor triggers, e.g., subgraphs, nodes, etc.) appear in the input\ndata, the backdoor embedded in the GNN models is activated, which misclassifies\nthe input data into the target class label specified by the attacker, whereas\nwhen there are no backdoor triggers in the input, the backdoor embedded in the\nGNN models is not activated, and the models work normally. Backdoor attacks are\nhighly stealthy and expose GNN models to serious security risks. Currently,\nresearch on backdoor attacks against GNNs mainly focus on tasks such as graph\nclassification and node classification, and backdoor attacks against link\nprediction tasks are rarely studied. In this paper, we propose a backdoor\nattack against the link prediction tasks based on GNNs and reveal the existence\nof such security vulnerability in GNN models, which make the backdoored GNN\nmodels to incorrectly predict unlinked two nodes as having a link relationship\nwhen a trigger appear. The method uses a single node as the trigger and poison\nselected node pairs in the training graph, and then the backdoor will be\nembedded in the GNN models through the training process. In the inference\nstage, the backdoor in the GNN models can be activated by simply linking the\ntrigger node to the two end nodes of the unlinked node pairs in the input data,\ncausing the GNN models to produce incorrect link prediction results for the\ntarget node pairs.",
        "translated": "图形神经网络(GNN)是一类能够处理图形结构数据的深度学习模型，在各种实际应用中表现出了显著的性能。最近的研究发现，GNN 模型容易受到后门攻击。当特定的模式(称为后门触发器，例如子图，节点等)出现在输入数据中时，GNN 模型中嵌入的后门被激活，这会将输入数据错误地分类到攻击者指定的目标类标签中，而当输入中没有后门触发器时，GNN 模型中嵌入的后门就不会被激活，模型正常工作。后门攻击具有高度隐蔽性，使 GNN 模型面临严重的安全风险。目前针对 GNN 的后门攻击研究主要集中在图分类和节点分类等任务上，而针对链路预测任务的后门攻击研究较少。本文针对基于 GNN 的链路预测任务提出了一种后门攻击，揭示了 GNN 模型中存在的这种安全漏洞，使得后门 GNN 模型在触发器出现时错误地预测未链接的两个节点之间存在链路关系。该方法以单个节点作为训练图中的触发节点和毒素选择节点对，然后通过训练过程将后门嵌入到 GNN 模型中。在推理阶段，GNN 模型中的后门可以通过简单地将触发节点连接到输入数据中未链接节点对的两个末端节点来激活，从而导致 GNN 模型对目标节点对产生不正确的链接预测结果。"
    },
    {
        "title": "MalModel: Hiding Malicious Payload in Mobile Deep Learning Models with\n  Black-box Backdoor Attack",
        "url": "http://arxiv.org/abs/2401.02659v1",
        "pub_date": "2024-01-05",
        "summary": "Mobile malware has become one of the most critical security threats in the\nera of ubiquitous mobile computing. Despite the intensive efforts from security\nexperts to counteract it, recent years have still witnessed a rapid growth of\nidentified malware samples. This could be partly attributed to the\nnewly-emerged technologies that may constantly open up under-studied attack\nsurfaces for the adversaries. One typical example is the recently-developed\nmobile machine learning (ML) framework that enables storing and running deep\nlearning (DL) models on mobile devices. Despite obvious advantages, this new\nfeature also inadvertently introduces potential vulnerabilities (e.g.,\non-device models may be modified for malicious purposes). In this work, we\npropose a method to generate or transform mobile malware by hiding the\nmalicious payloads inside the parameters of deep learning models, based on a\nstrategy that considers four factors (layer type, layer number, layer coverage\nand the number of bytes to replace). Utilizing the proposed method, we can run\nmalware in DL mobile applications covertly with little impact on the model\nperformance (i.e., as little as 0.4% drop in accuracy and at most 39ms latency\noverhead).",
        "translated": "移动恶意软件已经成为无处不在的移动计算时代最严重的安全威胁之一。尽管安全专家做出了巨大的努力来对抗它，但是近年来确认的恶意软件样本仍然在快速增长。这可能部分归因于新出现的技术，这些技术可能不断地为对手打开未经研究的攻击表面。一个典型的例子是最近开发的移动机器学习(ML)框架，它支持在移动设备上存储和运行深度学习(DL)模型。尽管有明显的优势，这个新特性也无意中引入了潜在的漏洞(例如，为了恶意目的可能修改设备上的模型)。在这项工作中，我们提出了一种方法来生成或转换移动恶意软件，通过隐藏的恶意有效载荷内的深度学习模型的参数，基于一种策略，考虑四个因素(层类型，层数，层覆盖率和字节数替换)。利用所提出的方法，我们可以在 DL 移动应用中秘密运行恶意软件，对模型性能的影响很小(即，精度下降0.4% ，最多39ms 的延迟开销)。"
    },
    {
        "title": "Adaptive Discounting of Training Time Attacks",
        "url": "http://arxiv.org/abs/2401.02652v1",
        "pub_date": "2024-01-05",
        "summary": "Among the most insidious attacks on Reinforcement Learning (RL) solutions are\ntraining-time attacks (TTAs) that create loopholes and backdoors in the learned\nbehaviour. Not limited to a simple disruption, constructive TTAs (C-TTAs) are\nnow available, where the attacker forces a specific, target behaviour upon a\ntraining RL agent (victim). However, even state-of-the-art C-TTAs focus on\ntarget behaviours that could be naturally adopted by the victim if not for a\nparticular feature of the environment dynamics, which C-TTAs exploit. In this\nwork, we show that a C-TTA is possible even when the target behaviour is\nun-adoptable due to both environment dynamics as well as non-optimality with\nrespect to the victim objective(s). To find efficient attacks in this context,\nwe develop a specialised flavour of the DDPG algorithm, which we term\ngammaDDPG, that learns this stronger version of C-TTA. gammaDDPG dynamically\nalters the attack policy planning horizon based on the victim's current\nbehaviour. This improves effort distribution throughout the attack timeline and\nreduces the effect of uncertainty the attacker has about the victim. To\ndemonstrate the features of our method and better relate the results to prior\nresearch, we borrow a 3D grid domain from a state-of-the-art C-TTA for our\nexperiments. Code is available at \"bit.ly/github-rb-gDDPG\".",
        "translated": "对强化学习解决方案最阴险的攻击是训练时间攻击(tta) ，它在学习行为中制造漏洞和后门。不仅仅是简单的破坏，现在还有建设性的 TTA (C-TTA) ，攻击者可以将特定的目标行为强加给受训的 RL 代理(受害者)。然而，即使是最先进的 C-TTA 也把重点放在目标行为上，如果不是因为 C-TTA 利用的环境动力学的一个特殊特征，受害者可以自然而然地采用这种行为。在这项工作中，我们表明，C-TTA 是可能的，即使当目标行为是不可采用的，由于环境动态和非最优相对于受害者的目标。为了在这种情况下找到有效的攻击，我们开发了一种专门的 DDPG 算法，我们称之为 gammaDDPG，它学习了 C-TTA 的这个更强的版本。GammaDDPG 根据受害者当前的行为动态地改变攻击策略规划视界。这改进了整个攻击时间线的工作分配，并减少了攻击者对受害者的不确定性的影响。为了说明我们的方法的特点，并更好地将结果与先前的研究联系起来，我们借用了一个三维网格领域从最先进的 C-TTA 为我们的实验。密码可于「 bit.ly/github-rb-gddpg 」下载。"
    },
    {
        "title": "RNA-TransCrypt: Image Encryption Using Chaotic RNA Encoding, Novel\n  Transformative Substitution, and Tailored Cryptographic Operations",
        "url": "http://arxiv.org/abs/2401.04707v1",
        "pub_date": "2024-01-09",
        "summary": "Given the security concerns of Internet of Things (IoT) networks and limited\ncomputational resources of IoT devices, this paper presents RNA-TransCrypt, a\nnovel image encryption scheme that is not only highly secure but also efficient\nand lightweight. RNA-TransCrypt integrates the biocryptographic properties of\nRNA encoding with the non-linearity and unpredictability of chaos theory. This\nscheme introduces three novel contributions: 1) the two-base RNA encoding\nmethod, which transforms the image into RNA strands-like sequence, ensuring\nefficient scrambling; 2) the transformative substitution technique, which\ntransforms the s-box values before replacing the pixel values, and is\nresponsible for making the scheme lightweight; and 3) three mathematical\ncryptographic operations designed especially for image encryption that ensure\nthe effective transformation of the s-box values, resulting in a new outcome\neven for the same input values. These modules are key-dependent, utilizing\nchaotic keys generated by the De Jong Fractal Map and the Van der Pol\nOscillator. Extensive security analysis, including histogram analysis,\ncorrelation analysis, and the results of the statistical security parameters\nobtained from the Gray-Level Co-occurrence Matrix (GLCM) validate the efficacy\nof the proposed scheme in encrypting input images with close-to-ideal results\nof 7.997 entropy and 0.0006 correlation.",
        "translated": "针对物联网网络的安全问题和物联网设备计算资源有限的特点，提出了一种新的图像加密方案—— RNA-TransCrypt。RNA-TransCrypt 将 RNA 编码的生物密码学特性与混沌理论的非线性和不可预测性结合起来。该方案引入了三个新的贡献: (1)双碱基 RNA 编码方法，将图像转换为 RNA 链状序列，确保有效的置乱; (2)转换替换技术，在替换像素值之前转换 s 盒值，并负责使方案轻量化; (3)三个数学加密操作，特别是为图像加密设计，确保有效的 s 盒值转换，导致新的结果，即使相同的输入值。这些模块是密钥相关的，利用德容分形映射和范德波尔振荡器生成的混沌密钥。广泛的安全性分析，包括直方图分析，相关性分析和统计安全性参数的结果获得的灰度共生矩阵(GLCM)验证了该方案的加密输入图像的有效性与接近理想的结果7.997熵和0.0006相关。"
    },
    {
        "title": "The Devil Behind the Mirror: Tracking the Campaigns of Cryptocurrency\n  Abuses on the Dark Web",
        "url": "http://arxiv.org/abs/2401.04662v1",
        "pub_date": "2024-01-09",
        "summary": "The dark web has emerged as the state-of-the-art solution for enhanced\nanonymity. Just like a double-edged sword, it also inadvertently becomes the\nsafety net and breeding ground for illicit activities. Among them,\ncryptocurrencies have been prevalently abused to receive illicit income while\nevading regulations. Despite the continuing efforts to combat illicit\nactivities, there is still a lack of an in-depth understanding regarding the\ncharacteristics and dynamics of cryptocurrency abuses on the dark web. In this\nwork, we conduct a multi-dimensional and systematic study to track\ncryptocurrency-related illicit activities and campaigns on the dark web. We\nfirst harvest a dataset of 4,923 cryptocurrency-related onion sites with over\n130K pages. Then, we detect and extract the illicit blockchain transactions to\ncharacterize the cryptocurrency abuses, targeting features from\nsingle/clustered addresses and illicit campaigns. Throughout our study, we have\nidentified 2,564 illicit sites with 1,189 illicit blockchain addresses, which\naccount for 90.8 BTC in revenue. Based on their inner connections, we further\nidentify 66 campaigns behind them. Our exploration suggests that illicit\nactivities on the dark web have strong correlations, which can guide us to\nidentify new illicit blockchain addresses and onions, and raise alarms at the\nearly stage of their deployment.",
        "translated": "暗网已经成为提高匿名性的最先进的解决方案。就像一把双刃剑，它也在不经意间成为非法活动的安全网和滋生地。其中，加密货币普遍被滥用，以获取非法收入，同时规避监管。尽管继续努力打击非法活动，但对暗网上滥用加密货币的特点和动态仍缺乏深入了解。在这项工作中，我们进行了一项多层面和系统的研究，以跟踪与加密货币有关的非法活动和暗网上的活动。我们首先获得一个包含4,923个加密货币相关洋葱站点的数据集，其页面超过130K。然后，我们检测和提取非法区块链交易，以描述加密货币滥用，针对特征的单一/集群地址和非法活动。在我们的研究中，我们已经确定了2,564个非法网站和1,189个非法区块链地址，这占了90.8 BTC 的收入。根据他们的内部联系，我们进一步确定了他们背后的66个活动。我们的探索表明，暗网上的非法活动具有很强的相关性，这可以指导我们确定新的非法区块链地址和洋葱，并在其部署的早期阶段发出警报。"
    },
    {
        "title": "A Discrete Particle Swarm Optimizer for the Design of Cryptographic\n  Boolean Functions",
        "url": "http://arxiv.org/abs/2401.04567v1",
        "pub_date": "2024-01-09",
        "summary": "A Particle Swarm Optimizer for the search of balanced Boolean functions with\ngood cryptographic properties is proposed in this paper. The algorithm is a\nmodified version of the permutation PSO by Hu, Eberhart and Shi which preserves\nthe Hamming weight of the particles positions, coupled with the Hill Climbing\nmethod devised by Millan, Clark and Dawson to improve the nonlinearity and\ndeviation from correlation immunity of Boolean functions. The parameters for\nthe PSO velocity equation are tuned by means of two meta-optimization\ntechniques, namely Local Unimodal Sampling (LUS) and Continuous Genetic\nAlgorithms (CGA), finding that CGA produces better results. Using the\nCGA-evolved parameters, the PSO algorithm is then run on the spaces of Boolean\nfunctions from $n=7$ to $n=12$ variables. The results of the experiments are\nreported, observing that this new PSO algorithm generates Boolean functions\nfeaturing similar or better combinations of nonlinearity, correlation immunity\nand propagation criterion with respect to the ones obtained by other\noptimization methods.",
        "translated": "提出了一种用于搜索具有良好密码学性质的平衡布尔函数的粒子群优化算法。该算法是 Hu，Eberhart 和 Shi 对置换粒子群算法(PSO)的改进，保留了粒子位置的 Hamming 权重，并结合 Millan，Clark 和 Dawson 设计的爬山方法，改善了布尔函数的非线性和偏离相关免疫性。采用局部单峰采样(LUS)和连续遗传算法(CGA)两种元优化技术对 PSO 速度方程的参数进行了优化，结果表明，CGA 能够取得较好的优化效果。使用 CGA 进化的参数，PSO 算法在从 $n = 7 $到 $n = 12 $变量的布尔函数空间上运行。实验结果表明，这种新的粒子群优化算法产生的布尔函数与其他优化方法产生的布尔函数具有相似或更好的非线性、相关免疫和传播准则的组合。"
    },
    {
        "title": "UBfuzz: Finding Bugs in Sanitizer Implementations",
        "url": "http://arxiv.org/abs/2401.04538v1",
        "pub_date": "2024-01-09",
        "summary": "In this paper, we propose a testing framework for validating sanitizer\nimplementations in compilers. Our core components are (1) a program generator\nspecifically designed for producing programs containing undefined behavior\n(UB), and (2) a novel test oracle for sanitizer testing. The program generator\nemploys Shadow Statement Insertion, a general and effective approach for\nintroducing UB into a valid seed program. The generated UB programs are\nsubsequently utilized for differential testing of multiple sanitizer\nimplementations. Nevertheless, discrepant sanitizer reports may stem from\neither compiler optimization or sanitizer bugs. To accurately determine if a\ndiscrepancy is caused by sanitizer bugs, we introduce a new test oracle called\ncrash-site mapping. We have incorporated our techniques into UBfuzz, a\npractical tool for testing sanitizers. Over a five-month testing period, UBfuzz\nsuccessfully found 31 bugs in both GCC and LLVM sanitizers. These bugs reveal\nthe serious false negative problems in sanitizers, where certain UBs in\nprograms went unreported. This research paves the way for further investigation\nin this crucial area of study.",
        "translated": "在本文中，我们提出了一个用于验证编译器中的消毒器实现的测试框架。我们的核心组件是(1)一个程序生成器，专门设计用于生产包含未定义行为(UB)的程序，(2)一个用于消毒剂测试的新型测试预言程序。程序生成器使用了 Shadow 语句插入，这是一种将 UB 引入到有效种子程序中的通用而有效的方法。生成的 UB 程序随后用于多个消毒剂实现的差异测试。然而，不一致的消毒剂报告可能源于编译器最佳化或消毒剂的错误。为了准确地确定差异是否是由消毒程序错误引起的，我们引入了一个称为崩溃站点映射的新测试预言。我们已经将我们的技术整合到 UBfuzz 中，这是一个测试消毒剂的实用工具。在五个月的测试期间，UBfuzz 成功地在 GCC 和 LLVM 消毒器中发现了31个 bug。这些错误揭示了消毒剂中严重的假阴性问题，程序中的某些 UB 没有被报告。这项研究为进一步研究这一关键领域铺平了道路。"
    },
    {
        "title": "Differential experiments using parallel alternative operations",
        "url": "http://arxiv.org/abs/2401.04495v1",
        "pub_date": "2024-01-09",
        "summary": "The use of alternative operations in differential cryptanalysis, or\nalternative notions of differentials, are lately receiving increasing\nattention. Recently, Civino et al. managed to design a block cipher which is\nsecure w.r.t. classical differential cryptanalysis performed using\nXOR-differentials, but weaker with respect to the attack based on an\nalternative difference operation acting on the first s-box of the block. We\nextend this result to parallel alternative operations, i.e. acting on each\ns-box of the block. First, we recall the mathematical framework needed to\ndefine and use such operations. After that, we perform some differential\nexperiments against a toy cipher and compare the effectiveness of the attack\nw.r.t. the one that uses XOR-differentials.",
        "translated": "在差分密码分析中使用可选操作，或差分的可选概念，最近受到越来越多的关注。最近，Civino 等人设法设计了一种分组密码，它使用 XOR 差分进行经典差分密码分析是安全的，但是对于基于作用于分组的第一个 s 盒的替代差分操作的攻击来说是较弱的。我们将这个结果扩展到并行替代操作，例如对块的每个 s 框进行操作。首先，我们回想一下定义和使用这些操作所需的数学框架。然后，我们对一个玩具密码进行一些差分实验，比较使用异或差分的攻击 W.R.T。的效果。"
    },
    {
        "title": "Hiding Information for Secure and Covert Data Storage in Commercial\n  ReRAM Chips",
        "url": "http://arxiv.org/abs/2401.04411v1",
        "pub_date": "2024-01-09",
        "summary": "This article introduces a novel, low-cost technique for hiding data in\ncommercially available resistive-RAM (ReRAM) chips. The data is kept hidden in\nReRAM cells by manipulating its analog physical properties through switching\n($\\textit{set/reset}$) operations. This hidden data, later, is retrieved by\nsensing the changes in cells' physical properties (i.e., $\\textit{set/reset}$\ntime of the memory cells). The proposed system-level hiding technique does not\naffect the normal memory operations and does not require any hardware\nmodifications. Furthermore, the proposed hiding approach is robust against\ntemperature variations and the aging of the devices through normal read/write\noperation. The silicon results show that our proposed data hiding technique is\nacceptably fast with ${\\sim}0.4bit/min$ of encoding and ${\\sim}15.625bits/s$ of\nretrieval rates, and the hidden message is unrecoverable without the knowledge\nof the secret key, which is used to enhance the security of hidden information.",
        "translated": "本文介绍了一种新颖的、低成本的在商用电阻 RAM 芯片中隐藏数据的技术。通过切换($textit { set/reset } $)操作来操纵数据的模拟物理属性，从而将数据隐藏在 ReRAM 单元中。稍后，通过感知单元格物理属性的变化(即，内存单元格的 $textit { set/reset } $time)来检索这些隐藏数据。提出的系统级隐藏技术不影响正常的内存操作，也不需要对硬件进行任何修改。此外，通过正常的读写操作，提出的隐藏方法对温度变化和器件老化具有鲁棒性。实验结果表明，本文提出的数据隐藏技术在编码速度为0.4 bit/min $，检索速度为15.625 bit/s $的情况下，具有较高的速度，且在没有密钥知识的情况下，隐藏信息是不可恢复的，从而提高了隐藏信息的安全性。"
    },
    {
        "title": "SoK: Facial Deepfake Detectors",
        "url": "http://arxiv.org/abs/2401.04364v1",
        "pub_date": "2024-01-09",
        "summary": "Deepfakes have rapidly emerged as a profound and serious threat to society,\nprimarily due to their ease of creation and dissemination. This situation has\ntriggered an accelerated development of deepfake detection technologies.\nHowever, many existing detectors rely heavily on lab-generated datasets for\nvalidation, which may not effectively prepare them for novel, emerging, and\nreal-world deepfake techniques. In this paper, we conduct an extensive and\ncomprehensive review and analysis of the latest state-of-the-art deepfake\ndetectors, evaluating them against several critical criteria. These criteria\nfacilitate the categorization of these detectors into 4 high-level groups and\n13 fine-grained sub-groups, all aligned with a unified standard conceptual\nframework. This classification and framework offer deep and practical insights\ninto the factors that affect detector efficacy. We assess the generalizability\nof 16 leading detectors across various standard attack scenarios, including\nblack-box, white-box, and gray-box settings. Our systematized analysis and\nexperimentation lay the groundwork for a deeper understanding of deepfake\ndetectors and their generalizability, paving the way for future research\nfocused on creating detectors adept at countering various attack scenarios.\nAdditionally, this work offers insights for developing more proactive defenses\nagainst deepfakes.",
        "translated": "深度造假已迅速成为对社会的深刻和严重威胁，主要是因为它们易于创造和传播。这种情况引发了深度伪造检测技术的加速发展。然而，许多现有的检测器严重依赖于实验室生成的数据集进行验证，这可能无法有效地为新颖的、新兴的和真实世界的深度伪造技术做好准备。在本文中，我们进行了广泛和全面的审查和分析，最新的国家的深伪探测器，评估他们对几个关键的标准。这些标准有助于将这些检测器分为4个高级别组和13个细粒度子组，所有这些都与一个统一的标准概念框架相一致。这种分类和框架为探测器效能的影响因素提供了深入和实用的见解。我们评估了16个主要检测器在各种标准攻击场景中的通用性，包括黑盒、白盒和灰盒设置。我们的系统化分析和实验为更深入地理解深度伪装检测器及其普遍性奠定了基础，为将来的研究重点创建能够抵御各种攻击场景的检测器铺平了道路。此外，这项工作为开发更积极的防御深度造假提供了见解。"
    },
    {
        "title": "WebGPU-SPY: Finding Fingerprints in the Sandbox through GPU Cache\n  Attacks",
        "url": "http://arxiv.org/abs/2401.04349v1",
        "pub_date": "2024-01-09",
        "summary": "Microarchitectural attacks on CPU structures have been studied in native\napplications, as well as in web browsers. These attacks continue to be a\nsubstantial threat to computing systems at all scales.\n  With the proliferation of heterogeneous systems and integration of hardware\naccelerators in every computing system, modern web browsers provide the support\nof GPU-based acceleration for the graphics and rendering processes. Emerging\nweb standards also support the GPU acceleration of general-purpose computation\nwithin web browsers.\n  In this paper, we present a new attack vector for microarchitectural attacks\nin web browsers. We use emerging GPU accelerating APIs in modern browsers\n(specifically WebGPU) to launch a GPU-based cache side channel attack on the\ncompute stack of the GPU that spies on victim activities on the graphics\n(rendering) stack of the GPU. Unlike prior works that rely on JavaScript APIs\nor software interfaces to build timing primitives, we build the timer using GPU\nhardware resources and develop a cache side channel attack on Intel's\nintegrated GPUs. We leverage the GPU's inherent parallelism at different levels\nto develop high-resolution parallel attacks. We demonstrate that GPU-based\ncache attacks can achieve a precision of 90 for website fingerprinting of 100\ntop websites. We also discuss potential countermeasures against the proposed\nattack to secure the systems at a critical time when these web standards are\nbeing developed and before they are widely deployed.",
        "translated": "微架构对 CPU 结构的攻击已经在本地应用程序和 web 浏览器中得到了研究。这些攻击继续对各种规模的计算系统构成重大威胁。随着各种异构系统的不断发展和硬件加速器在各种计算系统中的集成，现代网络浏览器为图形和渲染过程提供了基于 GPU 的加速支持。新兴的网络标准也支持 GPU 在网络浏览器中加速通用计算。本文针对浏览器中的微体系结构攻击提出了一种新的攻击向量。我们在现代浏览器(特别是 WebGPU)中使用新兴的 GPU 加速 API，对 GPU 的计算堆栈发起基于 GPU 的缓存端通道攻击，监视 GPU 的图形(渲染)堆栈上的受害者活动。与之前依赖 JavaScript API 或软件接口构建计时原语的工作不同，我们使用 GPU 硬件资源构建计时器，并针对 Intel 的集成 GPU 开发缓存端信道攻击。我们利用 GPU 在不同层次上固有的并行性来开发高分辨率的并行攻击。我们证明，基于 GPU 的缓存攻击可以达到90精度的网站指纹的100个顶级网站。我们还讨论了在开发这些 Web 标准的关键时刻以及在它们被广泛部署之前，针对所提议的攻击的潜在对策，以确保系统的安全。"
    },
    {
        "title": "Private Fine-tuning of Large Language Models with Zeroth-order\n  Optimization",
        "url": "http://arxiv.org/abs/2401.04343v1",
        "pub_date": "2024-01-09",
        "summary": "Fine-tuning large pretrained models on private datasets may run the risk of\nviolating privacy. Differential privacy is a framework for mitigating privacy\nrisks by enforcing algorithmic stability. DP-SGD enables training models with\nprivate data in a privacy-preserving manner, but raises new obstacles in the\nform of performance loss and significant engineering challenges. We introduce\nDP-ZO, a new method for fine-tuning large language models that preserves the\nprivacy of training data by privatizing zeroth-order optimization. A key\ninsight into the design of our method is that the direction of the gradient in\nSPSA, the zeroth-order algorithm we use, is always random and the only\ninformation that depends on private data is the step size, i.e., a scalar.\nTherefore, we only need to privatize the scalar step size, which is\nmemory-efficient. DP-ZO, which can be instantiated with either Laplace or\nGaussian noise, provides a strong privacy-utility trade-off across different\ntasks, and model sizes, under conservative privacy budgets. One noteworthy\nresult is that DP-ZO exhibits just $1.86\\%$ performance degradation due to\nprivacy at $(1,10^{-5})$-DP when fine-tuning OPT-66B on 1000 training samples\nfrom SQuAD.",
        "translated": "对私有数据集上的大型预先训练的模型进行微调可能存在侵犯隐私的风险。差分隐私是一个通过加强算法稳定性来减少隐私风险的框架。DP-SGD 以保护隐私的方式支持使用私有数据的培训模型，但是以性能损失和重大工程挑战的形式提出了新的障碍。我们介绍了 DP-ZO，一种通过私有化零阶优化来保护训练数据隐私性的大型语言模型微调新方法。我们方法设计的一个关键点在于，SPSA 的梯度方向，我们使用的零级算法，总是随机的，唯一依赖于私人数据的信息是步长，也就是一个标量。因此，我们只需要对标量步长进行私有化，这样可以节省内存。DP-zO 可以用拉普拉斯或高斯噪声来实例化，在保守的隐私预算下，它可以在不同任务和型号之间进行强有力的隐私效用权衡。一个值得注意的结果是，当对来自 SQuAD 的1000个训练样本的 OPT-66B 进行微调时，DP-ZO 仅表现出 $1.86% 的性能下降，这是由于在 $(1,10 ^ {-5}) $- DP 时的隐私造成的。"
    },
    {
        "title": "Deep Efficient Private Neighbor Generation for Subgraph Federated\n  Learning",
        "url": "http://arxiv.org/abs/2401.04336v1",
        "pub_date": "2024-01-09",
        "summary": "Behemoth graphs are often fragmented and separately stored by multiple data\nowners as distributed subgraphs in many realistic applications. Without harming\ndata privacy, it is natural to consider the subgraph federated learning\n(subgraph FL) scenario, where each local client holds a subgraph of the entire\nglobal graph, to obtain globally generalized graph mining models. To overcome\nthe unique challenge of incomplete information propagation on local subgraphs\ndue to missing cross-subgraph neighbors, previous works resort to the\naugmentation of local neighborhoods through the joint FL of missing neighbor\ngenerators and GNNs. Yet their technical designs have profound limitations\nregarding the utility, efficiency, and privacy goals of FL. In this work, we\npropose FedDEP to comprehensively tackle these challenges in subgraph FL.\nFedDEP consists of a series of novel technical designs: (1) Deep neighbor\ngeneration through leveraging the GNN embeddings of potential missing\nneighbors; (2) Efficient pseudo-FL for neighbor generation through embedding\nprototyping; and (3) Privacy protection through noise-less\nedge-local-differential-privacy.\n  We analyze the correctness and efficiency of FedDEP, and provide theoretical\nguarantees on its privacy.\n  Empirical results on four real-world datasets justify the clear benefits of\nproposed techniques.",
        "translated": "在许多现实的应用程序中，巨兽图常常被多个数据所有者作为分布式子图分割和单独存储。在不损害数据隐私的情况下，考虑子图联邦学习(子图 FL)场景(其中每个局部客户端拥有整个全局图的一个子图) ，可以很自然地获得全局广义的图挖掘模型。为了克服由于缺少交叉子图邻居而导致局部子图上信息传播不完全的独特挑战，以往的工作都是通过缺少邻居生成器和 GNN 的联合 FL 来增强局部邻居。然而，他们的技术设计在实用性、效率和隐私目标方面有很大的局限性。在这项工作中，我们提出 FedDEP 来全面解决这些挑战的子图 FL。FedDEP 包括一系列新颖的技术设计: (1)利用潜在缺失邻居的 GNN 嵌入进行深度邻居生成; (2)通过嵌入原型进行有效的伪 FL 邻居生成; (3)通过无噪声边局部差分隐私保护。分析了 FedDEP 的正确性和有效性，为其隐私性提供了理论保证。对四个真实世界数据集的实验结果证明了所提出的技术的明显好处。"
    }
]